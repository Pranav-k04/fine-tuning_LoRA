[
    {
    "instruction": "Get `grep` to not output file name\n\nWhen I use \ngrep -o\n to search in multiple files, it outputs each result prefixed with the file name. How can I prevent this prefix? I want the results without the file names.",
    "input": "",
    "output": "With the GNU implementation of \ngrep\n (the one that also introduced \n-o\n) or compatible, you can use the \n-h\n option.\n\n\n\n> ```bash\n> -h, --no-filename\n> Suppress the prefixing of file names on  output.   This  is  the\n> default  when there is only one file (or only standard input) to\n> search.\n> ```\n\n\n\nWith other implementations, you can always concatenate the files with \ncat\n and \ngrep\n that output:\n\n\n\n```bash\ncat ./*.txt | grep regexp\n```\n\n\n\nOr use \nsed\n or \nawk\n instead of \ngrep\n:\n\n\n\n```bash\nawk '/regexp/' ./*.txt\n```\n\n\n\n(extended regexps like with \ngrep -E\n).\n\n\n\n```bash\nsed '/regexp/!d/' ./*.txt\n```\n\n\n\n(basic regexps like with \ngrep\n without \n-E\n. Many \nsed\n implementations now also support a \n-E\n option for extended regexps)."
  },
  {
    "instruction": "How to cycle through reverse-i-search in Bash?\n\nIn the GNU bash shell, I can type \nCtrl\n + \nR\n to search for a matching command previously run. E.g., if I type \nCtrl\n + \nR\n and then \"\ngrep\n\", it lists my last \ngrep\n command, and I can hit \nEnter\n to use it.\n\n\nThis only gives one suggestion though. Is there any way to cycle through other previously typed matching commands?",
    "input": "",
    "output": "If I understand the question correctly you should be able to cycle through\nalternatives by repeatedly hitting \nCtrl\n + \nR\n.\n\n\nE.g.:\n\n\n\n\nCtrl\n + \nR\n\n\ngrep\n\n\nCtrl\n + \nR\n\n\nCtrl\n + \nR\n\n...\n\n\n\n\nThat searches backwards through your history.  To search forward instead, use \nCtrl\n + \nS\n, but you may need to have set: \nstty -ixon\n (either by \n.bash_profile\n or manually) prior to that to disable the XON/XOFF feature which takes over \nCtrl\n + \nS\n. If it happens anyway, use \nCtrl\n + \nQ\n to re-enable screen output (More details \nhere\n.)"
  },
  {
    "instruction": "Is `git diff` related to `diff`?\n\nIs \ngit diff\n related to \ndiff\n?\n\n\n\n\nIs \ngit diff\n implemented based on \ndiff\n?\n\n\nIs \nthe command line syntax of \ngit diff\n similar to \nthat of \ndiff\n? Does learning one help  using the other much?\n\n\nare their output files following the same format? Can they be both used by \ngit patch\n and \npatch\n? (Is there \ngit patch\n? How is it related to \npatch\n?)\n\n\n\n\nThanks.",
    "input": "",
    "output": "The file format is interoperable.  Git uses the best format, \ndiff -u\n.  It also extends it to represent additional types of changes.\n\n\nThe equivalent to \npatch\n is \ngit apply\n.  It stages the changes in the index as well as applying them to the working tree.\n\n\nI remember \ngit apply\n being stricter than \npatch\n, although the reference documentation doesn't seem to make an explicit comparison.  It does mention several tests / errors which can be enabled or disabled.\n\n\nThe reference documentation also suggests that it could be used as \"a replacement for GNU patch\" - even outside of a git repository, if you use a certain option."
  },
  {
    "instruction": "Determine if Git working directory is clean from a script\n\nI have a script which runs \nrsync\n with a Git working directory as destination. I want the script to have different behavior depending on if the working directory is clean (no changes to commit), or not. For instance, if the output of \ngit status\n is as below, I want the script to exit:\n\n\n\n```bash\ngit status\nAlready up-to-date.\n# On branch master\nnothing to commit (working directory clean)\nEverything up-to-date\n```\n\n\n\nIf the directory is not clean then I would like it to execute some more commands.\n\n\nHow can I check for output like the above in a shell script?",
    "input": "",
    "output": "Parsing the output of \ngit status\n is a bad idea because the output is intended to be human readable, not machine-readable. There's no guarantee that the output will remain the same in future versions of Git or in differently configured environments.\n\n\nUVVs comment\n is on the right track, but unfortunately the return code of \ngit status\n doesn't change when there are uncommitted changes. It does, however, provide the \n--porcelain\n option, which causes the output of \ngit status --porcelain\n to be formatted in an easy-to-parse format for scripts, \nand\n will remain stable across Git versions and regardless of user configuration.\n\n\nWe can use empty output of \ngit status --porcelain\n as an indicator that there are no changes to be committed:\n\n\n\n```bash\nif [ -z \"$(git status --porcelain)\" ]; then \n  # Working directory clean\nelse \n  # Uncommitted changes\nfi\n```\n\n\n\nIf we do not care about untracked files in the working directory, we can use the \n--untracked-files=no\n option to disregard those:\n\n\n\n```bash\nif [ -z \"$(git status --untracked-files=no --porcelain)\" ]; then \n  # Working directory clean excluding untracked files\nelse \n  # Uncommitted changes in tracked files\nfi\n```\n\n\n\nTo make this more robust against conditions which \nactually\n cause \ngit status\n to fail without output to \nstdout\n, we can refine the check to:\n\n\n\n```bash\nif output=$(git status --porcelain) && [ -z \"$output\" ]; then\n  # Working directory clean\nelse \n  # Uncommitted changes\nfi\n```\n\n\n\nIt's also worth noting that, although \ngit status\n does not give meaningful exit code when the working directory is unclean, \ngit diff\n provides the \n--exit-code\n option, which makes it behave similar to the \ndiff\n utility, that is, exiting with status \n1\n when there were differences and \n0\n when none were found.\n\n\nUsing this, we can check for unstaged changes with:\n\n\n\n```bash\ngit diff --exit-code\n```\n\n\n\nand staged, but not committed changes with:\n\n\n\n```bash\ngit diff --cached --exit-code\n```\n\n\n\nAlthough \ngit diff\n can report on untracked files in submodules via appropriate arguments to \n--ignore-submodules\n, unfortunately it seems that there is no way to have it report on untracked files in the actual working directory. If untracked files in the working directory are relevant, \ngit status --porcelain\n is probably the best bet."
  },
  {
    "instruction": "On-the-fly stream compression that doesn&#39;t spill over into hardware resources?\n\nI have 200 GB free disk space, 16 GB of RAM (of which ~1 GB is occupied by the  desktop and kernel) and 6 GB of swap.\n\n\nI have a 240 GB external SSD, with 70 GB used\n1\n and the rest free, which I need to back up to my disk.\n\n\nNormally, I would \ndd if=/dev/sdb of=Desktop/disk.img\n the disk first, and then compress it, but making the image first is not an option since doing so would require far more disk space than I have, even though the compression step will result in the free space being squashed so the final archive can easily fit on my disk.\n\n\ndd\n writes to STDOUT by default, and \ngzip\n can read from STDIN, so in theory I can write \ndd if=/dev/sdb | gzip -9 -\n, but \ngzip\n takes significantly longer to read bytes than \ndd\n can produce them.\n\n\nFrom \nman pipe\n: \n\n\n\n> Data written to the write end of the pipe is buffered by the kernel until it is read from the read end of the pipe.\n\n\n\nI visualise a \n|\n as being like a real pipe -- one application shoving data in and the other taking data out of the pipe's queue as quickly as possible. \n\n\nWhat when the program on the left side writes more data more quickly than the other side of the pipe can hope to process it? Will it cause extreme memory or swap usage, or will the kernel try to create a FIFO on disk, thereby filling up the disk? Or will it just fail with \nSIGPIPE Broken pipe\n if the buffer is too large?\n\n\nBasically, this boils down to two questions:\n \n\n\n\n\nWhat are the implications and outcomes of shoving more data into a pipe than is read at a time?\n\n\nWhat's the reliable way to compress a datastream to disk without putting the entire uncompressed datastream on the disk?\n\n\n\n\nNote 1: I cannot just copy exactly the first 70 used GB and expect to get a working system or filesystem, because of fragmentation and other things which will require the full contents to be intact.",
    "input": "",
    "output": "dd\n reads and writes data one block at a time, and it only ever has one block outstanding. So\n\n\n\n```bash\nvalgrind dd if=/dev/zero status=progress of=/dev/null bs=1M\n```\n\n\n\nshows that \ndd\n uses approximately 1MB of memory. You can play around with the block size, and drop \nvalgrind\n, to see the effect on \ndd\n’s speed.\n\n\nWhen you pipe into \ngzip\n, \ndd\n simply slows down to match \ngzip\n’s speed. Its memory usage doesn’t increase, nor does it cause the kernel to store the buffers on disk (the kernel doesn’t know how to do that, except \nvia\n swap). A broken pipe only happens when one of the ends of the pipe dies; see \nsignal(7)\n and \nwrite(2)\n for details.\n\n\nThus\n\n\n\n```bash\ndd if=... iconv=fullblock bs=1M | gzip -9 > ...\n```\n\n\n\nis a safe way to do what you’re after.\n\n\nWhen piping, the writing process ends up being blocked by the kernel if the reading process isn’t keeping up. You can see this by running\n\n\n\n```bash\nstrace dd if=/dev/zero bs=1M | (sleep 60; cat > /dev/null)\n```\n\n\n\nYou’ll see that \ndd\n reads 1MB, then issues a \nwrite()\n which sits there waiting for one minute while \nsleep\n runs. That’s how both sides of a pipe balance out: the kernel blocks writes if the writing process is too fast, and it blocks reads if the reading process is too fast."
  },
  {
    "instruction": "tar --exclude doesn&#39;t exclude. Why?\n\nI have this very simple line in a bash script which executes successfully (i.e. producing the \n_data.tar\n file), except that it \ndoesn't\n exclude the sub-directories it is told exclude via the \n--exclude\n option:\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  --exclude='/data/sub1/*'  --exclude='/data/sub2/*' --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'  /data\n```\n\n\n\nInstead, it produces a \n_data.tar\n file that contains everything under /data, including the files in the subdirectories I wanted to exclude.\n\n\nAny idea why? and how to fix this?\n\n\nUpdate\n I implemented my observations based on the link provided in the first answer below (top level dir first, no whitespace after last exclude):\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1/*'  --exclude='/data/sub2/*'  --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'\n```\n\n\n\nBut that didn't help. All \"excluded\" sub-directories are present in the resulting \n_data.tar\n file.\n\n\nThis is puzzling. Whether this is a bug in current tar (GNU tar 1.23, on a CentOS 6.2, Linux 2.6.32)  or \"extreme sensitivity\" of tar to whitespaces and other easy-to-miss typos, I consider this a bug. For now.\n\n\nThis is horrible\n: I tried the insight suggested below (no trailing \n/*\n) and it still doesn't work in the production script:\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1'  --exclude='/data/sub2'  --exclude='/data/sub3'  --exclude='/data/sub4'\n```\n\n\n\nI can't see any difference between what I tried and what @Richard Perrin tried, except for the quotes and 2 spaces instead of 1. I am going to try this (must wait for the nightly script to run as the directory to be backed up is huge) and report back.\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  /data --exclude=/data/sub1 --exclude=/data/sub2 --exclude=/data/sub3 --exclude=/data/sub4\n```\n\n\n\nI am beginning to think that all these \ntar --exclude\n sensitivities aren't tar's but something in my environment, but then what could that be?\n\n\nIt worked!\n The last variation tried (no single-quotes and single-space instead of double-space between the \n--exclude\ns) tested working. Weird but accepting.\n\n\nUnbelievable!\n It turns out that an older version of \ntar\n (1.15.1) would only exclude if the top-level dir is \nlast\n on the command line. This is the exact opposite of how version 1.23 requires. FYI.",
    "input": "",
    "output": "It may be that your version of \ntar\n requires that the \n--exclude\n options have to be placed at the beginning of the \ntar\n command.\n\n\nSee: \nhttps://stackoverflow.com/q/984204\n\n\n\n```bash\ntar --exclude='./folder' --exclude='./upload/folder2' \\\n    -zcvf /backup/filename.tgz .\n```\n\n\n\nSee: \nhttp://mandrivausers.org/index.php?/topic/8585-multiple-exclude-in-tar/\n\n\n\n```bash\ntar --exclude=<first> --exclude=<second> -cjf backupfile.bz2 /home/*\n```\n\n\n\nAlternative:\n\n\n\n```bash\nEXCLD='first second third'\ntar -X <(for i in ${EXCLD}; do echo $i; done) -cjf backupfile.bz2 /home/*\n```\n\n\n\nYet another \ntar\n command tip is from \nhere\n:\n\n\n\n```bash\ntar cvfz myproject.tgz --exclude='path/dir_to_exclude1' \\\n                       --exclude='path/dir_to_exclude2' myproject\n```"
  },
  {
    "instruction": "Dereferencing hard links\n\nIn the manual page of \ntar\n command, an option for following hard links is listed.\n\n\n\n```bash\n-h, --dereference\n      follow symlinks; archive and dump the files they point to\n\n--hard-dereference\n      follow hard links; archive and dump the files they refer to\n```\n\n\n\nHow does \ntar\n know that a file is a hard link? How does it \nfollow\n it?\n\n\nWhat if I don't choose this option? How does it \nnot\n hard-dereference?",
    "input": "",
    "output": "By default, if you tell \ntar\n to archive a file with hard links, and more than one such link is included among the files to be archived, it archives the file only once, and records the second (and any additional names) as hard links.  This means that when you extract that archive, the hard links will be restored.\n\n\nIf you use the \n--hard-dereference\n option, then \ntar\n does \nnot\n preserve hard links.  Instead, it treats them as independent files that just happen to have the same contents and metadata.  When you extract the archive, the files will be independent.\n\n\nNote:  It recognizes hard links by first checking the link count of the file.  It records the device number and inode of each file with more than one link, and uses that to detect when the same file is being archived again.  (When you use \n--hard-dereference\n, it does not do this.)"
  },
  {
    "instruction": "How do I compress files in-place?\n\nI have a machine with 90% hard-disk usage.  I want to compress its 500+ log files into a smaller new file. However, the hard disk is too small to keep both the original files and the compressed ones.\n\n\nSo what I need is to compress all log files into a single new file one by one, deleting each original once compressed.\n\n\nHow can I do that in Linux?",
    "input": "",
    "output": "gzip\n or \nbzip2\n will compress the file and remove the non-compressed one automatically (this is their default behaviour).\n\n\nHowever, keep in mind that while the compressing process, both files will exists.\n\n\nIf you want to compress log files (ie: files containing text), you may prefer \nbzip2\n, since it has a better ratio for text files.\n\n\n\n```bash\nbzip2 -9 myfile       # will produce myfile.bz2\n```\n\n\n\nComparison and examples:\n\n\n\n```bash\n$ ls -l myfile\n-rw-rw-r-- 1 apaul apaul 585999 29 april 10:09 myfile\n\n$ bzip2 -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 115780 29 april 10:09 myfile.bz2\n\n$ bunzip2 myfile.bz2\n\n$ gzip -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 146234 29 april 10:09 myfile.gz\n```\n\n\n\nUPDATE\n as @Jjoao told me in a comment, interestingly, \nxz\n seems to have a best ratio on plain files with its default options:\n\n\n\n```bash\n$ xz -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 109384 29 april 10:09 myfile.xz\n```\n\n\n\nFor more informations, here is an interesting benchmark for different tools: \nhttp://binfalse.de/2011/04/04/comparison-of-compression/\n\n\nFor the example above, I use \n-9\n for a best compression ratio, but if the time needed to compress data is more important than the ratio, you'd better not use it (use a lower option, ie \n-1\n, or something between)."
  },
  {
    "instruction": "How to fix &quot;Hunk #1 FAILED at 1 (different line endings)&quot; message?\n\nI am trying to create a patch with the command\n\n\n\n```bash\ngit diff sourcefile >/var/lib/laymab/overlay/category/ebuild/files/thepatch.patch\n```\n\n\n\nwhen I apply the patch, it gives me \n\n\n\n```bash\n$ patch -v\nGNU patch 2.7.5\n\n$ /usr/bin/patch -p1 </var/lib/laymab/overlay/category/ebuild/files/thepatch.patch\npatching file sourcefile\nHunk #1 FAILED at 1 (different line endings).\nHunk #2 FAILED at 23 (different line endings).\nHunk #3 FAILED at 47 (different line endings).\nHunk #4 FAILED at 65 (different line endings).\nHunk #5 FAILED at 361 (different line endings).\n5 out of 5 hunks FAILED -- saving rejects to file sourcefile.rej\n```\n\n\n\nI tried to apply dos2unix to both src file and patch file, but the message don't gone...\n\n\nUPD: --ignore-whitespace doesn't help too\n\n\n\n```bash\nPATCH COMMAND:  patch -p1 -g0 -E --no-backup-if-mismatch --ignore-whitespace --dry-run -f < '/var/lib/layman/dotnet/dev-dotnet/slntools/files/remove-wix-project-from-sln-file-v2.patch'\n\n=====================================================\nchecking file Main/SLNTools.sln\nHunk #1 FAILED at 14 (different line endings).\nHunk #2 FAILED at 49 (different line endings).\nHunk #3 FAILED at 69 (different line endings).\nHunk #4 FAILED at 102 (different line endings).\n4 out of 4 hunks FAILED\n```\n\n\n\nUPD: found a very good article: \nhttps://stackoverflow.com/a/4425433/1709408",
    "input": "",
    "output": "I had the same problem using the \npatch\ncommand that comes with MSYS2 on Windows. In my case both the source file and the patch had CRLF line-ending, and converting both to LF didn't work either. What worked was the following: \n\n\n\n```bash\n$ dos2unix patch-file.patch\n$ patch -p1 < patch-file.patch\n$ unix2dos modified-files...\n```\n\n\n\npatch\n will convert the line-endings to LF on all the patched files, so it's necessary to convert them back to CRLF.\n\n\nObs: the \npatch\n version I'm using is 2.7.5"
  },
  {
    "instruction": "How to return both file name and line number with find ... -exec grep?\n\nWhen using \nfind\n, how do I return the file name and the line number when searching for a string? I manage to return the file name in one command and the line numbers with another one, but I can't seem to combine them.\n\n\nFile names: \nfind . -type f -exec grep -l 'string to search' {} \\;\n\n\nLine numbers: \nfind . -type f -exec grep -n 'string to search' {} \\;",
    "input": "",
    "output": "The command line switch \n-H\n forces \ngrep\n to print the file name, even with just one file.\n\n\n\n```bash\n% grep -n 7 test.in\n7:7\n% grep -Hn 7 test.in\ntest.in:7:7\n```\n\n\n\n\n\n\n```bash\n-H, --with-filename\n          Print the filename for each match.\n```\n\n\n\nNote that as \nKojiro\n says in \na comment\n, this is not part of the POSIX standard; it is in both GNU and BSD grep, but it's possible some systems don't have it (e.g. Solaris)."
  }
]