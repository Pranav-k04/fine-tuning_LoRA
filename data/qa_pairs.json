[
  {
    "question_title": "How to correctly add a path to PATH?",
    "question_body": "<p>I'm wondering where a new path has to be added to the <code>PATH</code> environment variable. I know this can be accomplished by editing <code>.bashrc</code> (for example), but it's not clear how to do this.</p>\n\n<p>This way:</p>\n\n<pre class=\"lang-bsh prettyprint-override\"><code>export PATH=~/opt/bin:$PATH\n</code></pre>\n\n<p>or this?</p>\n\n\n\n<pre><code>export PATH=$PATH:~/opt/bin\n</code></pre>\n",
    "answer": "<h3>The simple stuff</h3>\n<pre><code>PATH=$PATH:~/opt/bin\n</code></pre>\n<p>or</p>\n<pre><code>PATH=~/opt/bin:$PATH\n</code></pre>\n<p>depending on whether you want to add <code>~/opt/bin</code> at the end (to be searched after all other directories, in case there is a program by the same name in multiple directories) or at the beginning (to be searched before all other directories).</p>\n<p>You can add multiple entries at the same time. <code>PATH=$PATH:~/opt/bin:~/opt/node/bin</code> or variations on the ordering work just fine. Don't put <code>export</code> at the beginning of the line as it has additional complications (see below under “Notes on shells other than bash”).</p>\n<p>If your <code>PATH</code> gets built by many different components, you might end up with duplicate entries. See <a href=\"https://unix.stackexchange.com/questions/25605/how-to-add-home-directory-path-to-be-discovered-by-unix-which-command\">How to add home directory path to be discovered by Unix which command?</a> and <a href=\"https://unix.stackexchange.com/questions/40749/remove-duplicate-path-entries-with-awk-command\">Remove duplicate $PATH entries with awk command</a> to avoid adding duplicates or remove them.</p>\n<p>Some distributions automatically put <code>~/bin</code> in your PATH if it exists, by the way.</p>\n<h3>Where to put it</h3>\n<p>Put the line to modify <code>PATH</code> in <code>~/.profile</code>, or in <code>~/.bash_profile</code> or if that's what you have. (If your login shell is zsh and not bash, put it in <code>~/.zprofile</code> instead.)</p>\n<p>The profile file is read by login shells, so it will only take effect the next time you log in. (Some systems configure terminals to read a login shell; in that case you can start a new terminal window, but the setting will take effect only for programs started via a terminal, and how to set <code>PATH</code> for all programs depends on the system.)</p>\n<p>Note that <code>~/.bash_rc</code> is not read by any program, and <code>~/.bashrc</code> is the configuration file of interactive instances of bash. You should not define environment variables in <code>~/.bashrc</code>. The right place to define environment variables such as <code>PATH</code> is <code>~/.profile</code> (or <code>~/.bash_profile</code> if you don't care about shells other than bash). See <a href=\"https://superuser.com/questions/183870/difference-between-bashrc-and-bash-profile/183980#183980\">What's the difference between them and which one should I use?</a></p>\n<p>Don't put it in <code>/etc/environment</code> or <code>~/.pam_environment</code>: these are not shell files, you can't use substitutions like <code>$PATH</code> in there. In these files, you can only override a variable, not add to it.</p>\n<h3>Potential complications in some system scripts</h3>\n<p>You don't need <code>export</code> if the variable is already in the environment: any change of the value of the variable is reflected in the environment.¹ <code>PATH</code> is pretty much always in the environment; all unix systems set it very early on (usually in the very first process, in fact).</p>\n<p>At login time, you can rely on <code>PATH</code> being already in the environment, and already containing some system directories. If you're writing a script that may be executed early while setting up some kind of virtual environment, you may need to ensure that <code>PATH</code> is non-empty and exported: if <code>PATH</code> is still unset, then something like <code>PATH=$PATH:/some/directory</code> would set <code>PATH</code> to <code>:/some/directory</code>, and the empty component at the beginning means the current directory (like <code>.:/some/directory</code>).</p>\n<pre><code>if [ -z &quot;${PATH-}&quot; ]; then export PATH=/usr/local/bin:/usr/bin:/bin; fi\n</code></pre>\n<h3>Notes on shells other than bash</h3>\n<p>In bash, ksh and zsh, <code>export</code> is special syntax, and both <code>PATH=~/opt/bin:$PATH</code> and <code>export PATH=~/opt/bin:$PATH</code> do the right thing even. In other Bourne/POSIX-style shells such as dash (which is <code>/bin/sh</code> on many systems), <code>export</code> is parsed as an ordinary command, which implies two differences:</p>\n<ul>\n<li><code>~</code> is only parsed at the beginning of a word, except in assignments (see <a href=\"https://unix.stackexchange.com/questions/25605/how-to-add-home-directory-path-to-be-discovered-by-unix-which-command/25704#25704\">How to add home directory path to be discovered by Unix which command?</a> for details);</li>\n<li><code>$PATH</code> outside double quotes <a href=\"https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters\">breaks if <code>PATH</code> contains whitespace or <code>\\[*?</code></a>.</li>\n</ul>\n<p>So in shells like dash, <strike><code>export PATH=~/opt/bin:$PATH</code></strike> sets <code>PATH</code> to the literal string <code>~/opt/bin/:</code> followed by the value of <code>PATH</code> up to the first space.\n<code>PATH=~/opt/bin:$PATH</code> (a bare assignment) <a href=\"https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary/68748#68748\">doesn't require quotes</a> and does the right thing. If you want to use <code>export</code> in a portable script, you need to write <code>export PATH=&quot;$HOME/opt/bin:$PATH&quot;</code>, or <code>PATH=~/opt/bin:$PATH; export PATH</code> (or <code>PATH=$HOME/opt/bin:$PATH; export PATH</code> for portability to even the Bourne shell that didn't accept <code>export var=value</code> and didn't do tilde expansion).</p>\n<p>¹ <sub> This wasn't true in Bourne shells (as in the actual Bourne shell, not modern POSIX-style shells), but you're highly unlikely to encounter such old shells these days. </sub></p>\n",
    "url": "https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path"
  },
  {
    "question_title": "How to cycle through reverse-i-search in Bash?",
    "question_body": "<p>In the GNU bash shell, I can type <kbd>Ctrl</kbd> + <kbd>R</kbd> to search for a matching command previously run. E.g., if I type <kbd>Ctrl</kbd> + <kbd>R</kbd> and then &quot;<code>grep</code>&quot;, it lists my last <code>grep</code> command, and I can hit <kbd>Enter</kbd> to use it.</p>\n<p>This only gives one suggestion though. Is there any way to cycle through other previously typed matching commands?</p>\n",
    "answer": "<p>If I understand the question correctly you should be able to cycle through\nalternatives by repeatedly hitting <kbd>Ctrl</kbd> + <kbd>R</kbd>.</p>\n<p>E.g.:</p>\n<ul>\n<li><kbd>Ctrl</kbd> + <kbd>R</kbd></li>\n<li><code>grep</code></li>\n<li><kbd>Ctrl</kbd> + <kbd>R</kbd></li>\n<li><kbd>Ctrl</kbd> + <kbd>R</kbd>\n...</li>\n</ul>\n<p>That searches backwards through your history.  To search forward instead, use <kbd>Ctrl</kbd> + <kbd>S</kbd>, but you may need to have set: <code>stty -ixon</code> (either by <code>.bash_profile</code> or manually) prior to that to disable the XON/XOFF feature which takes over <kbd>Ctrl</kbd> + <kbd>S</kbd>. If it happens anyway, use <kbd>Ctrl</kbd> + <kbd>Q</kbd> to re-enable screen output (More details <a href=\"https://stackoverflow.com/a/791800/162094\">here</a>.)</p>\n",
    "url": "https://unix.stackexchange.com/questions/73498/how-to-cycle-through-reverse-i-search-in-bash"
  },
  {
    "question_title": "How to get execution time of a script effectively?",
    "question_body": "<p>I would like to display the completion time of a script. </p>\n\n<p>What I currently do is -</p>\n\n<pre><code>#!/bin/bash\ndate  ## echo the date at start\n# the script contents\ndate  ## echo the date at end\n</code></pre>\n\n<p>This just show's the time of start and end of the script. Would it be possible to display a fine grained output like processor time/ io time , etc?</p>\n",
    "answer": "<p>Just use <code>time</code> when you call the script:</p>\n<pre><code>time yourscript.sh\n</code></pre>\n<p>Output (&quot;# comments not really in the output&quot;):</p>\n<pre><code>real    2m5.034s # &lt;-- Actual time taken from start to finish.\nuser    0m0.000s # &lt;-- CPU time user-space.\nsys     0m0.003s # &lt;-- CPU time kernel-space.\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/52313/how-to-get-execution-time-of-a-script-effectively"
  },
  {
    "question_title": "Preserve bash history in multiple terminal windows",
    "question_body": "<p>I consistently have more than one terminal open. Anywhere from two to ten, doing various bits and bobs. Now let's say I restart and open up another set of terminals. Some remember certain things, some forget.</p>\n\n<p>I want a history that:</p>\n\n<ul>\n<li>Remembers everything from every terminal</li>\n<li>Is instantly accessible from every terminal (eg if I <code>ls</code> in one, switch to another already-running terminal and then press up, <code>ls</code> shows up)</li>\n<li>Doesn't forget command if there are spaces at the front of the command.</li>\n</ul>\n\n<p>Anything I can do to make bash work more like that?</p>\n",
    "answer": "<p>Add the following to your <code>~/.bashrc</code>:</p>\n<pre class=\"lang-sh prettyprint-override\"><code># Avoid duplicates\nHISTCONTROL=ignoredups:erasedups\n# When the shell exits, append to the history file instead of overwriting it\nshopt -s histappend\n\n# After each command, append to the history file and reread it\nPROMPT_COMMAND=&quot;${PROMPT_COMMAND:+$PROMPT_COMMAND$'\\n'}history -a; history -c; history -r&quot;\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/1288/preserve-bash-history-in-multiple-terminal-windows"
  },
  {
    "question_title": "Using &quot;${a:-b}&quot; for variable assignment in scripts",
    "question_body": "<p>I have been looking at a few scripts other people wrote (specifically Red Hat), and a lot of their variables are assigned using the following notation\n<code>VARIABLE1=\"${VARIABLE1:-some_val}\"</code>\nor some expand other variables\n<code>VARIABLE2=\"${VARIABLE2:-`echo $VARIABLE1`}\"</code></p>\n\n<p>What is the point of using this notation instead of just declaring the values directly (e.g., <code>VARIABLE1=some_val</code>)?</p>\n\n<p>Are there benefits to this notation or possible errors that would be prevented?</p>\n\n<p>Does the <code>:-</code> have specific meaning in this context?</p>\n",
    "answer": "<p>This technique allows for a variable to be assigned a value if another variable is either empty or is undefined. <strong>NOTE:</strong> This &quot;other variable&quot; can be the same or another variable.</p>\n<p><em>excerpt</em></p>\n<pre><code>${parameter:-word}\n    If parameter is unset or null, the expansion of word is substituted. \n    Otherwise, the value of parameter is substituted.\n</code></pre>\n<p><strong>NOTE:</strong> This form also works, <code>${parameter-word}</code>. According to <a href=\"https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html\" rel=\"noreferrer\">the Bash documentation</a>, for all such expansions:</p>\n<blockquote>\n<p>Omitting the colon results in a test only for a parameter that is unset. Put another way, if the colon is included, the operator tests for both <em>parameter</em>’s existence and that its value is not null; if the colon is omitted, the operator tests only for existence.</p>\n</blockquote>\n<p>If you'd like to see a full list of all forms of parameter expansion available within Bash then I highly suggest you take a look at this topic in the Bash Hacker's wiki titled: &quot;<a href=\"https://web.archive.org/web/20200309072646/https://wiki.bash-hackers.org/syntax/pe\" rel=\"noreferrer\">Parameter expansion</a>&quot;.</p>\n<h3>Examples</h3>\n<h4><em>variable doesn't exist</em></h4>\n<pre><code>$ echo &quot;$VAR1&quot;\n\n$ VAR1=&quot;${VAR1:-default value}&quot;\n$ echo &quot;$VAR1&quot;\ndefault value\n</code></pre>\n<h4><em>variable exists</em></h4>\n<pre><code>$ VAR1=&quot;has value&quot;\n$ echo &quot;$VAR1&quot;\nhas value\n\n$ VAR1=&quot;${VAR1:-default value}&quot;\n$ echo &quot;$VAR1&quot;\nhas value\n</code></pre>\n<p>The same thing can be done by evaluating other variables, or running commands within the default value portion of the notation.</p>\n<pre><code>$ VAR2=&quot;has another value&quot;\n$ echo &quot;$VAR2&quot;\nhas another value\n$ echo &quot;$VAR1&quot;\n\n$\n\n$ VAR1=&quot;${VAR1:-$VAR2}&quot;\n$ echo &quot;$VAR1&quot;\nhas another value\n</code></pre>\n<h3>More Examples</h3>\n<p>You can also use a slightly different notation where it's just <code>VARX=${VARX-&lt;def. value&gt;}</code>.</p>\n<pre><code>$ echo &quot;${VAR1-0}&quot;\nhas another value\n$ echo &quot;${VAR2-0}&quot;\nhas another value\n$ echo &quot;${VAR3-0}&quot;\n0\n</code></pre>\n<p>In the above <code>$VAR1</code> &amp; <code>$VAR2</code> were already defined with the string &quot;has another value&quot; but <code>$VAR3</code> was undefined, so the default value was used instead, <code>0</code>.</p>\n<h3>Another Example</h3>\n<pre><code>$ VARX=&quot;${VAR3-0}&quot;\n$ echo &quot;$VARX&quot;\n0\n</code></pre>\n<h3>Checking and assigning using <code>:=</code> notation</h3>\n<p>Lastly I'll mention the handy operator, <code>:=</code>. This will do a check and assign a value if the variable under test is empty or undefined.</p>\n<h3>Example</h3>\n<p>Notice that <code>$VAR1</code> is now set. The operator <code>:=</code> did the test and the assignment in a single operation.</p>\n<pre><code>$ unset VAR1\n$ echo &quot;$VAR1&quot;\n\n$ echo &quot;${VAR1:=default}&quot;\ndefault\n$ echo &quot;$VAR1&quot;\ndefault\n</code></pre>\n<p>However if the value is set prior, then it's left alone.</p>\n<pre><code>$ VAR1=&quot;some value&quot;\n$ echo &quot;${VAR1:=default}&quot;\nsome value\n$ echo &quot;$VAR1&quot;\nsome value\n</code></pre>\n<h3>Handy Dandy Reference Table</h3>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th></th>\n<th>Parameter set and not null</th>\n<th>Parameter set but null</th>\n<th>Parameter unset</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>${parameter:-word}</code></td>\n<td>substitute <em>parameter</em></td>\n<td>substitute <em>word</em></td>\n<td>substitute <em>word</em></td>\n</tr>\n<tr>\n<td><code>${parameter-word}</code></td>\n<td>substitute <em>parameter</em></td>\n<td>substitute <em>null</em></td>\n<td>substitute <em>word</em></td>\n</tr>\n<tr>\n<td><code>${parameter:=word}</code></td>\n<td>substitute <em>parameter</em></td>\n<td>assign <em>word</em></td>\n<td>assign <em>word</em></td>\n</tr>\n<tr>\n<td><code>${parameter=word}</code></td>\n<td>substitute <em>parameter</em></td>\n<td>substitute <em>null</em></td>\n<td>assign <em>word</em></td>\n</tr>\n<tr>\n<td><code>${parameter:?word}</code></td>\n<td>substitute <em>parameter</em></td>\n<td>error, exit</td>\n<td>error, exit</td>\n</tr>\n<tr>\n<td><code>${parameter?word}</code></td>\n<td>substitute <em>parameter</em></td>\n<td>substitute <em>null</em></td>\n<td>error, exit</td>\n</tr>\n<tr>\n<td><code>${parameter:+word}</code></td>\n<td>substitute <em>word</em></td>\n<td>substitute <em>null</em></td>\n<td>substitute <em>null</em></td>\n</tr>\n<tr>\n<td><code>${parameter+word}</code></td>\n<td>substitute <em>word</em></td>\n<td>substitute <em>word</em></td>\n<td>substitute <em>null</em></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>(<a href=\"https://i.sstatic.net/T2Fp8.png\" rel=\"noreferrer\">Screenshot of source table</a>)</p>\n<p><a href=\"https://unix.stackexchange.com/questions/657434/bash-parameter-expansion-substituted-versus-assigned\">This</a> makes the difference between <em>assignment</em> and <em>substitution</em> explicit: Assignment sets a value for the variable whereas substitution doesn't.</p>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://web.archive.org/web/20200309072646/https://wiki.bash-hackers.org/syntax/pe\" rel=\"noreferrer\">Parameter Expansions - Bash Hackers Wiki</a></li>\n<li><a href=\"https://www.tldp.org/LDP/abs/html/parameter-substitution.html\" rel=\"noreferrer\">10.2. Parameter Substitution</a></li>\n<li><a href=\"https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html\" rel=\"noreferrer\">Bash Parameter Expansions</a></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/122845/using-a-b-for-variable-assignment-in-scripts"
  },
  {
    "question_title": "How can I resolve a hostname to an IP address in a Bash script?",
    "question_body": "<p>What's the most concise way to resolve a hostname to an IP address in a Bash script? I'm using <a href=\"http://en.wikipedia.org/wiki/Arch_Linux\">Arch Linux</a>.</p>\n",
    "answer": "<p>You can use <code>getent</code>, which comes with <code>glibc</code> (so you almost certainly have it on Linux). This resolves using gethostbyaddr/gethostbyname2, and so also will check <code>/etc/hosts</code>/NIS/etc:</p>\n\n<pre><code>getent hosts unix.stackexchange.com | awk '{ print $1 }'\n</code></pre>\n\n<p>Or, as Heinzi said below, you can use <code>dig</code> with the <code>+short</code> argument (queries DNS servers directly, does not look at <code>/etc/hosts</code>/NSS/etc) :</p>\n\n<pre><code>dig +short unix.stackexchange.com\n</code></pre>\n\n<p>If <code>dig +short</code> is unavailable, any one of the following should work. All of these query DNS directly and ignore other means of resolution:</p>\n\n<pre><code>host unix.stackexchange.com | awk '/has address/ { print $4 }'\nnslookup unix.stackexchange.com | awk '/^Address: / { print $2 }'\ndig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 }'\n</code></pre>\n\n<p>If you want to only print one IP, then add the <code>exit</code> command to <code>awk</code>'s workflow.</p>\n\n<pre><code>dig +short unix.stackexchange.com | awk '{ print ; exit }'\ngetent hosts unix.stackexchange.com | awk '{ print $1 ; exit }'\nhost unix.stackexchange.com | awk '/has address/ { print $4 ; exit }'\nnslookup unix.stackexchange.com | awk '/^Address: / { print $2 ; exit }'\ndig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 ; exit }'\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/20784/how-can-i-resolve-a-hostname-to-an-ip-address-in-a-bash-script"
  },
  {
    "question_title": "In a bash script, using the conditional &quot;or&quot; in an &quot;if&quot; statement",
    "question_body": "<p>This question is a sequel of sorts to my <a href=\"https://unix.stackexchange.com/questions/47557/in-a-bash-shell-script-writing-a-for-loop-that-iterates-over-string-values\">earlier question</a>.  The users on this site kindly helped me determine how to write a bash <code>for</code> loop that iterates over string values.  For example, suppose that a loop control variable <code>fname</code> iterates over the strings <code>&quot;a.txt&quot; &quot;b.txt&quot; &quot;c.txt&quot;</code>.  I would like to <code>echo</code> &quot;yes!&quot; when <code>fname</code> has the value <code>&quot;a.txt&quot;</code> or <code>&quot;c.txt&quot;</code>, and <code>echo</code> &quot;no!&quot; otherwise.  I have tried the following bash shell script:</p>\n<pre><code>#!/bin/bash\n\nfor fname in &quot;a.txt&quot; &quot;b.txt&quot; &quot;c.txt&quot;\ndo\n  echo $fname\n  if [ &quot;$fname&quot; = &quot;a.txt&quot; ] | [ &quot;$fname&quot; = &quot;c.txt&quot; ]; then\n    echo &quot;yes!&quot;\n  else\n    echo &quot;no!&quot;\n  fi\ndone\n</code></pre>\n<p>I obtain the output:</p>\n<blockquote>\n<p>a.txt</p>\n<p>no!</p>\n<p>b.txt</p>\n<p>no!</p>\n<p>c.txt</p>\n<p>yes!</p>\n</blockquote>\n<p>Why does the <code>if</code> statement apparently yield true when <code>fname</code> has the value <code>&quot;a.txt&quot;</code>?  Have I used <code>|</code> incorrectly?</p>\n",
    "answer": "<p>If you want to say <code>OR</code> use double pipe (<code>||</code>).</p>\n\n<pre><code>if [ \"$fname\" = \"a.txt\" ] || [ \"$fname\" = \"c.txt\" ]\n</code></pre>\n\n<p>(The original OP code using <code>|</code> was simply piping the output of the left side to the right side, in the same way any ordinary pipe works.)</p>\n\n<hr>\n\n<p>After many years of comments and misunderstanding, allow me to clarify.</p>\n\n<p>To do <code>OR</code> you use <code>||</code>.</p>\n\n<p>Whether you use <code>[</code> or <code>[[</code> or <code>test</code> or <code>((</code> all depends on what you need on a case by case basis. It's wrong to say that one of those is preferred in all cases. Sometimes <code>[</code> is right and <code>[[</code> is wrong. But that's not what the question was. OP asked why <code>|</code> didn't work. The answer is because it should be <code>||</code> instead.</p>\n",
    "url": "https://unix.stackexchange.com/questions/47584/in-a-bash-script-using-the-conditional-or-in-an-if-statement"
  },
  {
    "question_title": "What is the difference between the Bash operators [[ vs [ vs ( vs ((?",
    "question_body": "<p>I am a little bit confused on what do these operators do differently when used in bash (brackets, double brackets, parenthesis and double parenthesis).</p>\n<pre><code>[[ , [ , ( , ((\n</code></pre>\n<p>I have seen people use them on <code>if</code> statements like this:</p>\n<pre><code>if [[ condition ]]\n\nif [ condition ]\n\nif ((condition))\n\nif (condition)\n</code></pre>\n",
    "answer": "<p>In Bourne-like shells, an <code>if</code> statement typically looks like</p>\n<pre><code>if\n   command-list1\nthen\n   command-list2\nelse\n   command-list3\nfi\n</code></pre>\n<p>The <code>then</code> clause is executed if the exit code of the <code>command-list1</code> list of commands is zero.  If the exit code is nonzero, then the <code>else</code> clause is executed.  <code>command-list1</code> can be\nsimple or complex.  It can, for example, be a sequence of one or more pipelines separated by one of the operators <code>;</code>, <code>&amp;</code>, <code>&amp;&amp;</code>, <code>||</code> or newline.  The <code>if</code> conditions shown below are just special cases of <code>command-list1</code>:</p>\n<ol>\n<li><p><code>if [ condition ]</code></p>\n<p><code>[</code> is another name for the traditional <code>test</code> command.  <code>[</code> / <code>test</code> is a standard POSIX utility. All POSIX shells have it builtin (though that's not required by POSIX²).  The <code>test</code> command sets an exit code and the <code>if</code> statement acts accordingly.  Typical tests are whether a file exists or one number is equal to another.</p>\n</li>\n<li><p><code>if [[ condition ]]</code></p>\n<p>This is a new upgraded variation on <code>test</code>¹ from <em>ksh</em> that <em>bash</em>, <em>zsh</em>, <em>yash</em>, <em>busybox sh</em> also support.  This <code>[[ ... ]]</code> construct also sets an exit code and the <code>if</code> statement acts accordingly.  Among its extended features, it can test whether a string matches a wildcard pattern (not in <em>busybox sh</em>).</p>\n</li>\n<li><p><code>if ((condition))</code></p>\n<p>Another <em>ksh</em> extension that <em>bash</em> and <em>zsh</em> also support. This performs arithmetic.  As the result of the arithmetic, an exit code is set and  the <code>if</code> statement acts accordingly.  It returns an exit code of zero (true) if the result of the arithmetic calculation is nonzero.  Like <code>[[...]]</code>, this form is not POSIX and therefore not portable.</p>\n</li>\n<li><p><code>if (command)</code></p>\n<p>This runs command in a subshell.  When command completes, it sets an exit code  and  the <code>if</code> statement acts accordingly.</p>\n<p>A typical reason for using a subshell like this is to limit side-effects of <code>command</code> if <code>command</code> required variable assignments or other changes to the shell's environment.  Such changes do not remain after the subshell completes.</p>\n</li>\n<li><p><code>if command</code></p>\n<p>command is executed and the <code>if</code> statement acts according to its exit code.</p>\n</li>\n</ol>\n<p>Note that <code>[ ... ]</code> and <code>[[ ... ]]</code> require whitespace around them, while <code>(...)</code> and <code>((...))</code> do not.</p>\n<hr />\n<p><sup>¹ though not really a command but a special shell construct with its own separate syntax from that of normal command, and varying significantly between shell implementations</sup></p>\n<p><sup>² POSIX does require that there be a standalone <code>test</code> and <code>[</code> utilities on the system however, though in the case of <code>[</code>, several Linux distributions have been known to be missing it.</sup></p>\n",
    "url": "https://unix.stackexchange.com/questions/306111/what-is-the-difference-between-the-bash-operators-vs-vs-vs"
  },
  {
    "question_title": "How to conditionally do something if a command succeeded or failed",
    "question_body": "<p>How can I do something like this in bash?</p>\n\n<pre><code>if \"`command` returns any error\";\nthen\n    echo \"Returned an error\"\nelse\n    echo \"Proceed...\"\nfi\n</code></pre>\n",
    "answer": "<blockquote>\n  <p><strong>How to conditionally do something if a command succeeded or failed</strong></p>\n</blockquote>\n\n<p>That's exactly what bash's <code>if</code> statement does:</p>\n\n<pre><code>if command ; then\n    echo \"Command succeeded\"\nelse\n    echo \"Command failed\"\nfi\n</code></pre>\n\n<p>Adding information from comments: you <em>don't</em> need to use the <code>[</code> ... <code>]</code> syntax in this case. <code>[</code> is itself a command, very nearly equivalent to <code>test</code>. It's probably the most common command to use in an <code>if</code>, which can lead to the assumption that it's part of the shell's syntax. But if you want to test whether a command succeeded or not, use the command itself directly with <code>if</code>, as shown above.</p>\n",
    "url": "https://unix.stackexchange.com/questions/22726/how-to-conditionally-do-something-if-a-command-succeeded-or-failed"
  },
  {
    "question_title": "How do I loop through only directories in bash?",
    "question_body": "<p>I have a folder with some directories and some files (some are hidden, beginning with dot).</p>\n<pre><code>for d in *; do\n echo $d\ndone\n</code></pre>\n<p>will loop through all files and directories, but I want to loop only through directories. How do I do that?</p>\n",
    "answer": "<p>You can specify a slash at the end to match only directories:</p>\n<pre><code>for d in */ ; do\n    echo &quot;$d&quot;\ndone\n</code></pre>\n<p>If you want to exclude symlinks, use a test to <code>continue</code> the loop if the current entry is a link. You need to remove the trailing slash from the name in order for <code>-L</code> to be able to recognise it as a symbolic link:</p>\n<pre><code>for d in */ ; do\n    [ -L &quot;${d%/}&quot; ] &amp;&amp; continue\n    echo &quot;$d&quot;\ndone\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/86722/how-do-i-loop-through-only-directories-in-bash"
  },
  {
    "question_title": "How do I change the extension of multiple files?",
    "question_body": "<p>I would like to change a file extension from <code>*.txt</code> to <code>*.text</code>. I tried using the <a href=\"https://man7.org/linux/man-pages/man1/basename.1.html\" rel=\"noreferrer\"><code>basename</code></a> command, but I'm having trouble changing more than one file.</p>\n<p>Here's my code:</p>\n<pre><code>files=`ls -1 *.txt`\n\nfor x in $files\ndo\n    mv $x &quot;`basename $files .txt`.text&quot;\ndone\n</code></pre>\n<p>I'm getting this error:</p>\n<blockquote>\n<p>basename: too many arguments Try basename --help' for more information</p>\n</blockquote>\n",
    "answer": "<p>Straight from <a href=\"https://mywiki.wooledge.org/BashFAQ/030\" rel=\"noreferrer\">Greg's Wiki</a>:</p>\n<pre class=\"lang-sh prettyprint-override\"><code># Rename all *.txt to *.text\nfor file in *.txt; do\n    mv -- &quot;$file&quot; &quot;${file%.txt}.text&quot;\ndone\n</code></pre>\n<p><code>*.txt</code> is <a href=\"https://mywiki.wooledge.org/glob\" rel=\"noreferrer\">a globbing pattern</a>, using <code>*</code> as a wildcard to match any string. <code>*.txt</code> matches all filenames ending with '.txt'.</p>\n<p><code>--</code> <a href=\"https://unix.stackexchange.com/questions/11376/what-does-double-dash-mean-also-known-as-bare-double-dash\">marks the end of the option list</a>. This avoids issues with filenames starting with hyphens.</p>\n<p><code>${file%.txt}</code> is <a href=\"https://stackoverflow.com/a/9559024/1709587\">a parameter expansion</a>, replaced by the value of the <code>file</code> variable with <code>.txt</code> removed from the end.</p>\n<p>Also see the entry on why you <a href=\"https://mywiki.wooledge.org/ParsingLs\" rel=\"noreferrer\">shouldn't parse <code>ls</code></a>.</p>\n<p>If you have to use <code>basename</code>, your syntax would be:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>for file in *.txt; do\n    mv -- &quot;$file&quot; &quot;$(basename -- &quot;$file&quot; .txt).text&quot;\ndone\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/19654/how-do-i-change-the-extension-of-multiple-files"
  },
  {
    "question_title": "How to remove a single line from history?",
    "question_body": "<p>I'm working in Mac OSX, so I guess I'm using bash...?</p>\n\n<p>Sometimes I enter something that I don't want to be remembered in the history. How do I remove it?</p>\n",
    "answer": "<h1>Preventative measures</h1>\n\n<p>If you want to run a command without saving it in history, prepend it with an extra space</p>\n\n<pre><code>prompt$ echo saved\nprompt$  echo not saved \\\n&gt; #     ^ extra space\n</code></pre>\n\n<p>For this to work you need either <code>ignorespace</code> or <code>ignoreboth</code> in <code>HISTCONTROL</code>.  For example, run</p>\n\n<pre><code>HISTCONTROL=ignorespace\n</code></pre>\n\n<p>To make this setting persistent, put it in your <code>.bashrc</code>.</p>\n\n<h1>Post-mortem clean-up</h1>\n\n<p>If you've already run the command, and want to remove it from history, first use</p>\n\n<pre><code>history\n</code></pre>\n\n<p>to display the list of commands in your history.  Find the number next to the one you want to delete (e.g. 1234) and run </p>\n\n<pre><code>history -d 1234\n</code></pre>\n\n<p>Additionally, if the line you want to delete has already been written to your $HISTFILE (which typically happens when you end a session by default), you will need to write back to $HISTFILE, or the line will reappear when you open a new session:</p>\n\n<pre><code>history -w\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/49214/how-to-remove-a-single-line-from-history"
  },
  {
    "question_title": "In Bash, when to alias, when to script and when to write a function?",
    "question_body": "<p>Noone should need 10 years for asking this question, like I did. If I were just starting out with Linux, I'd want to know: <em>When to alias, when to script and when to write a function?</em></p>\n<p>Where aliases are concerned, I use aliases for very simple operations that don't take arguments.</p>\n<pre><code>alias houston='cd /home/username/.scripts/'\n</code></pre>\n<p>That seems obvious. But some people do this:</p>\n<pre><code>alias command=&quot;bash bashscriptname&quot;\n</code></pre>\n<p>(and add it to the <code>.bashrc</code> file).</p>\n<p>Is there a good reason to do that? I didn't come across a circumstance for this. If there is an edge case where that would make a difference, please answer below.</p>\n<p>That's where I would just put something in my PATH and <code>chmod +x</code> it, which is another thing that came after years of Linux trial-and-error.</p>\n<p>Which brings me to the next topic. For instance, I added a hidden folder (<code>.scripts/</code>) in the home directory to my PATH by just adding a line to my <code>.bashrc</code> (<code>PATH=$PATH:/home/username/.scripts/</code>), so anything executable in there automagically autocompletes.</p>\n<p>I don't really need that, do I? I would only use that for languages which are not the shell, like Python. If it's the shell, I can just write a function inside the very same <code>.bashrc</code>:</p>\n<pre><code>funcname () {\n  somecommand -someARGS &quot;$@&quot;\n}\n</code></pre>\n<p>Did I miss anything?<br />\nWhat would you tell a beginning Linux user about when to alias, when to script and when to write a function?</p>\n<p><em>If it's not obvious, I'm assuming the people who answer this will make use of all three options. If you only use one or two of these three (aliases, scripts, functions), this question isn't really aimed at you.</em></p>\n",
    "answer": "<p>The other answers provide some soft general guidelines based on personal taste, but ignore many pertinent <em>facts</em> that one should consider when deciding between scripts, functions, or aliases.</p>\n\n<h3>Aliases and Functions ¹</h3>\n\n<ul>\n<li>The entire contents of aliases and functions are stored in the shell's memory.  </li>\n<li>A natural consequence of this is aliases and functions can <em>only</em> be used by the current shell, and not by any other programs you may invoke from the shell like text editors, scripts, or even child instances of the same shell. </li>\n<li>Aliases and functions are executed by the current shell, i.e. they run within and affect the shell's current environment.²  No separate process is necessary to run an alias or function.</li>\n</ul>\n\n<h3>Scripts</h3>\n\n<ul>\n<li>Shells do not keep scripts in memory.  Instead, scripts are read from the files where they are stored every time they are needed.  If the script is found via a <code>$PATH</code> search, many shells store a hash of its path name in memory to save time on future <code>$PATH</code> look-ups, but that is the extent of a script's memory footprint when not in use.</li>\n<li><p>Scripts can be invoked in more ways than functions and aliases can.  They can be passed as an argument to an interpreter, like <code>sh script</code>, or invoked directly as an executable, in which case the interpreter in the shebang line (e.g. <code>#!/bin/sh</code>) is invoked to run it.  In both cases, the script is run by a separate interpreter process with its own environment separate from that of your shell, whose environment the script cannot affect in any way.  Indeed, the interpreter shell does not even have to match the invoking shell.  Because scripts invoked this way appear to behave like any ordinary executable, they can be used by any program.  </p>\n\n<p>Finally, a script can be read and run by the current shell with <code>.</code>, or in some shells, <code>source</code>.  In this case, the script behaves much like a function that is read on-demand instead of being constantly kept in memory.</p></li>\n</ul>\n\n<h3>Application</h3>\n\n<p>Given the above, we can come up with some general guidelines for whether to make something a script or function / alias.</p>\n\n<ul>\n<li><p><em>Do other programs besides your shell need to be able to use it?</em>  If so, it has to be a script.</p></li>\n<li><p><em>Do you only want it to be available from an interactive shell?</em>  It's common to want to change the default behavior of many commands when run interactively without affecting external commands / scripts.  For this case, use an alias / function set in the shell's \"interactive-mode-only\" rc file (for <code>bash</code> this is <code>.bashrc</code>).</p></li>\n<li><p><em>Does it need to change the shell's environment?</em>  Both a function / alias or a sourced script are possible choices.</p></li>\n<li><p><em>Is it something you use frequently?</em>  It's probably more efficient to keep it in memory, so make it a function / alias if possible.</p></li>\n<li><p><em>Conversely, is it something you use only rarely?</em>  In that case, there's no sense having it hog memory when you don't need it, so make it a script.</p></li>\n</ul>\n\n<hr>\n\n<p><sup>¹ While functions and aliases have some important differences, they are grouped together because functions can do everything aliases can.  Aliases can not have local variables nor can they process arguments, and they are inconvenient for anything longer than one line.</sup></p>\n\n<p><sup>² Every running process in a Unix system has an <em>environment</em> consisting of a bunch of <code>variable=value</code> pairs which often contain global configuration settings, like <code>LANG</code> for the default locale and <code>PATH</code> for specifying executable search path.</sup></p>\n",
    "url": "https://unix.stackexchange.com/questions/30925/in-bash-when-to-alias-when-to-script-and-when-to-write-a-function"
  },
  {
    "question_title": "How to view the output of a running process in another bash session?",
    "question_body": "<p>I have left a script running on a remote machine from when I was locally working at it. I can connect over SSH to the machine as the same user and see the script running in <code>ps</code>. </p>\n\n<pre><code>$ ps aux | grep ipcheck\nmyuser  18386  0.0  0.0  18460  3476 pts/0    S+   Dec14   1:11 /bin/bash ./ipchecker.sh\n</code></pre>\n\n<p>It is simply outputting to stdout on a local session (I ran <code>./ipchecker.sh</code> form a local terminal window, no redirection, no use of <code>screen</code> etc).</p>\n\n<p>Is there anyway from an SSH session I can view the output of this running command (without stopping it)?</p>\n\n<p>So far the best I have found is to use <code>strace -p 18386</code> but I get hordes of text flying up the screen, its far too detailed. I can stop <code>strace</code> and then sift through the output and find the text bring printed to stdout but its very long and confusing, and obviously whilst it's stopped I might miss something. I would like to find a way to see the script output live as if I was working locally.</p>\n\n<p>Can anyone improve on this? The obvious answer is to restart the script with redirection or in a <code>screen</code> session etc, this isn't a mission critical script so I could do that. Rather though, I see this as a fun learning exercise.</p>\n",
    "answer": "<p>You can access the output via the <code>proc</code> filesystem.</p>\n<pre><code>tail -f /proc/&lt;pid&gt;/fd/1\n</code></pre>\n<p><code>1</code> = stdout, <code>2</code> = stderr</p>\n<p>(or like @jmhostalet says: <code>cat /proc/&lt;pid&gt;/fd/1</code> if tail doesn't work)</p>\n",
    "url": "https://unix.stackexchange.com/questions/58550/how-to-view-the-output-of-a-running-process-in-another-bash-session"
  },
  {
    "question_title": "How can I get the size of a file in a bash script?",
    "question_body": "<p>How can I get the size of a file in a bash script? </p>\n\n<p>How do I assign this to a bash variable so I can use it later?</p>\n",
    "answer": "<p>Your best bet if on a GNU system:</p>\n<pre><code>stat --printf=&quot;%s&quot; file.any\n</code></pre>\n<p>From <a href=\"https://linux.die.net/man/1/stat\" rel=\"noreferrer\">man stat</a>:</p>\n<blockquote>\n<p>%s total size, in bytes</p>\n</blockquote>\n<p>In a bash script :</p>\n<pre><code>#!/bin/bash\nFILENAME=/home/heiko/dummy/packages.txt\nFILESIZE=$(stat -c%s &quot;$FILENAME&quot;)\necho &quot;Size of $FILENAME = $FILESIZE bytes.&quot;\n</code></pre>\n<p>NOTE: see <a href=\"https://unix.stackexchange.com/a/185039/9071\">@chbrown's answer</a> for how to use <code>stat</code> on BSD or macOS systems.</p>\n",
    "url": "https://unix.stackexchange.com/questions/16640/how-can-i-get-the-size-of-a-file-in-a-bash-script"
  },
  {
    "question_title": "How to define &#39;tab&#39; delimiter with &#39;cut&#39; in Bash?",
    "question_body": "<p>Here is an example of using <code>cut</code> to break input into fields using a space delimiter, and obtaining the second field:</p>\n\n<p><code>cut -f2 -d' '</code> </p>\n\n<p>How can the delimiter be defined as a tab, instead of a space?</p>\n",
    "answer": "<p>Two ways:</p>\n<p>Press <kbd>Ctrl</kbd>+<kbd>V</kbd> and then <kbd>Tab</kbd> to use &quot;verbatim&quot; <a href=\"https://www.gnu.org/software/bash/manual/html_node/Commands-For-Text.html\" rel=\"noreferrer\">quoted insert</a>.</p>\n<pre><code>cut -f2 -d'   ' infile\n</code></pre>\n<p>or write it like this to use <a href=\"https://www.gnu.org/software/bash/manual/html_node/ANSI_002dC-Quoting.html\" rel=\"noreferrer\">ANSI-C quoting</a>:</p>\n<pre><code>cut -f2 -d$'\\t' infile\n</code></pre>\n<p>The <code>$'...'</code> form of quotes isn't part of the POSIX shell language (<a href=\"http://austingroupbugs.net/view.php?id=249\" rel=\"noreferrer\">not yet</a>), but works at least in ksh, mksh, zsh and Busybox in addition to Bash.</p>\n",
    "url": "https://unix.stackexchange.com/questions/35369/how-to-define-tab-delimiter-with-cut-in-bash"
  },
  {
    "question_title": "Why does my shell script choke on whitespace or other special characters?",
    "question_body": "<p>… or an introductory guide to robust filename handling and other string passing in shell scripts.</p>\n<p>I wrote a shell script which works well most of the time. But it chokes on some inputs (e.g. on some file names).</p>\n<p>I encountered a problem such as the following:</p>\n<ul>\n<li>I have a file name containing a space <code>hello world</code>, and it was treated as two separate files <code>hello</code> and <code>world</code>.</li>\n<li>I have an input line with two consecutive spaces and they shrank to one in the input.</li>\n<li>Leading and trailing whitespace disappears from input lines.</li>\n<li>Sometimes, when the input contains one of the characters <code>\\[*?</code>, they are replaced by some text which is actually the names of some files.</li>\n<li>There is an apostrophe <code>'</code> (or a double quote <code>&quot;</code>) in the input, and things got weird after that point.</li>\n<li>There is a backslash in the input (or: I am using Cygwin and some of my file names have Windows-style <code>\\</code> separators).</li>\n</ul>\n<p>What is going on, and how do I fix this?</p>\n",
    "answer": "<h2>Always use double quotes around variable substitutions and command substitutions: <code>&quot;$foo&quot;</code>, <code>&quot;$(foo)&quot;</code></h2>\n<p>If you use <code>$foo</code> unquoted, your script will choke on input or parameters (or command output, with <code>$(foo)</code>) containing whitespace or <code>\\[*?</code>.</p>\n<p>There, you can stop reading. Well, ok, here are a few more:</p>\n<ul>\n<li><code>read</code> — <strong>To read input line by line with the <code>read</code> builtin, use <code>while IFS= read -r line; do …</code></strong><br />\nPlain <code>read</code> treats backslashes and whitespace specially.</li>\n<li><code>xargs</code> — <strong>Avoid <code>xargs</code></strong>. If you must use <code>xargs</code>, make that <code>xargs -0</code>. Instead of <code>find … | xargs</code>, <strong>prefer <code>find … -exec …</code></strong>.<br />\n<code>xargs</code> treats whitespace and the characters <code>\\&quot;'</code> specially.</li>\n</ul>\n<p>This answer applies to Bourne/POSIX-style shells (<code>sh</code>, <code>ash</code>, <code>dash</code>, <code>bash</code>, <code>ksh</code>, <code>mksh</code>, <code>yash</code>…). Zsh users should skip it and read the end of <a href=\"https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary/68748#68748\">When is double-quoting necessary?</a> instead. If you want the whole nitty-gritty, <a href=\"http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06\" rel=\"noreferrer\">read the standard</a> or your shell's manual.</p>\n<hr />\n<p>Note that the explanations below contains a few approximations (statements that are true in most conditions but can be affected by the surrounding context or by configuration).</p>\n<h2>Why do I need to write <code>&quot;$foo&quot;</code>? What happens without the quotes?</h2>\n<p><code>$foo</code> does not mean “take the value of the variable <code>foo</code>”. It means something much more complex:</p>\n<ul>\n<li>First, take the value of the variable.</li>\n<li>Field splitting: treat that value as a whitespace-separated list of fields, and build the resulting list. For example, if the variable contains <code>foo *  bar ​</code> then the result of this step is the 3-element list <code>foo</code>, <code>*</code>, <code>bar</code>.</li>\n<li>Filename generation: treat each field as a glob, i.e. as a wildcard pattern, and replace it by the list of file names that match this pattern. If the pattern doesn't match any files, it is left unmodified. In our example, this results in the list containing <code>foo</code>, following by the list of files in the current directory, and finally <code>bar</code>. If the current directory is empty, the result is <code>foo</code>, <code>*</code>, <code>bar</code>.</li>\n</ul>\n<p>Note that the result is a list of strings. There are two contexts in shell syntax: list context and string context. Field splitting and filename generation only happen in list context, but that's most of the time. Double quotes delimit a string context: the whole double-quoted string is a single string, not to be split. (Exception: <code>&quot;$@&quot;</code> to expand to the list of positional parameters, e.g. <code>&quot;$@&quot;</code> is equivalent to <code>&quot;$1&quot; &quot;$2&quot; &quot;$3&quot;</code> if there are three positional parameters. See <a href=\"https://unix.stackexchange.com/questions/41571/what-is-the-difference-between-and/94200#94200\">What is the difference between $* and $@?</a>)</p>\n<p>The same happens to command substitution with <code>$(foo)</code> or with <code>`foo`</code>. On a side note, don't use <code>`foo`</code>: its quoting rules are weird and non-portable, and all modern shells support <code>$(foo)</code> which is absolutely equivalent except for having intuitive quoting rules.</p>\n<p>The output of arithmetic substitution also undergoes the same expansions, but that isn't normally a concern as it only contains non-expandable characters (assuming <code>IFS</code> doesn't contain digits or <code>-</code>).</p>\n<p>See <a href=\"https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary\">When is double-quoting necessary?</a> for more details about the cases when you can leave out the quotes.</p>\n<p>Unless you mean for all this rigmarole to happen, just remember to always use double quotes around variable and command substitutions. Do take care: leaving out the quotes can lead not just to errors but to <a href=\"https://unix.stackexchange.com/questions/171346/security-implications-of-forgetting-to-quote-a-variable-in-bash-posix-shells/171347#171347\">security holes</a>.</p>\n<h3>How do I process a list of file names?</h3>\n<p>If you write <code>myfiles=&quot;file1 file2&quot;</code>, with spaces to separate the files, this can't work with file names containing spaces. Unix file names can contain any character other than <code>/</code> (which is always a directory separator) and null bytes (which you can't use in shell scripts with most shells).</p>\n<p>Same problem with <code>myfiles=*.txt; … process $myfiles</code>. When you do this, the variable <code>myfiles</code> contains the 5-character string <code>*.txt</code>, and it's when you write <code>$myfiles</code> that the wildcard is expanded. This example will actually work, until you change your script to be <code>myfiles=&quot;$someprefix*.txt&quot;; … process $myfiles</code>. If <code>someprefix</code> is set to <code>final report</code>, this won't work.</p>\n<p>To process a list of any kind (such as file names), put it in an array. This requires mksh, ksh93, yash or bash (or zsh, which doesn't have all these quoting issues); a plain POSIX shell (such as ash or dash) doesn't have array variables.</p>\n<pre><code>myfiles=(&quot;$someprefix&quot;*.txt)\nprocess &quot;${myfiles[@]}&quot;\n</code></pre>\n<p>Ksh88 has array variables with a different assignment syntax <code>set -A myfiles &quot;someprefix&quot;*.txt</code> (see <a href=\"https://stackoverflow.com/questions/22862038/assignation-variable-under-different-ksh-environment\">assignation variable under different ksh environment</a> if you need ksh88/bash portability). Bourne/POSIX-style shells have a single one array, the array of positional parameters <code>&quot;$@&quot;</code> which you set with <code>set</code> and which is local to a function:</p>\n<pre><code>set -- &quot;$someprefix&quot;*.txt\nprocess -- &quot;$@&quot;\n</code></pre>\n<h3>What about file names that begin with <code>-</code>?</h3>\n<p>On a related note, keep in mind that file names can begin with a <code>-</code> (dash/minus), which most commands interpret as denoting an option. Some commands (like <code>sh</code>, <code>set</code> or <code>sort</code>) also accept options that start with <code>+</code>. If you have a file name that begins with a variable part, be sure to pass <code>--</code> before it, as in the snippet above. This indicates to the command that it has reached the end of options, so anything after that is a file name even if it starts with <code>-</code> or <code>+</code>.</p>\n<p>Alternatively, you can make sure that your file names begin with a character other than <code>-</code>. Absolute file names begin with <code>/</code>, and you can add <code>./</code> at the beginning of relative names. The following snippet turns the content of the variable <code>f</code> into a “safe” way of referring to the same file that's guaranteed not to start with <code>-</code> nor <code>+</code>.</p>\n<pre><code>case &quot;$f&quot; in -* | +*) &quot;f=./$f&quot;;; esac\n</code></pre>\n<p>On a final note on this topic, beware that some commands interpret <code>-</code> as meaning standard input or standard output, even after <code>--</code>. If you need to refer to an actual file named <code>-</code>, or if you're calling such a program and you don't want it to read from stdin or write to stdout, make sure to rewrite <code>-</code> as above. See <a href=\"https://unix.stackexchange.com/questions/110750/what-is-the-difference-between-du-sh-and-du-sh/110756#110756\">What is the difference between &quot;du -sh *&quot; and &quot;du -sh ./*&quot;?</a> for further discussion.</p>\n<h2>How do I store a command in a variable?</h2>\n<p>“Command” can mean three things: a command name (the name as an executable, with or without full path, or the name of a function, builtin or alias), a command name with arguments, or a piece of shell code. There are accordingly different ways of storing them in a variable.</p>\n<p>If you have a command name, just store it and use the variable with double quotes as usual.</p>\n<pre><code>command_path=&quot;$1&quot;\n…\n&quot;$command_path&quot; --option --message=&quot;hello world&quot;\n</code></pre>\n<p>If you have a command with arguments, the problem is the same as with a list of file names above: this is a list of strings, not a string. You can't just stuff the arguments into a single string with spaces in between, because if you do that you can't tell the difference between spaces that are part of arguments and spaces that separate arguments. If your shell has arrays, you can use them.</p>\n<pre><code>cmd=(/path/to/executable --option --message=&quot;hello world&quot; --)\ncmd=(&quot;${cmd[@]}&quot; &quot;$file1&quot; &quot;$file2&quot;)\n&quot;${cmd[@]}&quot;\n</code></pre>\n<p>What if you're using a shell without arrays? You can still use the positional parameters, if you don't mind modifying them.</p>\n<pre><code>set -- /path/to/executable --option --message=&quot;hello world&quot; --\nset -- &quot;$@&quot; &quot;$file1&quot; &quot;$file2&quot;\n&quot;$@&quot;\n</code></pre>\n<p>What if you need to store a complex shell command, e.g. with redirections, pipes, etc.? Or if you don't want to modify the positional parameters? Then you can build a string containing the command, and use the <code>eval</code> builtin.</p>\n<pre><code>code='/path/to/executable --option --message=&quot;hello world&quot; -- /path/to/file1 | grep &quot;interesting stuff&quot;'\neval &quot;$code&quot;\n</code></pre>\n<p>Note the nested quotes in the definition of <code>code</code>: the single quotes <code>'…'</code> delimit a string literal, so that the value of the variable <code>code</code> is the string <code>/path/to/executable --option --message=&quot;hello world&quot; -- /path/to/file1</code>. The <code>eval</code> builtin tells the shell to parse the string passed as an argument as if it appeared in the script, so at that point the quotes and pipe are parsed, etc.</p>\n<p>Using <code>eval</code> is tricky. Think carefully about what gets parsed when. In particular, you can't just stuff a file name into the code: you need to quote it, just like you would if it was in a source code file. There's no direct way to do that. Something like <code>code=&quot;$code $filename&quot;</code> breaks if the file name contains any shell special character (spaces, <code>$</code>, <code>;</code>, <code>|</code>, <code>&lt;</code>, <code>&gt;</code>, etc.). <code>code=&quot;$code \\&quot;$filename\\&quot;&quot;</code> still breaks on <code>&quot;$\\`</code>. Even <code>code=&quot;$code '$filename'&quot;</code> breaks if the file name contains a <code>'</code>. There are two solutions.</p>\n<ul>\n<li><p>Add a layer of quotes around the file name. The easiest way to do that is to add single quotes around it, and replace single quotes by <code>'\\''</code>.</p>\n<pre><code> quoted_filename=$(printf %s. &quot;$filename&quot; | sed &quot;s/'/'\\\\\\\\''/g&quot;)\n code=&quot;$code '${quoted_filename%.}'&quot;\n</code></pre>\n</li>\n<li><p>Keep the variable expansion inside the code, so that it's looked up when the code is evaluated, not when the code fragment is built. This is simpler but only works if the variable is still around with the same value at the time the code is executed, not e.g. if the code is built in a loop.</p>\n<pre><code> code=&quot;$code \\&quot;\\$filename\\&quot;&quot;\n</code></pre>\n</li>\n</ul>\n<p>Finally, do you really need a variable containing code? The most natural way to give a name to a code block is to define a function.</p>\n<h2>What's up with <code>read</code>?</h2>\n<p>Without <code>-r</code>, <code>read</code> allows continuation lines — this is a single logical line of input:</p>\n<pre><code>hello \\\nworld\n</code></pre>\n<p><code>read</code> splits the input line into fields delimited by characters in <code>$IFS</code> (without <code>-r</code>, backslash also escapes those). For example, if the input is a line containing three words, then <code>read first second third</code> sets <code>first</code> to the first word of input, <code>second</code> to the second word and <code>third</code> to the third word. If there are more words, the last variable contains everything that's left after setting the preceding ones. Leading and trailing whitespace are trimmed.</p>\n<p>Setting <code>IFS</code> to the empty string avoids any trimming. See <a href=\"https://unix.stackexchange.com/questions/18886/why-is-while-ifs-read-used-so-often-instead-of-ifs-while-read/18936#18936\">Why is `while IFS= read` used so often, instead of `IFS=; while read..`?</a> for a longer explanation.</p>\n<h2>What's wrong with <code>xargs</code>?</h2>\n<p>The input format of <code>xargs</code> is whitespace-separated strings which can optionally be single- or double-quoted. No standard tool outputs this format.</p>\n<p><code>xargs -L1</code> or <code>xargs -l</code> is not to split the input on <em>lines</em>, but to run one command per line of input (that line still split to make up the arguments, and continued on the next line if ending in blanks).</p>\n<p><code>xargs -I PLACEHOLDER</code> does use one line of input to substitute the <code>PLACEHOLDER</code> but quotes and backslashes are still processed and leading blanks trimmed.</p>\n<p>You can use <code>xargs -r0</code> where applicable (and where available: GNU (Linux, Cygwin), BusyBox, BSDs, OSX, but it isn't in POSIX). That's safe, because null bytes can't appear in most data, in particular in file names and external command arguments. To produce a null-separated list of file names, use <code>find … -print0</code> (or you can use <code>find … -exec …</code> as explained below).</p>\n<h3>How do I process files found by <code>find</code>?</h3>\n<pre><code>find … -exec some_command a_parameter another_parameter {} +\n</code></pre>\n<p><code>some_command</code> needs to be an external command, it can't be a shell function or alias. If you need to invoke a shell to process the files, call <code>sh</code> explicitly.</p>\n<pre><code>find … -exec sh -c '\n  for x do\n    … # process the file &quot;$x&quot;\n  done\n' find-sh {} +\n</code></pre>\n<h2>I have some other question</h2>\n<p>Browse the <a href=\"/questions/tagged/quoting\" class=\"post-tag\" title=\"show questions tagged &#39;quoting&#39;\" rel=\"tag\">quoting</a> tag on this site, or <a href=\"/questions/tagged/shell\" class=\"post-tag\" title=\"show questions tagged &#39;shell&#39;\" rel=\"tag\">shell</a> or <a href=\"/questions/tagged/shell-script\" class=\"post-tag\" title=\"show questions tagged &#39;shell-script&#39;\" rel=\"tag\">shell-script</a>. (Click on “learn more…” to see some general tips and a hand-selected list of common questions.) If you've searched and you can't find an answer, <a href=\"https://unix.stackexchange.com/questions/ask\">ask away</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters"
  },
  {
    "question_title": "How do I clear Bash&#39;s cache of paths to executables?",
    "question_body": "<p>When I execute a program without specifying the full path to the executable, and Bash must search the directories in <code>$PATH</code> to find the binary, it seems that Bash remembers the path in some sort of cache. For example, I installed a build of Subversion from source to <code>/usr/local</code>, then typed <code>svnsync help</code> at the Bash prompt. Bash located the binary <code>/usr/local/bin/svnsync</code> for \"svnsync\" and executed it. Then when I deleted the installation of Subversion in <code>/usr/local</code> and re-ran <code>svnsync help</code>, Bash responds:</p>\n\n<pre><code>bash: /usr/local/bin/svnsync: No such file or directory\n</code></pre>\n\n<p>But, when I start a new instance of Bash, it finds and executes <code>/usr/bin/svnsync</code>.</p>\n\n<p>How do I clear the cache of paths to executables?</p>\n",
    "answer": "<p><code>bash</code> does cache the full path to a command.  You can verify that the command you are trying to execute is hashed with the <code>type</code> command:</p>\n\n<pre><code>$ type svnsync\nsvnsync is hashed (/usr/local/bin/svnsync)\n</code></pre>\n\n<p>To clear the entire cache:</p>\n\n<pre><code>$ hash -r\n</code></pre>\n\n<p>Or just one entry:</p>\n\n<pre><code>$ hash -d svnsync\n</code></pre>\n\n<p>For additional information, consult <code>help hash</code> and <code>man bash</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/5609/how-do-i-clear-bashs-cache-of-paths-to-executables"
  },
  {
    "question_title": "How can I get the current working directory?",
    "question_body": "<p>I want to have a script that takes the current working directory to a variable. The section that needs the directory is like this <code>dir = pwd</code>. It just prints <code>pwd</code> how do I get the current working directory into a variable?</p>\n",
    "answer": "<p>There's no need to do that, it's already <em>in</em> a variable:</p>\n<pre><code>$ echo &quot;$PWD&quot;\n/home/terdon\n</code></pre>\n<p>The <code>PWD</code> variable is <a href=\"http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html\" rel=\"noreferrer\">defined by POSIX</a> and will work on all POSIX-compliant shells:</p>\n<blockquote>\n<p>PWD</p>\n</blockquote>\n<blockquote>\n<p>Set by the shell and by the cd utility. In the shell the value\nshall be initialized from the environment as follows. If a value for\nPWD is passed to the shell in the environment when it is executed, the\nvalue is an absolute pathname of the current working directory that is\nno longer than {PATH_MAX} bytes including the terminating null byte,\nand the value does not contain any components that are dot or dot-dot,\nthen the shell shall set PWD to the value from the environment.\nOtherwise, if a value for PWD is passed to the shell in the\nenvironment when it is executed, the value is an absolute pathname of\nthe current working directory, and the value does not contain any\ncomponents that are dot or dot-dot, then it is unspecified whether the\nshell sets PWD to the value from the environment or sets PWD to the\npathname that would be output by pwd -P. Otherwise, the sh utility\nsets PWD to the pathname that would be output by pwd -P. In cases\nwhere PWD is set to the value from the environment, the value can\ncontain components that refer to files of type symbolic link. In cases\nwhere PWD is set to the pathname that would be output by pwd -P, if\nthere is insufficient permission on the current working directory, or\non any parent of that directory, to determine what that pathname would\nbe, the value of PWD is unspecified. Assignments to this variable may\nbe ignored. If an application sets or unsets the value of PWD, the\nbehaviors of the cd and pwd utilities are unspecified.</p>\n</blockquote>\n<hr />\n<p>For the more general answer, the way to save the output of a command in a variable is to enclose the command in <code>$()</code> or <code>` `</code> (backticks):</p>\n<pre><code>var=$(command)\n</code></pre>\n<p>or</p>\n<pre><code>var=`command`\n</code></pre>\n<p>Of the two, the <code>$()</code> is preferred since it is easier to build complex commands like:</p>\n<pre><code>command0 &quot;$(command1 &quot;$(command2 &quot;$(command3)&quot;)&quot;)&quot;\n</code></pre>\n<p>Whose backtick equivalent would look like:</p>\n<pre><code>command0 &quot;`command1 \\&quot;\\`command2 \\\\\\&quot;\\\\\\`command3\\\\\\`\\\\\\&quot;\\`\\&quot;`&quot;\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/188182/how-can-i-get-the-current-working-directory"
  },
  {
    "question_title": "Why doesn&#39;t my Bash script recognize aliases?",
    "question_body": "<p>In my <code>~/.bashrc</code> file reside two definitions:</p>\n\n<ol>\n<li><code>commandA</code>, which is an alias to a longer path</li>\n<li><code>commandB</code>, which is an alias to a Bash script</li>\n</ol>\n\n<p>I want to process the same file with these two commands, so I wrote the following Bash script:</p>\n\n<hr>\n\n<pre><code>#!/bin/bash\n\nfor file in \"$@\"\n    do\n    commandA $file\n    commandB $file\ndone\n</code></pre>\n\n<p>Even after logging out of my session and logging back in, Bash prompts me with <code>command not found</code> errors for both commands when I run this script.</p>\n\n<p>What am I doing wrong?</p>\n",
    "answer": "<p>If you look into the bash manpage you find:</p>\n\n<blockquote>\n  <p>Aliases are not expanded when the\n  shell is not interactive, unless the \n  expand_aliases shell  option  is  set\n  using shopt (see the description of\n  shopt under SHELL BUILTIN COMMANDS\n  below).</p>\n</blockquote>\n\n<p>So put a</p>\n\n<pre><code>shopt -s expand_aliases\n</code></pre>\n\n<p>in your script.</p>\n\n<p>Make sure to source your aliases file after setting this in your script.</p>\n\n<pre><code>shopt -s expand_aliases\nsource ~/.bash_aliases\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/1496/why-doesnt-my-bash-script-recognize-aliases"
  },
  {
    "question_title": "What does &quot;rc&quot; in .bashrc stand for?",
    "question_body": "<p>Is it \"resource configuration\", by any chance?</p>\n",
    "answer": "<p>As is often the case with obscure terms, the <a href=\"http://www.catb.org/jargon/html/R/rc-file.html\">Jargon File</a> has an answer:</p>\n\n<blockquote>\n  <p>[Unix: from runcom files on the CTSS system 1962-63, via the startup script /etc/rc] Script file containing startup instructions for an application program (or an entire operating system), usually a text file containing commands of the sort that might have been invoked manually once the system was running but are to be executed automatically each time the system starts up.</p>\n</blockquote>\n\n<p>Thus, it would seem that the \"rc\" part stands for \"runcom\", which I believe can be expanded to \"run commands\".  In fact, this is exactly what the file contains, commands that bash should run.</p>\n",
    "url": "https://unix.stackexchange.com/questions/3467/what-does-rc-in-bashrc-stand-for"
  },
  {
    "question_title": "How to get the pid of the last executed command in shell script?",
    "question_body": "<p>I want to have a shell script like this:</p>\n\n<pre><code>my-app &amp;\necho $my-app-pid\n</code></pre>\n\n<p>But I do not know how the get the pid of the just executed command.</p>\n\n<p>I know I can just use the <code>jobs -p my-app</code> command to grep the pid. But if I want to execute the shell multiple times, this method will not work. Because the <em>jobspec</em> is ambiguous.</p>\n",
    "answer": "<p>The PID of the most recently executed background (asynchronous) command is in the <a href=\"https://www.gnu.org/software/bash/manual/html_node/Special-Parameters.html#index-_0021-1\" rel=\"noreferrer\"><code>$!</code></a> shell variable:</p>\n<pre><code>my-app &amp;\necho $!\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/30370/how-to-get-the-pid-of-the-last-executed-command-in-shell-script"
  },
  {
    "question_title": "How can I make &quot;Press any key to continue&quot;",
    "question_body": "<p>I'm making a script to install my theme, after it finished installing it will appear the changelog and there will be \"Press any key to continue\" so that after users read the changelog then press any key to continue</p>\n",
    "answer": "<p>You can use the <code>read</code> command. If you are using <code>bash</code>:</p>\n<pre><code>read -p &quot;Press enter to continue&quot;\n</code></pre>\n<p>In other shells, you can do:</p>\n<pre><code>printf &quot;%s &quot; &quot;Press enter to continue&quot;\nread ans\n</code></pre>\n<p>As mentioned in the comments above, this command does actually require the user to press <kbd>enter</kbd>; a solution that works with any key in <code>bash</code> would be:</p>\n<pre><code>read -n 1 -s -r -p &quot;Press any key to continue&quot;\n</code></pre>\n<h2>Explanation by <a href=\"https://unix.stackexchange.com/users/10938/rayne\">Rayne</a> and <a href=\"https://unix.stackexchange.com/users/37380/wchargin\">wchargin</a></h2>\n<p><code>-n</code> defines the required character count to stop reading</p>\n<p><code>-s</code> hides the user's input</p>\n<p><code>-r</code> causes the string to be interpreted &quot;raw&quot; (without considering backslash escapes)</p>\n",
    "url": "https://unix.stackexchange.com/questions/293940/how-can-i-make-press-any-key-to-continue"
  },
  {
    "question_title": "What is the &quot;eval&quot; command in bash?",
    "question_body": "<p>What can you do with the <code>eval</code> command? Why is it useful? Is it some kind of a built-in function in bash? There is no <code>man</code> page for it..</p>\n",
    "answer": "<p><code>eval</code> is part of POSIX. It's an interface which can be a shell built-in.</p>\n<p>It's described in the &quot;POSIX Programmer's Manual&quot;: <a href=\"http://www.unix.com/man-page/posix/1posix/eval/\" rel=\"nofollow noreferrer\">http://www.unix.com/man-page/posix/1posix/eval/</a></p>\n<pre><code>eval - construct command by concatenating arguments\n</code></pre>\n<p>It will take an argument and construct a command of it, which will then be executed by the shell. This is the example from the manpage:</p>\n<pre><code>foo=10 x=foo    # 1\ny='$'$x         # 2\necho $y         # 3\n$foo\neval y='$'$x    # 5\necho $y         # 6\n10\n</code></pre>\n<ol>\n<li>In the first line you define <code>$foo</code> with the value <code>'10'</code> and <code>$x</code> with the value <code>'foo'</code>.</li>\n<li>Now define <code>$y</code>, which consists of the string <code>'$foo'</code>. The dollar sign must be escaped\nwith <code>'$'</code>.</li>\n<li>To check the result, <code>echo $y</code>.</li>\n<li>The result of 1)-3) will be the string <code>'$foo'</code></li>\n<li>Now we repeat the assignment with <code>eval</code>. It will first evaluate <code>$x</code> to the string <code>'foo'</code>. Now we have the statement <code>y=$foo</code> which will get evaluated to <code>y=10</code>.</li>\n<li>The result of <code>echo $y</code> is now the value <code>'10'</code>.</li>\n</ol>\n<p>This is a common function in many languages:</p>\n<ul>\n<li><a href=\"http://perldoc.perl.org/functions/eval.html\" rel=\"nofollow noreferrer\">Perl</a></li>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/eval\" rel=\"nofollow noreferrer\">Javascript</a></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/23111/what-is-the-eval-command-in-bash"
  },
  {
    "question_title": "How to colorize output of git?",
    "question_body": "<p>Is there a way to color output for git (or any command)?</p>\n\n<p>Consider:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>baller@Laptop:~/rails/spunky-monkey$ git status\n# On branch new-message-types\n# Changes not staged for commit:\n#   (use \"git add &lt;file&gt;...\" to update what will be committed)\n#   (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n#\n#       modified:   app/models/message_type.rb\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nballer@Laptop:~/rails/spunky-monkey$ git add app/models\n</code></pre>\n\n<p>And</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>baller@Laptop:~/rails/spunky-monkey$ git status\n# On branch new-message-types\n# Changes to be committed:\n#   (use \"git reset HEAD &lt;file&gt;...\" to unstage)\n#\n#       modified:   app/models/message_type.rb\n#\n</code></pre>\n\n<p>The output looks the same, but the information is totally different: the file has gone from unstaged to staged for commit.</p>\n\n<p>Is there a way to colorize the output? For example, files that are unstaged are red, staged are green? </p>\n\n<p>Or even <code>Changes not staged for commit:</code> to red and <code># Changes to be committed:</code> to green?</p>\n\n<p>Working in Ubuntu.</p>\n\n<p>EDIT: Googling found this answer which works great: <code>git config --global --add color.ui true</code>.</p>\n\n<p>However, is there any more general solution for adding color to a command output?</p>\n",
    "answer": "<p>You can create a section <code>[color]</code> in your <code>~/.gitconfig</code> with e.g. the following content</p>\n<pre><code>[color]\n  diff = auto\n  status = auto\n  branch = auto\n  interactive = auto\n  ui = true\n  pager = true\n</code></pre>\n<p>You can also fine control what you want to have coloured in what way, e.g.</p>\n<pre><code>[color &quot;status&quot;]\n  added = green\n  changed = red bold\n  untracked = magenta bold\n\n[color &quot;branch&quot;]\n  remote = yellow\n</code></pre>\n<p>I hope this gets you started. And of course, you need a terminal which supports colour.</p>\n<p>Also see <a href=\"https://unix.stackexchange.com/a/44297/64235\">this answer</a> for a way to add colorization directly from the command line.</p>\n",
    "url": "https://unix.stackexchange.com/questions/44266/how-to-colorize-output-of-git"
  },
  {
    "question_title": "Colorizing your terminal and shell environment?",
    "question_body": "<p>I spend most of my time working in Unix environments and using terminal emulators. I try to use color on the command line, because color makes the output more useful and intuitive.</p>\n\n<p>What options exist to add color to my terminal environment? What tricks do you use? What pitfalls have you encountered?</p>\n\n<p>Unfortunately, support for color varies depending on terminal type, OS, TERM setting, utility, buggy implementations, etc.</p>\n\n<p>Here are some tips from my setup, after a lot of experimentation:</p>\n\n<ol>\n<li>I tend to set <code>TERM=xterm-color</code>, which is supported on most hosts (but not all).</li>\n<li>I work on a number of different hosts, different OS versions, etc. I use everything from macOS X, Ubuntu Linux, RHEL/CentOS/Scientific Linux and FreeBSD. I'm trying to keep things simple and generic, if possible.</li>\n<li>I do a bunch of work using GNU <code>screen</code>, which adds another layer of fun.</li>\n<li>Many OSs set things like <code>dircolors</code> and by default, and I don't want to modify this on a hundred different hosts. So I try to stick with the defaults. Instead, I tweak my terminal's color configuration.</li>\n<li><p>Use color for some <a href=\"http://www.debian.org/doc/manuals/reference/ch09.en.html#_colorized_commands\" rel=\"noreferrer\">Unix commands</a> (<code>ls</code>, <code>grep</code>, <code>less</code>, <code>vim</code>) and the <a href=\"http://www.cyberciti.biz/faq/bash-shell-change-the-color-of-my-shell-prompt-under-linux-or-unix/\" rel=\"noreferrer\">Bash prompt</a>. These commands seem to use the standard \"<a href=\"http://en.wikipedia.org/wiki/ANSI_escape_code\" rel=\"noreferrer\">ANSI escape sequences</a>\". For example:</p>\n\n<pre><code>alias less='less --RAW-CONTROL-CHARS'\nexport LS_OPTS='--color=auto'\nalias ls='ls ${LS_OPTS}'\n</code></pre></li>\n</ol>\n\n<p>I'll post my <code>.bashrc</code> and answer my own question Jeopardy Style.</p>\n",
    "answer": "<p>Here are a couple of things you can do:</p>\n\n<p><strong>Editors + Code</strong><br>\nA lot of editors have syntax highlighting support. <code>vim</code> and <code>emacs</code> have it on by default. You can also <a href=\"https://askubuntu.com/questions/90013/how-do-i-enable-syntax-highlighting-in-nano\">enable it under <code>nano</code></a>.</p>\n\n<p>You can also syntax highlight code on the terminal by using <a href=\"http://pygments.org/faq/#how-can-i-use-pygments\" rel=\"noreferrer\">Pygments</a> as a command-line tool.</p>\n\n<p><strong>grep</strong><br>\n<code>grep --color=auto</code> highlights all matches. You can also use <code>export GREP_OPTIONS='--color=auto'</code> to make it persistent without an alias. If you use <code>--color=always</code>, it'll <a href=\"http://linuxcommando.blogspot.com/2007/10/grep-with-color-output.html\" rel=\"noreferrer\">use colour even when piping</a>, which confuses things.</p>\n\n<p><strong>ls</strong></p>\n\n<p><code>ls --color=always</code></p>\n\n<p>Colors specified by:</p>\n\n<pre><code>export LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33'\n</code></pre>\n\n<p>(hint: <code>dircolors</code> can be helpful)</p>\n\n<p><strong>PS1</strong><br>\nYou can set your PS1 (shell prompt) to use colours. For example:</p>\n\n<pre><code>PS1='\\e[33;1m\\u@\\h: \\e[31m\\W\\e[0m\\$ '\n</code></pre>\n\n<p>Will produce a PS1 like:</p>\n\n<p>[yellow]lucas@ubuntu: [red]~[normal]$ </p>\n\n<p>You can get really creative with this. As an idea:</p>\n\n<pre><code>PS1='\\e[s\\e[0;0H\\e[1;33m\\h    \\t\\n\\e[1;32mThis is my computer\\e[u[\\u@\\h:  \\w]\\$ '\n</code></pre>\n\n<p>Puts a bar at the top of your terminal with some random info. (For best results, also use <code>alias clear=\"echo -e '\\e[2J\\n\\n'\"</code>.)</p>\n\n<p><strong>Getting Rid of Escape Sequences</strong></p>\n\n<p>If something is stuck outputting colour when you don't want it to, I use this <code>sed</code> line to strip the escape sequences:</p>\n\n<pre><code>sed \"s/\\[^[[0-9;]*[a-zA-Z]//gi\"\n</code></pre>\n\n<p>If you want a more authentic experience, you can also get rid of lines starting with <code>\\e[8m</code>, which instructs the terminal to hide the text. (Not widely supported.)</p>\n\n<pre><code>sed \"s/^\\[^[8m.*$//gi\"\n</code></pre>\n\n<p>Also note that those ^[s should be actual, literal ^[s. You can type them by pressing ^V^[ in bash, that is <kbd>Ctrl</kbd> + <kbd>V</kbd>, <kbd>Ctrl</kbd> + <kbd>[</kbd>. </p>\n",
    "url": "https://unix.stackexchange.com/questions/148/colorizing-your-terminal-and-shell-environment"
  },
  {
    "question_title": "How can I test if a variable is empty or contains only spaces?",
    "question_body": "<p>The following bash syntax verifies if <code>param</code> isn't empty:</p>\n\n<pre><code> [[ !  -z  $param  ]]\n</code></pre>\n\n<p>For example:</p>\n\n<pre><code>param=\"\"\n[[ !  -z  $param  ]] &amp;&amp; echo \"I am not zero\"\n</code></pre>\n\n<p>No output and its fine.</p>\n\n<p>But when <code>param</code> is empty except for one (or more) space characters, then the case is different:</p>\n\n<pre><code>param=\" \" # one space\n[[ !  -z  $param  ]] &amp;&amp; echo \"I am not zero\"\n</code></pre>\n\n<p>\"I am not zero\" is output.</p>\n\n<p>How can I change the test to consider variables that contain only space characters as empty?</p>\n",
    "answer": "<p>First, note that the <a href=\"https://www.gnu.org/software/bash/manual/bashref.html#Bash-Conditional-Expressions\"><code>-z</code> test</a> is explicitly for:</p>\n\n<blockquote>\n  <p>the length of string is zero</p>\n</blockquote>\n\n<p>That is, a string containing only spaces should <strong>not</strong> be true under <code>-z</code>, because it has a non-zero length.</p>\n\n<p>What you want is to remove the spaces from the variable using the <a href=\"https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion\">pattern replacement parameter expansion</a>:</p>\n\n<pre><code>[[ -z \"${param// }\" ]]\n</code></pre>\n\n<p>This expands the <code>param</code> variable and replaces all matches of the pattern <code></code> (a single space) with nothing, so a string that has only spaces in it will be expanded to an empty string.</p>\n\n<hr>\n\n<p>The <a href=\"https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion\">nitty-gritty of how that works</a> is that <code>${var/pattern/string}</code> replaces the first longest match of <code>pattern</code> with <code>string</code>. When <code>pattern</code> starts with <code>/</code> (as above) then it replaces <em>all</em> the matches. Because the replacement is empty, we can omit the final <code>/</code> and the <code>string</code> value:</p>\n\n<blockquote>\n  <p>${parameter/pattern/string}</p>\n  \n  <p>The <em>pattern</em> is expanded to produce a pattern just as in filename expansion. <em>Parameter</em> is expanded and the longest match of <em>pattern</em> against its value is replaced with <em>string</em>. If <em>pattern</em> begins with ‘/’, all matches of <em>pattern</em> are replaced with <em>string</em>. Normally only the first match is replaced. ... If <em>string</em> is null, matches of <em>pattern</em> are deleted and the / following <em>pattern</em> may be omitted.</p>\n</blockquote>\n\n<p>After all that, we end up with <code>${param// }</code> to delete all spaces.</p>\n\n<p><sub>Note that though present in <code>ksh</code> (where it originated), <code>zsh</code> and <code>bash</code>, that syntax is not POSIX and should not be used in <code>sh</code> scripts.</sub></p>\n",
    "url": "https://unix.stackexchange.com/questions/146942/how-can-i-test-if-a-variable-is-empty-or-contains-only-spaces"
  },
  {
    "question_title": "What is the meaning of IFS=$&#39;\\n&#39; in bash scripting?",
    "question_body": "<p>At the beginning of a bash shell script is the following line: </p>\n\n<pre><code>IFS=$'\\n'\n</code></pre>\n\n<p>What is the meaning behind this collection of symbols? </p>\n",
    "answer": "<p><code>IFS</code> stands for \"internal field separator\". It is used by the shell to determine how to do word splitting, i. e. how to recognize word boundaries.</p>\n\n<p>Try this in a shell like bash (other shells may handle this differently, for example zsh):</p>\n\n<pre><code>mystring=\"foo:bar baz rab\"\nfor word in $mystring; do\n  echo \"Word: $word\"\ndone\n</code></pre>\n\n<p>The default value for <code>IFS</code> consists of whitespace characters (to be precise: space, tab and newline). Each character can be a word boundary. So, with the default value of <code>IFS</code>, the loop above will print:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>Word: foo:bar\nWord: baz\nWord: rab\n</code></pre>\n\n<p>In other words, the shell thinks that whitespace is a word boundary.</p>\n\n<p>Now, try setting <code>IFS=:</code> before executing the loop. This time, the result is:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>Word: foo\nWord: bar baz rab\n</code></pre>\n\n<p>Now, the shell splits <code>mystring</code> into words as well -- but now, it only treats a colon as the word boundary.</p>\n\n<p>The first character of <code>IFS</code> is special: It is used to delimit words in the output when using the special <code>$*</code> variable (example taken from the <a href=\"http://tldp.org/LDP/abs/html/internalvariables.html\" rel=\"noreferrer\">Advanced Bash Scripting Guide</a>, where you can also find more information on special variables like that one):</p>\n\n<pre><code>$ bash -c 'set w x y z; IFS=\":-;\"; echo \"$*\"'\nw:x:y:z\n</code></pre>\n\n<p>Compare to:</p>\n\n<pre><code>$ bash -c 'set w x y z; IFS=\"-:;\"; echo \"$*\"'\nw-x-y-z\n</code></pre>\n\n<p>Note that in both examples, the shell will still treat all of the characters <code>:</code>, <code>-</code> and <code>;</code> as word boundaries. The only thing that changes is the behaviour of <code>$*</code>.</p>\n\n<p>Another important thing to know is how so-called \"IFS whitespace\" <a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_05\" rel=\"noreferrer\">is treated</a>. Basically, as soon as <code>IFS</code> includes whitespace characters, leading and trailing whitespace is stripped from the string to be split before processing it and a <strong>sequence</strong> of consecutive whitespace characters delimits fields as well. However, this only applies to those whitespace characters which are actually present in <code>IFS</code>.</p>\n\n<p>For example, let's look at the string <code>\"a:b:: c  d \"</code> (trailing space and two space characters between <code>c</code> and <code>d</code>).</p>\n\n<ol>\n<li>With <code>IFS=:</code> it would be split into four fields: <code>\"a\"</code>, <code>\"b\"</code>, <code>\"\"</code> (empty string) and <code>\" c  d \"</code> (again, two spaces between <code>c</code> and <code>d</code>). Note the leading and trailing whitespace in the last field.</li>\n<li>With <code>IFS=' :'</code>, it would be split into five fields: <code>\"a\"</code>, <code>\"b\"</code>, <code>\"\"</code> (empty string), <code>\"c\"</code> and <code>\"d\"</code>. No leading and trailing whitespace anywhere.</li>\n</ol>\n\n<p>Note how multiple, consecutive whitespace characters delimit two fields in the second example, while multiple, consecutive colons don't (since they are not whitespace characters).</p>\n\n<p>As for <code>IFS=$'\\n'</code>, that is a <code>ksh93</code> syntax also supported by <code>bash</code>, <code>zsh</code>, <code>mksh</code> and FreeBSD <code>sh</code> (with variations between all shells). Quoting the bash manpage:</p>\n\n<blockquote>\n  <p>Words of the form $'string' are treated specially.  The word expands to \"string\", with backslash-escaped characters replaced as specified by the ANSI C standard.</p>\n</blockquote>\n\n<p><code>\\n</code> is the escape sequence for a newline, so <code>IFS</code> ends up being set to a single newline character.</p>\n",
    "url": "https://unix.stackexchange.com/questions/184863/what-is-the-meaning-of-ifs-n-in-bash-scripting"
  },
  {
    "question_title": "How to determine where an environment variable came from?",
    "question_body": "<p>I have a Linux instance that I set up some time ago.  When I fire it up and log in as <code>root</code> there are some environment variables that I set up but I can't remember or find where they came from.  </p>\n\n<ul>\n<li>I've checked <code>~/.bash_profile</code>, <code>/etc/.bash_rc</code>, and all the startup\nscripts. </li>\n<li>I've run <code>find</code> and <code>grep</code> to no avail.</li>\n</ul>\n\n<p>I feel like I must be forgetting to look in some place obvious. Is there a trick for figuring this out?</p>\n",
    "answer": "<p>If <code>zsh</code> is your login shell:</p>\n<pre><code>zsh -xl\n</code></pre>\n<p>With <code>bash</code>:</p>\n<pre><code>PS4='+$BASH_SOURCE&gt; ' BASH_XTRACEFD=7 bash -xl 7&gt;&amp;2\n</code></pre>\n<p>That will simulate a login shell and show everything that is done (except in areas where stderr is redirected with <code>zsh</code>) along with the name of the file currently being interpreted.</p>\n<p>So all you need to do is look for the name of your environment variable in that output. (you can use the <code>script</code> command to help you store the whole shell session output, or for the <code>bash</code> approach, use <code>7&gt; file.log</code> instead of <code>7&gt;&amp;2</code> to store the <code>xtrace</code> output to <code>file.log</code> instead of on the terminal).</p>\n<p>If your variable is not in there, then probably the shell inherited it on startup, so it was set before like in PAM configuration, in <code>~/.ssh/environment</code>, or things read upon your X11 session startup (<code>~/.xinitrc</code>, <code>~/.xsession</code>) or set upon the service definition that started your login manager or even earlier in some boot script. Then a <code>find /etc -type f -exec grep -Fw THE_VAR {} +</code> may help.</p>\n",
    "url": "https://unix.stackexchange.com/questions/813/how-to-determine-where-an-environment-variable-came-from"
  },
  {
    "question_title": "How to add a newline to the end of a file?",
    "question_body": "<p>Using version control systems I get annoyed at the noise when the diff says <code>No newline at end of file</code>.</p>\n\n<p>So I was wondering: How to add a newline at the end of a file to get rid of those messages?</p>\n",
    "answer": "<p><a href=\"https://github.com/l0b0/tilde/blob/e7ccc9a6a3aba2b0216c8e5141554e8a729389ec/.bash_aliases#L474-L494\" rel=\"noreferrer\">Here you go</a>:</p>\n<pre><code>sed -i -e '$a\\' file\n</code></pre>\n<p>And alternatively for OS X <code>sed</code>:</p>\n<pre><code>sed -i '' -e '$a\\' file\n</code></pre>\n<p>This adds <code>\\n</code> at the end of the file <em>only</em> if it doesn’t already end with a newline. So if you run it twice, it will not add another newline:</p>\n<pre><code>$ cd &quot;$(mktemp -d)&quot;\n$ printf foo &gt; test.txt\n$ sed -e '$a\\' test.txt &gt; test-with-eol.txt\n$ diff test*\n1c1\n&lt; foo\n\\ No newline at end of file\n---\n&gt; foo\n$ echo $?\n1\n$ sed -e '$a\\' test-with-eol.txt &gt; test-still-with-one-eol.txt\n$ diff test-with-eol.txt test-still-with-one-eol.txt\n$ echo $?\n0\n</code></pre>\n<p>How it works:</p>\n<ul>\n<li><code>$</code> denotes the end of file</li>\n<li><code>a\\</code> appends the following text (which is nothing, in this case) on a new line</li>\n</ul>\n<p>In other words, if the last line contains a character that is not newline, append a newline.</p>\n",
    "url": "https://unix.stackexchange.com/questions/31947/how-to-add-a-newline-to-the-end-of-a-file"
  },
  {
    "question_title": "Execute a command once per line of piped input?",
    "question_body": "<p>I want to run a java command once for every match of <code>ls | grep pattern -</code>. In this case, I think I could do <code>find pattern -exec java MyProg '{}' \\;</code> but I'm curious about the general case - is there an easy way to say \"run a command once for every line of standard input\"? (In fish or bash.)</p>\n",
    "answer": "<p><a href=\"https://unix.stackexchange.com/a/7560\">The accepted answer</a> has the right idea, but the key is to pass <code>xargs</code> the <code>-n1</code> switch, which means <a href=\"https://www.gnu.org/software/findutils/manual/html_node/find_html/xargs-options.html#xargs-options\" rel=\"noreferrer\">&quot;Use at most 1 argument per command line&quot;</a>:</p>\n<pre><code>cat file... | xargs -n1 command\n</code></pre>\n<p>Or, for a single input file you can avoid the pipe from <code>cat</code> entirely and just go with:</p>\n<pre><code>&lt;file xargs -n1 command\n</code></pre>\n<p><strong>Updated 2020-08-05:</strong></p>\n<p>I would also like to respond to the advice found in user Jander's comment, which was heavily upvoted, despite containing some amount of misinformation as I will now explain.</p>\n<p>Do not be so hasty to recommend the <code>-L</code> option of <code>xargs</code>, without mentioning the trouble that its (so called) <em>trailing blank(s) feature</em> can lead to. In my opinion this switch causes more harm than good and is certainly a stretch to use to mean, for the case of <code>-L 1</code>, <em>act on one non-empty line at a time</em>. To be fair, the man page for <code>xargs</code> does spell out the <em>features</em> (i.e., issues) that come for the ride with the <code>-L</code> switch.</p>\n<p>Since Jander made no mention of the issues when mentioning <code>-L</code> to perhaps a hasty unsuspecting StackOverflow audience seeking quick tips and not having the time for such tedious things as reading man pages rather than accepting comments and answers as gospel, I will now present my case for why <code>-L</code> is a very bad suggestion without a careful understanding of all of the baggage that it brings along for the ride.</p>\n<p>To illustrate my disdain for <code>-L</code>, let's consider a simple input file that consists of the following text someone carelessly entered (perhaps a high-school summer intern that created this data file as part of his/her training, as evidenced by its <em>Windowish</em> filename. As luck (karma?) would have it, you have been selected by management to be its new custodian):</p>\n<p><code>testdata.txt</code></p>\n<pre><code>1\n2␠\n3\n</code></pre>\n<p>Because of the fact that the line that contains the digit <code>2</code> has a space character (shown as a Unicode <code>SYMBOL FOR SPACE</code> glyph after the digit <code>2</code> in the preceding code, in case your browser's font does not have a visual representation for this character), a command that uses <code>xargs -L1</code>, such as:</p>\n<pre><code>&lt;testdata.txt xargs -L1 echo\n</code></pre>\n<p>..., would produce the following (perhaps surprising) output:</p>\n<pre><code>1\n2 3\n</code></pre>\n<p>This is caused by the fact that the <code>-L</code> switch instructs <code>xargs</code> to <em>append subsequent lines to those that end with blanks</em>, a behavior that may only effect the resulting output in those oddball moments where lines are not properly trimmed of trailing blanks - a time bomb bug waiting for the right input file to present itself.</p>\n<p>On the other hand, the same command using the <code>-n 1</code> switch of <code>xargs</code>, instead of <code>-L 1</code> would produce a far more acceptable output of:</p>\n<pre><code>1\n2␠\n3\n</code></pre>\n<p>And that's not even the worst of it! The <code>-L</code> switch unlike <code>-n</code> forces the &quot;dreaded&quot; <code>-x</code> option of <code>xargs</code> to go into effect. This causes termination of the <code>xargs</code> process if a command line is encountered that it deems too long for the environment on which it is run.</p>\n<p>An input file consisting of many lines with trailing blanks in succession could, as instructed by the <code>-L</code> switch and its use of the chemical agent known as Agent <code>-x</code> into the mix, potentially cause <code>xargs</code> to terminate midstream if the concatenation of all of these into one <em>superline</em> exceeds <code>xargs</code>' definition of <em>line is too long</em> for a command line. If things are beginning to look murky, consider that <em>line is too long</em> is a size determined by <code>xargs</code> based on the max length specified for the platform on which it is run and further offset by a seemingly <em>arbitrary constant</em> as explained in more detail in the man page. Remember those pesky indefinite integrals from Calculus and their <em>arbitrary constants</em> and losing a point on a quiz or test, because you forgot to write <code>+ C</code> after your solution for an indefinite integral? Well, that phrase is back with a vengeance, to bite you on the rear once again, if add <code>-L</code> to your handy <code>xargs</code> toolkit.</p>\n<p>A <code>-n</code> value of <code>1</code>, on the other hand, would just chop up those long lines into (hopefully) small bite sized one-line chunks and execute the command supplied to <code>xargs</code> for each of them, one at a time, without any consideration given to whether they end with blanks or not. No more long lines and no more of <code>xargs</code> stabbing you in the back by terminating abruptly - Et tu, <strike>Brute</strike> <code>-x</code>?</p>\n<p><em>An optional segue regarding phrasing in the xargs man page</em></p>\n<p>I don't know why the ambiguous and non-standard word <em>blanks</em> was used throughout the <code>xargs</code> man page, instead of far better defined and less ambiguous options such as:</p>\n<ul>\n<li>space(s), if <em>blanks</em> means one or more <em>ASCII space</em> characters</li>\n<li>whitespace(s) other than new-lines (if that's what <em>blanks</em> implies)</li>\n<li>one or more non-printable characters from the set: <em>{space, horizontal tab}</em> (if <em>blanks</em> was used as a synonym for this gruesome twosome)</li>\n</ul>\n<p><strong>Updated 2021-06-15:</strong></p>\n<p>User @BjornW asked how <code>xargs</code> could be used to run a command once per <em>line</em> of input and not just word of input. (See, I do read the comments, and I'll just blame the seven months it took to respond on Covid :P ).</p>\n<p>In the spirit of the original question, as asked, and to make my answer applicable to a greater number of use cases, I would like to address this particular scenario in detail.</p>\n<p>Consider the following input file. It is chock-full of various edge cases one may actually encounter in the <em>Real World</em>™ (e.g., leading/trailing spaces, lines consisting only of spaces, empty lines, lines beginning with a hyphen [which should not get interpreted as the introduction of a switch], etc.):</p>\n<p><code>lines.txt</code></p>\n<pre><code>a1 a22 a333 a4444\nb4444 b333 b22 b1\n␠␠c d e f g\n␣\nhhh\nii jj kk␠\n␣\n␠␠␠\n-L and -x are the gruesome twosome\n␣\n␣\n␣\n</code></pre>\n<p>In the preceding input file, the Unicode character <code>OPEN BOX</code> U+2423 was used to mark empty lines and the Unicode <code>SYMBOL FOR SPACE</code> was used for leading and trailing spaces, in order to make them more prominent.</p>\n<p>Let's say we want to run a command on each line of input, taken as a whole, and passed to our command as a single argument, regardless of content (including no content). We would go about this using <code>xargs</code>, as follows (Note: <code>printf</code> will be our sample command and the <code>%q</code> format specifier will be used in order to enclose the supplied argument in apostrophes for clarity, when spaces are present or the argument is an empty string - all in, only our <code>hhh</code> input line was left &quot;unscathed,&quot; by <code>%q</code>, as you will see in the output, presented shortly. Had any non-printable characters been present, they would have also gotten escaped by <code>%q</code> using the POSIX <code>$''</code> quoting syntax]):</p>\n<pre><code>&lt;lines.txt xargs -n1 -d'\\n' printf -- 'Input line: %q\\n'\n</code></pre>\n<p>The output is as follows:</p>\n<pre><code>Input line: 'a1 a22 a333 a4444'\nInput line: 'b4444 b333 b22 b1'\nInput line: '   c d e f g'\nInput line: ''\nInput line: hhh\nInput line: 'ii jj kk '\nInput line: ''\nInput line: '   '\nInput line: '-L and -x are the gruesome twosome'\nInput line: ''\nInput line: ''\nInput line: ''\n</code></pre>\n<p>So, there you have it. Using the <code>-d</code> switch, we can specify the delimiter that <code>xargs</code> should look for in our input file to indicate where an argument ends and the next one begins. By setting it to <code>'\\n'</code> which <code>xargs</code> itself is smart enough to interpret as a <em>C-style character escape</em>, as stated in the description of the <code>-d</code> switch on its <code>man</code> page, we can use <code>xargs</code> to forward entire lines of input to our command of choice as arguments, with minimal effort on our part.</p>\n<p>I would like to also mention that <code>xargs</code> can be used to concatenate multiple lines of input (with a caveat I will detail at the end of this paragraph), for the rare cases when such behavior is desired, and forward them as a single argument to our command. This can be accomplished by setting the number passed to the <code>-n</code> switch to <code>xargs</code> in the above invocation command, to a value that is indicative of the number of lines of input that should get merged into a single argument with their <code>\\n</code> line endings removed as part of the process. Unfortunately, this new-line stripping behavior makes the aforementioned <code>xargs</code> approach unsuitable for many use cases, because information that is indicative of where one line ends and the next begins, gets lost in the process.</p>\n",
    "url": "https://unix.stackexchange.com/questions/7558/execute-a-command-once-per-line-of-piped-input"
  },
  {
    "question_title": "Is there a one-liner that allows me to create a directory and move into it at the same time?",
    "question_body": "<p>I find myself repeating a lot of:</p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>mkdir longtitleproject\ncd longtitleproject\n</code></pre>\n\n<p>Is there a way of doing it in one line without repeating the directory name? I'm on bash here.</p>\n",
    "answer": "<p>This is the one-liner that you need. No other config needed:</p>\n\n<pre><code>mkdir longtitleproject &amp;&amp; cd $_\n</code></pre>\n\n<p>The <code>$_</code> variable, in bash, is the last argument given to the previous command. In this case, the name of the directory you just created. As explained in <code>man bash</code>:</p>\n\n<pre><code>_         At  shell  startup,  set to the absolute pathname used to invoke\n          the shell or shell script being executed as passed in the  envi‐\n          ronment  or  argument  list.   Subsequently, expands to the last\n          argument to the previous command, after expansion.  Also set  to\n          the  full  pathname  used  to  invoke  each command executed and\n          placed in the environment exported to that command.  When check‐\n          ing  mail,  this  parameter holds the name of the mail file cur‐\n          rently being checked.\"$_\" is the last argument of the previous command.\n</code></pre>\n\n<p>Use <code>cd $_</code> to retrieve the last argument of the previous command instead of <code>cd !$</code> because <code>cd !$</code> gives the last argument of previous command <strong>in the shell history</strong>:</p>\n\n<pre><code>cd ~/\nmkdir folder &amp;&amp; cd !$\n</code></pre>\n\n<p>you end up home (or ~/ )</p>\n\n<pre><code>cd ~/\nmkdir newfolder &amp;&amp; cd $_\n</code></pre>\n\n<p>you end up in newfolder under home !! ( or ~/newfolder )</p>\n",
    "url": "https://unix.stackexchange.com/questions/9123/is-there-a-one-liner-that-allows-me-to-create-a-directory-and-move-into-it-at-th"
  },
  {
    "question_title": "Passing named arguments to shell scripts",
    "question_body": "<p>Is there any easy way to pass (receive) named parameters to a shell script?</p>\n\n<p>For example, </p>\n\n<pre><code>my_script -p_out '/some/path' -arg_1 '5'\n</code></pre>\n\n<p>And inside <code>my_script.sh</code> receive them as:</p>\n\n<pre><code># I believe this notation does not work, but is there anything close to it?\np_out=$ARGUMENTS['p_out']\narg1=$ARGUMENTS['arg_1']\n\nprintf \"The Argument p_out is %s\" \"$p_out\"\nprintf \"The Argument arg_1 is %s\" \"$arg1\"\n</code></pre>\n\n<p>Is this possible in Bash or Zsh?</p>\n",
    "answer": "<p>If you don't mind being limited to single-letter argument names i.e. <code>my_script -p '/some/path' -a5</code>, then in bash you could use the built-in <code>getopts</code>, e.g.</p>\n<pre><code>#!/bin/bash\n\nwhile getopts &quot;:a:p:&quot; opt; do\n  case $opt in\n    a) arg_1=&quot;$OPTARG&quot;\n    ;;\n    p) p_out=&quot;$OPTARG&quot;\n    ;;\n    \\?) echo &quot;Invalid option -$OPTARG&quot; &gt;&amp;2\n    exit 1\n    ;;\n  esac\n\n  case $OPTARG in\n    -*) echo &quot;Option $opt needs a valid argument&quot;\n    exit 1\n    ;;\n  esac\ndone\n\nprintf &quot;Argument p_out is %s\\n&quot; &quot;$p_out&quot;\nprintf &quot;Argument arg_1 is %s\\n&quot; &quot;$arg_1&quot;\n</code></pre>\n<p>Then you can do</p>\n<pre><code>$ ./my_script -p '/some/path' -a5\nArgument p_out is /some/path\nArgument arg_1 is 5\n</code></pre>\n<p>There is a helpful <a href=\"https://web.archive.org/web/20230324055145/https://wiki.bash-hackers.org/doku.php?id=howto:getopts_tutorial&amp;rev=1521590837\" rel=\"noreferrer\">Small getopts tutorial</a> or you can type <code>help getopts</code> at the shell prompt.</p>\n<p>Edit: The second <code>case</code> statement in <code>while</code> loop triggers if the <code>-p</code> option has no arguments and is followed by another option, e.g. <code>my_script -p -a5</code>, and <code>exit</code>s the program.</p>\n",
    "url": "https://unix.stackexchange.com/questions/129391/passing-named-arguments-to-shell-scripts"
  },
  {
    "question_title": "What is the purpose of .bashrc and how does it work?",
    "question_body": "<p>I found the <code>.bashrc</code> file and I want to know the purpose/function of it. Also how and when is it used?</p>\n",
    "answer": "<p><code>.bashrc</code> is a Bash <a href=\"http://en.wikipedia.org/wiki/Shell_script\" rel=\"noreferrer\">shell script</a> that Bash runs whenever it is started interactively. It initializes an interactive shell session. You can put any command in that file that you could type at the command prompt.</p>\n<p>You put commands here to set up the shell for use in your particular environment, or to customize things to your preferences. A common thing to put in <code>.bashrc</code> are <a href=\"http://en.wikipedia.org/wiki/Alias_(command)\" rel=\"noreferrer\">aliases</a> that you want to always be available.</p>\n<p><code>.bashrc</code> runs on <em>every</em> interactive shell launch. If you say:</p>\n<pre><code>$ bash ; bash ; bash\n</code></pre>\n<p>and then hit <kbd>Ctrl-D</kbd> three times, <code>.bashrc</code> will run three times.  But if you say this instead:</p>\n<pre><code>$ bash -c exit ; bash -c exit ; bash -c exit\n</code></pre>\n<p>then <code>.bashrc</code> won't run at all, since <code>-c</code> makes the Bash call non-interactive. The same is true when you run a shell script from a file.</p>\n<p>Contrast <code>.bash_profile</code> and <code>.profile</code> which are only run at the start of a new login shell. (<code>bash -l</code>) You choose whether a command goes in <code>.bashrc</code> vs <code>.bash_profile</code> depending on whether you want it to run once or for every interactive shell start.</p>\n<p>As a counterexample to aliases, which I prefer to put in <code>.bashrc</code>, you want to do <code>PATH</code> adjustments in <code>.bash_profile</code> instead, since these changes are typically not <a href=\"http://en.wikipedia.org/wiki/Idempotent\" rel=\"noreferrer\">idempotent</a>:</p>\n<pre><code>export PATH=&quot;$PATH:/some/addition&quot;\n</code></pre>\n<p>If you put that in <code>.bashrc</code> instead, every time you launched an interactive sub-shell, <code>:/some/addition</code> would get tacked onto the end of the <code>PATH</code> again, creating extra work for the shell when you mistype a command.</p>\n<p>You get a new interactive Bash shell whenever you <a href=\"http://web.physics.ucsb.edu/%7Epcs/apps/editors/vi/vi_unix.html\" rel=\"noreferrer\">shell out of <code>vi</code></a> with <code>:sh</code>, for example.</p>\n",
    "url": "https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work"
  },
  {
    "question_title": "Security implications of forgetting to quote a variable in bash/POSIX shells",
    "question_body": "<p>If you've been following unix.stackexchange.com for a while, you\nshould hopefully know by now that leaving a variable\nunquoted in list context (as in <code>echo $var</code>) in Bourne/POSIX\nshells (zsh being the exception) has a very special meaning and\nshouldn't be done unless you have a very good reason to.</p>\n<p>It's discussed at length in a number of Q&amp;A here (Examples: <a href=\"https://unix.stackexchange.com/questions/131766\">Why does my shell script choke on whitespace or other special characters?</a>, <a href=\"https://unix.stackexchange.com/q/68694\">When is double-quoting necessary?</a>, <a href=\"https://unix.stackexchange.com/q/108963\">Expansion of a shell variable and effect of glob and split on it</a>, <a href=\"https://unix.stackexchange.com/q/78914\">Quoted vs unquoted string expansion</a><b></b>)</p>\n<p>That has been the case since the initial release of the Bourne\nshell in the late 70s and hasn't been changed by the Korn shell\n(one of <a href=\"https://news.slashdot.org/story/01/02/06/2030205/david-korn-tells-all\" rel=\"noreferrer\">David Korn's biggest\nregrets (question #7)</a>) or <code>bash</code> which mostly\ncopied the Korn shell, and that's how that has been specified by POSIX/Unix.</p>\n<p>Now, we're still seeing a number of answers here and even\noccasionally publicly released shell code where\nvariables are not quoted. You'd have thought people would have\nlearnt by now.</p>\n<p>In my experience, there are mainly 3 types of people who omit to\nquote their variables:</p>\n<ul>\n<li><p>beginners. Those can be excused as admittedly it's a\ncompletely unintuitive syntax. And it's our role on this site\nto educate them.</p>\n</li>\n<li><p>forgetful people.</p>\n</li>\n<li><p>people who are not convinced even after repeated hammering,\nwho think that <em>surely the Bourne shell author did not\nintend us to quote all our variables</em>.</p>\n</li>\n</ul>\n<p>Maybe we can convince them if we expose the risk associated with\nthis kind of behaviours.</p>\n<p>What's the worst thing that can possibly happen if you\nforget to quote your variables. Is it really <em>that</em> bad?</p>\n<p>What kind of vulnerability are we talking of here?</p>\n<p>In what contexts can it be a problem?</p>\n",
    "answer": "<h2>Preamble</h2>\n<p>First, I'd say it's not the right way to address the problem.\nIt's a bit like saying &quot;<em>you should not murder people because\notherwise you'll go to jail</em>&quot;.</p>\n<p>Similarly, you don't quote your variable because otherwise\nyou're introducing security vulnerabilities. You quote your\nvariables because it is wrong not to (but if the fear of the jail can help, why not).</p>\n<p>A little summary for those who've just jumped on the train.</p>\n<p>In most shells, leaving a variable expansion unquoted (though\nthat (and the rest of this answer) also applies to command\nsubstitution (<code>`...`</code> or <code>$(...)</code>) and arithmetic expansion (<code>$((...))</code> or <code>$[...]</code>)) has a very special\nmeaning. A good way to describe it is that it is like\ninvoking some sort of implicit <em>split+glob</em> operator¹.</p>\n<pre><code>cmd $var\n</code></pre>\n<p>in another language would be written something like:</p>\n<pre><code>cmd(glob(split($var)))\n</code></pre>\n<p><code>$var</code> is first split into a list of words according to complex\nrules involving the <code>$IFS</code> special parameter (the <em>split</em> part)\nand then each word resulting of that splitting is considered as\na <em>pattern</em> which is expanded to a list of files that match it\n(the <em>glob</em> part).</p>\n<p>As an example, if <code>$var</code> contains <code>*.txt,/var/*.xml</code> and <code>$IFS</code>\ncontains <code>,</code>, <code>cmd</code> would be called with a number of arguments,\nthe first one being <code>cmd</code> and the next ones being the <code>txt</code>\nfiles in the current directory and the <code>xml</code> files in <code>/var</code>.</p>\n<p>If you wanted to call <code>cmd</code> with just the two literal arguments <code>cmd</code>\nand <code>*.txt,/var/*.xml</code>, you'd write:</p>\n<pre><code>cmd &quot;$var&quot;\n</code></pre>\n<p>which would be in your other more familiar language:</p>\n<pre><code>cmd($var)\n</code></pre>\n<h2>What do we mean by <em>vulnerability in a shell</em>?</h2>\n<p>After all, it's been known since the dawn of time that shell\nscripts should not be used in security-sensitive contexts.\nSurely,  OK, leaving a variable unquoted is a bug but that can't\ndo that much harm, can it?</p>\n<p>Well, despite the fact that anybody would tell you that shell\nscripts should never be used for web CGIs, or that thankfully\nmost systems don't allow setuid/setgid shell scripts nowadays,\none thing that shellshock (the remotely exploitable bash bug\nthat made the headlines in September 2014) revealed is that\nshells are still extensively used where they probably shouldn't:\nin CGIs, in DHCP client hook scripts, in sudoers commands,\ninvoked <em>by</em> (if not <em>as</em>) setuid commands...</p>\n<p>Sometimes unknowingly. For instance <code>system('cmd $PATH_INFO')</code>\nin a <code>php</code>/<code>perl</code>/<code>python</code> CGI script does invoke a shell to interpret that command line (not to\nmention the fact that <code>cmd</code> itself may be a shell script and its\nauthor may have never expected it to be called from a CGI).</p>\n<p>You've got a vulnerability when there's a path for privilege\nescalation, that is when someone (let's call him <em>the attacker</em>)\nis able to do something they are not meant to.</p>\n<p>Invariably that means <em>the attacker</em> providing data, that data\nbeing processed by a privileged user/process which inadvertently\ndoes something it shouldn't be doing, in most of the cases because\nof a bug.</p>\n<p>Basically, you've got a problem when your buggy code processes\ndata under the control of <em>the attacker</em>.</p>\n<p>Now, it's not always obvious where that <em>data</em> may come from,\nand it's often hard to tell if your code will ever get to\nprocess untrusted data.</p>\n<p>As far as variables are concerned, In the case of a CGI script,\nit's quite obvious, the data are the CGI GET/POST parameters and\nthings like cookies, path, host... parameters.</p>\n<p>For a setuid script (running as one user when invoked by\nanother), it's the arguments or environment variables.</p>\n<p>Another very common vector is file names. If you're getting a\nfile list from a directory, it's possible that files have been\nplanted there by <em>the attacker</em>.</p>\n<p>In that regard, even at the prompt of an interactive shell, you\ncould be vulnerable (when processing files in <code>/tmp</code> or <code>~/tmp</code>\nfor instance).</p>\n<p>Even a <code>~/.bashrc</code> can be vulnerable (for instance, <code>bash</code> will\ninterpret it when invoked over <code>ssh</code> to run a <code>ForcedCommand</code>\nlike in <code>git</code> server deployments with some variables under the\ncontrol of the client).</p>\n<p>Now, a script may not be called directly to process untrusted\ndata, but it may be called by another command that does. Or your\nincorrect code may be copy-pasted into scripts that do (by you 3\nyears down the line or one of your colleagues). One place where it's\nparticularly <em>critical</em> is in answers in Q&amp;A sites as you'll\nnever know where copies of your code may end up.</p>\n<h2>Down to business; how bad is it?</h2>\n<p>Leaving a variable (or command substitution) unquoted is by far\nthe number one source of security vulnerabilities associated\nwith shell code. Partly because those bugs often translate to\nvulnerabilities but also because it's so common to see unquoted\nvariables.</p>\n<p>Actually, when looking for vulnerabilities in shell code, the\nfirst thing to do is look for unquoted variables. It's easy to\nspot, often a good candidate, generally easy to track back to\nattacker-controlled data.</p>\n<p>There's an infinite number of ways an unquoted variable can turn\ninto a vulnerability. I'll just give a few common trends here.</p>\n<h3>Information disclosure</h3>\n<p>Most people will bump into bugs associated with unquoted\nvariables because of the <em>split</em> part (for instance, it's\ncommon for files to have spaces in their names nowadays and space\nis in the default value of <code>$IFS</code>). Many people will overlook the\n<em>glob</em> part. The <em>glob</em> part is at least as dangerous as the\n<em>split</em> part.</p>\n<p>Globbing done upon unsanitised external input means <em>the\nattacker</em> can make you read the content of any directory.</p>\n<p>In:</p>\n<pre><code>echo You entered: $unsanitised_external_input\n</code></pre>\n<p>if <code>$unsanitised_external_input</code> contains <code>/*</code>, that means <em>the\nattacker</em> can see the content of <code>/</code>. No big deal. It becomes\nmore interesting though with <code>/home/*</code> which gives you a list of\nuser names on the machine, <code>/tmp/*</code>,  <code>/home/*/.forward</code> for\nhints at other dangerous practises, <code>/etc/rc*/*</code> for enabled\nservices... No need to name them individually. A value of <code>/* /*/* /*/*/*...</code> will just list the whole file system.</p>\n<h3>Denial of service vulnerabilities.</h3>\n<p>Taking the previous case a bit too far and we've got a DoS.</p>\n<p>Actually, any unquoted variable in list context with unsanitized\ninput is <em>at least</em> a DoS vulnerability.</p>\n<p>Even expert shell scripters commonly forget to quote things\nlike:</p>\n<pre><code>#! /bin/sh -\n: ${QUERYSTRING=$1}\n</code></pre>\n<p><code>:</code> is the no-op command. What could possibly go wrong?</p>\n<p>That's meant to assign <code>$1</code> to <code>$QUERYSTRING</code> if <code>$QUERYSTRING</code>\nwas unset. That's a quick way to make a CGI script callable from\nthe command line as well.</p>\n<p>That <code>$QUERYSTRING</code> is still expanded though and because it's\nnot quoted, the <em>split+glob</em> operator is invoked.</p>\n<p>Now, there are some globs that are particularly expensive to\nexpand. The <code>/*/*/*/*</code> one is bad enough as it means listing\ndirectories up to 4 levels down. In addition to the disk and CPU\nactivity, that means storing tens of thousands of file paths\n(40k here on a minimal server VM, 10k of which directories).</p>\n<p>Now <code>/*/*/*/*/../../../../*/*/*/*</code> means 40k x 10k and\n<code>/*/*/*/*/../../../../*/*/*/*/../../../../*/*/*/*</code> is enough to\nbring even the mightiest machine to its knees.</p>\n<p>Try it for yourself (though be prepared for your machine to\ncrash or hang):</p>\n<pre><code>a='/*/*/*/*/../../../../*/*/*/*/../../../../*/*/*/*' sh -c ': ${a=foo}'\n</code></pre>\n<p>Of course, if the code is:</p>\n<pre><code>echo $QUERYSTRING &gt; /some/file\n</code></pre>\n<p>Then you can fill up the disk.</p>\n<p>Just do a google search on <a href=\"https://www.google.co.uk/search?q=shell+cgi\" rel=\"noreferrer\">shell\ncgi</a> or <a href=\"https://www.google.co.uk/search?q=bash+cgi\" rel=\"noreferrer\">bash\ncgi</a> or <a href=\"https://www.google.co.uk/search?q=ksh+cgi\" rel=\"noreferrer\">ksh\ncgi</a>, and you'll find\na few pages that show you how to write CGIs in shells. Notice\nhow half of those that process parameters are vulnerable.</p>\n<p>Even <a href=\"https://web.archive.org/web/20150430175848/http://www2.research.att.com/%7Eastopen/download/ksh/scripts/cgi-lib.ksh\" rel=\"noreferrer\">David Korn's\nown\none</a>\nis vulnerable (look at the cookie handling).</p>\n<h3>up to arbitrary code execution vulnerabilities</h3>\n<p>Arbitrary code execution is the worst type of vulnerability,\nsince if <em>the attacker</em> can run any command, there's no limit on\nwhat they may do.</p>\n<p>That's generally the <em>split</em> part that leads to those. That\nsplitting results in several arguments to be passed to commands\nwhen only one is expected. While the first of those will be used\nin the expected context, the others will be in a different context\nso potentially interpreted differently. Better with an example:</p>\n<pre><code>awk -v foo=$external_input '$2 == foo'\n</code></pre>\n<p>Here, the intention was to assign the content of the\n<code>$external_input</code> shell variable to the <code>foo</code> <code>awk</code> variable.</p>\n<p>Now:</p>\n<pre><code>$ external_input='x BEGIN{system(&quot;uname&quot;)}'\n$ awk -v foo=$external_input '$2 == foo'\nLinux\n</code></pre>\n<p>The second word resulting of the splitting of <code>$external_input</code>\nis not assigned to <code>foo</code> but considered as <code>awk</code> code (here that\nexecutes an arbitrary command: <code>uname</code>).</p>\n<p>That's especially a problem for commands that can execute other\ncommands (<code>awk</code>, <code>env</code>, <code>sed</code> (GNU one), <code>perl</code>, <code>find</code>...) especially\nwith the GNU variants (which accept options after arguments).\nSometimes, you wouldn't suspect commands to be able to execute\nothers like <code>ksh</code>, <code>bash</code> or <code>zsh</code>'s <code>[</code> or <code>printf</code>...</p>\n<pre><code>for file in *; do\n  [ -f $file ] || continue\n  something-that-would-be-dangerous-if-$file-were-a-directory\ndone\n</code></pre>\n<p>If we create a directory called <code>x -o yes</code>, then the test\nbecomes positive, because it's a completely different\nconditional expression we're evaluating.</p>\n<p>Worse, if we create a file called <code>x -a a[0$(uname&gt;&amp;2)] -gt 1</code>,\nwith all ksh implementations at least (which includes the <code>sh</code>\nof most commercial Unices and some BSDs), that executes <code>uname</code>\nbecause those shells perform arithmetic evaluation on the\nnumerical comparison operators of the <code>[</code> command.</p>\n<pre><code>$ touch x 'x -a a[0$(uname&gt;&amp;2)] -gt 1'\n$ ksh -c 'for f in *; do [ -f $f ]; done'\nLinux\n</code></pre>\n<p>Same with <code>bash</code> for a filename like <code>x -a -v a[0$(uname&gt;&amp;2)]</code>.</p>\n<p>Of course, if they can't get arbitrary execution, <em>the attacker</em> may\nsettle for lesser damage (which may help to get arbitrary\nexecution). Any command that can write files or change\npermissions, ownership or have any main or side effect could be exploited.</p>\n<p>All sorts of things can be done with file names.</p>\n<pre><code>$ touch -- '-R ..'\n$ for file in *; do [ -f &quot;$file&quot; ] &amp;&amp; chmod +w $file; done\n</code></pre>\n<p>And you end up making <code>..</code> writeable (recursively with GNU\n<code>chmod</code>).</p>\n<p>Scripts doing automatic processing of files in publicly writable areas like <code>/tmp</code> are to be written very carefully.</p>\n<h3>What about <code>[ $# -gt 1 ]</code></h3>\n<p>That's something I find exasperating. Some people go down all\nthe trouble of wondering whether a particular expansion may be\nproblematic to decide if they can omit the quotes.</p>\n<p>It's like saying. <em>Hey, it looks like <code>$#</code> cannot be subject to\nthe split+glob operator, let's ask the shell to split+glob it</em>.\nOr <em>Hey, let's write incorrect code just because the bug is\nunlikely to be hit</em>.</p>\n<p>Now how unlikely is it? OK, <code>$#</code> (or <code>$!</code>, <code>$?</code> or any\narithmetic substitution) may only contain digits (or <code>-</code> for\nsome²) so the <em>glob</em> part is out. For the <em>split</em> part to do\nsomething though, all we need is for <code>$IFS</code> to contain digits (or <code>-</code>).</p>\n<p>With some shells, <code>$IFS</code> may be inherited from the environment,\nbut if the environment is not safe, it's game over anyway.</p>\n<p>Now if you write a function like:</p>\n<pre><code>my_function() {\n  [ $# -eq 2 ] || return\n  ...\n}\n</code></pre>\n<p>What that means is that the behaviour of your function depends\non the context in which it is called. Or in other words, <code>$IFS</code>\nbecomes one of the inputs to it. Strictly speaking, when you\nwrite the API documentation for your function, it should be\nsomething like:</p>\n<pre><code># my_function\n#   inputs:\n#     $1: source directory\n#     $2: destination directory\n#   $IFS: used to split $#, expected not to contain digits...\n</code></pre>\n<p>And code calling your function needs to make sure <code>$IFS</code> doesn't\ncontain digits. All that because you didn't feel like typing\nthose 2 double-quote characters.</p>\n<p>Now, for that <code>[ $# -eq 2 ]</code> bug to become a vulnerability,\nyou'd need somehow for the value of <code>$IFS</code> to become under\ncontrol of <em>the attacker</em>. Conceivably, that would not normally\nhappen unless <em>the attacker</em> managed to exploit another bug.</p>\n<p>That's not unheard of though. A common case is when people\nforget to sanitize data before using it in arithmetic\nexpression. We've already seen above that it can allow\narbitrary code execution in some shells, but in all of them, it allows\n<em>the attacker</em> to give any variable an integer value.</p>\n<p>For instance:</p>\n<pre><code>n=$(($1 + 1))\nif [ $# -gt 2 ]; then\n  echo &gt;&amp;2 &quot;Too many arguments&quot;\n  exit 1\nfi\n</code></pre>\n<p>And with a <code>$1</code> with value <code>(IFS=-1234567890)</code>, that arithmetic\nevaluation has the side effect of settings <code>$IFS</code> and the next <code>[</code>\ncommand fails which means the check for <em>too many args</em> is\nbypassed.</p>\n<h3>What about when the <em>split+glob</em> operator is not invoked?</h3>\n<p>There's another case where quotes are needed around variables and other expansions: when it's used as a pattern.</p>\n<pre><code>[[ $a = $b ]]   # a `ksh` construct also supported by `bash`\ncase $a in ($b) ...; esac\n</code></pre>\n<p>do not test whether <code>$a</code> and <code>$b</code> are the same (except with <code>zsh</code>) but if <code>$a</code> matches the pattern in <code>$b</code>. And you need to quote <code>$b</code> if you want to compare as strings (same thing in <code>&quot;${a#$b}&quot;</code> or <code>&quot;${a%$b}&quot;</code> or <code>&quot;${a##*$b*}&quot;</code> where <code>$b</code> should be quoted if it's not to be taken as a pattern).</p>\n<p>What that means is that <code>[[ $a = $b ]]</code> may return true in cases where <code>$a</code> is different from <code>$b</code> (for instance when <code>$a</code> is <code>anything</code> and <code>$b</code> is <code>*</code>) or may return false when they are identical (for instance when both <code>$a</code> and <code>$b</code> are <code>[a]</code>).</p>\n<p>Can that make for a security vulnerability? Yes, like any bug. Here, <em>the attacker</em> can alter your script's logical code flow and/or break the assumptions that your script are making. For instance, with a code like:</p>\n<pre><code>if [[ $1 = $2 ]]; then\n   echo &gt;&amp;2 '$1 and $2 cannot be the same or damage will incur'\n   exit 1\nfi\n</code></pre>\n<p><em>The attacker</em> can bypass the check by passing <code>'[a]' '[a]'</code>.</p>\n<p>Now, if neither that pattern matching nor the <em>split+glob</em> operator apply, what's the danger of leaving a variable unquoted?</p>\n<p>I have to admit that I do write:</p>\n<pre><code>a=$b\ncase $a in...\n</code></pre>\n<p>There, quoting doesn't harm but is not strictly necessary.</p>\n<p>However, one side effect of omitting quotes in those cases (for instance in Q&amp;A answers) is that it can send a wrong message to beginners: <strike><em>that it may be all right not to quote variables</em></strike>.</p>\n<p>For instance, they may start thinking that if <code>a=$b</code> is OK, then <strike><code>export a=$b</code></strike> would be as well (which <a href=\"/a/97569/135943\">it's not in many shells</a> as it's in arguments to the <code>export</code> command so in list context) or <strike><code>env a=$b</code></strike>.</p>\n<p>There are a few places though where quotes are not accepted. The main one being inside Korn-style arithmetic expressions in many shells like in <code>echo &quot;$(( $1 + 1 ))&quot; &quot;${array[$1 + 1]}&quot; &quot;${var:$1 + 1}&quot;</code> where the <code>$1</code> must not be quoted (being in a list context --the arguments to a simple command-- the overall expansions still needs to be quoted though).</p>\n<p>Inside those, the shell understands a separate language altogether inspired from C. In AT&amp;T <code>ksh</code> for instance <code>$(( 'd' - 'a' ))</code> expands to 3 like it does in C and not the same as <code>$(( d - a ))</code> would. Double quotes are ignored in ksh93 but cause a syntax error in many other shells. In C, <code>&quot;d&quot; - &quot;a&quot;</code> would return the difference between pointers to C strings. Doing the same in shell would not make sense.</p>\n<h3>What about <code>zsh</code>?</h3>\n<p><code>zsh</code> did fix most of those design awkwardnesses. In <code>zsh</code> (at least when not in sh/ksh emulation mode), if you want <em>splitting</em>, or <em>globbing</em>, or <em>pattern matching</em>, you have to request it explicitly: <code>$=var</code> to split, and <code>$~var</code> to glob or for the content of the variable to be treated as a pattern.</p>\n<p>However, splitting (but not globbing) is still done implicitly upon unquoted command substitution (as in <code>echo $(cmd)</code>).</p>\n<p>Also, a sometimes unwanted side effect of not quoting variable is the <em>empties removal</em>. The <code>zsh</code> behaviour is similar to what you can achieve in other shells by disabling globbing altogether (with <code>set -f</code>) and splitting (with <code>IFS=''</code>). Still, in:</p>\n<pre><code>cmd $var\n</code></pre>\n<p>There will be no <em>split+glob</em>, but if <code>$var</code> is empty, instead of receiving one empty argument, <code>cmd</code> will receive no argument at all.</p>\n<p>That can cause bugs (like the obvious <code>[ -n $var ]</code>). That can possibly break a script's expectations and assumptions and cause vulnerabilities.</p>\n<p>As the empty variable can cause an argument to be just <em>removed</em>, that means the  next argument could be interpreted in the wrong context.</p>\n<p>As an example,</p>\n<pre><code>printf '[%d] &lt;%s&gt;\\n' 1 $attacker_supplied1 2 $attacker_supplied2\n</code></pre>\n<p>If <code>$attacker_supplied1</code> is empty,  then <code>$attacker_supplied2</code> will be interpreted as an arithmetic expression (for <code>%d</code>) instead of a string (for <code>%s</code>) and <a href=\"/q/172103\">any unsanitized data used in an arithmetic expression is a command injection vulnerability in Korn-like shells such as zsh</a>.</p>\n<pre><code>$ attacker_supplied1='x y' attacker_supplied2='*'\n$ printf '[%d] &lt;%s&gt;\\n' 1 $attacker_supplied1 2 $attacker_supplied2\n[1] &lt;x y&gt;\n[2] &lt;*&gt;\n</code></pre>\n<p>fine, but:</p>\n<pre><code>$ attacker_supplied1='' attacker_supplied2='psvar[$(uname&gt;&amp;2)0]'\n$ printf '[%d] &lt;%s&gt;\\n' 1 $attacker_supplied1 2 $attacker_supplied2\nLinux\n[1] &lt;2&gt;\n[0] &lt;&gt;\n</code></pre>\n<p>The <code>uname</code> <em>arbitrary command</em> was run.</p>\n<p>Also note that while <code>zsh</code> doesn't do globbing upon substitutions by default, as globs in zsh are much more powerful than in other shells, that means they can do a lot more damage if ever you enabled the <code>globsubst</code> option at the same time of the <code>extendedglob</code> one, or without disabling <code>bareglobqual</code> and left some variables unintentionally unquoted.</p>\n<p>For instance, even:</p>\n<pre><code>set -o globsubst\necho $attacker_controlled\n</code></pre>\n<p>Would be an arbitrary command execution vulnerability, because commands can be executed as part of glob expansions, for instance with the <code>e</code>valuation glob qualifier:</p>\n<pre><code>$ set -o globsubst\n$ attacker_controlled='.(e[uname])'\n$ echo $attacker_controlled\nLinux\n.\n</code></pre>\n<pre><code>emulate sh # or ksh\necho $attacker_controlled\n</code></pre>\n<p>doesn't cause an ACE vulnerability (though it still a DoS one like in sh) because <code>bareglobqual</code> is disabled in sh/ksh emulation. There's no good reason to enable <code>globsubst</code> other than in those sh/ksh emulations when wanting to interpret sh/ksh code.</p>\n<h3>What about when you <em>do</em> need the <em>split+glob</em> operator?</h3>\n<p>Yes, that's typically when you do want to leave your variable unquoted. But then you need to make sure you tune your <em>split</em> and <em>glob</em> operators correctly before using it. If you only want the <em>split</em> part and not the <em>glob</em> part (which is the case most of the time), then you do need to disable globbing (<code>set -o noglob</code>/<code>set -f</code>) and fix <code>$IFS</code>. Otherwise you'll cause vulnerabilities as well (like David Korn's CGI example mentioned above).</p>\n<h2>Conclusion</h2>\n<p>In short, leaving a variable (or command substitution or\narithmetic expansion) unquoted in shells can be very dangerous\nindeed especially when done in the wrong contexts, and it's very\nhard to know which are those wrong contexts.</p>\n<p>That's one of the reasons why it is considered <em>bad practice</em>.</p>\n<p>Thanks for reading so far. If it goes over your head, don't\nworry. One can't expect everyone to understand all the implications of\nwriting their code the way they write it. That's why we have\n<em>good practice recommendations</em>, so they can be followed without\nnecessarily understanding why.</p>\n<p>(and in case that's not obvious yet, please avoid writing\nsecurity sensitive code in shells).</p>\n<p>And <strong>please quote your variables on your answers on this site!</strong></p>\n<hr />\n<p><sup>¹In <code>ksh93</code> and <code>pdksh</code> and derivatives, <em>brace expansion</em> is also performed unless globbing is disabled (in the case of <code>ksh93</code> versions up to ksh93u+, even when the <code>braceexpand</code> option is disabled).</sup></p>\n<p><sup>² In <code>ksh93</code> and <code>yash</code>, arithmetic expansions can also include things like <code>1,2</code>, <code>1e+66</code>, <code>inf</code>, <code>nan</code>. There are even more in <code>zsh</code>, including <code>#</code> which is a glob operator with <code>extendedglob</code>, but <code>zsh</code> never does split+glob upon arithmetic expansion, even in <code>sh</code> emulation</sup></p>\n",
    "url": "https://unix.stackexchange.com/questions/171346/security-implications-of-forgetting-to-quote-a-variable-in-bash-posix-shells"
  },
  {
    "question_title": "How can I delete a word backward at the command line (bash and zsh)?",
    "question_body": "<p>How can I delete a word backward at the command line?  I'm truly used to some editors deleting the last 'word' using <kbd>Ctrl</kbd>+<kbd>Backspace</kbd>, and I'd like that functionality at the command line too.</p>\n\n<p>I am using Bash at the moment and although I could jump backward a word and then delete forward a word, I'd rather have this as a quick-key, or event as <kbd>Ctrl</kbd>+<kbd>Backspace</kbd>.</p>\n\n<p>How can accomplish this?</p>\n",
    "answer": "<p><kbd>Ctrl</kbd>+<kbd>W</kbd> is the standard &quot;kill word&quot; (aka <code>werase</code>).\n<kbd>Ctrl</kbd>+<kbd>U</kbd> kills the whole line (<code>kill</code>).</p>\n<p>You can change them with <code>stty</code>.</p>\n<pre><code>-bash-4.2$ stty -a\nspeed 38400 baud; 24 rows; 80 columns;\nlflags: icanon isig iexten echo echoe -echok echoke -echonl echoctl\n        -echoprt -altwerase -noflsh -tostop -flusho pendin -nokerninfo\n        -extproc -xcase\niflags: -istrip icrnl -inlcr -igncr -iuclc ixon -ixoff ixany imaxbel\n        -ignbrk brkint -inpck -ignpar -parmrk\noflags: opost onlcr -ocrnl -onocr -onlret -olcuc oxtabs -onoeot\ncflags: cread cs8 -parenb -parodd hupcl -clocal -cstopb -crtscts -mdmbuf\ncchars: discard = ^O; dsusp = ^Y; eof = ^D; eol = &lt;undef&gt;;\n        eol2 = &lt;undef&gt;; erase = ^?; intr = ^C; kill = ^U; lnext = ^V;\n        min = 1; quit = ^\\; reprint = ^R; start = ^Q; status = &lt;undef&gt;;\n        stop = ^S; susp = ^Z; time = 0; werase = ^W;\n-bash-4.2$ stty werase ^p\n-bash-4.2$ stty kill ^a\n-bash-4.2$\n</code></pre>\n<p>Note that one does not have to put the actual control character on the line, <code>stty</code> understands putting <code>^</code> and then the character you would hit with control.</p>\n<p>After doing this, if I hit <kbd>Ctrl</kbd>+<kbd>P</kbd> it will erase a word from the line.  And if I hit <kbd>Ctrl</kbd>+<kbd>A</kbd>, it will erase the whole line.</p>\n",
    "url": "https://unix.stackexchange.com/questions/94331/how-can-i-delete-a-word-backward-at-the-command-line-bash-and-zsh"
  },
  {
    "question_title": "Combined `mkdir` and `cd`?",
    "question_body": "<p>is there any way (what is the easiest way in bash) to combine the following:</p>\n\n<pre><code>mkdir foo\ncd foo\n</code></pre>\n\n<p>The manpage for <code>mkdir</code> does not describe anything like that, maybe there is a fancy version of <code>mkdir</code>? I know that <code>cd</code> has to be shell builtin, so the same would be true for the fancy <code>mkdir</code>...</p>\n\n<p>Aliasing?</p>\n",
    "answer": "<p>I think creating a function is the most appropriate way to do this, but just for listing all alternative ways, you could write:</p>\n\n<pre><code>mkdir foo &amp;&amp; cd \"$_\"\n</code></pre>\n\n<p><code>$_</code>is a special parameter that holds the last argument of the previous command. The quote around <code>$_</code> make sure it works even if the folder name contains spaces.</p>\n\n<h2>Why use double quotes?</h2>\n\n<p>In some shells, such as <code>zsh</code>, the double quotes surrounding the <code>$_</code> are not necessary even when the directory name contains spaces. They are required for this command to work in <code>bash</code>, however.</p>\n\n<p>For example, running this command in bash 3.2.57 on macOS 10.13.6:</p>\n\n<pre class=\"lang-bsh prettyprint-override\"><code>mkdir \"my directory\" &amp;&amp; cd $_\n</code></pre>\n\n<p>results in this output:</p>\n\n<pre class=\"lang-bsh prettyprint-override\"><code>bash: cd: my: No such file or directory\n</code></pre>\n\n<p>However, if we surround <code>$_</code> with double quotes, the command returns successfully.</p>\n\n<pre class=\"lang-bsh prettyprint-override\"><code>bash-3.2$ mkdir \"my directory\" &amp;&amp; cd \"$_\"\nbash-3.2$ echo $?\n0\nbash-3.2$\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/125385/combined-mkdir-and-cd"
  },
  {
    "question_title": "Terminal prompt not wrapping correctly",
    "question_body": "<p>I have an issue where if I type in very long commands in bash the terminal will not render what I'm typing correctly. I'd expect that if I had a command like the following:</p>\n\n<pre><code>username@someserver ~/somepath $ ssh -i /path/to/private/key\nmyusername@something.someserver.com</code></pre>\n\n<p>The command should render on two lines. Instead it will often wrap around and start writing over the top of my prompt, somewhat like this:</p>\n\n<pre><code>myreallylongusername@something.somelongserver.comh -i /path/to/private/key</code></pre>\n\n<p>If I decide to go back and change some argument there's no telling where the cursor will show up, sometimes in the middle of the prompt, but usually on the line <em>above</em> where I'm typing. </p>\n\n<p>Additional fun happens when when I <kbd>Up</kbd> to a previous command.\nI've tried this in both gnome-terminal and terminator and on i3 and Cinnamon. Someone suggested it was my prompt, so here that is:</p>\n\n<pre><code>\\[\\033[01;32m\\]\\u:\\[\\033[01;34m\\] \\W\\033[01;34m \\$\\[\\033[00m\\]</code></pre>\n\n<p><kbd>Ctrl</kbd><kbd>l</kbd>, <code>reset</code>, and <code>clear</code> all do what they say, but when I type the command back in or <kbd>Up</kbd> the same things happens.</p>\n\n<p>I checked and <code>checkwinsize</code> is enabled in bash. This happens on 80x24 and other window sizes.</p>\n\n<p>Is this just something I learn to live with? Is there some piece of magic which I should know? I've settled for just using a really short prompt, but that doesn't fix the issue.</p>\n",
    "answer": "<p>Non-printable sequences should be <a href=\"http://www.gnu.org/software/bash/manual/bashref.html#Controlling-the-Prompt\" rel=\"noreferrer\">enclosed in <code>\\[</code> and <code>\\]</code></a>. Looking at your <em>PS1</em> it has a unenclosed sequence after <code>\\W</code>. But, the second entry is redundant as well as it repeats the previous statement <em>&quot;1;34&quot;</em>.</p>\n<pre><code>\\[\\033[01;32m\\]\\u:\\[\\033[01;34m\\] \\W\\033[01;34m \\$\\[\\033[00m\\]\n                  |_____________|               |_|\n                         |                       |\n                         +--- Let this apply to this as well.\n</code></pre>\n<p>As such this should have intended coloring:</p>\n<pre><code>\\[\\033[1;32m\\]\\u:\\[\\033[1;34m\\] \\W \\$\\[\\033[0m\\]\n                               |_____|\n                                  |\n                                  +---- Bold blue.\n</code></pre>\n<p>Keeping the <em>&quot;original&quot;</em> this should also work:</p>\n<pre><code>\\[\\033[1;32m\\]\\u:\\[\\033[1;34m\\] \\W\\[\\033[1;34m\\] \\$\\[\\033[0m\\]\n                                  |_|         |_|\n                                   |           |\n                                   +-----------+-- Enclose in \\[ \\]\n</code></pre>\n<hr />\n<h3>Edit:</h3>\n<p>The reason for the behavior is because <code>bash</code> believes the prompt is longer then it actually is. As a simple example, if one use:</p>\n<pre><code>PS1=&quot;\\033[0;34m$&quot;\n       1 2345678\n</code></pre>\n<p>The prompt is believed to be 8 characters and not 1. As such if terminal window is 20 columns, after typing 12 characters, it is believed to be 20 and wraps around. This is also evident if one then try to do backspace or <kbd>Ctrl+u</kbd>. It stops at column 9.</p>\n<p>However it also does not start new line unless one are on last column, as a result the first line is overwritten.</p>\n<p>If one keep typing the line should wrap to next line after 32 characters.</p>\n",
    "url": "https://unix.stackexchange.com/questions/105958/terminal-prompt-not-wrapping-correctly"
  },
  {
    "question_title": "What does env x=&#39;() { :;}; command&#39; bash do and why is it insecure?",
    "question_body": "<p>There is apparently a vulnerability (CVE-2014-6271) in bash: <a href=\"https://securityblog.redhat.com/2014/09/24/bash-specially-crafted-environment-variables-code-injection-attack/\">Bash specially crafted environment variables code injection attack</a></p>\n\n<p>I am trying to figure out what is happening, but I'm not entirely sure I understand it. How can the <code>echo</code> be executed as it is in single quotes?</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>$ env x='() { :;}; echo vulnerable' bash -c \"echo this is a test\"\nvulnerable\nthis is a test\n</code></pre>\n\n<hr>\n\n<p><strong>EDIT 1</strong>: A patched system looks like this:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>$ env x='() { :;}; echo vulnerable' bash -c \"echo this is a test\"\nbash: warning: x: ignoring function definition attempt\nbash: error importing function definition for `x'\nthis is a test\n</code></pre>\n\n<p><strong>EDIT 2</strong>: There is a related vulnerability / patch: <a href=\"https://access.redhat.com/articles/1200223\">CVE-2014-7169</a> which uses a slightly different test:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>$ env 'x=() { :;}; echo vulnerable' 'BASH_FUNC_x()=() { :;}; echo vulnerable' bash -c \"echo test\"\n</code></pre>\n\n<p><em>unpatched output</em>:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>vulnerable\nbash: BASH_FUNC_x(): line 0: syntax error near unexpected token `)'\nbash: BASH_FUNC_x(): line 0: `BASH_FUNC_x() () { :;}; echo vulnerable'\nbash: error importing function definition for `BASH_FUNC_x'\ntest\n</code></pre>\n\n<p><em>partially (early version) patched output</em>:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>bash: warning: x: ignoring function definition attempt\nbash: error importing function definition for `x'\nbash: error importing function definition for `BASH_FUNC_x()'\ntest\n</code></pre>\n\n<p><em>patched output</em> up to and including CVE-2014-7169:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>bash: warning: x: ignoring function definition attempt\nbash: error importing function definition for `BASH_FUNC_x'\ntest\n</code></pre>\n\n<p><strong>EDIT 3</strong>: story continues with:</p>\n\n<ul>\n<li><a href=\"http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-7186\">CVE-2014-7186</a></li>\n<li><a href=\"http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-7187\">CVE-2014-7187</a></li>\n<li><a href=\"http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-6277\">CVE-2014-6277</a></li>\n</ul>\n",
    "answer": "<p>bash stores exported function definitions as environment variables. Exported functions look like this:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>$ foo() { bar; }\n$ export -f foo\n$ env | grep -A1 foo\nfoo=() {  bar\n}\n</code></pre>\n\n<p>That is, the environment variable <code>foo</code> has the literal contents:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>() {  bar\n}\n</code></pre>\n\n<p>When a new instance of bash launches, it looks for these specially crafted environment variables, and interprets them as function definitions. You can even write one yourself, and see that it still works:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>$ export foo='() { echo \"Inside function\"; }'\n$ bash -c 'foo'\nInside function\n</code></pre>\n\n<p>Unfortunately, the parsing of function definitions from strings (the environment variables) can have wider effects than intended. In unpatched versions, it also interprets arbitrary commands that occur after the termination of the function definition. This is due to insufficient constraints in the determination of acceptable function-like strings in the environment. For example:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>$ export foo='() { echo \"Inside function\" ; }; echo \"Executed echo\"'\n$ bash -c 'foo'\nExecuted echo\nInside function\n</code></pre>\n\n<p>Note that the echo outside the function definition has been unexpectedly executed during bash startup. The function definition is just a step to get the evaluation and exploit to happen, the function definition itself and the environment variable used are arbitrary. The shell looks at the environment variables, sees <code>foo</code>, which looks like it meets the constraints it knows about what a function definition looks like, and it evaluates the line, unintentionally also executing the echo (which could be any command, malicious or not).</p>\n\n<p>This is considered insecure because variables are not typically allowed or expected, by themselves, to directly cause the invocation of arbitrary code contained in them. Perhaps your program sets environment variables from untrusted user input. It would be highly unexpected that those environment variables could be manipulated in such a way that the user could run arbitrary commands without your explicit intent to do so using that environment variable for such a reason declared in the code.</p>\n\n<p>Here is an example of a viable attack. You run a web server that runs a vulnerable shell, somewhere, as part of its lifetime. This web server passes environment variables to a bash script, for example, if you are using CGI, information about the HTTP request is often included as environment variables from the web server. For example, <code>HTTP_USER_AGENT</code> might be set to the contents of your user agent. This means that if you spoof your user agent to be something like '() { :; }; echo foo', when that shell script runs, <code>echo foo</code> will be executed. Again, <code>echo foo</code> could be anything, malicious or not.</p>\n",
    "url": "https://unix.stackexchange.com/questions/157329/what-does-env-x-command-bash-do-and-why-is-it-insecure"
  },
  {
    "question_title": "There are stopped jobs (on bash exit)",
    "question_body": "<p>I get the message <code>There are stopped jobs.</code> when I try to exit a bash shell sometimes. Here is a reproducible scenario in python 2.x:</p>\n\n<ul>\n<li><kbd>ctrl</kbd>+<kbd>c</kbd> is handled by the interpreter as an exception.</li>\n<li><kbd>ctrl</kbd>+<kbd>z</kbd> 'stops' the process.</li>\n<li><kbd>ctrl</kbd>+<kbd>d</kbd> exits python for reals.</li>\n</ul>\n\n<p>Here is some real-world terminal output:</p>\n\n<pre><code>example_user@example_server:~$ python\nPython 2.7.3 (default, Sep 26 2013, 20:03:06) \n[GCC 4.6.3] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n</code></pre>\n\n<p><kbd>ctrl+z</kbd></p>\n\n<pre><code>[1]+  Stopped                 python\nexample_user@example_server:~$ exit\nlogout\nThere are stopped jobs.\n</code></pre>\n\n<p>Bash did not exit, I must <code>exit</code> again to exit the bash shell.</p>\n\n<ul>\n<li><strong>Q:</strong> What is a 'stopped job', or what does this mean?</li>\n<li><strong>Q:</strong> Can a stopped process be resumed?</li>\n<li><strong>Q:</strong> Does the first <code>exit</code> kill the stopped jobs?</li>\n<li><strong>Q:</strong> Is there a way to exit the shell the first time? (without entering <code>exit</code> twice)</li>\n</ul>\n",
    "answer": "<p>A stopped job is one that has been temporarily put into the background and is no longer running, but is still using resources (i.e. system memory). Because that job is not attached to the current terminal, it cannot produce output and is not receiving input from the user.</p>\n\n<p>You can see jobs you have running using the <code>jobs</code> builtin command in bash, probably other shells as well. Example:</p>\n\n<pre><code>user@mysystem:~$ jobs\n[1] + Stopped                python\nuser@mysystem:~$ \n</code></pre>\n\n<p>You can resume a stopped job by using the <code>fg</code> (foreground) bash built-in command. If you have multiple commands that have been stopped you must specify which one to resume by passing jobspec number on the command line with <code>fg</code>. If only one program is stopped, you may use <code>fg</code> alone:</p>\n\n<pre><code>user@mysystem:~$ fg 1\npython\n</code></pre>\n\n<p>At this point you are back in the python interpreter and may exit by using control-D.</p>\n\n<p>Conversely, you may <code>kill</code> the command with either it's jobspec or PID. For instance:</p>\n\n<pre><code>user@mysystem:~$ ps\n  PID TTY          TIME CMD\n16174 pts/3    00:00:00 bash\n17781 pts/3    00:00:00 python\n18276 pts/3    00:00:00 ps\nuser@mysystem:~$ kill 17781\n[1]+  Killed                  python\nuser@mysystem:~$ \n</code></pre>\n\n<p>To use the jobspec, precede the number with the percent (%) key:</p>\n\n<pre><code>user@mysystem:~$ kill %1\n[1]+  Terminated              python\n</code></pre>\n\n<p>If you issue an exit command with stopped jobs, the warning you saw will be given. The jobs will be left running for safety. That's to make sure you are aware you are attempting to kill jobs you might have forgotten you stopped. The second time you use the exit command the jobs are terminated and the shell exits. This may cause problems for some programs that aren't intended to be killed in this fashion.</p>\n\n<p>In bash it seems you can use the <code>logout</code> command which will kill stopped processes and exit. This may cause unwanted results.</p>\n\n<p>Also note that some programs may not exit when terminated in this way, and your system could end up with a lot of orphaned processes using up resources if you make a habit of doing that.</p>\n\n<p>Note that you can create background process that will stop if they require user input:</p>\n\n<pre><code>user@mysystem:~$ python &amp;\n[1] 19028\nuser@mysystem:~$ jobs\n[1]+  Stopped                 python\n</code></pre>\n\n<p>You can resume and kill these jobs in the same way you did jobs that you stopped with the <code>Ctrl-z</code> interrupt.</p>\n",
    "url": "https://unix.stackexchange.com/questions/116959/there-are-stopped-jobs-on-bash-exit"
  },
  {
    "question_title": "Are there naming conventions for variables in shell scripts?",
    "question_body": "<p>Most languages have naming conventions for variables, the most common style I see in shell scripts is <code>MY_VARIABLE=foo</code>.  Is this the convention or is it only for global variables?  What about variables local to the script?</p>\n",
    "answer": "<p>Environment variables or shell variables introduced by the operating system, shell startup scripts, or the shell itself, etc., are usually all in <code>CAPITALS</code><sup>1</sup>.</p>\n<p>To prevent your variables from conflicting with these variables, it is a good practice to use <code>lower_case</code> variable names.</p>\n<hr />\n<p><sup>1</sup><sub>A notable exception that may be worth knowing about is the <code>path</code> array, used by the <code>zsh</code> shell.  This is the same as the common <code>PATH</code> variable but represented as an array.</sub></p>\n",
    "url": "https://unix.stackexchange.com/questions/42847/are-there-naming-conventions-for-variables-in-shell-scripts"
  },
  {
    "question_title": "How do I remove a directory and all its contents?",
    "question_body": "<p>In bash all I know is that </p>\n\n<pre><code>rmdir directoryname\n</code></pre>\n\n<p>will remove the directory but only if it's empty. Is there a way to force remove subdirectories?</p>\n",
    "answer": "<p>The following command will do it for you. Use caution though if this isn't your intention as this also removes files in the directory and subdirectories.</p>\n<pre><code>rm -rf directoryname\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/45676/how-do-i-remove-a-directory-and-all-its-contents"
  },
  {
    "question_title": "Looping through files with spaces in the names?",
    "question_body": "<p>I wrote the following script to diff the outputs of two directores with all the same files in them as such:</p>\n\n<pre><code>#!/bin/bash\n\nfor file in `find . -name \"*.csv\"`  \ndo\n     echo \"file = $file\";\n     diff $file /some/other/path/$file;\n     read char;\ndone\n</code></pre>\n\n<p>I know there are other ways to achieve this.  Curiously though, this script fails when the files have spaces in them.  How can I deal with this?</p>\n\n<p>Example output of find:</p>\n\n<pre><code>./zQuery - abc - Do Not Prompt for Date.csv\n</code></pre>\n",
    "answer": "<p><strong>Short answer (closest to your answer, but handles spaces)</strong></p>\n<pre><code>OIFS=&quot;$IFS&quot;\nIFS=$'\\n'\nfor file in `find . -type f -name &quot;*.csv&quot;`  \ndo\n     echo &quot;file = $file&quot;\n     diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n     read line\ndone\nIFS=&quot;$OIFS&quot;\n</code></pre>\n<p><strong>Better answer (also handles wildcards and newlines in file names)</strong></p>\n<pre><code>find . -type f -name &quot;*.csv&quot; -print0 | while IFS= read -r -d '' file; do\n    echo &quot;file = $file&quot;\n    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n    read line &lt;/dev/tty\ndone\n</code></pre>\n<p><strong>Best answer (based on <a href=\"https://unix.stackexchange.com/questions/9496/looping-through-files-with-spaces-in-the-names/9500#9500\">Gilles' answer</a>)</strong></p>\n<pre><code>find . -type f -name '*.csv' -exec sh -c '\n  file=&quot;$0&quot;\n  echo &quot;$file&quot;\n  diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n  read line &lt;/dev/tty\n' exec-sh {} ';'\n</code></pre>\n<p>Or even better, to avoid running one <code>sh</code> per file:</p>\n<pre><code>find . -type f -name '*.csv' -exec sh -c '\n  for file do\n    echo &quot;$file&quot;\n    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n    read line &lt;/dev/tty\n  done\n' exec-sh {} +\n</code></pre>\n<hr />\n<p><strong>Long answer</strong></p>\n<p>You have three problems:</p>\n<ol>\n<li>By default, the shell splits the output of a command on spaces, tabs, and newlines</li>\n<li>Filenames could contain wildcard characters which would get expanded</li>\n<li>What if there is a directory whose name ends in <code>*.csv</code>?</li>\n</ol>\n<p><strong>1. Splitting only on newlines</strong></p>\n<p>To figure out what to set <code>file</code> to, the shell has to take the output of <code>find</code> and interpret it somehow, otherwise <code>file</code> would just be the entire output of <code>find</code>.</p>\n<p>The shell reads the <code>IFS</code> variable, which is set to <code>&lt;space&gt;&lt;tab&gt;&lt;newline&gt;</code> by default.</p>\n<p>Then it looks at each character in the output of <code>find</code>.  As soon as it sees any character that's in <code>IFS</code>, it thinks that marks the end of the file name, so it sets <code>file</code> to whatever characters it saw until now and runs the loop.  Then it starts where it left off to get the next file name, and runs the next loop, etc., until it reaches the end of output.</p>\n<p>So it's effectively doing this:</p>\n<pre><code>for file in &quot;zquery&quot; &quot;-&quot; &quot;abc&quot; ...\n</code></pre>\n<p>To tell it to only split the input on newlines, you need to do</p>\n<pre><code>IFS=$'\\n'\n</code></pre>\n<p>before your <code>for ... find</code> command.</p>\n<p>That sets <code>IFS</code> to a single newline, so it only splits on newlines, and not spaces and tabs as well.</p>\n<p>If you are using <code>sh</code> or <code>dash</code> instead of <code>ksh93</code>, <code>bash</code> or <code>zsh</code>, you need to write <code>IFS=$'\\n'</code> like this instead:</p>\n<pre><code>IFS='\n'\n</code></pre>\n<p>That is probably enough to get your script working, but if you're interested to handle some other corner cases properly, read on...</p>\n<p><strong>2. Expanding <code>$file</code> without wildcards</strong></p>\n<p>Inside the loop where you do</p>\n<pre><code>diff $file /some/other/path/$file\n</code></pre>\n<p>the shell tries to expand <code>$file</code> (again!).</p>\n<p>It could contain spaces, but since we already set <code>IFS</code> above, that won't be a problem here.</p>\n<p>But it could also contain wildcard characters such as <code>*</code> or <code>?</code>, which would lead to unpredictable behavior.  (Thanks to Gilles for pointing this out.)</p>\n<p>To tell the shell not to expand wildcard characters, put the variable inside double quotes, e.g.</p>\n<pre><code>diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n</code></pre>\n<p>The same problem could also bite us in</p>\n<pre><code>for file in `find . -name &quot;*.csv&quot;`\n</code></pre>\n<p>For example, if you had these three files</p>\n<pre><code>file1.csv\nfile2.csv\n*.csv\n</code></pre>\n<p>(very unlikely, but still possible)</p>\n<p>It would be as if you had run</p>\n<pre><code>for file in file1.csv file2.csv *.csv\n</code></pre>\n<p>which will get expanded to</p>\n<pre><code>for file in file1.csv file2.csv *.csv file1.csv file2.csv\n</code></pre>\n<p>causing <code>file1.csv</code> and <code>file2.csv</code> to be processed twice.</p>\n<p>Instead, we have to do</p>\n<pre><code>find . -name &quot;*.csv&quot; -print | while IFS= read -r file; do\n    echo &quot;file = $file&quot;\n    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n    read line &lt;/dev/tty\ndone\n</code></pre>\n<p><code>read</code> reads lines from standard input, splits the line into words according to <code>IFS</code> and stores them in the variable names that you specify.</p>\n<p>Here, we're telling it not to split the line into words, and to store the line in <code>$file</code>.</p>\n<p>Also note that <code>read line</code> has changed to <code>read line &lt;/dev/tty</code>.</p>\n<p>This is because inside the loop, standard input is coming from <code>find</code> via the pipeline.</p>\n<p>If we just did <code>read</code>, it would be consuming part or all of a file name, and some files would be skipped.</p>\n<p><code>/dev/tty</code> is the terminal where the user is running the script from.  Note that this will cause an error if the script is run via cron, but I assume this is not important in this case.</p>\n<p>Then, what if a file name contains newlines?</p>\n<p>We can handle that by changing <code>-print</code> to <code>-print0</code> and using <code>read -d ''</code> on the end of a pipeline:</p>\n<pre><code>find . -name &quot;*.csv&quot; -print0 | while IFS= read -r -d '' file; do\n    echo &quot;file = $file&quot;\n    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n    read char &lt;/dev/tty\ndone\n</code></pre>\n<p>This makes <code>find</code> put a null byte at the end of each file name.  Null bytes are the only characters not allowed in file names, so this should handle all possible file names, no matter how weird.</p>\n<p>To get the file name on the other side, we use <code>IFS= read -r -d ''</code>.</p>\n<p>Where we used <code>read</code> above, we used the default line delimiter of newline, but now, <code>find</code> is using null as the line delimiter. In <code>bash</code>, you can't pass a NUL character in an argument to a command (even builtin ones), but <code>bash</code> understands <code>-d ''</code> as meaning <em>NUL delimited</em>. So we use <code>-d ''</code> to make <code>read</code> use the same line delimiter as <code>find</code>. Note that <code>-d $'\\0'</code>, incidentally, works as well, because <code>bash</code> not supporting NUL bytes treats it as the empty string.</p>\n<p>To be correct, we also add <code>-r</code>, which says don't handle backslashes in file names specially.  For example, without <code>-r</code>, <code>\\&lt;newline&gt;</code> are removed, and <code>\\n</code> is converted into <code>n</code>.</p>\n<p>A more portable way of writing this that doesn't require <code>bash</code> or <code>zsh</code> or remembering all the above rules about null bytes (again, thanks to Gilles):</p>\n<pre><code>find . -name '*.csv' -exec sh -c '\n  file=&quot;$0&quot;\n  echo &quot;$file&quot;\n  diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n  read char &lt;/dev/tty\n' exec-sh {} ';'\n</code></pre>\n<p>*<em>3. Skipping directories whose names end in <em>.csv</em></em></p>\n<pre><code>find . -name &quot;*.csv&quot;\n</code></pre>\n<p>will also match directories that are called <code>something.csv</code>.</p>\n<p>To avoid this, add <code>-type f</code> to the <code>find</code> command.</p>\n<pre><code>find . -type f -name '*.csv' -exec sh -c '\n  file=&quot;$0&quot;\n  echo &quot;$file&quot;\n  diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n  read line &lt;/dev/tty\n' exec-sh {} ';'\n</code></pre>\n<p>As <a href=\"https://unix.stackexchange.com/users/4667/glenn-jackman\">glenn jackman</a> points out, in both of these examples, the commands to execute for each file are being run in a subshell, so if you change any variables inside the loop, they will be forgotten.</p>\n<p>If you need to set variables and have them still set at the end of the loop, you can rewrite it to use process substitution like this:</p>\n<pre><code>i=0\nwhile IFS= read -r -d '' file; do\n    echo &quot;file = $file&quot;\n    diff &quot;$file&quot; &quot;/some/other/path/$file&quot;\n    read line &lt;/dev/tty\n    i=$((i+1))\ndone &lt; &lt;(find . -type f -name '*.csv' -print0)\necho &quot;$i files processed&quot;\n</code></pre>\n<p>Note that if you try copying and pasting this at the command line, <code>read line</code> will consume the <code>echo &quot;$i files processed&quot;</code>, so that command won't get run.</p>\n<p>To avoid this, you could remove <code>read line &lt;/dev/tty</code> and send the result to a pager like <code>less</code>.</p>\n<hr />\n<p><strong>NOTES</strong></p>\n<p>I removed the semi-colons (<code>;</code>) inside the loop.  You can put them back if you want, but they are not needed.</p>\n<p>These days, <code>$(command)</code> is more common than <code>`command`</code>.  This is mainly because it's easier to write <code>$(command1 $(command2))</code> than <code>`command1 \\`command2\\``</code>.</p>\n<p><code>read char</code> doesn't really read a character.  It reads a whole line so I changed it to <code>read line</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/9496/looping-through-files-with-spaces-in-the-names"
  },
  {
    "question_title": "remove particular characters from a variable using bash",
    "question_body": "<p>I want to parse a <strong>variable</strong> (in my case it's development kit version) to make it dot(<code>.</code>) free. If <code>version='2.3.3'</code>, desired output is <code>233</code>.</p>\n\n<p>I tried as below, but it requires <code>.</code> to be replaced with another character giving me <code>2_3_3</code>. It would have been fine if <code>tr . ''</code> would have worked.</p>\n\n<pre><code>  1 VERSION='2.3.3' \n  2 echo \"2.3.3\" | tr . _\n</code></pre>\n",
    "answer": "<p>There is no need to execute an external program. <code>bash</code>'s <a href=\"http://tldp.org/LDP/abs/html/string-manipulation.html\" rel=\"noreferrer\">string manipulation</a> can handle it (also available in <code>ksh93</code> (where it comes from), <code>zsh</code> and recent versions of <code>mksh</code>, <code>yash</code> and busybox <code>sh</code> (at least)):</p>\n\n<pre><code>$ VERSION='2.3.3'\n$ echo \"${VERSION//.}\"\n233\n</code></pre>\n\n<p>(In those shells' manuals you can generally find this in the <em>parameter expansion</em> section.)</p>\n",
    "url": "https://unix.stackexchange.com/questions/104881/remove-particular-characters-from-a-variable-using-bash"
  },
  {
    "question_title": "How to debug a bash script?",
    "question_body": "<p>I'm having some problems with some scripts in bash, about errors and unexpected behaviors. I would like to investigate the causes of the problems so I can apply fixes. Is there a way I can turn some kind of \"debug-mode\" for bash, to get more information?</p>\n",
    "answer": "<p>Start your bash script with <code>bash -x ./script.sh</code> or add in your script <code>set -x</code> to see debug output.</p>\n\n<hr />\n\n<p>Additional with <code>bash</code> 4.1 or later:</p>\n\n<p>If you want to write the debug output to a separate file, add this to your script:</p>\n\n<pre><code>exec 5&gt; debug_output.txt\nBASH_XTRACEFD=\"5\"\n</code></pre>\n\n<p>See: <a href=\"https://stackoverflow.com/a/25593226/3776858\">https://stackoverflow.com/a/25593226/3776858</a></p>\n\n<hr />\n\n<p>If you want to see line numbers add this:</p>\n\n<pre><code>PS4='$LINENO: '\n</code></pre>\n\n<p><hr />\nIf you have access to <code>logger</code> command then you can use this to write debug output via your syslog with timestamp, script name and line number:</p>\n\n<pre><code>#!/bin/bash\n\nexec 5&gt; &gt;(logger -t $0)\nBASH_XTRACEFD=\"5\"\nPS4='$LINENO: '\nset -x\n\n# Place your code here\n</code></pre>\n\n<p>You can use option <code>-p</code> of <code>logger</code> command to set an individual facility and level to write output via local syslog to its own logfile.</p>\n",
    "url": "https://unix.stackexchange.com/questions/155551/how-to-debug-a-bash-script"
  },
  {
    "question_title": "How to use watch command with a piped chain of commands/programs",
    "question_body": "<p>I usually use <code>watch</code> Linux utility to watch the output of a command repeatedly every <code>n</code> seconds, like in <code>watch df -h /some_volume/</code>.</p>\n<p>But I seem not to be able to use <code>watch</code> with a piped series of command like:</p>\n<pre><code>$ watch ls -ltr|tail -n 1\n</code></pre>\n<p>If I do that, <code>watch</code> is really watching <code>ls -ltr</code> and the output is being passed to <code>tail -n 1</code> which doesn't output anything.</p>\n<p>If I try this:</p>\n<pre><code>$ watch (ls -ltr|tail -n 1)\n</code></pre>\n<p>I get</p>\n<pre><code>$ watch: syntax error near unexpected token `ls'\n</code></pre>\n<p>And any of the following fails some reason or another:</p>\n<pre><code>$ watch &lt;(ls -ltr|tail -n 1)\n\n$ watch &lt; &lt;(ls -ltr|tail -n 1)\n\n$ watch $(ls -ltr|tail -n 1)\n\n$ watch `ls -ltr|tail -n 1)`\n</code></pre>\n<p>And finally if do this:</p>\n<pre><code>$ watch echo $(ls -ltr|tail -n 1)\n</code></pre>\n<p>I see no change in the output at the given interval because the command inside <code>$()</code> is run just once and the resulting output string is always printed (&quot;watched&quot;) as a literal.</p>\n<p><strong>So, how do I make the <code>watch</code> command work with a piped chain of commands [other that putting them inside a script]?</strong></p>\n",
    "answer": "<pre><code>watch 'command | othertool | yet-another-tool'\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/318859/how-to-use-watch-command-with-a-piped-chain-of-commands-programs"
  },
  {
    "question_title": "What does &lt;&lt;&lt; mean?",
    "question_body": "<p>What does &lt;&lt;&lt; mean? Here is an example:</p>\n\n<pre><code>$ sed 's/a/b/g' &lt;&lt;&lt; \"aaa\"\nbbb\n</code></pre>\n\n<p>Is it something general that works with more Linux commands?</p>\n\n<p>It looks like it's feeding the <code>sed</code> program with the string <code>aaa</code>, but isn't &lt;&lt; or &lt; usually used for that?</p>\n",
    "answer": "<p>Others have answered the basic question: What is it? (Answer: It's a <a href=\"https://www.gnu.org/software/bash/manual/bash.html#Here-Strings\" rel=\"noreferrer\">here string</a>.)</p>\n<p>Let's look at why it's useful.</p>\n<p>You can also feed a string to a command's stdin like this:</p>\n<pre><code>echo &quot;$string&quot; | command\n</code></pre>\n<p>However in Bash, introducing a pipe means the individual commands are run in subshells. Consider this:</p>\n<pre><code>echo &quot;hello world&quot; | read first second\necho $second $first\n</code></pre>\n<p>The output of the 2nd echo command prints just a single space. Whaaaa? What happened to my variables? Because the read command is in a pipeline, it is run in a subshell. It correctly reads 2 words from its stdin and assigns to the variables. But then the command completes, <em>the subshell exits</em> and the variables are lost.</p>\n<p>Sometimes you can work around this with braces:</p>\n<pre><code>echo &quot;hello world&quot; | {\n    read first second\n    echo $second $first\n}\n</code></pre>\n<p>That's OK if your need for the values is contained, but you still don't have those variables in the current shell of your script.</p>\n<p>To remedy this confusing situation, use a here string:</p>\n<pre><code>read first second &lt;&lt;&lt; &quot;hello world&quot;\necho $second $first\n</code></pre>\n<p>Ah, much better!</p>\n",
    "url": "https://unix.stackexchange.com/questions/80362/what-does-mean"
  },
  {
    "question_title": "Can I redirect output to a log file and background a process at the same time?",
    "question_body": "<p>Can I redirect output to a log file and a background process at the same time?</p>\n<p>In other words, can I do something like this?</p>\n<pre><code>nohup java -jar myProgram.jar 2&gt;&amp;1 &gt; output.log &amp;\n</code></pre>\n<p>Or, is that not a legal command?  Or, do I need to manually move it to the background, like this:</p>\n<pre><code>java -jar myProgram.jar 2&gt;$1 &gt; output.log\njobs\n[CTRL-Z]\nbg 1\n</code></pre>\n",
    "answer": "<p>One problem with your first command is that you redirect stderr to where stdout is (if you changed the $ to a &amp; as suggested in the comment) and then, you redirected stdout to some log file, but that does not pull along the redirected stderr. You must do it in the other order, first send stdout to where you want it to go, and then send stderr to the address stdout is at</p>\n\n<pre><code>some_cmd &gt; some_file 2&gt;&amp;1 &amp;\n</code></pre>\n\n<p>and then you could throw the &amp; on to send it to the background. Jobs can be accessed with the <code>jobs</code> command. <code>jobs</code> will show you the running jobs, and number them. You could then talk about the jobs using a % followed by the number like <code>kill %1</code> or so.  </p>\n\n<p>Also, without the &amp; on the end you can suspend the command with <kbd>Ctrl</kbd><kbd>z</kbd>, use the <code>bg</code> command to put it in the background and <code>fg</code> to bring it back to the foreground.  In combination with the <code>jobs</code> command, this is powerful.</p>\n\n<p>to clarify the above part about the order you write the commands. Suppose stderr is address 1002, stdout is address 1001, and the file is 1008. The command reads left to right, so the first thing it sees in yours is <code>2&gt;&amp;1</code> which moves stderr to the address 1001, it then sees <code>&gt; file</code> which moves stdout to 1008, but keeps stderr at 1001. It does not pull everything pointing at 1001 and move it to 1008, but simply references stdout and moves it to the file.<br>\nThe other way around, it moves stdout to 1008, and then moves stderr to the point that stdout is pointing to, 1008 as well. This way both can point to the single file.</p>\n",
    "url": "https://unix.stackexchange.com/questions/74520/can-i-redirect-output-to-a-log-file-and-background-a-process-at-the-same-time"
  },
  {
    "question_title": "How to export variables from a file?",
    "question_body": "<p>I have a <code>tmp.txt</code> file containing variables to be exported, for example:</p>\n\n<pre><code>a=123\nb=\"hello world\"\nc=\"one more variable\"\n</code></pre>\n\n<p>How can I export all these variables using the <code>export</code> command, so that they can later be used by child processes?</p>\n",
    "answer": "<pre><code>set -a\n. ./tmp.txt\nset +a\n</code></pre>\n<p><code>set -a</code> causes variables¹ defined from now on to be automatically exported. It's available in any Bourne-like shell. <code>.</code> is the standard and Bourne name for the <code>source</code> command so I prefer it for portability (<code>source</code> comes from <code>csh</code> and is now available in most modern Bourne-like shells including <code>bash</code> though (sometimes with a slightly different behaviour)).</p>\n<p>In POSIX shells, you can also use</p>\n<pre><code>set -o allexport\n. ./tmp.txt\nset +o allexport\n</code></pre>\n<p>as a more descriptive alternative way to write it.</p>\n<p>You can make it a function with:</p>\n<pre><code>export_from() {\n  # local is not a standard command but is pretty common. It's needed here\n  # for this code to be re-entrant (for the case where sourced files to\n  # call export_from). We still use _export_from_ prefix to namespace\n  # those variables to reduce the risk of those variables being some of\n  # those exported by the sourced file.\n  local _export_from_ret _export_from_restore _export_from_file\n\n  _export_from_ret=0\n\n  # record current state of the allexport option. Some shells (ksh93/zsh)\n  # have support for local scope for options, but there's no standard\n  # equivalent.\n  case $- in\n    (*a*) _export_from_restore=;;\n    (*)   _export_from_restore='set +a';;\n  esac\n\n  for _export_from_file do\n    # using the command prefix removes the &quot;special&quot; attribute of the &quot;.&quot;\n    # command so that it doesn't exit the shell when failing.\n    command . &quot;$_export_from_file&quot; || _export_from_ret=&quot;$?&quot;\n  done\n  eval &quot;$_export_from_restore&quot;\n  return &quot;$_export_from_ret&quot;\n}\n</code></pre>\n<hr />\n<p><sup>¹ In <code>bash</code>, beware that it also causes all <strong>functions</strong> declared while <code>allexport</code> is on to be exported to the environment (as <code>BASH_FUNC_myfunction%%</code> environment variables that are then imported by all <code>bash</code> shells run in that environment, even when running as <code>sh</code>).</sup></p>\n",
    "url": "https://unix.stackexchange.com/questions/79064/how-to-export-variables-from-a-file"
  },
  {
    "question_title": "Precedence of the shell logical operators &amp;&amp;, ||",
    "question_body": "<p>I am trying to understand how the logical operator precedence works in bash. For example, I would have expected, that the following command does not echo anything.</p>\n\n<pre><code>true || echo aaa &amp;&amp; echo bbb\n</code></pre>\n\n<p>However, contrary to my expectation, <code>bbb</code> gets printed. </p>\n\n<p>Can somebody please explain, how can I make sense of compounded <code>&amp;&amp;</code> and <code>||</code> operators in bash?</p>\n",
    "answer": "<p>In many computer languages, operators with the same precedence are <a href=\"http://en.wikipedia.org/wiki/Operator_associativity\" rel=\"noreferrer\">left-associative</a>. That is, in the absence of grouping structures, leftmost operations are executed first. Bash is <a href=\"http://www.gnu.org/software/bash/manual/bashref.html#Lists\" rel=\"noreferrer\">no exception</a> to this rule.</p>\n<p>This is important because in Bash and other shells <code>&amp;&amp;</code> and <code>||</code> have the same precedence. This is different from most other programming languages which usually give <code>&amp;&amp;</code> a higher precedence than <code>||</code>.</p>\n<p>So what happens in your example is that the leftmost operation (<code>||</code>) is carried out first:</p>\n<pre><code>true || echo aaa\n</code></pre>\n<p>Since <code>true</code> is obviously true, the <code>||</code> operator short-circuits and the whole statement is considered true without the need to evaluate <code>echo aaa</code> as you would expect. Now it remains to do the rightmost operation:</p>\n<pre><code>(...) &amp;&amp; echo bbb\n</code></pre>\n<p>Since the first operation evaluated to true (i.e. had a 0 exit status), it's as if you're executing</p>\n<pre><code>true &amp;&amp; echo bbb\n</code></pre>\n<p>so the <code>&amp;&amp;</code> will not short-circuit, which is why you see <code>bbb</code> echoed.</p>\n<p>You would get the same behavior with</p>\n<pre><code>false &amp;&amp; echo aaa || echo bbb\n</code></pre>\n<p><strong>Notes based on the comments</strong></p>\n<ul>\n<li><p>You should note that the left-associativity rule is <em>only</em> followed when both operators have <em>the same</em> precedence. This is not the case when you use these operators in conjunction with keywords such as <code>[[...]]</code> or <code>((...))</code> or use the <code>-o</code> and <code>-a</code> operators as arguments to the <code>test</code> or <code>[</code> commands. In such cases, AND (<code>&amp;&amp;</code> or <code>-a</code>) takes precedence over OR (<code>||</code> or <code>-o</code>). Thanks to Stephane Chazelas' comment for clarifying this point.</p>\n</li>\n<li><p>It seems that <a href=\"http://www.difranco.net/compsci/C_Operator_Precedence_Table.htm\" rel=\"noreferrer\">in C</a> and C-like languages <code>&amp;&amp;</code> has higher precedence than <code>||</code> which is probably why you expected your original construct to behave like</p>\n<pre><code>true || (echo aaa &amp;&amp; echo bbb). \n</code></pre>\n</li>\n</ul>\n<p>This is not the case with Bash, however, in which both operators have the same precedence, which is why Bash parses your expression using the left-associativity rule. Thanks to Kevin's comment for bringing this up.</p>\n<ul>\n<li>There might also be cases where <em>all 3</em> expressions are evaluated. If the first command returns a non-zero exit status, the <code>||</code> won't short circuit and goes on to execute the second command. If the second command returns with a zero exit status, then the <code>&amp;&amp;</code> won't short-circuit as well and the third command will be executed. Thanks to Ignacio Vazquez-Abrams' comment for bringing this up.</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/88850/precedence-of-the-shell-logical-operators"
  },
  {
    "question_title": "Run a command that is shadowed by an alias",
    "question_body": "<p>Let's say I have the following alias in bash - <code>alias ls='ls --color=auto'</code> - and I want to call ordinary <code>ls</code> without options. Is the only way to do that is to unalias, do the command and then alias again? Or there is some nifty trick or workaround?</p>\n",
    "answer": "<p>You can also prefix a back slash to disable the alias: <code>\\ls</code></p>\n\n<p>Edit: Other ways of doing the same include:</p>\n\n<p>Use \"command\": <code>command ls</code> as per <a href=\"https://unix.stackexchange.com/a/39302/9427\">Mikel</a>.</p>\n\n<p>Use the full path: <code>/bin/ls</code> as per <a href=\"https://unix.stackexchange.com/a/39292/9427\">uther</a>.</p>\n\n<p>Quote the command: <code>\"ls\"</code> or <code>'ls'</code> as per Mikel comment.</p>\n\n<p>You can remove the alias temporarily for that terminal session with <code>unalias command_name</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/39291/run-a-command-that-is-shadowed-by-an-alias"
  },
  {
    "question_title": "What color codes can I use in my Bash PS1 prompt?",
    "question_body": "<p>I used several colors in my bash PS1 prompt such as:</p>\n<pre><code>\\033]01;31\\] # pink\n\\033]00m\\]   # white\n\\033]01;36\\] # bold green\n\\033]02;36\\] # green\n\\033]01;34\\] # blue\n\\033]01;33\\] # bold yellow\n</code></pre>\n<p>Where can I find a list of the color codes I can use?</p>\n<p>I looked at <a href=\"https://unix.stackexchange.com/questions/74024/colorize-bash-console-color\">Colorize Bash Console Color</a> but it didn't answer my question about a list of the actual codes.</p>\n<p>It would be nice if there was a more readable form also.</p>\n<p>See also: <a href=\"https://unix.stackexchange.com/a/127800/10043\">How can I get my PS1 prompt to show time, user, host, directories, and Git branch</a></p>\n",
    "answer": "<p>Those are <a href=\"http://en.wikipedia.org/wiki/ANSI_escape_code#Colors\" rel=\"noreferrer\">ANSI escape sequences</a>; that link is to a chart of color codes but there are other interesting things on that Wikipedia page as well.  Not all of them work on (e.g.) a normal Linux console.</p>\n\n<p>This is incorrect:</p>\n\n<blockquote>\n  <p><code>\\033]00m\\]   # white</code></p>\n</blockquote>\n\n<p><code>0</code> resets the terminal to its default (which is probably white).  The actual code for white foreground is 37.  Also, the escaped closing brace at the end (<code>\\]</code>) is not part of the color sequence (see the last few paragraphs below for an explanation of their purpose in setting a prompt).</p>\n\n<p>Note that some GUI terminals allow you to specify a customized color scheme. This will affect the output.</p>\n\n<p>There's <a href=\"http://misc.flogisoft.com/bash/tip_colors_and_formatting\" rel=\"noreferrer\">a list here</a> which adds 7 foreground and 7 background colors I had not seen before, but they seem to work:</p>\n\n<pre><code># Foreground colors\n90   Dark gray  \n91   Light red  \n92   Light green    \n93   Light yellow   \n94   Light blue \n95   Light magenta  \n96   Light cyan  \n\n# Background colors\n100  Dark gray  \n101  Light red  \n102  Light green    \n103  Light yellow   \n104  Light blue \n105  Light magenta  \n106  Light cyan \n</code></pre>\n\n<p>In addition, if you have a 256 color GUI terminal (I think most of them are now), you can apply colors from this chart: </p>\n\n<p><img src=\"https://i.sstatic.net/UQVe5.png\" alt=\"xterm  256 color chart\"></p>\n\n<p>The ANSI sequence to select these, using the number in the bottom left corner, starts <code>38;5;</code> for the foreground and <code>48;5;</code> for the background, then the color number, so e.g.:</p>\n\n<pre><code>echo -e \"\\\\033[48;5;95;38;5;214mhello world\\\\033[0m\"\n</code></pre>\n\n<p>Gives me a light orange on tan (meaning, the color chart is roughly approximated).</p>\n\n<p>You can see the colors in this chart<sup>1</sup> as they would appear on your terminal fairly easily:</p>\n\n<pre><code>#!/bin/bash\n\ncolor=16;\n\nwhile [ $color -lt 245 ]; do\n    echo -e \"$color: \\\\033[38;5;${color}mhello\\\\033[48;5;${color}mworld\\\\033[0m\"\n    ((color++));\ndone  \n</code></pre>\n\n<p>The output is self-explanatory.  </p>\n\n<p>Some systems set the $TERM variable to <code>xterm-256color</code> if you are on a 256 color terminal via some shell code in <code>/etc/profile</code>.  On others, you should be able to configure your terminal to use this.  That will let TUI applications know there are 256 colors, and allow you to add something like this to your <code>~/.bashrc</code>:</p>\n\n<pre><code>if [[ \"$TERM\" =~ 256color ]]; then\n     PS1=\"MyCrazyPrompt...\"\nfi\n</code></pre>\n\n<p>Beware that when you use color escape sequences in your prompt, you should enclose them in escaped (<code>\\</code> prefixed) square brackets, like this:</p>\n\n<pre><code>PS1=\"\\[\\033[01;32m\\]MyPrompt: \\[\\033[0m\\]\"\n</code></pre>\n\n<p>Notice the <code>[</code>'s interior to the color sequence are not escaped, but the enclosing ones are.  The purpose of the latter is to indicate to the shell that the enclosed sequence does not count toward the character length of the prompt.  If that count is wrong, weird things will happen when you scroll back through the history, e.g., if it is too long, the excess length of the last scrolled string will appear attached to your prompt and you won't be able to backspace into it (it's ignored the same way the prompt is).</p>\n\n<p>Also note that if you want to include the output of a command run every time the prompt is used (as opposed to just once when the prompt is set), you should set it as a literal string with single quotes, e.g.:</p>\n\n<pre><code>PS1='\\[\\033[01;32m\\]$(date): \\[\\033[0m\\]'\n</code></pre>\n\n<p>Although this is not a great example if you are happy with using bash's special <code>\\d</code> or <code>\\D{format}</code> prompt escapes -- which are not the topic of the question but can be found in <code>man bash</code> under <code>PROMPTING</code>.  There are various other useful escapes such as <code>\\w</code> for current directory, <code>\\u</code> for current user, etc.</p>\n\n<hr>\n\n<p><sup>1. The main portion of this chart, colors 16 - 231 (notice they are not in numerical order) are a 6 x 6 x 6 RGB color cube. \"Color cube\" refers to the fact that an RGB color space can be represented using a three dimensional array (with one axis for red, one for green, and one for blue).  Each color in the cube here can be represented as coordinates in a 6 x 6 x 6 array, and the index in the chart calculated thusly:</sup></p>\n\n<pre><code>    16 + R * 36 + G * 6 + B\n</code></pre>\n\n<p><sup>The first color in the cube, at index 16 in the chart, is black (RGB 0, 0, 0).  You could use this formula in shell script:</sup></p>\n\n<pre><code>#!/bin/sh                                                         \n\nfunction RGBcolor {                                               \n    echo \"16 + $1 * 36 + $2 * 6 + $3\" | bc                        \n}                                                                 \n\nfg=$(RGBcolor 1 0 2)  # Violet                                            \nbg=$(RGBcolor 5 3 0)  # Bright orange.                                            \n\necho -e \"\\\\033[1;38;5;$fg;48;5;${bg}mviolet on tangerine\\\\033[0m\"\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/124407/what-color-codes-can-i-use-in-my-bash-ps1-prompt"
  },
  {
    "question_title": "Generate File of a certain size?",
    "question_body": "<p>I'd like to generate a file with the name <code>example.file</code>. I could use</p>\n\n<pre><code>touch example.file\n</code></pre>\n\n<p>but I want the file to be exactly 24MB in size. I already checked the manpage of touch, but there is no parameter like this. Is there an easy way to generate files of a certain size?</p>\n",
    "answer": "<p>You can use dd:</p>\n\n<pre><code>dd if=/dev/zero of=output.dat  bs=24M  count=1\n</code></pre>\n\n<p>or</p>\n\n<pre><code>dd if=/dev/zero of=output.dat  bs=1M  count=24\n</code></pre>\n\n<p>or, on Mac,</p>\n\n<pre><code>dd if=/dev/zero of=output.dat  bs=1m  count=24\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/101332/generate-file-of-a-certain-size"
  },
  {
    "question_title": "How to suspend and resume processes",
    "question_body": "<p>In the bash terminal I can hit <kbd>Control</kbd>+<kbd>Z</kbd> to suspend any running process... then I can type <code>fg</code> to resume the process.</p>\n\n<p>Is it possible to suspend a process if I only have it's PID? And if so, what command should I use?</p>\n\n<p>I'm looking for something like:</p>\n\n<pre><code>suspend-process $PID_OF_PROCESS\n</code></pre>\n\n<p>and then to resume it with</p>\n\n<pre><code>resume-process $PID_OF_PROCESS\n</code></pre>\n",
    "answer": "<p>You can use <code>kill</code> to stop the process.</p>\n<p>For a 'polite' stop to the process (prefer this for normal use), send <code>SIGTSTP</code>:</p>\n<pre><code>kill -TSTP [pid]\n</code></pre>\n<p>For a 'hard' stop, send <code>SIGSTOP</code>:</p>\n<pre><code>kill -STOP [pid]\n</code></pre>\n<p>Note that if the process you are trying to stop by PID is in your shell's job table, it may remain visible there, but terminated, until the process is <code>fg</code>'d again.</p>\n<p>To resume execution of the process, sent <code>SIGCONT</code>:</p>\n<pre><code>kill -CONT [pid]\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/2107/how-to-suspend-and-resume-processes"
  },
  {
    "question_title": "Is there any way to execute commands from history?",
    "question_body": "<p>For example:</p>\n\n<pre><code>[root@ip-10-0-7-125 ~]# history | grep free\n  594  free -m\n  634  free -m | xargs | awk '{print \"free/total memory\" $17 \" / \" $ 8}'\n  635  free -m\n  636  free -m | xargs | awk '{print \"free/total memory\" $9 \" / \" $ 10}'\n  736  df -h | xargs |  awk '{print \"free/total disk: \" $11 \" / \" $9}'\n  740  df -h | xargs |  awk '{print \"free/total disk: \" $11 \" / \" $8}'\n  741  free -m | xargs | awk '{print \"free/total memory: \" $17 \" / \" $8 \" MB\"}'\n</code></pre>\n\n<p>I'm just wondering if there any way to execute the 636 command without typing it again, just type something plus the number, like history 636 or something.</p>\n",
    "answer": "<p>In bash, just <code>!636</code> will be ok.</p>\n",
    "url": "https://unix.stackexchange.com/questions/275053/is-there-any-way-to-execute-commands-from-history"
  },
  {
    "question_title": "Batch renaming files",
    "question_body": "<p>I have a directory full of images:</p>\n\n<pre><code>image0001.png\nimage0002.png\nimage0003.png\n...\n</code></pre>\n\n<p>And I would like a one-liner to rename them to (say).</p>\n\n<pre><code>0001.png\n0002.png\n0003.png\n...\n</code></pre>\n\n<p>How do I do this?</p>\n",
    "answer": "<p>On Debian and derivatives, <a href=\"https://unix.stackexchange.com/a/727288/12574\">Perl's <code>rename</code> commandline</a> works similarly to <code>sed</code> like this:</p>\n<pre><code>  rename -v 's/image//' ./*.png\n</code></pre>\n<p>There's also the <code>rename</code> from <code>util-linux</code> that works like this, instead:</p>\n<pre><code>  rename -- image '' *.png\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/1136/batch-renaming-files"
  },
  {
    "question_title": "Temporarily suspend bash_history on a given shell?",
    "question_body": "<p>Is there a way to temporarily suspend history tracking in bash, so as to enter a sort of \"incognito\" mode? I'm entering stuff into my terminal that I don't want recorded, sensitive financial info. </p>\n",
    "answer": "<p>This will prevent bash from saving any new history when exiting the shell:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>unset HISTFILE\n</code></pre>\n<p>Specifically, according to <code>man bash</code>:</p>\n<blockquote>\n<p>If HISTFILE is unset, or if the history file is unwritable, the history is not saved.</p>\n</blockquote>\n<p>Note that if you re-set <code>HISTFILE</code>, history will be saved normally. This only affects the moment the shell session ends.</p>\n<p>Alternatively, if you want to toggle it off and then back on again during a session, it may be easier to use <a href=\"https://ss64.com/bash/set.html\" rel=\"noreferrer\"><code>set</code></a>:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>set +o history # temporarily turn off history\n\n# commands here won't be saved\n\nset -o history # turn it back on\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/10922/temporarily-suspend-bash-history-on-a-given-shell"
  },
  {
    "question_title": "how can I add (subtract, etc.) two numbers with bash?",
    "question_body": "<p>I can read the numbers and operation in with:</p>\n\n<pre><code>echo \"First number please\"\nread num1\necho \"Second number please\"\nread num2\necho \"Operation?\"\nread op\n</code></pre>\n\n<p>but then all my attempts to add the numbers fail:</p>\n\n<pre><code>case \"$op\" in\n  \"+\")\n    echo num1+num2;;\n  \"-\")\n    echo `num1-num2`;;\nesac\n</code></pre>\n\n<p>Run:</p>\n\n<pre><code>First number please\n1\nSecond mumber please\n2\nOperation?\n+\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>num1+num2\n</code></pre>\n\n<p>...or...</p>\n\n<pre><code>echo $num1+$num2;;\n\n# results in: 1+2    \n</code></pre>\n\n<p>...or...</p>\n\n<pre><code>echo `$num1`+`$num2`;;\n\n# results in: ...line 9: 1: command not found\n</code></pre>\n\n<p>Seems like I'm getting strings still perhaps when I try add add (\"2+2\" instead of \"4\").</p>\n",
    "answer": "<p><a href=\"http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_04\" rel=\"noreferrer\">Arithmetic in POSIX shells</a> is done with <code>$</code> and double parentheses <code>(( ))</code>:</p>\n<pre><code>echo &quot;$(($num1+$num2))&quot;\n</code></pre>\n<p>You can assign from that; also note the <code>$</code> operators on the variable names inside <code>(())</code> are optional):</p>\n<pre><code>num1=&quot;$((num1+num2))&quot;\n</code></pre>\n<p>There is also <code>expr</code>:</p>\n<pre><code>expr $num1 + $num2\n</code></pre>\n<p>In scripting <code>$(())</code> is preferable since it avoids a fork/execute for the <code>expr</code> command. In addition, <code>expr</code>’s exit status can be confusing: it’s 0 if the result is <em>not</em> empty or 0, 1 if the result is empty or 0 (this is less confusing if one considers that 0 is conventionally a &quot;normal&quot; exit status and != 0 an error or unusual state).</p>\n",
    "url": "https://unix.stackexchange.com/questions/93029/how-can-i-add-subtract-etc-two-numbers-with-bash"
  },
  {
    "question_title": "Quoting within $(command substitution) in Bash",
    "question_body": "\n<p>In my Bash environment I use variables containing spaces, and I use these variables within command substitution.</p>\n<p>What is the correct way to quote my variables? And how should I do it if these are nested?</p>\n<pre class=\"lang-sh prettyprint-override\"><code>DIRNAME=$(dirname &quot;$FILE&quot;)\n</code></pre>\n<p>or do I quote outside the substitution?</p>\n<pre class=\"lang-sh prettyprint-override\"><code>DIRNAME=&quot;$(dirname $FILE)&quot;\n</code></pre>\n<p>or both?</p>\n<pre class=\"lang-sh prettyprint-override\"><code>DIRNAME=&quot;$(dirname &quot;$FILE&quot;)&quot;\n</code></pre>\n<p>or do I use back-ticks?</p>\n<pre class=\"lang-sh prettyprint-override\"><code>DIRNAME=`dirname &quot;$FILE&quot;`\n</code></pre>\n<p>What is the right way to do this? And how can I easily check if the quotes are set right?</p>\n",
    "answer": "\n<p>In order from worst to best:</p>\n<ul>\n<li><code>DIRNAME=&quot;$(dirname $FILE)&quot;</code> <a href=\"https://mywiki.wooledge.org/Quotes\" rel=\"noreferrer\">will not do what you want</a> if <code>$FILE</code> contains whitespace (or whatever characters <code>$IFS</code> currently contains) or globbing characters <code>\\[?*</code>.</li>\n<li><code>DIRNAME=`dirname &quot;$FILE&quot;`</code> is technically correct, but <a href=\"https://mywiki.wooledge.org/BashFAQ/082\" rel=\"noreferrer\">backticks are not recommended for command expansion</a> because of the extra complexity when nesting them and the extra backslash processing that happens within them.</li>\n<li><code>DIRNAME=$(dirname &quot;$FILE&quot;)</code> is correct, but <a href=\"https://mywiki.wooledge.org/BashPitfalls#for_i_in_.24.28ls_.2A.mp3.29/BashPitfalls#for_i_in_.24.28ls_.2A.mp3.29\" rel=\"noreferrer\">only because this is an assignment to a scalar (not array) variable</a>. If you use the command substitution in any other context, such as <code>export DIRNAME=$(dirname &quot;$FILE&quot;)</code> or <code>du $(dirname -- &quot;$FILE&quot;)</code>, the lack of quotes will cause trouble if the result of the expansion contain whitespace or globbing characters.</li>\n<li><code>DIRNAME=&quot;$(dirname &quot;$FILE&quot;)&quot;</code> (except for the missing <code>--</code>, see below) is the recommended way. You can replace <code>DIRNAME=</code> with a command and a space without changing anything else, and <code>dirname</code> receives the correct string.</li>\n</ul>\n<p>To improve even further:</p>\n<ul>\n<li><code>DIRNAME=&quot;$(dirname -- &quot;$FILE&quot;)&quot;</code> works if <code>$FILE</code> starts with a dash.</li>\n<li><code>DIRNAME=&quot;$(dirname -- &quot;$FILE&quot; &amp;&amp; printf x)&quot; &amp;&amp; DIRNAME=&quot;${DIRNAME%?x}&quot; || exit</code> works even if <code>$FILE</code>'s dirname ends with a newline, since <code>$()</code> chops off newlines at the end of output, both the one added by <code>dirname</code> and the ones that may be part of the actual data.</li>\n</ul>\n<p>You can nest command expansions as much as you like. With <code>$()</code> you always create a new quoting context, so you can do things like this:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>foo &quot;$(bar &quot;$(baz &quot;$(ban &quot;bla&quot;)&quot;)&quot;)&quot;\n</code></pre>\n<p>You do <em>not</em> want to try that with backticks.</p>\n",
    "url": "https://unix.stackexchange.com/questions/118433/quoting-within-command-substitution-in-bash"
  },
  {
    "question_title": "How to combine 2 -name conditions in find?",
    "question_body": "<p>I would like to search for files that would not match 2 <code>-name</code> conditions. I can do it like so : </p>\n\n<pre><code>find /media/d/ -type f -size +50M ! -name \"*deb\" ! -name \"*vmdk\"\n</code></pre>\n\n<p>and this will yield proper result but can I join these 2 condition with OR somehow ?</p>\n",
    "answer": "<p>yes, you can:</p>\n\n<pre><code>find /media/d/ -type f -size +50M ! \\( -name \"*deb\" -o -name \"*vmdk\" \\)\n</code></pre>\n\n<p>Explanation from the <a href=\"http://pubs.opengroup.org/onlinepubs/009695399/utilities/find.html\">POSIX spec</a>:</p>\n\n<blockquote>\n  <p><strong>!</strong>  <em>expression</em> : Negation of a primary; the unary NOT operator.</p>\n  \n  <p>( <em>expression</em> ): True if expression is true. </p>\n  \n  <p><em>expression</em> <strong>-o</strong>  <em>expression</em>: Alternation of primaries; the OR operator. The second expression shall not be evaluated if the first expression is true.</p>\n</blockquote>\n\n<p>Note that parenthesis, both opening and closing, are prefixed by a backslash (<code>\\</code>) to prevent evaluation by the shell.</p>\n",
    "url": "https://unix.stackexchange.com/questions/50612/how-to-combine-2-name-conditions-in-find"
  },
  {
    "question_title": "How can we run a command stored in a variable?",
    "question_body": "<pre><code>$ ls -l /tmp/test/my\\ dir/\ntotal 0\n</code></pre>\n\n<p>I was wondering why the following ways to run the above command fail or succeed?</p>\n\n<pre><code>$ abc='ls -l \"/tmp/test/my dir\"'\n\n$ $abc\nls: cannot access '\"/tmp/test/my': No such file or directory\nls: cannot access 'dir\"': No such file or directory\n\n$ \"$abc\"\nbash: ls -l \"/tmp/test/my dir\": No such file or directory\n\n$ bash -c $abc\n'my dir'\n\n$ bash -c \"$abc\"\ntotal 0\n\n$ eval $abc\ntotal 0\n\n$ eval \"$abc\"\ntotal 0\n</code></pre>\n",
    "answer": "<p>This has been discussed in a number of questions on unix.SE, I'll try to collect all issues I can come up with here. Below is</p>\n<ul>\n<li>a description of why and how the various attempts fail,</li>\n<li>a way to do it properly with a function (for a fixed command), or</li>\n<li>with shell arrays (Bash/ksh/zsh) or the <code>$@</code> pseudo-array (POSIX sh), both of which also allow building the command line pieces, if you e.g. only need to vary some optoins</li>\n<li>and notes about using <code>eval</code> to do this.</li>\n</ul>\n<p>Some references at the end.</p>\n<p>For the purposes here, it doesn't matter much if it's only the command arguments or also the command name that is to be stored in a variable. They're processed similarly up to the point where the command is launched, at which point the shell just takes the first word as the name of the command to run.</p>\n<hr />\n<h3>Why it fails</h3>\n<p>The reason you face those problems is the fact that <a href=\"https://mywiki.wooledge.org/WordSplitting\" rel=\"noreferrer\">word splitting</a> is quite simple and doesn't lend itself to complex cases, and the fact that quotes expanded from variables don't act as quotes, but are just ordinary characters.</p>\n<p><sub>(Note that the part about quotes is similar to every other programming language: e.g. <code>char *s = &quot;foo()&quot;; printf(&quot;%s\\n&quot;, s)</code> does not call the function <code>foo()</code> in C, but just prints the string <code>foo()</code>. That's different in macro processors, like m4, the C preprocessor, or Make (to some extent). The shell is a programming language, not a macro processor.)</sub></p>\n<p>On Unix-like systems, it's the shell that processes quotes and variable expansions on the command line, turning it from a single string into the list of strings that the underlying system call passes to the launched command. The program itself doesn't see the quotes the shell processed. E.g. if given the command <code>ls -l &quot;foo bar&quot;</code>, the shell turns that into the three strings <code>ls</code>, <code>-l</code> and <code>foo bar</code> (removing the quotes), and passes those to <code>ls</code>. <sub>(Even the command name is passed, though not all programs use it.)</sub></p>\n<p><strong>The cases presented in the question:</strong></p>\n<p>The assignment here assigns the single string <code>ls -l &quot;/tmp/test/my dir&quot;</code> to <code>abc</code>:</p>\n<pre><code>$ abc='ls -l &quot;/tmp/test/my dir&quot;'\n</code></pre>\n<p>Below, <code>$abc</code> is split on whitespace, and <code>ls</code> gets the three arguments <code>-l</code>, <code>&quot;/tmp/test/my</code> and <code>dir&quot;</code>. The quotes here are just data, so there's one at the front of the second argument and another at the back of the third. The option works, but the path gets incorrectly processed as <code>ls</code> sees the quotes as part of the filenames:</p>\n<pre><code>$ $abc\nls: cannot access '&quot;/tmp/test/my': No such file or directory\nls: cannot access 'dir&quot;': No such file or directory\n</code></pre>\n<p>Here, the expansion is quoted, so it's kept as a single word. The shell tries to find a program literally called <code>ls -l &quot;/tmp/test/my dir&quot;</code>, spaces and quotes included.</p>\n<pre><code>$ &quot;$abc&quot;\nbash: ls -l &quot;/tmp/test/my dir&quot;: No such file or directory\n</code></pre>\n<p>And here, <code>$abc</code> is split, and only the first resulting word is taken as the argument to <code>-c</code>, so Bash just runs <code>ls</code> in the current directory. The other words are arguments to bash, and are used to fill <code>$0</code>, <code>$1</code>, etc.</p>\n<pre><code>$ bash -c $abc\n'my dir'\n</code></pre>\n<p>With <code>bash -c &quot;$abc&quot;</code>, and <code>eval &quot;$abc&quot;</code>, there's an additional shell processing step, which does make the quotes work, but <em>also causes all shell expansions to be processed again</em>, so there's a risk of accidentally running e.g. a command substitution from user-provided data, unless you're very careful about quoting.</p>\n<hr />\n<h3>Better ways to do it</h3>\n<p>The two better ways to store a command are a) use a function instead, b) use an array variable (or the positional parameters).</p>\n<p><strong>Using functions:</strong></p>\n<p>Simply declare a function with the command inside, and run the function as if it were a command. Expansions in commands within the function are only processed when the command runs, not when it's defined, and you don't need to quote the individual commands. Though this really only helps if you have a fixed command  you need to store (or more than one fixed command).</p>\n<pre><code># define it\nmyls() {\n    ls -l &quot;/tmp/test/my dir&quot;\n}\n\n# run it\nmyls\n</code></pre>\n<p>It's also possible to define multiple functions and use a variable to store the name of the function you want to run in the end.</p>\n<p><strong>Using an array:</strong></p>\n<p>Arrays allow creating multi-word variables where the individual words contain white space. Here, the individual words are stored as distinct array elements, and the <code>&quot;${array[@]}&quot;</code> expansion expands each element as separate shell words:</p>\n<pre><code># define the array\nmycmd=(ls -l &quot;/tmp/test/my dir&quot;)\n\n# expand the array, run the command\n&quot;${mycmd[@]}&quot;\n</code></pre>\n<p>The command is written inside the parentheses exactly as it would be written when running the command. The processing the shell does is the same in both cases, just in one it only saves the resulting list of strings, instead of using it to run a program.</p>\n<p>The syntax for expanding the array later is slightly horrible, though, and the quotes around it are important.</p>\n<p>Arrays also allow you to build the command line piece-by-piece. For example:</p>\n<pre><code>mycmd=(ls)               # initial command\nif [ &quot;$want_detail&quot; = 1 ]; then\n    mycmd+=(-l)          # optional flag, append to array\nfi\nmycmd+=(&quot;$targetdir&quot;)    # the filename\n\n&quot;${mycmd[@]}&quot;\n</code></pre>\n<p>or keep parts of the command line constant and use the array fill just a part of it, like options or filenames:</p>\n<pre><code>options=(-x -v)\nfiles=(file1 &quot;file name with whitespace&quot;)\ntarget=/somedir\n\nsomecommand &quot;${options[@]}&quot; &quot;${files[@]}&quot; &quot;$target&quot;\n</code></pre>\n<p><sub>(<code>somecommand</code> being a generic placeholder name here, not any real command.)</sub></p>\n<p>The downside of arrays is that they're not a standard feature, so plain POSIX shells (like <code>dash</code>, the default <code>/bin/sh</code> in Debian/Ubuntu) don't support them (but see below). Bash, ksh and zsh do, however, so it's likely your system has some shell that supports arrays.</p>\n<p><strong>Using <code>&quot;$@&quot;</code></strong></p>\n<p>In shells with no support for named arrays, one can still use the positional parameters (the pseudo-array <code>&quot;$@&quot;</code>) to hold the arguments of a command.</p>\n<p>The following should be portable script bits that do the equivalent of the code bits in the previous section. The array is replaced with <code>&quot;$@&quot;</code>, the list of positional parameters.  Setting <code>&quot;$@&quot;</code> is done with <code>set</code>, and the double quotes around <code>&quot;$@&quot;</code> are important (these cause the elements of the list to be individually quoted).</p>\n<p>First, simply storing a command with arguments in <code>&quot;$@&quot;</code> and running it:</p>\n<pre><code>set -- ls -l &quot;/tmp/test/my dir&quot;\n&quot;$@&quot;\n</code></pre>\n<p>Conditionally setting parts of the command line options for a command:</p>\n<pre><code>set -- ls\nif [ &quot;$want_detail&quot; = 1 ]; then\n    set -- &quot;$@&quot; -l\nfi\nset -- &quot;$@&quot; &quot;$targetdir&quot;\n\n&quot;$@&quot;\n</code></pre>\n<p>Only using <code>&quot;$@&quot;</code> for options and operands:</p>\n<pre><code>set -- -x -v\nset -- &quot;$@&quot; file1 &quot;file name with whitespace&quot;\nset -- &quot;$@&quot; /somedir\n\nsomecommand &quot;$@&quot;\n</code></pre>\n<p>Of course, <code>&quot;$@&quot;</code> is usually filled with the arguments to the script itself, so you'll have to save them somewhere before re-purposing <code>&quot;$@&quot;</code>.</p>\n<p>To conditionally pass a single argument, you can also use the alternate value expansion <code>${var:+word}</code> with some careful quoting. Here, we include <code>-f</code>  and the filename only if the filename is nonempty:</p>\n<pre><code>file=&quot;foo bar&quot;\nsomecommand ${file:+-f &quot;$file&quot;}\n</code></pre>\n<hr />\n<h3>Using <code>eval</code> (be careful here!)</h3>\n<p><code>eval</code> takes a string and runs it as a command, just like if it was entered on the shell command line. This includes all quote and expansion processing, which is both useful and dangerous.</p>\n<p>In the simple case, it allows doing just what we want:</p>\n<pre><code>cmd='ls -l &quot;/tmp/test/my dir&quot;'\neval &quot;$cmd&quot;\n</code></pre>\n<p>With <code>eval</code>, the quotes are processed, so <code>ls</code> eventually sees just the two arguments <code>-l</code> and <code>/tmp/test/my dir</code>, like we want. <code>eval</code> is also smart enough to concatenate any arguments it gets, so <code>eval $cmd</code> could also work in some cases, but e.g. all runs of whitespace would be changed to single spaces. It's still better to quote the variable there as that will ensure it gets unmodified to <code>eval</code>.</p>\n<p>However, <strong>it's dangerous to include user input in the command string to <code>eval</code></strong>. For example, this seems to work:</p>\n<pre><code>read -r filename\ncmd=&quot;ls -ld '$filename'&quot;\neval &quot;$cmd&quot;;\n</code></pre>\n<p><strong>But if the user gives input that contains single quotes, they can break out of the quoting and run arbitrary commands!</strong> E.g. with the input <code>'$(whatever)'.txt</code>, your script happily runs the command substitution. That it could have been <code>rm -rf</code> (or worse) instead.</p>\n<p>The issue there is that the value of <code>$filename</code> was embedded in the command line that <code>eval</code> runs. It was expanded before <code>eval</code>, which saw e.g. the command <code>ls -l ''$(whatever)'.txt'</code>. You would need to pre-process the input to be safe.</p>\n<p>If we do it the other way, keeping the filename in the variable, and letting the <code>eval</code> command expand it, it's safer again:</p>\n<pre><code>read -r filename\ncmd='ls -ld &quot;$filename&quot;'\neval &quot;$cmd&quot;;\n</code></pre>\n<p>Note the outer quotes are now single quotes, so expansions within do not happen. Hence, <code>eval</code> sees the command <code>ls -l &quot;$filename&quot;</code> and expands the filename safely itself.</p>\n<p>But that's not much different from just storing the command in a function or an array. With functions or arrays, there is no such problem since the words are kept separate for the whole time, and there's no quote or other processing for the contents of <code>filename</code>.</p>\n<pre><code>read -r filename\ncmd=(ls -ld -- &quot;$filename&quot;)\n&quot;${cmd[@]}&quot;\n</code></pre>\n<p>Pretty much the only reason to use <code>eval</code> is one where the varying part involves shell syntax elements that can't be brought in via variables (pipelines, redirections, etc.). However, you'll then need to quote/escape everything <em>else</em> on the command line that needs protection from the additional parsing step (see link below). In any case, it's best to <strong>avoid</strong> embedding input from the user in the <code>eval</code> command!</p>\n<hr />\n<h3>References</h3>\n<ul>\n<li><a href=\"https://mywiki.wooledge.org/WordSplitting\" rel=\"noreferrer\">Word Splitting</a> in <a href=\"https://mywiki.wooledge.org/BashGuide\" rel=\"noreferrer\">BashGuide</a></li>\n<li><a href=\"http://mywiki.wooledge.org/BashFAQ/050\" rel=\"noreferrer\">BashFAQ/050 or &quot;I'm trying to put a command in a variable, but the complex cases always fail!&quot;</a></li>\n<li>The question <a href=\"https://unix.stackexchange.com/q/131766/170373\">Why does my shell script choke on whitespace or other special characters?</a>, which discusses a number of issues related to quoting and whitespace, including storing commands.</li>\n<li><a href=\"https://unix.stackexchange.com/q/379181/170373\">Escape a variable for use as content of another script</a></li>\n<li><a href=\"https://unix.stackexchange.com/q/562870/170373\">How can I conditionally pass an argument from a POSIX shell script?</a></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/444946/how-can-we-run-a-command-stored-in-a-variable"
  },
  {
    "question_title": "How do I add X days to date and get new date?",
    "question_body": "<p>I  have Linux ( RH 5.3) machine</p>\n\n<p>I need to add/calculate 10 days plus date so then I will get new date (expiration date))</p>\n\n<p>for example</p>\n\n<pre><code> # date \n Sun Sep 11 07:59:16 IST 2012\n</code></pre>\n\n<p>So I need to get </p>\n\n<pre><code>     NEW_expration_DATE = Sun Sep 21 07:59:16 IST 2012\n</code></pre>\n\n<p>Please advice how to calculate the new expiration date ( with bash , ksh , or manipulate date command ?)</p>\n",
    "answer": "<p>You can just use the <code>-d</code> switch and provide a date to be calculated</p>\n<pre><code>date\nSun Sep 23 08:19:56 BST 2012\nNEW_expration_DATE=$(date -d &quot;+10 days&quot;)\necho $NEW_expration_DATE\nWed Oct 3 08:12:33 BST 2012 \n</code></pre>\n<blockquote>\n<pre><code>  -d, --date=STRING\n</code></pre>\n</blockquote>\n<pre><code>          display time described by STRING, not ‘now’\n</code></pre>\n<p>This is quite a powerful tool as you can do things like</p>\n<pre><code>date -d &quot;Sun Sep 11 07:59:16 IST 2012+10 days&quot;\nFri Sep 21 03:29:16 BST 2012\n</code></pre>\n<p>or</p>\n<pre><code>TZ=IST date -d &quot;Sun Sep 11 07:59:16 IST 2012+10 days&quot;\nFri Sep 21 07:59:16 IST 2012\n</code></pre>\n<p>or</p>\n<pre><code>prog_end_date=`date '+%C%y%m%d' -d &quot;$end_date+10 days&quot;`\n</code></pre>\n<p>So if <code>$end_date</code> = <code>20131001</code> then <code>$prog_end_date</code> = <code>20131011</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/49053/how-do-i-add-x-days-to-date-and-get-new-date"
  },
  {
    "question_title": "How to uppercase the command line argument?",
    "question_body": "<p>I searched SO and found that to uppercase a string following would work</p>\n\n<pre><code>str=\"Some string\"\necho ${str^^}\n</code></pre>\n\n<p>But I tried to do a similar thing on a command-line argument, which gave me the following error</p>\n\n<h3>Tried</h3>\n\n<pre><code>#!/bin/bash\n             ## Output\necho ${1^^}  ## line 3: ${1^^}: bad substitution\necho {$1^^}  ## No error, but output was still smaller case i.e. no effect\n</code></pre>\n\n<p>How could we do this?</p>\n",
    "answer": "<p>The syntax <code>str^^</code> which you are trying is available from Bash 4.0 and above. Perhaps yours is an older version (or you ran the script with <code>sh</code> explicitly):</p>\n\n<p>Try this:</p>\n\n<pre><code>str=\"Some string\"\nprintf '%s\\n' \"$str\" | awk '{ print toupper($0) }'\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/51983/how-to-uppercase-the-command-line-argument"
  },
  {
    "question_title": "How do I delete the first n lines of an ascii file using shell commands?",
    "question_body": "<p>I have multiple files that contain ascii text information in the first 5-10 lines, followed by well-tabulated matrix information.  In a shell script, I want to remove these first few lines of text so that I can use the pure matrix information in another program.  How can I use bash shell commands to do this?  </p>\n\n<p>If it's any help, I'm using RedHat and an Ubuntu linux systems.</p>\n",
    "answer": "<p>As long as the file is not a symlink or hardlink, you can use sed, tail, or awk. Example below.</p>\n\n<pre><code>$ cat t.txt\n12\n34\n56\n78\n90\n</code></pre>\n\n<h2>sed</h2>\n\n<pre><code>$ sed -e '1,3d' &lt; t.txt\n78\n90\n</code></pre>\n\n<p>You can also use sed in-place without a temp file: <code>sed -i -e 1,3d yourfile</code>. This won't echo anything, it will just modify the file in-place. If you don't need to pipe the result to another command, this is easier.</p>\n\n<h2>tail</h2>\n\n<pre><code>$ tail -n +4 t.txt\n78\n90\n</code></pre>\n\n<h2>awk</h2>\n\n<pre><code>$ awk 'NR &gt; 3 { print }' &lt; t.txt\n78\n90\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/37790/how-do-i-delete-the-first-n-lines-of-an-ascii-file-using-shell-commands"
  },
  {
    "question_title": "Can I &quot;export&quot; functions in bash?",
    "question_body": "<pre><code>source some_file\n</code></pre>\n\n<p>some_file:</p>\n\n<pre><code>doit ()\n{\n  echo doit $1\n}\nexport TEST=true\n</code></pre>\n\n<p>If I source <strong>some_file</strong> the function \"doit\" and the variable TEST are available on the command line. But running this script:</p>\n\n<p>script.sh:</p>\n\n<pre><code>#/bin/sh\necho $TEST\ndoit test2\n</code></pre>\n\n<p>Will return the value of TEST, but will generate an error about the unknown function \"doit\".</p>\n\n<p>Can I \"export\" the function, too, or do I have to source some_file in script.sh to use the function there?</p>\n",
    "answer": "<p>In Bash you can export function definitions to other shell scripts that your script calls with</p>\n<pre><code>export -f function_name\n</code></pre>\n<p>For example you can try this simple example:</p>\n<h3><code>./script1</code>:</h3>\n<pre><code>#!/bin/bash\n\nmyfun() {\n    echo &quot;Hello!&quot;\n}\n\nexport -f myfun\n./script2\n</code></pre>\n<h3><code>./script2</code>:</h3>\n<pre><code>#!/bin/bash\n\nmyfun\n</code></pre>\n<p>Then if you call <code>./script1</code> you will see the output <em>Hello!</em>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/22796/can-i-export-functions-in-bash"
  },
  {
    "question_title": "Why ZSH ends a line with a highlighted percent symbol?",
    "question_body": "<p>I've noticed this on occasion with a variety of applications. I've often thought it was because the output was cancelled early (ctrl+c, for example) or something similar, and zsh is filling in a new line character. But now curiosity has gotten the best of me, since it doesn't seem to do this in bash. </p>\n\n<p>zsh</p>\n\n<p><img src=\"https://i.sstatic.net/2nhNU.png\" alt=\"zsh\"></p>\n\n<p>bash</p>\n\n<p><img src=\"https://i.sstatic.net/msl0g.png\" alt=\"enter image description here\"></p>\n\n<p>The <code>Sequence</code> program is something I pulled from a book while reading on Java certifications and just wanted to see if it would compile and run. I did notice that it does not use the <code>println()</code> method from the <code>System.out</code> package/class. Instead it uses plain old <code>print()</code>.</p>\n\n<p>Is the lack of a new line character the reason I get this symbol? </p>\n",
    "answer": "<p>Yes, this happens because it is a \"partial line\". And by default <a href=\"http://zsh.sourceforge.net/Doc/Release/Options.html#Prompting\">zsh goes to the next line to avoid covering it with the prompt</a>.</p>\n\n<blockquote>\n  <p>When a partial line is preserved, by default you will see an\n  inverse+bold character at the end of the partial line: a \"%\" for a\n  normal user or a \"#\" for root. If set, the shell parameter\n  PROMPT_EOL_MARK can be used to customize how the end of partial lines\n  are shown.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/167582/why-zsh-ends-a-line-with-a-highlighted-percent-symbol"
  },
  {
    "question_title": "Can&#39;t use exclamation mark (!) in bash?",
    "question_body": "<p>I'm trying to use the curl command to access a http url with a exclamation mark (<code>!</code>) in its path. e.g:</p>\n\n<pre><code>curl -v \"http://example.org/!287s87asdjh2/somepath/someresource\"\n</code></pre>\n\n<p>the console replies with <code>bash: ... event not found</code>.</p>\n\n<p>What is going on here? and what would be the proper syntax to escape the exclamation mark?</p>\n",
    "answer": "<p>The exclamation mark is part of <a href=\"https://www.gnu.org/software/bash/manual/html_node/Event-Designators.html\" rel=\"noreferrer\">history expansion</a> in bash.  To use it you need it <a href=\"https://www.gnu.org/software/bash/manual/html_node/History-Interaction.html#:%7E:text=When%20using%20the%20shell%2C\" rel=\"noreferrer\">enclosed in single quotes</a> (eg: <code>'http://example.org/!132'</code>).</p>\n<p>You might try to directly escape it with a backslash (<code>\\</code>) before the character (eg: <code>&quot;http://example.org/\\!132&quot;</code>). However, even though a backslash before the exclamation mark does prevent history expansion, the backslash is <em>not</em> removed on some shells in such a case.  So it's better to use single quotes, so you're not passing a literal backslash to <code>curl</code> as part of the URL.</p>\n",
    "url": "https://unix.stackexchange.com/questions/33339/cant-use-exclamation-mark-in-bash"
  },
  {
    "question_title": "How to run a program in a clean environment in bash?",
    "question_body": "<p>I want to run a program in an empty environment (i.e. with no envariables set). How to do this in bash?</p>\n",
    "answer": "<p>You can do this with <code>env</code>:</p>\n\n<pre><code>env -i your_command\n</code></pre>\n\n<p>Contrary to comments below, this <em>does</em> completely clear out the environment, but it does not prevent <code>your_command</code> setting new variables. In particular, running a shell will cause the <code>/etc/profile</code> to run, and the shell may have some built in settings also.</p>\n\n<p>You can check this with:</p>\n\n<pre><code>env -i env\n</code></pre>\n\n<p>i.e. wipe the environment and then print it. The output will be blank.</p>\n",
    "url": "https://unix.stackexchange.com/questions/48994/how-to-run-a-program-in-a-clean-environment-in-bash"
  },
  {
    "question_title": "Bash - how to run a command after the previous finished?",
    "question_body": "<p>Currently I have a <code>script.sh</code> file with the following content:</p>\n\n<pre><code>#!/bin/bash\nwget -q http://exemple.com/page1.php;\nwget -q http://exemple.com/page2.php;\nwget -q http://exemple.com/page3.php;\n</code></pre>\n\n<p>I want to execute the commands one by one, when the previous finishes. Am I doing it in the right way? I've never worked with Linux before and tried to search for it but found no solutions.</p>\n",
    "answer": "<p>Yes, you're doing it the right way. Shell scripts will run each command sequentially, waiting for the first to finish before the next one starts. You can either join commands with <code>;</code> or have them on separate lines:</p>\n\n<pre><code>command1; command2\n</code></pre>\n\n<p>or </p>\n\n<pre><code>command1\ncommand2\n</code></pre>\n\n<p>There is no need for <code>;</code> if the commands are on separate lines. You can also choose to run the second command only if the first exited successfully. To do so, join them with <code>&amp;&amp;</code>:</p>\n\n<pre><code>command1 &amp;&amp; command2\n</code></pre>\n\n<p>or </p>\n\n<pre><code>command1 &amp;&amp;\ncommand2\n</code></pre>\n\n<p>For more information on the various control operators available to you, see <a href=\"https://unix.stackexchange.com/q/159513/22222\">here</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/184502/bash-how-to-run-a-command-after-the-previous-finished"
  },
  {
    "question_title": "How to redirect output to a file from within cron?",
    "question_body": "<p>I have a backup script which I need to run at a particular time of a day so I am using <code>cron</code> for this task and from within cron am also trying to redirect the output of backup script to a <code>logfile</code>.</p>\n\n<p><code>crontab -e</code></p>\n\n<pre><code>*/1 * * * * /home/ranveer/backup.sh &amp;&gt;&gt; /home/ranveer/backup.log\n</code></pre>\n\n<p>In the above cron entry I am redirecting both <code>stderr and stdout</code> to a log file.</p>\n\n<p>The above cron job executes fine according to <code>syslog</code>  and it performs the task mentioned in the <code>backup.sh</code> file but it doesn't write anything to the log file.</p>\n\n<p><code>/var/log/syslog</code></p>\n\n<pre><code>Oct 19 20:26:01 ranveer CRON[15214]: (ranveer) CMD (/home/ranveer/backup.sh &amp;&gt;&gt; /home/ranveer/backup.log)\n</code></pre>\n\n<p>When I run the script from cli it works as required and output is written to a log file</p>\n\n<pre><code>ranveer@ranveer:~$ ./backup.sh &amp;&gt;&gt; backup.log \nranveer@ranveer:~$ cat backup.log\nFri Oct 19 20:28:01 IST 2012\nsuccessfully copied testdir\ntest.txt successfully copied\n-------------------------------------------------------------------------------------\nranveer@ranveer:~$ \n</code></pre>\n\n<p>So, why the output of file is not getting redirected to the file from within cron.</p>\n",
    "answer": "<p>I solved the problem. There are two ways:</p>\n\n<p><strong>M1</strong></p>\n\n<p>Change the redirection from <code>&amp;&gt;&gt;</code> to <code>2&gt;&amp;1</code>. So now <code>crontab -e</code> looks like</p>\n\n<pre><code>*/1 * * * * /home/ranveer/vimbackup.sh &gt;&gt; /home/ranveer/vimbackup.log 2&gt;&amp;1\n</code></pre>\n\n<p><strong>I believe</strong> the above works because by default <code>cron</code> is using <code>sh</code> to run the task instead of <code>bash</code> so <code>&amp;&gt;&gt;</code> is not supported by <code>sh</code>.</p>\n\n<p><strong>M2</strong></p>\n\n<p>Change the default shell by adding <code>SHELL=/bin/bash</code> in the <code>crontab -e</code> file.</p>\n",
    "url": "https://unix.stackexchange.com/questions/52330/how-to-redirect-output-to-a-file-from-within-cron"
  },
  {
    "question_title": "Argument string to integer in bash",
    "question_body": "<p>Trying to figure out how to convert an argument to an integer to perform arithmetic on, and then print it out, say for <code>addOne.sh</code>:</p>\n\n<pre><code>echo $1 + 1\n&gt;&gt;sh addOne.sh 1\nprints 1 + 1\n</code></pre>\n",
    "answer": "<p>In bash, one does not &quot;convert an argument to an integer to perform arithmetic&quot;.  In bash, variables are treated as integer or string depending on context.</p>\n<p>(If you are using a variable in an integer context, then, obviously, the variable better contain a string that looks like a valid integer.  Otherwise, you'll get an error.)</p>\n<p>To perform arithmetic, you should invoke the arithmetic expansion operator <code>$((...))</code>.  For example:</p>\n<pre><code>$ a=2\n$ echo &quot;$a + 1&quot;\n2 + 1\n$ echo &quot;$(($a + 1))&quot;\n3\n</code></pre>\n<p>or generally preferred:</p>\n<pre><code>$ echo &quot;$((a + 1))&quot;\n3\n</code></pre>\n<p>You should be aware that bash (as opposed to ksh93, zsh or yash) only performs <em>integer</em> arithmetic.  If you have floating point numbers (numbers with decimals), then there are other tools to assist.  For example, use <code>bc</code>:</p>\n<pre><code>$ b=3.14\n$ echo &quot;$(($b + 1))&quot;\nbash: 3.14 + 1: syntax error: invalid arithmetic operator (error token is &quot;.14 + 1&quot;)\n$ echo &quot;$b + 1&quot; | bc -l\n4.14\n</code></pre>\n<p>Or you can use a shell with floating point arithmetic support instead of bash:</p>\n<pre><code>zsh&gt; echo $((3.14 + 1))\n4.14\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/232384/argument-string-to-integer-in-bash"
  },
  {
    "question_title": "When is double-quoting necessary?",
    "question_body": "<p>The old advice used to be to double-quote any expression involving a <code>$VARIABLE</code>, at least if one wanted it to be interpreted by the shell as one single item, otherwise, any spaces in the content of <code>$VARIABLE</code> would throw off the shell.</p>\n\n<p>I understand, however, that in more recent versions of shells, double-quoting is no longer always needed (at least for the purpose described above).  For instance, in <code>bash</code>:</p>\n\n<pre><code>% FOO='bar baz'\n% [ $FOO = 'bar baz' ] &amp;&amp; echo OK\nbash: [: too many arguments\n% [[ $FOO = 'bar baz' ]] &amp;&amp; echo OK\nOK\n% touch 'bar baz'\n% ls $FOO\nls: cannot access bar: No such file or directory\nls: cannot access baz: No such file or directory\n</code></pre>\n\n<p>In <code>zsh</code>, on the other hand, the same three commands succeed.  Therefore, based on this experiment, it seems that, in <code>bash</code>, one can omit the double quotes inside <code>[[ ... ]]</code>, but not inside <code>[ ... ]</code> nor in command-line arguments, whereas, in <code>zsh</code>, the double quotes may be omitted in all these cases.</p>\n\n<p>But inferring general rules from anecdotal examples like the above is a chancy proposition.  It would be nice to see a summary of when double-quoting is necessary.  I'm primarily interested in <code>zsh</code>, <code>bash</code>, and <code>/bin/sh</code>.</p>\n",
    "answer": "<p>First, separate zsh from the rest. It's not a matter of old vs modern shells: zsh behaves differently. The zsh designers decided to make it incompatible with traditional shells (Bourne, ksh, bash), but easier to use.</p>\n<p>Second, it is far easier to use double quotes all the time than to remember when they are needed. They are needed most of the time, so you'll need to learn when they aren't needed, not when they are needed.</p>\n<p>In a nutshell, <strong>double quotes are necessary wherever a list of words or a pattern is expected</strong>. They are optional in contexts where a single raw string is expected by the parser.</p>\n<h3>What happens without quotes</h3>\n<p>Note that without double quotes, two things happen.</p>\n<ol>\n<li>First, the result of the expansion (the value of the variable for a parameter substitution like <code>$foo</code> or <code>${foo}</code>, or the output of the command for a command substitution like <code>$(foo)</code>) is split into words wherever it contains whitespace.<br />\nMore precisely, the result of the expansion is split at each character that appears in the value of the <code>IFS</code> variable (separator character). If a sequence of separator characters contains whitespace (space, tab, or newline), the whitespace counts as a single character; leading, trailing, or repeated non-whitespace separators lead to empty fields. For example, with <code>IFS=&quot; :&quot;</code> (space and colon), <code>:one::two : three: :four </code> produces empty fields before <code>one</code>, between <code>one</code> and <code>two</code>, and (a single one) between <code>three</code> and <code>four</code>.</li>\n<li>Each field that results from splitting is interpreted as a glob (a wildcard pattern) if it contains one of the characters <code>[*?</code>. If that pattern matches one or more file names, the pattern is replaced by the list of matching file names.</li>\n</ol>\n<p>An unquoted variable expansion <code>$foo</code> is colloquially known as the “split+glob operator”, in contrast with <code>&quot;$foo&quot;</code> which just takes the value of the variable <code>foo</code>. The same goes for command substitution: <code>&quot;$(foo)&quot;</code> is a command substitution, <code>$(foo)</code> is a command substitution followed by split+glob.</p>\n<h3>Where you can omit the double quotes</h3>\n<p>Here are all the cases I can think of in a Bourne-style shell where you can write a variable or command substitution without double quotes, and the value is interpreted literally.</p>\n<ul>\n<li><p>On the right-hand side of a scalar (not array) variable assignment.</p>\n<pre><code> var=$stuff\n a_single_star=*\n</code></pre>\n<p>Note that you do need the double quotes after <code>export</code> or <code>readonly</code>, because in a few shells, they are still ordinary builtins, not a keyword. This is only true in some shells such as some older versions of dash, older versions of zsh (in sh emulation), yash, or posh; in bash, ksh, newer versions of dash and zsh <code>export</code> / <code>readonly</code> and co are treated specially as dual builtin / keyword (under some conditions) as POSIX now more clearly requires.</p>\n<pre><code> export VAR=&quot;$stuff&quot;\n</code></pre>\n</li>\n<li><p>In a <code>case</code> statement.</p>\n<pre><code> case $var in …\n</code></pre>\n<p>Note that you do need double quotes in a case pattern. Word splitting doesn't happen in a case pattern, but an unquoted variable is interpreted as a glob-style pattern whereas a quoted variable is interpreted as a literal string.</p>\n<pre><code> a_star='a*'\n case $var in\n   &quot;$a_star&quot;) echo &quot;'$var' is the two characters a, *&quot;;;\n    $a_star) echo &quot;'$var' begins with a&quot;;;\n esac\n</code></pre>\n</li>\n<li><p>Within double brackets. Double brackets are shell special syntax.</p>\n<pre><code> [[ -e $filename ]]\n</code></pre>\n<p>Except that you do need double quotes where a pattern or regular expression is expected: on the right-hand side of <code>=</code> or <code>==</code> or <code>!=</code> or <code>=~</code> (though for the latter, behaviour varies between shells).</p>\n<pre><code> a_star='a*'\n if [[ $var == &quot;$a_star&quot; ]]; then echo &quot;'$var' is the two characters a, *&quot;\n elif [[ $var == $a_star ]]; then echo &quot;'$var' begins with a&quot;\n fi\n</code></pre>\n<p>You do need double quotes as usual within single brackets <code>[ … ]</code> because they are ordinary shell syntax (it's a command that happens to be called <code>[</code>). See <a href=\"https://unix.stackexchange.com/questions/32210/using-single-or-double-bracket-bash/32227#32227\">Why does parameter expansion with spaces without quotes work inside double brackets &quot;[[&quot; but not inside single brackets &quot;[&quot;?</a>.</p>\n</li>\n<li><p>In a redirection in non-interactive POSIX shells (not <code>bash</code>, nor <code>ksh88</code>).</p>\n<pre><code> echo &quot;hello world&quot; &gt;$filename\n</code></pre>\n<p>Some shells, when interactive, do treat the value of the variable as a wildcard pattern. POSIX prohibits that behaviour in non-interactive shells, but a few shells including bash (except in POSIX mode) and ksh88  (including when found as the (supposedly) POSIX <code>sh</code> of some commercial Unices like Solaris) still do it there (<code>bash</code> does also attempt <em>splitting</em> and the redirection fails unless that <em>split+globbing</em> results in exactly one word), which is why it's better to quote targets of redirections in a <code>sh</code> script in case you want to convert it to a <code>bash</code> script some day, or run it on a system where <code>sh</code> is non-compliant on that point, or it may be <em>sourced</em> from interactive shells.</p>\n</li>\n<li><p>Inside an arithmetic expression. In fact, you need to leave the quotes out in order for a variable to be parsed as an arithmetic expression in several shells.</p>\n<pre><code> expr=2*2\n echo &quot;$(($expr))&quot;\n</code></pre>\n<p>However, you do need the quotes around the arithmetic expansion as it is subject to word splitting in most shells as POSIX requires (!?).</p>\n</li>\n<li><p>In an associative array subscript.</p>\n<pre><code> typeset -A a\n i='foo bar*qux'\n a[foo\\ bar\\*qux]=hello\n echo &quot;${a[$i]}&quot;\n</code></pre>\n</li>\n</ul>\n<p>An unquoted variable and command substitution can be useful in some rare circumstances:</p>\n<ul>\n<li>When the variable value or command output consists of a list of glob patterns and you want to expand these patterns to the list of matching files.</li>\n<li>When you know that the value doesn't contain any wildcard character, that <code>$IFS</code> was not modified and you want to split it at whitespace (well, only space, tab and newline) characters.</li>\n<li>When you want to split a value at a certain character: disable globbing with <code>set -o noglob</code> / <code>set -f</code>, set <code>IFS</code> to the separator character (or leave it alone to split at whitespace), then do the expansion.</li>\n</ul>\n<h3>Zsh</h3>\n<p>In zsh, you can omit the double quotes most of the time, with a few exceptions.</p>\n<ul>\n<li><p><code>$var</code> never expands to multiple words (assuming <code>var</code> isn't an array), but it expands to the empty list (as opposed to a list containing a single, empty word) if the value of <code>var</code> is the empty string. Contrast:</p>\n<pre><code> var=\n print -l -- $var foo        # prints just foo\n print -l -- &quot;$var&quot; foo      # prints an empty line, then foo\n</code></pre>\n<p>Similarly, <code>&quot;${array[@]}&quot;</code> expands to all the elements of the array, while <code>$array</code> only expands to the non-empty elements.</p>\n</li>\n<li><p>Like in ksh and bash, inside <code>[[ … ]]</code>, a variable in the right-hand side of a <code>==</code>, <code>!=</code> or <code>=~</code> operator needs to be double-quoted if it contains a string, and not quoted if it contains a pattern/regex: <code>p='a*'; [[ abc == $p ]]</code> is true but <code>p='a*'; [[ abc == &quot;$p&quot; ]]</code> is false.</p>\n</li>\n<li><p>The <code>@</code> parameter expansion flag sometimes requires double quotes around the whole substitution: <code>&quot;${(@)foo}&quot;</code>.</p>\n</li>\n<li><p>Command substitution undergoes field splitting if unquoted: <code>echo $(echo 'a'; echo '*')</code> prints <code>a *</code> (with a single space) whereas <code>echo &quot;$(echo 'a'; echo '*')&quot;</code> prints the unmodified two-line string. Use <code>&quot;$(somecommand)&quot;</code> to get the output of the command in a single word, sans final newlines. Use <code>&quot;${$(somecommand; echo .)%?}&quot;</code> to get the exact output of the command including final newlines. Use <code>&quot;${(@f)$(somecommand)}&quot;</code> to get an array of lines from the command's output (removing trailing empty lines if any though).</p>\n</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary"
  },
  {
    "question_title": "How can I set my default shell to start up tmux",
    "question_body": "<p>I would like my default bash shell to go straight into tmux instead of my always having to type tmux every time.</p>\n",
    "answer": "<p><a href=\"https://unix.stackexchange.com/a/43604/377345\">@StarNamer's answer</a> is generally accurate, though I typically include the following tests to make sure that</p>\n<ol>\n<li><code>tmux</code> exists on the system</li>\n<li>we're in an interactive shell, and</li>\n<li><code>tmux</code> doesn't try to run within itself</li>\n</ol>\n<p>So, I would add this to the <code>.bashrc</code>:</p>\n<pre><code>if command -v tmux &amp;&gt; /dev/null &amp;&amp; [ -n &quot;$PS1&quot; ] &amp;&amp; [[ ! &quot;$TERM&quot; =~ screen ]] &amp;&amp; [[ ! &quot;$TERM&quot; =~ tmux ]] &amp;&amp; [ -z &quot;$TMUX&quot; ]; then\n  exec tmux\nfi\n</code></pre>\n<hr />\n<p>References</p>\n<ul>\n<li>Using bash's <code>command</code> to check for existence of a command - <a href=\"http://man7.org/linux/man-pages/man1/bash.1.html#SHELL_BUILTIN_COMMANDS\" rel=\"noreferrer\">http://man7.org/linux/man-pages/man1/bash.1.html#SHELL_BUILTIN_COMMANDS</a></li>\n<li>Why to use <code>command</code> instead of <code>which</code> to check for the existence of commands - <a href=\"https://unix.stackexchange.com/a/85250\">https://unix.stackexchange.com/a/85250</a></li>\n<li>Using <code>$PS1</code> to check for interactive shell - <a href=\"https://www.gnu.org/software/bash/manual/html_node/Is-this-Shell-Interactive_003f.html\" rel=\"noreferrer\">https://www.gnu.org/software/bash/manual/html_node/Is-this-Shell-Interactive_003f.html</a></li>\n<li>Expected state of <code>$TERM</code> environment variable &quot;for all programs running inside tmux&quot; - <a href=\"http://man7.org/linux/man-pages/man1/tmux.1.html#WINDOWS_AND_PANES\" rel=\"noreferrer\">http://man7.org/linux/man-pages/man1/tmux.1.html#WINDOWS_AND_PANES</a></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/43601/how-can-i-set-my-default-shell-to-start-up-tmux"
  },
  {
    "question_title": "What is the difference between &quot;sort -u&quot; and &quot;sort | uniq&quot;?",
    "question_body": "<p>Everywhere I see someone needing to get a sorted, unique list, they always pipe to <code>sort | uniq</code>. I've never seen any examples where someone uses <code>sort -u</code> instead. Why not? What's the difference, and why is it better to use uniq than the unique flag to sort?</p>\n",
    "answer": "<p><code>sort | uniq</code> existed before <code>sort -u</code>, and is compatible with a wider range of systems, although almost all modern systems do support <code>-u</code> -- it's POSIX. It's mostly a throwback to the days when <code>sort -u</code> didn't exist (and people don't tend to change their methods if the way that they know continues to work, just look at <code>ifconfig</code> vs. <code>ip</code> adoption).</p>\n<p>The two were likely merged because removing duplicates within a file requires sorting (at least, in the standard case), and is an extremely common use case of sort. It is also faster internally as a result of being able to do both operations at the same time (and due to the fact that it doesn't require IPC (<a href=\"https://en.m.wikipedia.org/wiki/Pipeline_(Unix)\" rel=\"noreferrer\">Inter-process communication</a>) between <code>uniq</code> and <code>sort</code>). Especially if the file is big, <code>sort -u</code> will likely use fewer intermediate files to sort the data.</p>\n<p>On my system I consistently get results like this:</p>\n<pre><code>$ dd if=/dev/urandom of=/dev/shm/file bs=1M count=100\n100+0 records in\n100+0 records out\n104857600 bytes (105 MB) copied, 8.95208 s, 11.7 MB/s\n$ time sort -u /dev/shm/file &gt;/dev/null\n\nreal        0m0.500s\nuser        0m0.767s\nsys         0m0.167s\n$ time sort /dev/shm/file | uniq &gt;/dev/null\n\nreal        0m0.772s\nuser        0m1.137s\nsys         0m0.273s\n</code></pre>\n<p>It also doesn't mask the return code of <code>sort</code>, which may be important (in modern shells there are ways to get this, for example, <code>bash</code>'s <code>$PIPESTATUS</code> array, but this wasn't always true).</p>\n",
    "url": "https://unix.stackexchange.com/questions/76049/what-is-the-difference-between-sort-u-and-sort-uniq"
  },
  {
    "question_title": "Bash: What does &quot;&gt;|&quot; do?",
    "question_body": "<p>I have just seen this written down;</p>\n\n<pre><code>$ some-command &gt;| /tmp/output.txt\n</code></pre>\n\n<p>Vertical pipes are used in standard redirects \"piping\" the output of one command to another, is <code>&gt;|</code> in fact completely useless as it would be the same as just <code>&gt;</code> in this scenario?</p>\n",
    "answer": "<p>It's not useless - it's a specialised form of the plain <code>&gt;</code> redirect operator (and, perhaps confusingly, nothing to do with pipes). <code>bash</code> and most other modern shells have an option <code>noclobber</code>, which prevents redirection from overwriting or destroying a file that already exists. For example, if <code>noclobber</code> is true, and the file <code>/tmp/output.txt</code> already exists, then this should fail:</p>\n\n<pre><code>$ some-command &gt; /tmp/output.txt\n</code></pre>\n\n<p>However, you can explicitly override the setting of <code>noclobber</code> with the <code>&gt;|</code> redirection operator - the redirection will work, even if <code>noclobber</code> is set.</p>\n\n<p>You can find out if <code>noclobber</code> is set in your current environment with <code>set -o</code>.</p>\n\n<p>For the historical note, both the \"noclobber\" option and its bypass features come from <code>csh</code> (late 70s). <code>ksh</code> copied it (early 80s) but used <code>&gt;|</code> instead of <code>&gt;!</code>. POSIX specified the <code>ksh</code> syntax (so all POSIX shells including bash, newer ash derivatives used as sh on some systems support it). Zsh supports both syntaxes. I don't think it was added to any Bourne shell variant but I might be wrong.</p>\n",
    "url": "https://unix.stackexchange.com/questions/45201/bash-what-does-do"
  },
  {
    "question_title": "Create symlink - overwrite if one exists",
    "question_body": "<p>I want to take down data in <code>/path/to/data/folder/month/date/hour/minute/file</code> and symlink it to <code>/path/to/recent/file</code> and do this automatically every time a file is created.</p>\n\n<p>Assuming I will not know ahead of time if <code>/path/to/recent/file</code> exists, how can I go about creating it (if it doesn't exist) or replacing it (if it does exist)? I am sure I can just check if it exists and then do a delete, symlink, but I'm wondering if there is a simple command which will do what I want in one step.</p>\n",
    "answer": "<p>This is the purpose of <code>ln</code>'s <code>-f</code> option: it removes existing destination files, if any, before creating the link.</p>\n\n<pre><code>ln -sf /path/to/data/folder/month/date/hour/minute/file /path/to/recent/file\n</code></pre>\n\n<p>will create the symlink <code>/path/to/recent/file</code> pointing to <code>/path/to/data/folder/month/date/hour/minute/file</code>, replacing any existing file or symlink to a file if necessary (and working fine if nothing exists there already).</p>\n\n<p>If a directory, or symlink to a directory, already exists with the target name, the symlink will be created inside it (so you'd end up with <code>/path/to/recent/file/file</code> in the example above). The <code>-n</code> option, available in some versions of <code>ln</code>, will take care of symlinks to directories for you, replacing them as necessary:</p>\n\n<pre><code>ln -sfn /path/to/data/folder/month/date/hour/minute/file /path/to/recent/file\n</code></pre>\n\n<p><a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/utilities/ln.html\" rel=\"noreferrer\">POSIX <code>ln</code></a> doesn’t specify <code>-n</code> so you can’t rely on it generally. Much of <code>ln</code>’s behaviour is implementation-defined so you really need to check the specifics of the system you’re using. If you’re using <a href=\"https://www.gnu.org/software/coreutils/manual/html_node/ln-invocation.html\" rel=\"noreferrer\">GNU <code>ln</code></a>, you can use the <code>-t</code> and <code>-T</code> options too, to make its behaviour fully predictable in the presence of directories (<em>i.e.</em> fail instead of creating the link inside the existing directory with the same name).</p>\n",
    "url": "https://unix.stackexchange.com/questions/207294/create-symlink-overwrite-if-one-exists"
  },
  {
    "question_title": "How can I execute local script on remote machine and include arguments?",
    "question_body": "<p>I have written a script that runs fine when executed locally:</p>\n\n<pre><code>./sysMole -time Aug 18 18\n</code></pre>\n\n<p>The arguments <strong>\"-time\"</strong>, <strong>\"Aug\"</strong>, <strong>\"18\"</strong>, and <strong>\"18\"</strong> are successfully passed on to the script.</p>\n\n<p>Now, this script is designed to be executed on a remote machine but, from a local directory on the local machine. Example:</p>\n\n<pre><code>ssh root@remoteServer \"bash -s\" &lt; /var/www/html/ops1/sysMole\n</code></pre>\n\n<p>That also works fine. But the problem arises when I try to include those aforementioned arguments <strong>(-time Aug 18 18)</strong>, for example:</p>\n\n<pre><code>ssh root@remoteServer \"bash -s\" &lt; /var/www/html/ops1/sysMole -time Aug 18 18\n</code></pre>\n\n<p>After running that script I get the following error:</p>\n\n<pre><code>bash: cannot set terminal process group (-1): Invalid argument\nbash: no job control in this shell\n</code></pre>\n\n<p>Please tell me what I'm doing wrong, this greatly frustrating.</p>\n",
    "answer": "<p>You were pretty close with your example. It works just fine when you use it with arguments such as these.</p>\n\n<p><em><strong>Sample script:</em></strong></p>\n\n<pre><code>$ more ex.bash \n#!/bin/bash\n\necho $1 $2\n</code></pre>\n\n<p><em><strong>Example that works:</em></strong></p>\n\n<pre><code>$ ssh serverA \"bash -s\" &lt; ./ex.bash \"hi\" \"bye\"\nhi bye\n</code></pre>\n\n<p>But it fails for these types of arguments:</p>\n\n<pre><code>$ ssh serverA \"bash -s\" &lt; ./ex.bash \"--time\" \"bye\"\nbash: --: invalid option\n...\n</code></pre>\n\n<h3>What's going on?</h3>\n\n<p>The problem you're encountering is that the argument, <code>-time</code>, or <code>--time</code> in my example, is being interpreted as a switch to <code>bash -s</code>. You can pacify <code>bash</code> by terminating it from taking any of the remaining command line arguments for itself using the <code>--</code> argument.</p>\n\n<p>Like this:</p>\n\n<pre><code>$ ssh root@remoteServer \"bash -s\" -- &lt; /var/www/html/ops1/sysMole -time Aug 18 18\n</code></pre>\n\n<h3>Examples</h3>\n\n<p><em>#1:</em></p>\n\n<pre><code>$ ssh serverA \"bash -s\" -- &lt; ./ex.bash \"-time\" \"bye\"\n-time bye\n</code></pre>\n\n<p><em>#2:</em></p>\n\n<pre><code>$ ssh serverA \"bash -s\" -- &lt; ./ex.bash \"--time\" \"bye\"\n--time bye\n</code></pre>\n\n<p><em>#3:</em></p>\n\n<pre><code>$ ssh serverA \"bash -s\" -- &lt; ./ex.bash --time \"bye\"\n--time bye\n</code></pre>\n\n<p><em>#4:</em></p>\n\n<pre><code>$ ssh  &lt; ./ex.bash serverA \"bash -s -- --time bye\"\n--time bye\n</code></pre>\n\n<p><strong>NOTE:</strong> Just to make it clear that wherever the redirection appears on the command line makes no difference, because <code>ssh</code> calls a remote shell with the concatenation of its arguments anyway, quoting doesn't make much difference, except when you need quoting on the remote shell like in example <em>#4:</em></p>\n\n<pre><code>$ ssh  &lt; ./ex.bash serverA \"bash -s -- '&lt;--time bye&gt;' '&lt;end&gt;'\"\n&lt;--time bye&gt; &lt;end&gt;\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/87405/how-can-i-execute-local-script-on-remote-machine-and-include-arguments"
  },
  {
    "question_title": "How do I grep for multiple patterns with pattern having a pipe character?",
    "question_body": "<p>I want to find all lines in several files that match one of two patterns. I tried to find the patterns I'm looking for by typing </p>\n\n<pre><code>grep (foo|bar) *.txt\n</code></pre>\n\n<p>but the shell interprets the <code>|</code> as a pipe and complains when <code>bar</code> isn't an executable.</p>\n\n<p>How can I grep for multiple patterns in the same set of files?</p>\n",
    "answer": "<p>First, you need to protect the pattern from expansion by the shell. The easiest way to do that is to put single quotes around it. Single quotes prevent expansion of anything between them (including backslashes); the only thing you can't do then is have single quotes in the pattern.</p>\n\n<pre><code>grep -- 'foo*' *.txt\n</code></pre>\n\n<p>(also note the <code>--</code> end-of-option-marker to stop some <code>grep</code> implementations including GNU <code>grep</code> from treating a file called <code>-foo-.txt</code> for instance (that would be expanded by the shell from <code>*.txt</code>) to be taken as an option (even though it follows a non-option argument here)).</p>\n\n<p>If you do need a single quote, you can write it as <code>'\\''</code> (end string literal, literal quote, open string literal).</p>\n\n<pre><code>grep -- 'foo*'\\''bar' *.txt\n</code></pre>\n\n<p>Second, grep supports at least¹ two syntaxes for patterns. The old, default syntax (<a href=\"https://en.wikipedia.org/wiki/Regular_expression#POSIX_basic_and_extended\" rel=\"noreferrer\">basic regular expressions</a>) doesn't support the alternation (<code>|</code>) operator, though some versions have it as an extension, but written with a backslash.</p>\n\n<pre><code>grep -- 'foo\\|bar' *.txt\n</code></pre>\n\n<p>The portable way is to use the newer syntax, <a href=\"https://en.wikipedia.org/wiki/Regular_expression#POSIX_extended\" rel=\"noreferrer\">extended regular expressions</a>. You need to pass the <code>-E</code> option to <code>grep</code> to select it (formerly that was done with the <code>egrep</code> separate command²)</p>\n\n<pre><code>grep -E -- 'foo|bar' *.txt\n</code></pre>\n\n<p>Another possibility when you're just looking for any of several patterns (as opposed to building a complex pattern using disjunction) is to pass multiple patterns to <code>grep</code>. You can do this by preceding each pattern with the <code>-e</code> option.</p>\n\n<pre><code>grep -e foo -e bar -- *.txt\n</code></pre>\n\n<p>Or put patterns on several lines:</p>\n\n<pre><code>grep -- 'foo\nbar' *.txt\n</code></pre>\n\n<p>Or store those patterns in a file, one per line and run</p>\n\n<pre><code>grep -f that-file -- *.txt\n</code></pre>\n\n<p>Note that if <code>*.txt</code> expands to a single file, <code>grep</code> won't prefix matching lines with its name like it does when there are more than one file. To work around that, with some <code>grep</code> implementations like GNU <code>grep</code>, you can use the <code>-H</code> option, or with any implementation, you can pass <code>/dev/null</code> as an extra argument.</p>\n\n<hr>\n\n<p><sup>¹ some <code>grep</code> implementations support even more like perl-compatible ones with <code>-P</code>, or <em>augmented</em> ones with <code>-X</code>, <code>-K</code> for ksh wildcards...</sup></p>\n\n<p><sup>² while <code>egrep</code> has been deprecated by POSIX and is sometimes no longer found on some systems, on some other systems like Solaris when the POSIX or GNU utilities have not been installed, then <code>egrep</code> is your only option as its <code>/bin/grep</code> supports none of <code>-e</code>, <code>-f</code>, <code>-E</code>, <code>\\|</code> or multi-line patterns</sup></p>\n",
    "url": "https://unix.stackexchange.com/questions/37313/how-do-i-grep-for-multiple-patterns-with-pattern-having-a-pipe-character"
  },
  {
    "question_title": "Can grep output only specified groupings that match?",
    "question_body": "<p>Say I have a file:</p>\n\n<pre><code># file: 'test.txt'\nfoobar bash 1\nbash\nfoobar happy\nfoobar\n</code></pre>\n\n<p>I only want to know what words appear after \"foobar\", so I can use this regex:</p>\n\n<pre><code>\"foobar \\(\\w\\+\\)\"\n</code></pre>\n\n<p>The parenthesis indicate that I have a special interest in the word right after foobar.  But when I do a <code>grep \"foobar \\(\\w\\+\\)\" test.txt</code>, I get the entire lines that match the entire regex, rather than just \"the word after foobar\":</p>\n\n<pre><code>foobar bash 1\nfoobar happy\n</code></pre>\n\n<p>I would much prefer that the output of that command looked like this:</p>\n\n<pre><code>bash\nhappy\n</code></pre>\n\n<p>Is there a way to tell grep to only output the items that match the grouping (or a specific grouping) in a regular expression?</p>\n",
    "answer": "<p>GNU grep has the <code>-P</code> option for perl-style regexes, and the <code>-o</code> option to print only what matches the pattern. These can be combined using look-around assertions (described under <a href=\"http://perldoc.perl.org/perlre.html#Extended-Patterns\">Extended Patterns in the perlre manpage</a>) to remove part of the grep pattern from what is determined to have matched for the purposes of <code>-o</code>.</p>\n\n<pre><code>$ grep -oP 'foobar \\K\\w+' test.txt\nbash\nhappy\n$\n</code></pre>\n\n<p>The <code>\\K</code> is the short-form (and more efficient form) of <code>(?&lt;=pattern)</code> which you use as a zero-width look-behind assertion before the text you want to output. <code>(?=pattern)</code> can be used as a zero-width look-ahead assertion after the text you want to output.</p>\n\n<p>For instance, if you wanted to match the word between <code>foo</code> and <code>bar</code>, you could use:</p>\n\n<pre><code>$ grep -oP 'foo \\K\\w+(?= bar)' test.txt\n</code></pre>\n\n<p>or (for symmetry)</p>\n\n<pre><code>$ grep -oP '(?&lt;=foo )\\w+(?= bar)' test.txt\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/13466/can-grep-output-only-specified-groupings-that-match"
  },
  {
    "question_title": "Count total number of occurrences using grep",
    "question_body": "<p><code>grep -c</code> is useful for finding how many times a string occurs in a file, but it only counts each occurence once per line. How to count multiple occurences per line?</p>\n\n<p>I'm looking for something more elegant than:</p>\n\n<pre><code>perl -e '$_ = &lt;&gt;; print scalar ( () = m/needle/g ), \"\\n\"'\n</code></pre>\n",
    "answer": "<p>grep's <code>-o</code> will only output the matches, ignoring lines; <code>wc</code> can count them:</p>\n\n<pre><code>grep -o 'needle' file | wc -l\n</code></pre>\n\n<p>This will also match 'needles' or 'multineedle'.</p>\n\n<p>To match only single words use one of the following commands:</p>\n\n<pre><code>grep -ow 'needle' file | wc -l\ngrep -o '\\bneedle\\b' file | wc -l\ngrep -o '\\&lt;needle\\&gt;' file | wc -l\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/6979/count-total-number-of-occurrences-using-grep"
  },
  {
    "question_title": "How can I prevent &#39;grep&#39; from showing up in ps results?",
    "question_body": "<p>When I  search for some process that doesn't exist, e.g.</p>\n\n<pre><code>$ ps aux | grep fnord                          \nwayne    15745  0.0  0.0  13580   928 pts/6    S+   03:58   0:00 grep fnord\n</code></pre>\n\n<p>Obviously I don't care about grep - that makes as much sense as searching for the <code>ps</code> process!</p>\n\n<p>How can I prevent grep from showing up in the results?</p>\n",
    "answer": "<p>Turns out there's a solution found in <a href=\"http://www.ibm.com/developerworks/library/l-keyc3/#code10\">keychain</a>.</p>\n\n<pre><code>$ ps aux | grep \"[f]nord\"\n</code></pre>\n\n<p>By putting the brackets around the letter and quotes around the string you search for the regex, which says, \"Find the character 'f' followed by 'nord'.\"</p>\n\n<p>But since you put the brackets in the pattern 'f' is now followed by ']', so <code>grep</code> won't show up in the results list. Neato!</p>\n",
    "url": "https://unix.stackexchange.com/questions/74185/how-can-i-prevent-grep-from-showing-up-in-ps-results"
  },
  {
    "question_title": "What is the difference between `grep`, `egrep`, and `fgrep`?",
    "question_body": "<p>Can anyone tell me the technical differences between <code>grep</code>, <code>egrep</code>, and <code>fgrep</code> and provide suitable examples?</p>\n\n<p>When do I need to use <code>grep</code> over <code>egrep</code> and vice versa?</p>\n",
    "answer": "<ul>\n<li><code>egrep</code> is 100% equivalent to <a href=\"https://www.gnu.org/software/grep/manual/grep.html#index-matching-extended-regular-expressions\" rel=\"noreferrer\"><code>grep -E</code></a></li>\n<li><code>fgrep</code> is 100% equivalent to <a href=\"https://www.gnu.org/software/grep/manual/grep.html#index-matching-fixed-strings\" rel=\"noreferrer\"><code>grep -F</code></a></li>\n</ul>\n\n<p>Historically these switches were provided in separate binaries. On some really old Unix systems you will find that you need to call the separate binaries, but on all modern systems the switches are preferred. The man page for grep has details about this. </p>\n\n<p>As for what they do, <code>-E</code> switches grep into a special mode so that the expression is evaluated as an ERE (Extended Regular Expression) as opposed to its normal pattern matching. Details of this syntax are on the man page.</p>\n\n<blockquote>\n  <p><code>-E, --extended-regexp</code><br>\n     Interpret PATTERN as an extended regular expression</p>\n</blockquote>\n\n<p>The <code>-F</code> switch switches grep into a different mode where it accepts a pattern to match, but then splits that pattern up into one search string per line and does an OR search on any of the strings without doing any special pattern matching.</p>\n\n<blockquote>\n  <p><code>-F, --fixed-strings</code><br>\n     Interpret  PATTERN  as  a  list of fixed strings, separated by newlines, any of which is to be matched.</p>\n</blockquote>\n\n<p>Here are some example scenarios:</p>\n\n<ul>\n<li><p>You have a file with a list of say ten Unix usernames in plain text. You want to search the group file on your machine to see if any of the ten users listed are in any special groups:</p>\n\n<pre><code>grep -F -f user_list.txt /etc/group\n</code></pre>\n\n<p>The reason the <code>-F</code> switch helps here is that the usernames in your pattern file are interpreted as plain text strings. Dots for example would be interpreted as dots rather than wild-cards.</p></li>\n<li><p>You want to search using a fancy expression. For example parenthesis <code>()</code> can be used to indicate groups with <code>|</code> used as an OR operator. You could run this search using <code>-E</code>:</p>\n\n<pre><code>grep -E '^no(fork|group)' /etc/group\n</code></pre>\n\n<p>...to return lines that start with either \"nofork\" or \"nogroup\". Without the <code>-E</code> switch you would have to escape the special characters involved because with normal pattern matching they would just search for that exact pattern;</p>\n\n<pre><code>grep '^no\\(fork\\|group\\)' /etc/group\n</code></pre></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/17949/what-is-the-difference-between-grep-egrep-and-fgrep"
  },
  {
    "question_title": "How can I grep in PDF files?",
    "question_body": "<p>Is there a way to search PDF files using grep, without converting to text first in Ubuntu?</p>\n",
    "answer": "<p>Install the package <code>pdfgrep</code>, then use the command:</p>\n\n<pre><code>find /path -iname '*.pdf' -exec pdfgrep pattern {} +\n</code></pre>\n\n<p>——————</p>\n\n<p>Simplest way to do that:</p>\n\n<pre><code>pdfgrep 'pattern' *.pdf\npdfgrep 'pattern' file.pdf \n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/6704/how-can-i-grep-in-pdf-files"
  },
  {
    "question_title": "Can grep return true/false or are there alternative methods",
    "question_body": "<p>As a part of this script, I need to be able to check if the first argument given matches the first word of file. If it does, exit with an error message; if it doesn't, append the arguments to the file. I understand how to write the <code>if</code> statement, but not how to use <code>grep</code> within a script. I understand that <code>grep</code> will look something like this</p>\n\n<pre><code>grep ^$1 schemas.txt\n</code></pre>\n\n<p>I feel like this should be much easier than I am making it.</p>\n\n<p>I'm getting an error \"too many arguments\" on the <code>if</code> statement. I got rid of the space between <code>grep -q</code> and then got an error binary operator expected. </p>\n\n<pre><code>if [ grep -q ^$1 schemas.txt ]\nthen\n        echo \"Schema already exists. Please try again\"\n        exit 1\nelse\n        echo \"$@\" &gt;&gt; schemas.txt\nfi\n</code></pre>\n",
    "answer": "<p><code>grep</code> returns a different exit code if it found something (zero) vs. if it hasn't found anything (non-zero). In an <code>if</code> statement, a zero exit code is mapped to \"true\" and a non-zero exit code is mapped to false. In addition, grep has a <code>-q</code> argument to not output the matched text (but only return the exit status code)</p>\n\n<p>So, you can use grep like this:</p>\n\n<pre class=\"lang-shell prettyprint-override\"><code>if grep -q PATTERN file.txt; then\n    echo found\nelse\n    echo not found\nfi\n</code></pre>\n\n<p>As a quick note, when you do something like <code>if [ -z \"$var\" ]…</code>, it turns out that <code>[</code> is actually a command you're running, just like grep. On my system, it's <code>/usr/bin/[</code>. (Well, technically, your shell probably has it built-in, but that's an optimization. It behaves as if it were a command). It works the same way, <code>[</code> returns a zero exit code for true, a non-zero exit code for false. (<code>test</code> is the same thing as <code>[</code>, except for the closing <code>]</code>)</p>\n",
    "url": "https://unix.stackexchange.com/questions/48535/can-grep-return-true-false-or-are-there-alternative-methods"
  },
  {
    "question_title": "What makes grep consider a file to be binary?",
    "question_body": "<p>I have some database dumps from a Windows system on my box. They are text files. I'm using cygwin to grep through them. These appear to be plain text files; I open them with text editors such as notepad and wordpad and they look legible. However, when I run grep on them, it will say <code>binary file foo.txt matches</code>.</p>\n\n<p>I have noticed that the files contain some ascii <code>NUL</code> characters, which I believe are artifacts from the database dump.</p>\n\n<p>So what makes grep consider these files to be binary? The <code>NUL</code> character? Is there a flag on the filesystem? What do I need to change to get grep to show me the line matches?</p>\n",
    "answer": "<p>If there is a <code>NUL</code> character anywhere in the file, grep will consider it as a binary file.</p>\n\n<p>There might a workaround like this <code>cat file | tr -d '\\000' | yourgrep</code> to eliminate all null first, and then to search through file.</p>\n",
    "url": "https://unix.stackexchange.com/questions/19907/what-makes-grep-consider-a-file-to-be-binary"
  },
  {
    "question_title": "grep returns &quot;Binary file (standard input) matches&quot; when trying to find a string pattern in file",
    "question_body": "<p>I'm on Ubuntu and I typed <code>cat .bash_history | grep git</code> and it returned</p>\n<blockquote>\n<p>Binary file (standard input) matches</p>\n</blockquote>\n<p>My <code>bash_history</code> does exist and there are many lines in it that starts with <code>git</code>.</p>\n<p>What caused it to display this error and how can I fix it?</p>\n",
    "answer": "<p>You can use <code>grep -a 'pattern'</code>.</p>\n\n<p>from <a href=\"https://linux.die.net/man/1/grep\" rel=\"noreferrer\">man grep</a> page:</p>\n\n<blockquote>\n  <p><strong>-a, --text</strong><br>\n      Process a binary file as if it were text; this is equivalent to the <strong>--binary-files=text</strong> option.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/335716/grep-returns-binary-file-standard-input-matches-when-trying-to-find-a-string"
  },
  {
    "question_title": "Return only the portion of a line after a matching pattern",
    "question_body": "<p>So pulling open a file with <code>cat</code> and then using <code>grep</code> to get matching lines only gets me so far when I am working with the particular log set that I am dealing with. It need a way to match lines to a pattern, but only to return the portion of the line after the match. The portion before and after the match will consistently vary. I have played with using <code>sed</code> or <code>awk</code>, but have not been able to figure out how to filter the line to either delete the part before the match, or just return the part after the match, either will work.\nThis is an example of a line that I need to filter:</p>\n\n<pre><code>2011-11-07T05:37:43-08:00 &lt;0.4&gt; isi-udb5-ash4-1(id1) /boot/kernel.amd64/kernel: [gmp_info.c:1758](pid 40370=\"kt: gmp-drive-updat\")(tid=100872) new group: &lt;15,1773&gt;: { 1:0-25,27-34,37-38, 2:0-33,35-36, 3:0-35, 4:0-9,11-14,16-32,34-38, 5:0-35, 6:0-15,17-36, 7:0-16,18-36, 8:0-14,16-32,34-36, 9:0-10,12-36, 10-11:0-35, 12:0-5,7-30,32-35, 13-19:0-35, 20:0,2-35, down: 8:15, soft_failed: 1:27, 8:15, stalled: 12:6,31, 20:1 }\n</code></pre>\n\n<p>The portion I need is everything after \"stalled\".</p>\n\n<p>The background behind this is that I can find out how often something stalls:</p>\n\n<pre><code>cat messages | grep stalled | wc -l\n</code></pre>\n\n<p>What I need to do is find out how many times a certain node has stalled (indicated by the portion before each colon after \"stalled\". If I just grep for that (ie 20:) it may return lines that have soft fails, but no stalls, which doesn't help me. I need to filter only the stalled portion so I can then grep for a specific node out of those that have stalled.</p>\n\n<p>For all intents and purposes, this is a freebsd system with standard GNU core utils, but I cannot install anything extra to assist.</p>\n",
    "answer": "<p>The canonical tool for that would be <code>sed</code>.</p>\n<pre><code>sed -n -e 's/^.*stalled: //p'\n</code></pre>\n<p>Detailed explanation:</p>\n<ul>\n<li><code>-n</code> means not to print anything by default.</li>\n<li><code>-e</code> is followed by a sed command.</li>\n<li><code>s</code> is the pattern replacement command.</li>\n<li>The regular expression <code>^.*stalled: </code> matches the pattern you're looking for, plus any preceding text (<code>.*</code> meaning any text, with an initial <code>^</code> to say that the match begins at the beginning of the line). Note that if <code>stalled: </code> occurs several times on the line, this will match the last occurrence.</li>\n<li>The match, i.e. everything on the line up to <code>stalled: </code>, is replaced by the empty string (i.e. deleted).</li>\n<li>The final <code>p</code> means to print the transformed line.</li>\n</ul>\n<p>If you want to retain the matching portion, use a backreference: <code>\\1</code> in the replacement part designates what is inside a group <code>\\(…\\)</code> in the pattern. Here, you could write <code>stalled: </code> again in the replacement part; this feature is useful when the pattern you're looking for is more general than a simple string.</p>\n<pre><code>sed -n -e 's/^.*\\(stalled: \\)/\\1/p'\n</code></pre>\n<p>Sometimes you'll want to remove the portion of the line after the match. You can include it in the match by including <code>.*$</code> at the end of the pattern (any text <code>.*</code> followed by the end of the line <code>$</code>). Unless you put that part in a group that you reference in the replacement text, the end of the line will not be in the output.</p>\n<p>As a further illustration of groups and backreferences, this command swaps the part before the match and the part after the match.</p>\n<pre><code>sed -n -e 's/^\\(.*\\)\\(stalled: \\)\\(.*\\)$/\\3\\2\\1/p'\n</code></pre>\n<p>To get the part after the <em>first</em> occurrence of the string instead of last (for those lines where the string can occur several times), a common trick is to replace that string once with a newline character (which is the one character that won't occur inside a line), and then remove everything up to that newline:</p>\n<pre><code>sed -n '\n  /stalled: / {\n    s//\\\n/\n    s/.*\\n//p\n  }'\n</code></pre>\n<p>With some <code>sed</code> implementations, the first <code>s</code> command can be written <code>s//\\n/</code> though that's not standard/portable.</p>\n",
    "url": "https://unix.stackexchange.com/questions/24140/return-only-the-portion-of-a-line-after-a-matching-pattern"
  },
  {
    "question_title": "How do I grep recursively through .gz files?",
    "question_body": "<p>I am using a script to regularly download my gmail messages that compresses the raw .eml into .gz files. The script creates a folder for each day, and then compresses every message into its own file.</p>\n\n<p>I would like a way to search through this archive for a \"string.\"</p>\n\n<p>Grep alone doesn't appear to do it. I also tried SearchMonkey.</p>\n",
    "answer": "<p>If you want to grep recursively in all .eml.gz files in the current directory, you can use:</p>\n\n<pre><code>find . -name \\*.eml.gz -print0 | xargs -0 zgrep \"STRING\"\n</code></pre>\n\n<p>You have to escape the first <code>*</code> so that the shell does not interpret it. <code>-print0</code> tells find to print a null character after each file it finds; <code>xargs -0</code> reads from standard input and runs the command after it for each file; <code>zgrep</code> works like <code>grep</code>, but uncompresses the file first.</p>\n",
    "url": "https://unix.stackexchange.com/questions/187742/how-do-i-grep-recursively-through-gz-files"
  },
  {
    "question_title": "Grep: how to add an &quot;OR&quot; condition?",
    "question_body": "<p>How can I introduce a conditional OR into grep?  Something like, grepping a file's type for (JPEG OR JPG), and then sending only those files into the photos folder.  For example.  I know how to send the file where I want it, and get the file type, I just need some help with the grep part.</p>\n\n<p>I'm on OS X, which IMO seems to have modified/customized *nix utilities than what I'm used to in a *nix environment.  So hopefully the answers can be as generic/portable as possible.</p>\n",
    "answer": "<p>You can pass multiple regexes to <code>grep</code> by using the <code>-e</code> option more than once:</p>\n<pre>grep -e <i>regex1</i> -e <i>regex2</i>  <i>input_file</i></pre>\n<p>will match lines matching <em><code>regex1</code></em> or <em><code>regex2</code></em>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/25821/grep-how-to-add-an-or-condition"
  },
  {
    "question_title": "How to run grep with multiple AND patterns?",
    "question_body": "<p>I would like to get the multi pattern match with implicit <strong>AND</strong> between patterns, i.e. equivalent to running several greps in a sequence:</p>\n<pre><code>grep pattern1 | grep pattern2 | ...\n</code></pre>\n<p>So how to convert it to something like?</p>\n<pre><code>grep pattern1 &amp; pattern2 &amp; pattern3\n</code></pre>\n<p><em>I would like to use single grep because I am building arguments dynamically, so everything has to fit in one string. Using filter is system feature, not grep, so it is not an argument for it.</em></p>\n<hr />\n<p>Don't confuse this question with:</p>\n<pre><code>grep &quot;pattern1\\|pattern2\\|...&quot;\n</code></pre>\n<p>This is an <strong>OR</strong> multi pattern match. I am looking for an <strong>AND</strong> pattern match.</p>\n",
    "answer": "<p>To find the lines that match each and everyone of a list of patterns, <code>agrep</code> (the original one, now shipped with <a href=\"https://github.com/az143/glimpse\" rel=\"nofollow noreferrer\">glimpse</a>, not the unrelated one in the <a href=\"https://laurikari.net/tre/download/\" rel=\"nofollow noreferrer\">TRE regexp library</a>) can do it with this syntax:</p>\n<pre><code>agrep 'pattern1;pattern2'\n</code></pre>\n<p>With GNU <code>grep</code>, when built with PCRE support, you can use several lookahead assertions:</p>\n<pre><code>grep -P '^(?=.*pattern1)(?=.*pattern2)'\n</code></pre>\n<p>With <a href=\"https://github.com/att/ast/blob/master/src/cmd/re/grep.c\" rel=\"nofollow noreferrer\">ast <code>grep</code></a>:</p>\n<pre><code>grep -X '.*pattern1.*&amp;.*pattern2.*'\n</code></pre>\n<p>(adding <code>.*</code>s as <code>&lt;x&gt;&amp;&lt;y&gt;</code> matches strings that match both <code>&lt;x&gt;</code> and <code>&lt;y&gt;</code> <em>exactly</em>, <code>a&amp;b</code> would never match as there's no such string that can <em>be</em> both <code>a</code> and <code>b</code> at the same time).</p>\n<p>If the patterns don't overlap, you may also be able to do:</p>\n<pre><code>grep -e 'pattern1.*pattern2' -e 'pattern2.*pattern1'\n</code></pre>\n<p>The best portable way is probably with <code>awk</code> as already mentioned:</p>\n<pre><code>awk '/pattern1/ &amp;&amp; /pattern2/'\n</code></pre>\n<p>Or with <code>sed</code>:</p>\n<pre><code>sed -e '/pattern1/!d' -e '/pattern2/!d'\n</code></pre>\n<p>Or <code>perl</code>:</p>\n<pre><code>perl -ne 'print if /pattern1/ &amp;&amp; /pattern2/'\n</code></pre>\n<p>Please beware that all those will have different regular expression syntaxes.</p>\n<p>The <code>awk</code>/<code>sed</code>/<code>perl</code> ones don't reflect whether any line matched the patterns in their exit status. To so that you need:</p>\n<pre><code>awk '/pattern1/ &amp;&amp; /pattern2/ {print; found = 1}\n     END {exit !found}'\n</code></pre>\n<pre><code>perl -ne 'if (/pattern1/ &amp;&amp; /pattern2/) {print; $found = 1}\n          END {exit !$found}'\n</code></pre>\n<p>Or pipe the command to <code>grep '^'</code>.</p>\n<p>For potentially gzip-compressed files, you can use <code>zgrep</code> which is generally a shell script wrapper around <code>grep</code>, and use one of the <code>grep</code> solutions above (not the ast-open one as that <code>grep</code> implementation cannot be use by <code>zgrep</code>) or you could use the <code>PerlIO::gzip</code> module of <code>perl</code> which can transparently uncompress files upon input:</p>\n<pre><code>perl -MPerlIO::gzip -Mopen='IN,gzip(autopop)' -ne '\n  print &quot;$ARGV:$_&quot; if /pattern1/ &amp;&amp; /pattern2/' -- *.gz\n</code></pre>\n<p>(which if the files are small enough at least is even going to be more efficient than <code>zgrep</code> as the decompression is done internally without having to run <code>gunzip</code> for each file).</p>\n",
    "url": "https://unix.stackexchange.com/questions/55359/how-to-run-grep-with-multiple-and-patterns"
  },
  {
    "question_title": "Convince grep to output all lines, not just those with matches",
    "question_body": "<p>Say I have the following file:</p>\n\n<pre><code>$ cat test\n\ntest line 1\ntest line 2\nline without the search word\nanother line without it\ntest line 3 with two test words\ntest line 4\n</code></pre>\n\n<p>By default, <code>grep</code> returns each line that contains the search term:</p>\n\n<pre><code>$ grep test test\n\ntest line 1\ntest line 2\ntest line 3 with two test words\ntest line 4\n</code></pre>\n\n<p>Passing the <code>--color</code> parameter to <code>grep</code> will make it highlight the portion of the line that matches the search expression, but it still only returns lines that contain the expression. Is there a way to get <code>grep</code> to output every line in the source file, but highlight the matches?</p>\n\n<p>My current terrible hack to accomplish this (at least on files that don't have 10000+ consecutive lines with no matches) is:</p>\n\n<pre><code>$ grep -B 9999 -A 9999 test test\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/8auYL.png\" alt=\"Screenshot of the two commands\"></p>\n\n<p>If <code>grep</code> can't accomplish this, is there another command-line tool that offers the same functionality? I've fiddled with <a href=\"https://beyondgrep.com/\" rel=\"noreferrer\"><code>ack</code></a>, but it doesn't seem to have an option for it either.</p>\n",
    "answer": "<pre><code>grep --color -E \"test|$\" yourfile\n</code></pre>\n\n<p>What we're doing here is matching against the <code>$</code> pattern and the test pattern, obviously <code>$</code> doesn't have anything to colourize so only the test pattern gets color.  The <code>-E</code> just turns on extended regex matching.</p>\n\n<p>You can create a function out of it easily like this:</p>\n\n<pre><code>highlight () { grep --color -E \"$1|$\" \"${@:1}\" ; }\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/366/convince-grep-to-output-all-lines-not-just-those-with-matches"
  },
  {
    "question_title": "How to grep standard error stream (stderr)?",
    "question_body": "<p>I am using ffmpeg to get the meta info of an audio clip. But I am unable to grep it.</p>\n\n<pre><code>    $ ffmpeg -i 01-Daemon.mp3  |grep -i Duration\n    FFmpeg version SVN-r15261, Copyright (c) 2000-2008 Fabrice Bellard, et al.\n      configuration: --prefix=/usr --bindir=/usr/bin \n      --datadir=/usr/share/ffmpeg --incdir=/usr/include/ffmpeg --libdir=/usr/lib\n      --mandir=/usr/share/man --arch=i386 --extra-cflags=-O2 \n      ...\n</code></pre>\n\n<p>I checked, this ffmpeg output is directed to stderr.</p>\n\n<pre><code>$ ffmpeg -i 01-Daemon.mp3 2&gt; /dev/null\n</code></pre>\n\n<p>So I think that grep is unable to read error stream to catch matching lines. How can we enable grep to read error stream?</p>\n\n<p>Using <a href=\"http://www.cyberciti.biz/faq/redirecting-stderr-to-stdout/\">nixCraft</a> link, I redirected standard error stream to standard output stream, then grep worked.</p>\n\n<pre><code>$ ffmpeg -i 01-Daemon.mp3 2&gt;&amp;1 | grep -i Duration\n  Duration: 01:15:12.33, start: 0.000000, bitrate: 64 kb/s\n</code></pre>\n\n<p>But what if we do not want to redirect stderr to stdout?</p>\n",
    "answer": "<p>If you're using <code>bash</code> why not employ anonymous pipes, in essence shorthand for what @phunehehe said:</p>\n<pre class=\"lang-none prettyprint-override\"><code>ffmpeg -i 01-Daemon.mp3 2&gt; &gt;(grep -i Duration)\n</code></pre>\n<p>And, as suggested by @tlo, to keep the filtered output in <code>stderr</code>, use:</p>\n<pre class=\"lang-none prettyprint-override\"><code>ffmpeg -i 01-Daemon.mp3 2&gt; &gt;(grep -i Duration &gt;&amp;2)\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/3514/how-to-grep-standard-error-stream-stderr"
  },
  {
    "question_title": "How can I count the number of lines of a file with common tools?",
    "question_body": "<p>I am redirecting <code>grep</code> results to a file, and then using <code>cat</code> to show its contents on the screen. I want to know how many lines of results I have in my results file and then add it to some counter.</p>\n\n<p>What will be the best way? Any relevant flag to <code>grep</code> or <code>cat</code>?</p>\n",
    "answer": "<p>If you have already collected the <code>grep</code> output in a file, you could output a numbered list with:</p>\n<pre><code>cat -n myfile\n</code></pre>\n<p>If you only want the number of lines, simply do:</p>\n<pre><code>wc -l myfile\n</code></pre>\n<p>There is absolutely no reason to do:</p>\n<pre><code>cat myfile | wc -l\n</code></pre>\n<p>...as this needlessly does I/O (the <code>cat</code>) that <code>wc</code> has to repeat.  Besides, you have two processes where one suffices.</p>\n<p>If you want to <code>grep</code> to your terminal and print a count of the matches at the end, you can do:</p>\n<pre><code>grep whatever myfile | tee /dev/tty | wc -l\n</code></pre>\n<hr />\n<p>Note: <code>/dev/tty</code> is the controlling terminal. From the <a href=\"https://linux.die.net/man/4/tty\" rel=\"noreferrer\"><code>tty(4)</code> man page</a>:</p>\n<blockquote>\n<p>The file <code>/dev/tty</code> is a character file with major number 5 and minor number 0, usually of mode 0666 and <code>owner.group</code> <code>root.tty</code>. It is a synonym for the controlling terminal of a process, if any.</p>\n<p>In addition to the <a href=\"https://linux.die.net/man/2/ioctl\" rel=\"noreferrer\"><code>ioctl(2)</code></a> requests supported by the device that tty refers to, the <a href=\"https://linux.die.net/man/2/ioctl\" rel=\"noreferrer\"><code>ioctl(2)</code></a> request <code>TIOCNOTTY</code> is supported.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/25344/how-can-i-count-the-number-of-lines-of-a-file-with-common-tools"
  },
  {
    "question_title": "grep on a variable",
    "question_body": "<p>Let's say I have a variable</p>\n\n<pre><code>line=\"This is where we select from a table.\"\n</code></pre>\n\n<p>now I want to grep how many times does select occur in the sentence. </p>\n\n<pre><code>grep -ci \"select\" $line\n</code></pre>\n\n<p>I tried that, but it did not work. I also tried </p>\n\n<pre><code>grep -ci \"select\" \"$line\"\n</code></pre>\n\n<p>It still doesn't work.  I get the following error. </p>\n\n<pre><code>grep: This is where we select from a table.: No such file or directory\n</code></pre>\n",
    "answer": "<p>Have <code>grep</code> read on its standard input. There you go, using <a href=\"http://linuxcommand.org/lts0060.php\" rel=\"noreferrer\">a pipe</a>...</p>\n\n<pre><code>$ echo \"$line\" | grep select\n</code></pre>\n\n<p>... or <a href=\"http://linux.die.net/abs-guide/x15683.html\" rel=\"noreferrer\">a here string</a>...</p>\n\n<pre><code>$ grep select &lt;&lt;&lt; \"$line\"\n</code></pre>\n\n<p>Also, you might want to replace spaces by newlines before grepping :</p>\n\n<pre><code>$ echo \"$line\" | tr ' ' '\\n' | grep select\n</code></pre>\n\n<p>... or you could ask <code>grep</code> to print the match only:</p>\n\n<pre><code>$ echo \"$line\" | grep -o select\n</code></pre>\n\n<p>This will allow you to get rid of the rest of the line when there's a match.</p>\n\n<p><em>Edit:</em> Oops, read a little too fast, thanks <a href=\"https://unix.stackexchange.com/users/12779/marco\">Marco</a>. In order to count the occurences, just pipe any of these to <a href=\"http://linux.die.net/man/1/wc\" rel=\"noreferrer\"><code>wc(1)</code></a> ;)</p>\n\n<p><em>Another edit made after <a href=\"https://unix.stackexchange.com/users/12380/izkata\">lzkata</a>'s comment, quoting <code>$line</code> when using <code>echo</code>.</em></p>\n",
    "url": "https://unix.stackexchange.com/questions/163810/grep-on-a-variable"
  },
  {
    "question_title": "How do I pass a list of files to grep",
    "question_body": "<p>I am using <code>find</code> and getting a list of files I want to <code>grep</code> through. How do I pipe that list to <code>grep</code>?</p>\n",
    "answer": "<p>Well, the generic case that works with any command that writes to stdout is to use <code>xargs</code>, which will let you attach any number of command-line arguments to the end of a command:</p>\n<pre><code>$ find … | xargs grep 'search'\n</code></pre>\n<p>Or to embed the command in your <code>grep</code> line with backticks or <code>$()</code>, which will run the command and substitute its output:</p>\n<pre><code>$ grep 'search' $(find …)\n</code></pre>\n<p>Note that these commands don't work if the file names contain whitespace, or certain other “weird characters” (<code>\\'&quot;</code> for xargs, <code>\\[*?</code> for <code>$(find …)</code>).</p>\n<hr />\n<p>However, in the specific case of <code>find</code> the ability to execute a program on the given arguments is built-in:</p>\n<pre><code>$ find … -exec grep 'search' {} \\;\n</code></pre>\n<p>Everything between <code>-exec</code> and <code>;</code> is the command to execute; <code>{}</code> is replaced with the filename found by <code>find</code>. That will execute a separate <code>grep</code> for each file; since <code>grep</code> can take many filenames and search them all, you can change the <code>;</code> to <code>+</code> to tell find to pass all the matching filenames to <code>grep</code> at once:</p>\n<pre><code>$ find … -exec grep 'search' {} \\+\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/20262/how-do-i-pass-a-list-of-files-to-grep"
  },
  {
    "question_title": "How to grep lines which does not begin with &quot;#&quot; or &quot;;&quot;?",
    "question_body": "<p>I want to grep <code>smb.conf</code> and see only lines which are not commented.</p>\n",
    "answer": "<pre><code>grep \"^[^#;]\" smb.conf\n</code></pre>\n\n<p>The first <code>^</code> refers to the beginning of the line, so lines with comments starting after the first character will not be excluded.  <code>[^#;]</code> means any character which is not <code>#</code> or <code>;</code>.</p>\n\n<p>In other words, it reports lines that start with any character other than <code>#</code> and <code>;</code>. It's not the same as reporting the lines that don't start with <code>#</code> and <code>;</code> (for which you'd use <code>grep -v '^[#;]'</code>) in that it also excludes <em>empty</em> lines, but that's probably preferable in this case as I doubt you care about empty lines.</p>\n\n<p>If you wanted to ignore leading blank characters, you could change it to:</p>\n\n<pre><code>grep '^[[:blank:]]*[^[:blank:]#;]' smb.conf\n</code></pre>\n\n<p>or</p>\n\n<pre><code>grep -vxE '[[:blank:]]*([#;].*)?' smb.conf\n</code></pre>\n\n<p>Or</p>\n\n<pre><code>awk '$1 ~ /^[^;#]/' smb.conf\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/60994/how-to-grep-lines-which-does-not-begin-with-or"
  },
  {
    "question_title": "Match exact string using grep",
    "question_body": "<p>I have a text file:</p>\n\n<pre><code>deiauk 1611516 afsdf 765\nminkra 18415151 asdsf 4152\nlinkra sfsfdsfs sdfss 4555\ndeiauk1 sdfsfdsfs 1561 51\ndeiauk2 115151 5454 4\ndeiauk 1611516 afsdf ddfgfgd\nluktol1 4545 4 9\nluktol 1\n</code></pre>\n\n<p>and I want to match exactly <code>deiauk</code>.  When I do this:</p>\n\n<pre><code>grep \"deiauk\" file.txt\n</code></pre>\n\n<p>I get this result:</p>\n\n<pre><code>deiauk 1611516 afsdf 765\ndeiauk1 sdfsfdsfs 1561 51\ndeiauk2 115151 5454 4\n</code></pre>\n\n<p>but I only need this:</p>\n\n<pre><code>deiauk 1611516 afsdf 765\ndeiauk 1611516 afsdf ddfgfgd\n</code></pre>\n\n<p>I know there's a <code>-w</code> option, but then my string has to mach whole line.</p>\n",
    "answer": "<p>Try one of:</p>\n\n<pre><code>grep -w \"deiauk\" textfile\n\ngrep \"\\&lt;deiauk\\&gt;\" textfile\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/206903/match-exact-string-using-grep"
  },
  {
    "question_title": "Preventing grep from causing premature termination of &quot;bash -e&quot; script",
    "question_body": "<p>This script does not output <code>after</code>:</p>\n<pre><code>#!/bin/bash -e\n\necho &quot;before&quot;\n\necho &quot;anything&quot; | grep e # it would if I searched for 'y' instead\n\necho &quot;after&quot;\nexit\n</code></pre>\n<p>I could solve this by removing the <code>-e</code> option on the shebang line, but I wish to keep it so my script stops if there is an error. I do not consider <code>grep</code> finding no match as an error. How may I prevent it from exiting so abruptly?</p>\n",
    "answer": "<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;anything&quot; | { grep e || true; }\n</code></pre>\n<p>Explanation:</p>\n<ul>\n<li>This will throw an error\n<pre class=\"lang-shellsession prettyprint-override\"><code>$ echo &quot;anything&quot; | grep e\n$ echo $?\n1\n</code></pre>\n</li>\n<li>This will not throw an error\n<pre class=\"lang-shellsession prettyprint-override\"><code>$ echo &quot;anything&quot; | { grep e || true; }\n$ echo $?\n0\n</code></pre>\n</li>\n<li>DopeGhoti's &quot;no-op&quot; version (Potentially avoids spawning a process, if <code>true</code> is not a builtin), this will not throw an error\n<pre class=\"lang-shellsession prettyprint-override\"><code>$ echo &quot;anything&quot; | { grep e || :; }\n$ echo $?\n0\n</code></pre>\n</li>\n</ul>\n<p>The <code>||</code> means &quot;or&quot;. If the first part of the command &quot;fails&quot; (meaning <code>grep e</code> returns a non-zero exit code) then the part after the <code>||</code> is executed, succeeds and returns zero as the exit code  (<code>true</code> always returns zero).</p>\n",
    "url": "https://unix.stackexchange.com/questions/330660/preventing-grep-from-causing-premature-termination-of-bash-e-script"
  },
  {
    "question_title": "How can I suppress output from grep, so that it only returns the exit status?",
    "question_body": "<p>I have the <code>grep</code> command. I'm searching for a keyword from a file, but I don't want to display the match. I just want to know the exit status of the <code>grep</code>.</p>\n",
    "answer": "<p>Any <a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/utilities/grep.html\">POSIX compliant version of <code>grep</code></a> has the switch <code>-q</code> for quiet:</p>\n\n<pre><code>-q\n     Quiet. Nothing shall be written to the standard output, regardless\n     of matching lines. Exit with zero status if an input line is selected.\n</code></pre>\n\n<p>In GNU grep (and possibly others) you can use long-option synonyms as well:</p>\n\n<pre><code>-q, --quiet, --silent     suppress all normal output\n</code></pre>\n\n<h3>Example</h3>\n\n<p>String exists:</p>\n\n<pre><code>$ echo \"here\" | grep -q \"here\"\n$ echo $?\n0\n</code></pre>\n\n<p>String doesn't exist:</p>\n\n<pre><code>$ echo \"here\" | grep -q \"not here\"\n$ echo $?\n1\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/104641/how-can-i-suppress-output-from-grep-so-that-it-only-returns-the-exit-status"
  },
  {
    "question_title": "How to grep for same string but multiple files at the same time?",
    "question_body": "<p>I have a set of log files that I need to review and I would like to search specific strings on the same files at once Is this possible? Currently I am using </p>\n\n<pre><code>grep -E 'fatal|error|critical|failure|warning|' /path_to_file\n</code></pre>\n\n<p>How do I use this and search for the strings of multiple files at once? If this is something that needs to be scripted, can someone provide a simple script to do this?</p>\n",
    "answer": "<pre><code>grep -E 'fatal|error|critical|failure|warning|' *.log\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/171314/how-to-grep-for-same-string-but-multiple-files-at-the-same-time"
  },
  {
    "question_title": "Detecting pattern at the end of a line with grep",
    "question_body": "<p>If I do:</p>\n\n<pre><code>$ ls -R\n.:\n4Shared/  Cloud/\n\n./4Shared:\nUFAIZLV2R7.part3.rar\n\n./Cloud:\nUFAIZLV2R7.part2.rar.part\nUFAIZLV2R7.part1.rar.part\nUFAIZLV2R7.part4.rar.part\n</code></pre>\n\n<p>If I want to list the <code>.rar</code> files only, and I use <strong>grep</strong>, it will show me too the <code>.rar.part</code> files, what is not my wish.<br>\nI am solving this using <code>find</code> or <code>ls **/*.rar</code> as told in <a href=\"https://unix.stackexchange.com/questions/7169/what-is-the-linux-equivalent-of-dos-dir-s-b-filename\">this thread</a> and they work fine, but I would like to learn if it is possible to do it via <code>grep</code>.  </p>\n\n<p>I have tried (thinking about <code>EOL</code>):</p>\n\n<pre><code>ls -R | grep \".rar\\n\"\n</code></pre>\n\n<p>with no results.<br>\nI think that the problem lies in discover if the greping is found <strong>at the end of the line</strong>, but I am not sure.  </p>\n\n<p>Any help out here, please?</p>\n",
    "answer": "<p>The <code>$</code> anchor matches the end of a line.</p>\n\n<pre><code>ls -R | grep '\\.rar$'\n</code></pre>\n\n<p>You can also use <code>find</code> for this:</p>\n\n<pre><code>find . -name '*.rar'\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/124462/detecting-pattern-at-the-end-of-a-line-with-grep"
  },
  {
    "question_title": "Why doesn&#39;t grep ignore binary files by default?",
    "question_body": "<p>The manpage for <code>grep</code> describes the <code>-I</code> flag as follows:</p>\n\n<pre><code>-I      Ignore binary files.  This option is equivalent to \n        --binary-file=without-match option.\n</code></pre>\n\n<p>It also says this about binary files:</p>\n\n<pre><code> --binary-files=value Controls searching and printing of binary files.\n         Options are binary, the default: search binary files but do not print\n         them; without-match: do not search binary files; and text: treat all\n         files as text.\n</code></pre>\n\n<p>I cannot think of a scenario where I would care about matches in binary files. If such a scenario exists, surely it must be the exception rather than the norm. Why doesn't <code>grep</code> ignore binary files by default rather than requiring setting this flag to do so?</p>\n",
    "answer": "<p>Not everything that grep thinks is a binary file, is actually a binary file. e.g. puppet's logs have ansi color coding in them, which makes grep think they're binary. I'd still want to search them if I'm grepping through /var/log though.</p>\n",
    "url": "https://unix.stackexchange.com/questions/70438/why-doesnt-grep-ignore-binary-files-by-default"
  },
  {
    "question_title": "grep inside less?",
    "question_body": "<p>I'm currently sifting through a lot of unfamiliar logs looking for some issues.  The first file I look at is Events.log, and I get at least three pages in <code>less</code> which appear to display the same event at different times – an event that appears to be fairly benign.\nI would like to filter this event out, and currently I quit <code>less</code> and do something like</p>\n\n<pre><code>grep -v \"event text\" Events.log | less\n</code></pre>\n\n<p>This now brings a number of other common, uninteresting events that I would also like to filter out.  Is there a way I can <code>grep -v</code> <em>inside</em> of <code>less</code>?  Rather than having to do</p>\n\n<pre><code>egrep -v \"event text|something else|the other thing|foo|bar\" Events.log | less\n</code></pre>\n\n<p>It strikes me as a useful feature when looking at any kind of log file – and if <code>less</code> isn't the tool, is there another with the qualities I seek?  Just a <code>less</code>-style viewer with built in <code>grep</code>.</p>\n",
    "answer": "<p><code>less</code> has very powerful pattern matching.  From the <a href=\"http://linux.die.net/man/1/less\">man page</a>:</p>\n\n<blockquote>\n  <p><code>&amp;<i>pattern</i></code><ul>\n  Display only lines which match the <code><i>pattern</i></code>;\n  lines which do not match the <code><i>pattern</i></code>\n  are not displayed.  If <code><i>pattern</i></code> is empty\n  (if you type <code>&amp;</code> immediately followed by <kbd>ENTER</kbd>),\n  any filtering is turned off, and all lines are displayed. \n  While filtering is in effect,\n  an ampersand is displayed at the beginning of the prompt,\n  as a reminder that some lines in the file may be hidden.</p>\n  \n  <p>Certain characters are special as in the <code>/</code> command<sup>&dagger;</sup>:</p>\n  \n  <p><code>^N</code> or <code>!</code><ul>\n  Display only lines which do NOT match the <code><i>pattern</i></code>.</ul>\n  <code>^R</code><ul>\n      Don't interpret regular expression metacharacters;\n  that is, do a simple textual comparison.</ul><br>\n  ____________<br>\n  <sup>&dagger;</sup> Certain characters are special\n  if entered at the beginning of the <code><i>pattern</i></code>;\n  they modify the type of search\n  rather than become part of the <code><i>pattern</i></code>.\n  </ul></p>\n</blockquote>\n\n<p>   (Of course <code>^N</code> and <code>^R</code> represent <kbd>Ctrl</kbd>+<kbd>N</kbd>\nand <kbd>Ctrl</kbd>+<kbd>R</kbd>, respectively.) </p>\n\n<p>So, for example, <code>&amp;dns</code> will display only lines that match the pattern <code>dns</code>,\nand <code>&amp;!dns</code> will filter out (exclude) those lines,\ndisplaying only lines that don't match the pattern.</p>\n\n<p>It is noted in the description of the <code>/</code> command that</p>\n\n<blockquote>\n  <ul>The <code><i>pattern</i></code> is a regular expression,\n  as recognized by the regular expression library supplied by your system.</ul>\n</blockquote>\n\n<p>So</p>\n\n<ul>\n<li><code>&amp;eth[01]</code>  will display lines containing <code>eth0</code> or <code>eth1</code></li>\n<li><code>&amp;arp.*eth0</code> will display lines containing <code>arp</code> followed by <code>eth0</code></li>\n<li><code>&amp;arp|dns</code>  will display lines containing <code>arp</code> or <code>dns</code></li>\n</ul>\n\n<p>And the <code>!</code> can invert any of the above. \nSo the command you would want to use for the example in your question is:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>&amp;!event text|something else|the other thing|foo|bar\n</code></pre>\n\n<p>Also use <code>/<i>pattern</i></code> and <code>?<i>pattern</i></code>\nto search (and <code>n</code>/<code>N</code> to go to next/previous).</p>\n",
    "url": "https://unix.stackexchange.com/questions/179238/grep-inside-less"
  },
  {
    "question_title": "Is there a basic tutorial for grep, awk and sed?",
    "question_body": "<p>I've been a Linux user for a while, and I've a pretty decent understanding of most the common command line utilities. However, ones that come up and up again in relation to programming are <code>grep</code>, <code>awk</code>, and <code>sed</code>.</p>\n\n<p>About the only thing I've used grep for is piping stuff into it to find files in log files, the output of <code>ps</code> etc. I haven't used <code>awk</code> or <code>sed</code> at all. Are there any good tutorials for these utilities?</p>\n",
    "answer": "<p>AWK is particularly well suited for tabular data and has a lower learning curve than some alternatives.</p>\n<p><a href=\"http://www.grymoire.com/Unix/Awk.html\" rel=\"nofollow noreferrer\">AWK: A Tutorial and Introduction</a></p>\n<p><a href=\"http://vc.airvectors.net/tsawk.html\" rel=\"nofollow noreferrer\">An AWK Primer</a> (<a href=\"https://en.wikibooks.org/wiki/An_Awk_Primer\" rel=\"nofollow noreferrer\">alt link</a>)</p>\n<p><a href=\"http://www.regular-expressions.info/\" rel=\"nofollow noreferrer\">RegularExpressions.info</a></p>\n<p><a href=\"http://www.panix.com/%7Eelflord/unix/sed.html\" rel=\"nofollow noreferrer\">sed tutorial</a></p>\n<p><a href=\"http://www.panix.com/%7Eelflord/unix/grep.html\" rel=\"nofollow noreferrer\">grep tutorial</a></p>\n<p><code>info sed</code>, <code>info grep</code> and <code>info awk</code> or <code>info gawk</code></p>\n",
    "url": "https://unix.stackexchange.com/questions/2434/is-there-a-basic-tutorial-for-grep-awk-and-sed"
  },
  {
    "question_title": "How to print all lines after a match up to the end of the file?",
    "question_body": "<p>Input file1 is:</p>\n\n<pre><code>dog 123 4335\ncat 13123 23424 \ndeer 2131 213132\nbear 2313 21313\n</code></pre>\n\n<p>I give the match the  pattern from in <code>other file</code> ( like <code>dog 123 4335</code> from   file2).</p>\n\n<p>I match the pattern of the line is <code>dog 123 4335</code> and after printing\nall lines without match line my output is: </p>\n\n<pre><code>cat 13123 23424\ndeer 2131 213132\nbear 2313 21313\n</code></pre>\n\n<p>If only use without address of line only use the pattern, for example <code>1s</code>\nhow to  match and  print the lines?</p>\n",
    "answer": "<p>In practice, I'd probably use <a href=\"https://unix.stackexchange.com/questions/56429/how-to-print-all-lines-after-a-match-up-to-the-end-of-the-file/140314#140314\">Aet3miirah's answer</a> most of the time, and <a href=\"https://unix.stackexchange.com/questions/56429/how-to-print-all-lines-after-a-match-up-to-the-end-of-the-file/77650#77650\">alexey's answer</a> is wonderful for navigating through the lines (also, it works with <code>less</code>). OTOH, I really like another approach (which is kind of the reversed <a href=\"https://unix.stackexchange.com/questions/56429/how-to-print-all-lines-after-a-match-up-to-the-end-of-the-file/56517#56517\">Gilles' answer</a>):</p>\n<pre><code>sed -n '/dog 123 4335/,$p'\n</code></pre>\n<p>When called with the <code>-n</code> flag, <code>sed</code> does not print by default the lines it processes anymore. Then we use a 2-address form that says to apply a command from the line matching <code>/dog 123 4335/</code> until the end of the file (represented by <code>$</code>). The command in question is <code>p</code>, which prints the current line. So, this means &quot;print all lines from the one matching <code>/dog 123 4335/</code> until the end.&quot;</p>\n",
    "url": "https://unix.stackexchange.com/questions/56429/how-to-print-all-lines-after-a-match-up-to-the-end-of-the-file"
  },
  {
    "question_title": "Reading grep patterns from a file",
    "question_body": "<p>I have a couple of big text files and in the file <code>UNIQS.txt</code> I have a list of strings to <code>grep</code> from another file. The code I use is</p>\n\n<pre><code>grep -f UNIQS.txt EEP_VSL.uniqs.sam &gt; UNIQ_templates.sam\n</code></pre>\n\n<p>which does nothing - the file generated is empty. But when I do</p>\n\n<pre><code>grep -F -f UNIQS.txt EEP_VSL.uniqs.sam &gt; UNIQ_templates.sam\n</code></pre>\n\n<p>it works correctly. This confuses me because I didn't think <code>grep</code> would interpret the entries in <code>UNIQS.txt</code> as regexp patterns without quotes and slashes and so on being in the file (which there aren't). Is it the case in general that if you are getting the patterns from a file then it will automatically think that they are regexp patterns?</p>\n\n<p><strong>Edit:</strong> In the <code>UNIQS.txt</code> file, there are newline separated strings of the form</p>\n\n<pre><code>HWI-ST365:215:D0GH0ACXX:2:1101:10034:186783\n</code></pre>\n\n<p>(called template names) and the file <code>EEP_VSL...</code> tab separated columns, with about 14 columns and the first column is the template name, so basically I want to extract the line corresponding to each template in the file.</p>\n",
    "answer": "<p>The <code>-f</code> option specifies a file where grep reads patterns. That's just like passing patterns on the command line (with the <code>-e</code> option if there's more than one), except that when you're calling from a shell you may need to quote the pattern to protect special characters in it from being expanded by the shell.</p>\n\n<p>The argument <code>-E</code> or <code>-F</code> or <code>-P</code>, if any, tells grep which syntax the patterns are written in. With no argument, grep expects <a href=\"http://en.wikipedia.org/wiki/Regular_expression#POSIX_basic_and_extended\">basic regular expressions</a>; with <code>-E</code>, grep expects <a href=\"http://en.wikipedia.org/wiki/Regular_expression#POSIX_extended\">extended regular expressions</a>; with <code>-P</code> (if supported), grep expects <a href=\"http://en.wikipedia.org/wiki/Regular_expression#Standard_Perl\">Perl regular expressions</a>; and with <code>-F</code>, grep expects literal strings. Whether the patterns come from the command line or from a file doesn't matter.</p>\n\n<p>Note that the strings are substrings: if you pass <code>a+b</code> as a pattern then a line containing <code>a+b+c</code> is matched. If you want to search for lines containing exactly one of the supplied strings and no more, then pass the <code>-x</code> option.</p>\n",
    "url": "https://unix.stackexchange.com/questions/83260/reading-grep-patterns-from-a-file"
  },
  {
    "question_title": "How to read first and last line from cat output?",
    "question_body": "<p>I have text file. Task - get first and last line from file after </p>\n\n<pre><code>$ cat file | grep -E \"1|2|3|4\" | commandtoprint\n\n$ cat file\n1\n2\n3\n4\n5\n</code></pre>\n\n<p>Need this without cat output (only 1 and 5).</p>\n\n<pre><code>~$ cat file | tee &gt;(head -n 1) &gt;(wc -l)\n1\n2\n3\n4\n5\n5\n1\n</code></pre>\n\n<p>Maybe awk and more shorter solution exist...</p>\n",
    "answer": "<p><strong>sed</strong> Solution:</p>\n\n<pre><code>sed -e 1b -e '$!d' file\n</code></pre>\n\n<p>When reading from <code>stdin</code> if would look like this (for example <code>ps -ef</code>):</p>\n\n<pre><code>ps -ef | sed -e 1b -e '$!d'\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      1931  1837  0 20:05 pts/0    00:00:00 sed -e 1b -e $!d\n</code></pre>\n\n<p><strong>head &amp; tail</strong> Solution:</p>\n\n<pre><code>(head -n1 &amp;&amp; tail -n1) &lt;file\n</code></pre>\n\n<p>When data is coming from a command (<code>ps -ef</code>):</p>\n\n<pre><code>ps -ef 2&gt;&amp;1 | (head -n1 &amp;&amp; tail -n1)\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      2068  1837  0 20:13 pts/0    00:00:00 -bash\n</code></pre>\n\n<p><strong>awk</strong> Solution:</p>\n\n<pre><code>awk 'NR==1; END{print}' file\n</code></pre>\n\n<p>And also the piped example with <code>ps -ef</code>:</p>\n\n<pre><code>ps -ef | awk 'NR==1; END{print}'\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      1935  1837  0 20:07 pts/0    00:00:00 awk NR==1; END{print}\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/139089/how-to-read-first-and-last-line-from-cat-output"
  },
  {
    "question_title": "grep and tail -f?",
    "question_body": "<p>Is it possible to do a <code>tail -f</code> (or similar) on a file, and <code>grep</code> it at the same time? I wouldn't mind other commands just looking for that kind of behavior.</p>\n",
    "answer": "<p>Using GNU <code>tail</code> and GNU <code>grep</code>, I am able to grep a <code>tail -f</code> using the straight-forward syntax:</p>\n\n<pre><code>tail -f /var/log/file.log | grep search_term\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/3229/grep-and-tail-f"
  },
  {
    "question_title": "Limit grep context to N characters on line",
    "question_body": "<p>I have to grep through some JSON files in which the line lengths exceed a few thousand characters. <strong>How can I limit grep to display context up to N characters to the left and right of the match?</strong> Any tool other than grep would be fine as well, so long as it available in common Linux packages.</p>\n\n<p>This would be example output, for the <strong>imaginary grep switch Ф</strong>:</p>\n\n<pre><code>$ grep -r foo *\nhello.txt: Once upon a time a big foo came out of the woods.\n\n$ grep -Ф 10 -r foo *\nhello.txt: ime a big foo came of t\n</code></pre>\n",
    "answer": "<p>Try to use this one:</p>\n\n<pre><code>grep -r -E -o \".{0,10}wantedText.{0,10}\" *\n</code></pre>\n\n<p><strong>-E</strong> tells, that you want to use extended regex</p>\n\n<p><strong>-o</strong> tells, that you want to print only the match</p>\n\n<p><strong>-r</strong> grep is looking for result recursively in the folder</p>\n\n<p><strong>REGEX:</strong></p>\n\n<p><em>{0,10}</em> tells, how many arbitrary characters you want to print</p>\n\n<p><strong>.</strong> represents an arbitrary character (a character itself wasn't important here, just their number)</p>\n\n<p><strong>Edit:</strong> Oh, I see, that Joseph recommends almost the same solution as I do :D</p>\n",
    "url": "https://unix.stackexchange.com/questions/163726/limit-grep-context-to-n-characters-on-line"
  },
  {
    "question_title": "grep lines starting with &quot;1&quot; in Ubuntu",
    "question_body": "<p>I try to search for lines that start with \"1\" using </p>\n\n<pre><code>ls -1 | grep ^1*\n</code></pre>\n\n<p>but it returns lines that do not start with 1. What I am missing here? </p>\n",
    "answer": "<p>Your regular expression doesn't mean what you think it does. It matches all lines starting (^) with one (1) repeated zero or more (*) times. All strings match that regular expression. <code>grep '^1'</code> does what you want.</p>\n",
    "url": "https://unix.stackexchange.com/questions/59893/grep-lines-starting-with-1-in-ubuntu"
  },
  {
    "question_title": "Grep &#39;OR&#39; regex problem",
    "question_body": "<p>I am trying to use grep with a regex to find lines in a file that match 1 of 2 possible strings. Here is my grep:</p>\n\n<pre><code>$ grep \"^ID.*(ETS|FBS)\" my_file.txt\n</code></pre>\n\n<p>The above grep returns no results. However if I execute either:</p>\n\n<pre><code>$ grep \"^ID.*ETS\" my_file.txt  \n</code></pre>\n\n<p>or   </p>\n\n<pre><code>$ grep \"^ID.*FBS\" my_file.txt  \n</code></pre>\n\n<p>I do match specific lines. Why is my OR regex not matching? Thanks in advance for the help!</p>\n",
    "answer": "<p>With normal regex, the characters <code>(</code>, <code>|</code> and <code>)</code> need to be escaped. So you should use</p>\n\n<pre><code>$ grep \"^ID.*\\(ETS\\|FBS\\)\" my_file.txt\n</code></pre>\n\n<p>You don't need the escapes when you use the <em>extended regex</em> (<code>-E</code>)option. See <code>man grep</code>, section \"<code>Basic vs Extended Regular Expressions</code>\".</p>\n",
    "url": "https://unix.stackexchange.com/questions/21764/grep-or-regex-problem"
  },
  {
    "question_title": "Recursive grep vs find / -type f -exec grep {} \\; Which is more efficient/faster?",
    "question_body": "<p>Which is more efficient for finding which files in an entire filesystem contain a string: recursive grep or find with grep in an exec statement?  I assume find would be more efficient because you can at least do some filtering if you know the file extension or a regex that matches the file name, but when you only know <code>-type f</code> which is better? GNU grep 2.6.3; find (GNU findutils) 4.4.2</p>\n\n<p>Example:</p>\n\n<p><code>grep -r -i 'the brown dog' /</code></p>\n\n<p><code>find / -type f -exec grep -i 'the brown dog' {} \\;</code></p>\n",
    "answer": "<p>I'm not sure:</p>\n\n<blockquote>\n<pre><code>grep -r -i 'the brown dog' /*\n</code></pre>\n</blockquote>\n\n<p>is really what you meant. That would mean grep recursively in all the non-hidden files and dirs in <code>/</code> (but still look inside hidden files and dirs inside those).</p>\n\n<p>Assuming you meant:</p>\n\n<pre><code>grep -r -i 'the brown dog' /\n</code></pre>\n\n<p>A few things to note:</p>\n\n<ul>\n<li>Not all <code>grep</code> implementations support <code>-r</code>. And among those that do, the behaviours differ: some follow symlinks to directories when traversing the directory tree (which means you may end up looking several times in the same file or even run in infinite loops), some will not. Some will look inside device files (and it will take quite some time in <code>/dev/zero</code> for instance) or pipes or binary files..., some will not.</li>\n<li>It's efficient as <code>grep</code> starts looking inside files as soon as it discovers them. But while it looks in a file, it's no longer looking for more files to search in (which is probably just as well in most cases)</li>\n</ul>\n\n<p>Your:</p>\n\n<pre><code>find / -type f -exec grep -i 'the brown dog' {} \\;\n</code></pre>\n\n<p>(removed the <code>-r</code> which didn't make sense here) is terribly inefficient because you're running one <code>grep</code> per file. <code>;</code> should only be used for commands that accept only one argument. Moreover here, because <code>grep</code> looks only in one file, it will not print the file name, so you won't know where the matches are.</p>\n\n<p>You're not looking inside device files, pipes, symlinks..., you're not following symlinks, but you're still potentially looking inside things like <code>/proc/mem</code>.</p>\n\n<pre><code>find / -type f -exec grep -i 'the brown dog' {} +\n</code></pre>\n\n<p>would be a lot better because as few <code>grep</code> commands as possible would be run. You'd get the file name unless the last run has only one file. For that it's better to use:</p>\n\n<pre><code>find / -type f -exec grep -i 'the brown dog' /dev/null {} +\n</code></pre>\n\n<p>or with GNU <code>grep</code>:</p>\n\n<pre><code>find / -type f -exec grep -Hi 'the brown dog' {} +\n</code></pre>\n\n<p>Note that <code>grep</code> will not be started until <code>find</code> has found enough files for it to chew on, so there will be some initial delay. And <code>find</code> will not carry on searching for more files until the previous <code>grep</code> has returned. Allocating and passing the big file list has some (probably negligible) impact, so all in all it's probably going to be less efficient than a <code>grep -r</code> that doesn't follow symlink or look inside devices.</p>\n\n<p>With GNU tools:</p>\n\n<pre><code>find / -type f -print0 | xargs -r0 grep -Hi 'the brown dog'\n</code></pre>\n\n<p>As above, as few <code>grep</code> instances as possible will be run, but <code>find</code> will carry on looking for more files while the first <code>grep</code> invocation is looking inside the first batch. That may or may not be an advantage though. For instance, with data stored on rotational hard drives, <code>find</code> and <code>grep</code> accessing data stored at different locations on the disk will slow down the disk throughput by causing the disk head to move constantly. In a RAID setup (where <code>find</code> and <code>grep</code> may access different disks) or on SSDs, that might make a positive difference.</p>\n\n<p>In a RAID setup, running several <em>concurrent</em> <code>grep</code> invocations might also improve things. Still with GNU tools on RAID1 storage with 3 disks,</p>\n\n<pre><code>find / -type f -print0 | xargs -r0 -P2 grep -Hi 'the brown dog'\n</code></pre>\n\n<p>might increase the performance significantly. Note however that the second <code>grep</code> will only be started once enough files have been found to fill up the first <code>grep</code> command. You can add a <code>-n</code> option to <code>xargs</code> for that to happen sooner (and pass fewer files per <code>grep</code> invocation).</p>\n\n<p>Also note that if you're redirecting <code>xargs</code> output to anything but a terminal device, then the <code>greps</code>s will start buffering their output which means that the output of those <code>grep</code>s will probably be incorrectly interleaved. You'd have to use <code>stdbuf -oL</code> (where available like on GNU or FreeBSD) on them to work around that (you may still have problems with very long lines (typically >4KiB)) or have each write their output in a separate file and concatenate them all in the end.</p>\n\n<p>Here, the string you're looking for is fixed (not a regexp) so using the <code>-F</code> option might make a difference (unlikely as <code>grep</code> implementations know how to optimise that already).</p>\n\n<p>Another thing that could make a big difference is fixing the locale to C if you're in a multi-byte locale:</p>\n\n<pre><code>find / -type f -print0 | LC_ALL=C xargs -r0 -P2 grep -Hi 'the brown dog'\n</code></pre>\n\n<p>To avoid looking inside <code>/proc</code>, <code>/sys</code>..., use <code>-xdev</code> and specify the file systems you want to search in:</p>\n\n<pre><code>LC_ALL=C find / /home -xdev -type f -exec grep -i 'the brown dog' /dev/null {} +\n</code></pre>\n\n<p>Or prune the paths you want to exclude explicitly:</p>\n\n<pre><code>LC_ALL=C find / \\( -path /dev -o -path /proc -o -path /sys \\) -prune -o \\\n  -type f -exec grep -i 'the brown dog' /dev/null {} +\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/131535/recursive-grep-vs-find-type-f-exec-grep-which-is-more-efficient-faster"
  },
  {
    "question_title": "What is a &quot;loop device&quot; when mounting?",
    "question_body": "<p>I am mounting an ISO file, and looking at <a href=\"https://www.cyberciti.biz/tips/how-to-mount-iso-image-under-linux.html\" rel=\"noreferrer\">this tutorial</a>. They use the command:</p>\n\n<pre><code>$ mount -o loop disk1.iso /mnt/disk\n</code></pre>\n\n<p>I'm trying to understand the use of <code>-o loop</code>. I have two questions:</p>\n\n<ol>\n<li><p>When I look at the long man page for mount, it takes time to find that <code>-o</code> option. If I do <code>man mount | grep \"-o\"</code> I get an error, and when I look in the file I do not find any info that \"loop\" is a command text for option <code>-o</code>. Where is that documented?</p></li>\n<li><p>Also, what is the \"loop device\" concept for mounting?</p></li>\n</ol>\n",
    "answer": "<p>A loop device is a pseudo (&quot;fake&quot;) device (actually just a file) that acts as a <a href=\"https://en.wikipedia.org/wiki/Device_file#BLOCKDEV\" rel=\"noreferrer\">block-based device</a>. You want to mount a file <code>disk1.iso</code> that will act as an entire filesystem, so you use loop.</p>\n<p>The <code>-o</code> is short for <code>--options</code>.</p>\n<p>And the last thing, if you want to search for &quot;-o&quot; you need to escape the '-'.</p>\n<p>Try:</p>\n<pre><code>man mount | grep &quot;\\-o&quot;\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/4535/what-is-a-loop-device-when-mounting"
  },
  {
    "question_title": "How to Count the Number of Lines of an Output?",
    "question_body": "<p>Let say I have the program: </p>\n\n<pre><code>Calculate.py\n</code></pre>\n\n<p>Is there a unix command-line that counts the number of lines outputted from my program, Calculate.py?</p>\n",
    "answer": "<p>You can pipe the output in to <code>wc</code>. You can use the <code>-l</code> flag to count lines. Run the program normally and use a pipe to redirect to <code>wc.</code></p>\n\n<pre><code>python Calculate.py | wc -l\n</code></pre>\n\n<p>Alternatively, you can redirect the output of your program to a file, say <code>calc.out</code>, and run <code>wc</code> on that file.</p>\n\n<pre><code>python Calculate.py &gt; calc.out\nwc -l calc.out\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/248245/how-to-count-the-number-of-lines-of-an-output"
  },
  {
    "question_title": "How can I grep for this or that (2 things) in a file?",
    "question_body": "<p>I have a file that has \"then\"'s and \"there\"'s.</p>\n\n<p>I can </p>\n\n<pre><code>$ grep \"then \" x.x\nx and then some\nx and then some\nx and then some\nx and then some\n</code></pre>\n\n<p>and I can</p>\n\n<pre><code>$ grep \"there \" x.x\nIf there is no blob none some will be created\n</code></pre>\n\n<p>How can I search for both in one operation?\nI tried</p>\n\n<pre><code>$ grep (then|there) x.x\n</code></pre>\n\n<p>-bash: syntax error near unexpected token `('</p>\n\n<p>and</p>\n\n<pre><code>grep \"(then|there)\" x.x\ndurrantm.../code\n# (Nothing)\n</code></pre>\n",
    "answer": "<p>You need to put the expression in quotation marks. The error you are receiving is a result of bash interpretting the <code>(</code> as a special character.</p>\n\n<p>Also, you need to tell grep to use extended regular expressions.</p>\n\n<pre><code>$ grep -E '(then|there)' x.x\n</code></pre>\n\n<p>Without extended regular expressions, you have to escape the <code>|</code>, <code>(</code>, and <code>)</code>. Note that we use single quotation marks here. Bash treats backslashes within double quotation marks specially.</p>\n\n<pre><code>$ grep '\\(then\\|there\\)' x.x\n</code></pre>\n\n<p>The grouping isn't necessary in this case.</p>\n\n<pre><code>$ grep 'then\\|there' x.x\n</code></pre>\n\n<p>It would be necessary for something like this:</p>\n\n<pre><code>$ grep 'the\\(n\\|re\\)' x.x\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/82990/how-can-i-grep-for-this-or-that-2-things-in-a-file"
  },
  {
    "question_title": "How to show lines after each grep match until other specific match?",
    "question_body": "<p>I know that by using the <code>\"-A NUM\"</code> switch I can print specific number of trailing lines after each match. I am just wondering if it's possible to print trailing lines until a specific word is found after each match. e.g. When I search for \"Word A\" I want to see the line containing \"Word A\" and also the lines after it until the one containing \"Word D\".</p>\n\n<p>context:</p>\n\n<pre><code>Word A\nWord B\nWord C\nWord D\nWord E\nWord F\n</code></pre>\n\n<p>command:</p>\n\n<pre><code>grep -A10 'Word A'\n</code></pre>\n\n<p>I need this output:</p>\n\n<pre><code>Word A\nWord B\nWord C\nWord D\n</code></pre>\n",
    "answer": "<p>It seems that you want to print lines between '<code>Word A</code>' and '<code>Word D</code>' (inclusive). I suggest you to use <code>sed</code> instead of <code>grep</code>. It lets you to edit a range of input stream which starts and ends with patterns you want. You should just tell <code>sed</code> to print all lines in range and no other lines:</p>\n\n<pre><code>sed -n -e '/Word A/,/Word D/ p' file\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/21076/how-to-show-lines-after-each-grep-match-until-other-specific-match"
  },
  {
    "question_title": "How to use grep when file does not contain the string? Testing whether a user exists in &quot;passwd&quot; or not",
    "question_body": "<p>In my bash script I'm trying to print a line if a certain string does not exist in a file.</p>\n<pre><code>if grep -q &quot;$user2&quot; /etc/passwd; then\n    echo &quot;User does exist!!&quot;\n</code></pre>\n<p>This is how I wrote it if I wanted the string to exist in the file. But how can I change this to make it print &quot;user does not exist&quot; if the user is <strong>not</strong> found in the <code>/etc/passwd</code> file?</p>\n",
    "answer": "<p><code>grep</code> will return success if it finds at least one instance of the pattern and failure if it does not.  So you could either add an <code>else</code> clause if you want both \"does\" and \"does not\" prints, or you could just negate the <code>if</code> condition to only get failures.  An example of each:</p>\n\n<pre><code>if grep -q \"$user2\" /etc/passwd; then\n    echo \"User does exist!!\"\nelse\n    echo \"User does not exist!!\"\nfi\n\nif ! grep -q \"$user2\" /etc/passwd; then\n    echo \"User does not exist!!\"\nfi\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/223503/how-to-use-grep-when-file-does-not-contain-the-string-testing-whether-a-user-ex"
  },
  {
    "question_title": "How to pipe the result of a grep search into a new vi file",
    "question_body": "<p>I'm using <code>grep -e Peugeot -e PeuGeot carlist.txt</code> to search through carlist.txt and pull out some items and I presumed that <code>grep -e Peugeot -e PeuGeot carlist.txt | vi</code> would pipe it through for me but this is what I get:</p>\n\n<pre><code>Vim: Warning: Input is not from a terminal\nVim: Error reading input, exiting...\nVim: preserving files...\nVim: Finished.\n</code></pre>\n",
    "answer": "<p>Running vi or vim with '-' as an argument makes it read the file to edit from standard input. Hence:</p>\n\n<pre><code>grep -e Peugeot -e PeuGeot carlist.txt | vi -\n</code></pre>\n\n<p>will do what you need.</p>\n",
    "url": "https://unix.stackexchange.com/questions/102089/how-to-pipe-the-result-of-a-grep-search-into-a-new-vi-file"
  },
  {
    "question_title": "Counting occurrences of word in text file",
    "question_body": "<p>I have a text file containing tweets and I'm required to count the number of times a word is mentioned in the tweet. For example, the file contains:</p>\n\n<pre><code>Apple iPhone X is going to worth a fortune\nThe iPhone X is Apple's latest flagship iPhone. How will it pit against it's competitors?\n</code></pre>\n\n<p>And let's say I want to count how many times the word iPhone is mentioned in the file. So here's what I've tried.</p>\n\n<pre><code>cut -f 1 Tweet_Data | grep -i \"iPhone\" | wc -l\n</code></pre>\n\n<p>it certainly works but I'm confused about the 'wc' command in unix. What is the difference if I try something like:</p>\n\n<pre><code>cut -f 1 Tweet_Data | grep -c \"iPhone\"\n</code></pre>\n\n<p>where -c is used instead? Both of these yield different results in a large file full of tweets and I'm confused on how it works. Which method is the correct way of counting the occurrence?</p>\n",
    "answer": "<p>Given such a requirement, I would use a GNU grep (for the <a href=\"https://linux.die.net/man/1/grep\" rel=\"noreferrer\"><code>-o</code> option</a>), <em>then</em> pass it through <code>wc</code> to count the total number of occurrences:</p>\n<pre><code>$ grep -o -i iphone Tweet_Data | wc -l\n3\n</code></pre>\n<p>Plain <code>grep -c</code> on the data will count the number of <em>lines</em> that match, not the total number of <em>words</em> that match. Using the <code>-o</code> option tells grep to output each match on its own line, no matter how many times the match was found in the original line.</p>\n<p><code>wc -l</code> tells the <code>wc</code> utility to count the number of lines. After grep puts each match in its own line, this is the total number of occurrences of the word in the input.</p>\n<hr />\n<p>If GNU grep is not available (or desired), you could transform the input with <code>tr</code> so that each word is on its own line, then use <code>grep -c</code> to count:</p>\n<pre><code>$ tr '[:space:]' '[\\n*]' &lt; Tweet_Data | grep -i -c iphone\n3\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/398413/counting-occurrences-of-word-in-text-file"
  },
  {
    "question_title": "Grep from the end of a file to the beginning",
    "question_body": "<p>I have a file with about 30.000.000 lines (Radius Accounting) and I need to find  the last match of a given pattern.</p>\n\n<p>The command:</p>\n\n<pre><code>tac accounting.log | grep $pattern\n</code></pre>\n\n<p>gives what I need, but it's too slow because the OS has to first read the whole file and then send to the pipe.</p>\n\n<p>So, I need something fast that can read the file from the last line to the first.</p>\n",
    "answer": "<p><code>tac</code> only helps if you also use <code>grep -m 1</code> (assuming GNU <code>grep</code>) to have <code>grep</code> stop after the first match:</p>\n<pre><code>tac accounting.log | grep -m 1 foo\n</code></pre>\n<p>From <code>man grep</code>:</p>\n<pre><code>   -m NUM, --max-count=NUM\n          Stop reading a file after NUM matching lines.  \n</code></pre>\n<p>In the example in your question, both <code>tac</code> and <code>grep</code> need to process the entire file so using <code>tac</code> is kind of pointless.</p>\n<p>If you can't use <code>grep -m</code>, just parse the output of <code>grep</code> to get the last match:</p>\n<pre><code>grep foo accounting.log | tail -n 1 \n</code></pre>\n<hr />\n<p>Another approach would be to use Perl or any other scripting language. For example (where <code>$pattern=foo</code>):</p>\n<pre><code>perl -ne '$l=$_ if /foo/; END{print $l}' file\n</code></pre>\n<p>or</p>\n<pre><code>awk '/foo/{k=$0}END{print k}' file\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/112159/grep-from-the-end-of-a-file-to-the-beginning"
  },
  {
    "question_title": "What is the difference between grep -e and grep -E option?",
    "question_body": "<p>I am trying to understand the difference between <code>grep -e</code> and <code>grep -E</code>. Now from <code>grep manpage</code> I got:</p>\n<blockquote>\n<p>-E, --extended-regexp</p>\n<p>Interpret PATTERN as an extended regular expression (see below).</p>\n<p>-e PATTERN, --regexp=PATTERN</p>\n<p>Use PATTERN as the pattern; useful to protect patterns beginning with -</p>\n</blockquote>\n<p>The above explanation does not make sense for me.</p>\n<p>So, can someone explain it to me using <code>examples</code> what is the difference between the two  and when to use which option.</p>\n<p>PS: Version: grep (GNU grep) 2.10</p>\n",
    "answer": "<p><code>-e</code> is strictly the flag for indicating the pattern you want to match against. <code>-E</code> controls whether you need to escape certain special characters.</p>\n<p><code>man grep</code> explains <code>-E</code> it a bit more:</p>\n<pre><code>Basic vs Extended Regular Expressions\nIn basic regular expressions the meta-characters ?, +, {, |, (, and ) lose their \nspecial meaning; instead use the backslashed versions \\?, \\+, \\{, \\|, \\(, and \\).\n\nTraditional  egrep  did  not  support  the  {  meta-character, and some egrep \nimplementations support \\{ instead, so portable scripts should avoid { in grep -E\npatterns and should use [{] to match a literal {.\n\nGNU grep -E attempts to support traditional usage by assuming that { is not \nspecial if it would be the start of an invalid interval specification. \nFor example, the command grep -E '{1' searches for the two-character string {1 \ninstead of reporting a syntax error in the regular expression.  POSIX.2 allows \nthis behavior as an extension, but portable scripts should avoid it.\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/50512/what-is-the-difference-between-grep-e-and-grep-e-option"
  },
  {
    "question_title": "How to search text throughout entire file system?",
    "question_body": "<p>Assuming that the grep tool should be used, I'd like to search for the text string \"800x600\" throughout the entire file system.</p>\n\n<p>I tried:</p>\n\n<pre><code>grep -r 800x600 /\n</code></pre>\n\n<p>but it doesn't work.</p>\n\n<p>What I believe my command should do is grep recursively through all files/folders under root for the text \"800x600\" and list the search results.</p>\n\n<p>What am I doing wrong?</p>\n",
    "answer": "<p>I normally use this style of command to run <code>grep</code> over a number of files:</p>\n\n<pre><code>find / -xdev -type f -print0 | xargs -0 grep -H \"800x600\"\n</code></pre>\n\n<p>What this actually does is make a list of every file on the system, and then for each file, execute <code>grep</code> with the given arguments and the name of each file.</p>\n\n<p>The <code>-xdev</code> argument tells find that it must ignore other filesystems - this is good for avoiding special filesystems such as <code>/proc</code>. However it will also ignore normal filesystems too - so if, for example, your /home folder is on a different partition, it won't be searched - you would need to say <code>find / /home -xdev ...</code>.</p>\n\n<p><code>-type f</code> means search for files only, so directories, devices and other special files are ignored (it will still recurse into directories and execute <code>grep</code> on the files within - it just won't execute <code>grep</code> on the directory itself, which wouldn't work anyway). And the <code>-H</code> option to <code>grep</code> tells it to always print the filename in its output.</p>\n\n<p><code>find</code> accepts all sorts of options to filter the list of files. For example, <code>-name '*.txt'</code> processes only files ending in .txt. <code>-size -2M</code> means files that are smaller than 2 megabytes. <code>-mtime -5</code> means files modified in the last five days. Join these together with -a for <em>and</em> and -o for <em>or</em>, and use <code>'('</code> parentheses <code>')'</code> to group expressions (in quotes to prevent the shell from interpreting them). So for example:</p>\n\n<pre><code>find / -xdev '(' -type f -a -name '*.txt' -a -size -2M -a -mtime -5 ')' -print0 | xargs -0 grep -H \"800x600\"\n</code></pre>\n\n<p>Take a look at <code>man find</code> to see the full list of possible filters.</p>\n",
    "url": "https://unix.stackexchange.com/questions/16138/how-to-search-text-throughout-entire-file-system"
  },
  {
    "question_title": "Is there a way to modify a file in-place?",
    "question_body": "<p>I have a fairly large file (35Gb), and I would like to filter this file in situ (i.e. I don't have enough disk space for another file), specifically I want to grep and ignore some patterns — is there a way to do this without using another file?</p>\n\n<p>Let's say I want to filter out all the lines containing <code>foo:</code> for example...</p>\n",
    "answer": "<p>At the system call level this should be possible. A program can open your target file for writing without truncating it and start writing what it reads from stdin. When reading EOF, the output file can be truncated.</p>\n\n<p>Since you are filtering lines from the input, the output file write position should always be less than the read position. This means you should not corrupt your input with the new output.</p>\n\n<p>However, finding a program that does this is the problem. <code>dd(1)</code> has the option <code>conv=notrunc</code> that does not truncate the output file on open, but it also does not truncate at the end, leaving the original file contents after the grep contents (with a command like <code>grep pattern bigfile | dd of=bigfile conv=notrunc</code>)</p>\n\n<p>Since it is very simple from a system call perspective, I wrote a small program and tested it on a small (1MiB) full loopback filesystem. It did what you wanted, but you really want to test this with some other files first. It's always going to be risky overwriting a file.</p>\n\n<p><strong>overwrite.c</strong></p>\n\n<pre><code>/* This code is placed in the public domain by camh */\n\n#include &lt;sys/types.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;errno.h&gt;\n\nint main(int argc, char **argv)\n{\n        int outfd;\n        char buf[1024];\n        int nread;\n        off_t file_length;\n\n        if (argc != 2) {\n                fprintf(stderr, \"usage: %s &lt;output_file&gt;\\n\", argv[0]);\n                exit(1);\n        }\n        if ((outfd = open(argv[1], O_WRONLY)) == -1) {\n                perror(\"Could not open output file\");\n                exit(2);\n        }\n        while ((nread = read(0, buf, sizeof(buf))) &gt; 0) {\n                if (write(outfd, buf, nread) == -1) {\n                        perror(\"Could not write to output file\");\n                        exit(4);\n                }\n        }\n        if (nread == -1) {\n                perror(\"Could not read from stdin\");\n                exit(3);\n        }\n        if ((file_length = lseek(outfd, 0, SEEK_CUR)) == (off_t)-1) {\n                perror(\"Could not get file position\");\n                exit(5);\n        }\n        if (ftruncate(outfd, file_length) == -1) {\n                perror(\"Could not truncate file\");\n                exit(6);\n        }\n        close(outfd);\n        exit(0);\n}\n</code></pre>\n\n<p>You would use it as:</p>\n\n<pre><code>grep pattern bigfile | overwrite bigfile\n</code></pre>\n\n<p>I'm mostly posting this for others to comment on before you try it. Perhaps someone else knows of a program that does something similar that is more tested.</p>\n",
    "url": "https://unix.stackexchange.com/questions/11067/is-there-a-way-to-modify-a-file-in-place"
  },
  {
    "question_title": "Get `grep` to not output file name",
    "question_body": "<p>When I use <code>grep -o</code> to search in multiple files, it outputs each result prefixed with the file name. How can I prevent this prefix? I want the results without the file names.</p>\n",
    "answer": "<p>With the GNU implementation of <code>grep</code> (the one that also introduced <code>-o</code>) or compatible, you can use the <code>-h</code> option.</p>\n<blockquote>\n<pre><code>-h, --no-filename\n          Suppress the prefixing of file names on  output.   This  is  the\n          default  when there is only one file (or only standard input) to\n          search.\n</code></pre>\n</blockquote>\n<p>With other implementations, you can always concatenate the files with <code>cat</code> and <code>grep</code> that output:</p>\n<pre><code>cat ./*.txt | grep regexp\n</code></pre>\n<p>Or use <code>sed</code> or <code>awk</code> instead of <code>grep</code>:</p>\n<pre><code>awk '/regexp/' ./*.txt\n</code></pre>\n<p>(extended regexps like with <code>grep -E</code>).</p>\n<pre><code>sed '/regexp/!d/' ./*.txt\n</code></pre>\n<p>(basic regexps like with <code>grep</code> without <code>-E</code>. Many <code>sed</code> implementations now also support a <code>-E</code> option for extended regexps).</p>\n",
    "url": "https://unix.stackexchange.com/questions/204607/get-grep-to-not-output-file-name"
  },
  {
    "question_title": "How to do max-depth search in ack and grep?",
    "question_body": "<p>Is there any way to tell <code>ack</code> to only search for text on the current folder? (or specify a <code>max-depth</code> level?) And with <code>grep</code>?</p>\n",
    "answer": "<p>You can couple find with the <code>-exec</code> argument. Example:</p>\n<pre><code>find . -maxdepth 1 -exec grep foo {} \\;\n</code></pre>\n<p>This can be scaled, i.e. <code>-maxdepth 2</code>.</p>\n<p><strong>Edit</strong></p>\n<p>As mentioned in the [answer by @Stéphane Chazelas], it is advisable to restrict <code>find</code> to regular files so that <code>grep</code> doesn't produce an error when the argument <code>{}</code> actually is a directory path:</p>\n<pre><code>find . -maxdepth 1 -type f -exec grep -H foo {} \\;\n</code></pre>\n<ul>\n<li><code>-type f</code> is a filter for <code>find</code> that limits the search results to files</li>\n<li><code>-H</code> is a <code>grep</code> option used to print a filename for every match (desired behavior when more than one file match)</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/111722/how-to-do-max-depth-search-in-ack-and-grep"
  },
  {
    "question_title": "How to grep lines between start and end pattern?",
    "question_body": "<p>I have a file which is having following content:</p>\n\n<pre><code>zdk\naaa\nb12\ncdn\ndke\nkdn\n</code></pre>\n\n<p>Input1: <code>aaa</code> and <code>cdn</code></p>\n\n<p>Output 1:</p>\n\n<pre><code>aaa\nb12\ncdn\n</code></pre>\n\n<p>Input 2: <code>zdk</code> and <code>dke</code></p>\n\n<p>Output 2:</p>\n\n<pre><code>zdk\naaa\nb12\ncdn\ndke\n</code></pre>\n\n<p>I could use below commands to achieve:</p>\n\n<pre><code>grep -a aaa -A2 file # Output 1\ngrep -a aaa -A4 file # Output 2\n</code></pre>\n\n<p>But in the file I don't know what is the exact occurrence (position) of end string pattern (file is having 20000 rows)</p>\n\n<p>How can I achieve this?</p>\n",
    "answer": "<p><code>grep</code> won't help you here. This is a job better accomplished with <code>sed</code> using range expressions:</p>\n\n<pre><code>$ sed -n '/aaa/,/cdn/p' file\naaa\nb12\ncdn\n$ sed -n '/zdk/,/dke/p' file\nzdk\naaa\nb12\ncdn\ndke\n</code></pre>\n\n<p><code>sed -n</code> suppresses the automatic printing, so that lines are printed just if explicitly asked to. And this happens when the range <code>/aaa/,/cdn/</code> happens.</p>\n\n<p>These range expressions are also available in <code>awk</code>, where you can say:</p>\n\n<pre><code>awk '/zdk/,/dke/' file\n</code></pre>\n\n<p>Of course, all these conditions can be expanded to a more strict regex like <code>sed -n '/^aaa$/,/^cdn$/p' file</code> to check that the lines consist on exactly <code>aaa</code> and <code>cdn</code>, nothing else.</p>\n",
    "url": "https://unix.stackexchange.com/questions/236751/how-to-grep-lines-between-start-and-end-pattern"
  },
  {
    "question_title": "How do I pipe ls to grep, and delete the files filtered by grep?",
    "question_body": "<p>I wanted to delete some package in my home file, but the filename was too long (<code>google-chrome-stable_current_i386.deb</code>). So, I decided to use the command <code>ls|grep chrome|rm</code> to pipe the files to grep to filter out the chrome file, and then remove it. It didn't work, so I would like to see how I can do this.</p>\n",
    "answer": "<p>This almost made me wince.</p>\n\n<ol>\n<li>You might want to stop pointing that <a href=\"http://mywiki.wooledge.org/ParsingLs\">shotgun</a> at your foot. Basically any kind of parsing of <code>ls</code> is going to be more <a href=\"https://unix.stackexchange.com/questions/128985/why-not-parse-ls\">complicated and error-prone</a> than established methods like <a href=\"https://unix.stackexchange.com/a/247933/3645\"><code>find [...] -exec</code></a> or <a href=\"http://mywiki.wooledge.org/glob\">globs</a>.</li>\n<li>Unless someone installed a troll distro for you, your shell has <kbd>Tab</kbd> completion. Just type <code>rm google</code> and press <kbd>Tab</kbd>. If it doesn't complete immediately, press <kbd>Tab</kbd> again to see a list of matching files. Type more characters of the filename to narrow it down until it does complete, then run the command.</li>\n<li><a href=\"http://mywiki.wooledge.org/BashGuide/InputAndOutput#Pipes\">Pipes</a> != <a href=\"http://mywiki.wooledge.org/BashGuide/Parameters\">parameters</a>. Standard input is a <em>binary data stream</em> which can be fed to a command asynchronously. Parameters are space separated strings which are passed once and only once to a command when running it. These are very rarely interchangeable.</li>\n</ol>\n",
    "url": "https://unix.stackexchange.com/questions/247924/how-do-i-pipe-ls-to-grep-and-delete-the-files-filtered-by-grep"
  },
  {
    "question_title": "How to return both file name and line number with find ... -exec grep?",
    "question_body": "<p>When using <code>find</code>, how do I return the file name and the line number when searching for a string? I manage to return the file name in one command and the line numbers with another one, but I can't seem to combine them.</p>\n\n<p>File names: <code>find . -type f -exec grep -l 'string to search' {} \\;</code></p>\n\n<p>Line numbers: <code>find . -type f -exec grep -n 'string to search' {} \\;</code></p>\n",
    "answer": "<p>The command line switch <code>-H</code> forces <code>grep</code> to print the file name, even with just one file.</p>\n\n<pre><code>% grep -n 7 test.in\n7:7\n% grep -Hn 7 test.in\ntest.in:7:7\n</code></pre>\n\n<hr>\n\n<pre><code>   -H, --with-filename\n          Print the filename for each match.\n</code></pre>\n\n<p>Note that as <a href=\"https://unix.stackexchange.com/users/5377/kojiro\">Kojiro</a> says in <a href=\"https://unix.stackexchange.com/questions/53072/how-to-return-both-file-name-and-line-number-with-find-exec-grep/53074#comment74330_53074\">a comment</a>, this is not part of the POSIX standard; it is in both GNU and BSD grep, but it's possible some systems don't have it (e.g. Solaris).</p>\n",
    "url": "https://unix.stackexchange.com/questions/53072/how-to-return-both-file-name-and-line-number-with-find-exec-grep"
  },
  {
    "question_title": "I would like to grep all files except some file types?",
    "question_body": "<p>How do I recursively <code>grep</code> files within a given folders except a couple file types?  </p>\n\n<p>For example, I'm looking for a string within my workspace folder but it ends up searching inside sql files and generates serialized strings.</p>\n\n<p>So in this case, I'd like to <code>grep</code> the workspace folder except sql files.</p>\n\n<p>I'm preferably looking for a one-liner if possible.</p>\n",
    "answer": "<p>If you have GNU <code>grep</code> you can use the <code>--exclude=GLOB</code> option, like</p>\n\n<pre><code>grep -r --exclude='*.sql' pattern dir/\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/31224/i-would-like-to-grep-all-files-except-some-file-types"
  },
  {
    "question_title": "What is the actual purpose of GNU grep&#39;s -X option and why is it undocumented?",
    "question_body": "<p>By reading <a href=\"https://unix.stackexchange.com/q/392275/203203\">this question</a>, I have discovered that GNU <code>grep</code> has a <code>-X</code> option which expects an argument. Strangely, it is mentioned neither in the man page nor in the info page.</p>\n\n<p>Looking at the source code, there is that comment <a href=\"http://git.savannah.gnu.org/cgit/grep.git/tree/src/grep.c?id=dbf6efe68c6d97a66ad20a4d5d2935a520bf4bd7#n1925\" rel=\"noreferrer\">right in the middle of the <code>--help</code> output</a>:</p>\n\n<pre><code>/* -X is deliberately undocumented.  */\n</code></pre>\n\n<p>Looking further, it appears that the <code>-X matcher</code> option <a href=\"http://git.savannah.gnu.org/cgit/grep.git/tree/src/grep.c?id=dbf6efe68c6d97a66ad20a4d5d2935a520bf4bd7#n2504\" rel=\"noreferrer\">sets the engine used for the regexp</a>, <code>matcher</code> being <a href=\"http://git.savannah.gnu.org/cgit/grep.git/tree/src/grep.c?id=dbf6efe68c6d97a66ad20a4d5d2935a520bf4bd7#n2012\" rel=\"noreferrer\">one of</a> <code>grep</code>, <code>egrep</code>, <code>fgrep</code>, <code>awk</code>, <code>gawk</code>, <code>posixawk</code> and <code>perl</code> (as of version 2.25).</p>\n\n<p>Some of those values are strictly identical to existing options (namely <code>grep -G</code>, <code>grep -E</code>, <code>grep -F</code> and <code>grep -P</code>). On the other hand, the three <code>awk</code> variants have no corresponding options.</p>\n\n<p>Does someone know what is the actual purpose of this option, especially with one of the <code>awk</code> regexp engines? Can someone tell me why it is purposely not documented?</p>\n",
    "answer": "<p>Its purpose is to provide access to the various matchers implemented in GNU <code>grep</code> in one form or another, in particular AWK matchers which aren’t available otherwise, probably for testing purposes (see <a href=\"https://debbugs.gnu.org/cgi/bugreport.cgi?bug=16481\" rel=\"noreferrer\">bug 16481</a> which discusses adding the <code>gawk</code> and <code>posixawk</code> matchers).</p>\n\n<p>However it is currently buggy, which <a href=\"http://lists.gnu.org/archive/html/bug-grep/2005-01/msg00063.html\" rel=\"noreferrer\">is the reason why it’s documented as being undocumented</a>:</p>\n\n<blockquote>\n  <p>On Thu, Jan 27, 2005 at 04:06:04PM -0500, Charles Levert wrote:<br>\n  > The '-X' option, and in particular its use with the \"awk\" matcher<br>\n  > (\"-X awk\") is undocumented.</p>\n  \n  <p>please leave it undocumented.</p>\n  \n  <p>It doesn't provide any new functionality besides -X awk.</p>\n  \n  <p>And the implementation of awk regexps is not perfect, I think.</p>\n  \n  <p>The new GNU regex conatins some means to set AWK style syntax, yes.\n  Yet gawk doesn't use it directly: it parses the regex first.</p>\n  \n  <p>In particular, awk regexps allow escape sequences \\NNN, where NNN is\n  an octal value. So /\\040/ mathes space. grep -X awk doesn't seem to\n  support this.</p>\n  \n  <p>I'm afraid that regex.c doesn't support these escape sequences.</p>\n  \n  <p>We would have to make sure that the regexes are fully compatible with\n  awk regexes before we decided to document (and thus support) this\n  feature.</p>\n  \n  <p>I think it's not worth the trouble.</p>\n  \n  <p>Stepan</p>\n</blockquote>\n\n<p>A <a href=\"http://lists.gnu.org/archive/html/bug-grep/2005-01/msg00071.html\" rel=\"noreferrer\">follow-up</a> asked for the comment to be added, and provided a bit more background on the <code>-X</code> option:</p>\n\n<blockquote>\n  <p>My own inclination is to suggest just removing -X entirely. I suspect it was added by the original author mainly for testing purposes. If it's going to stay in, at least add a comment like this.</p>\n\n<pre><code>/* -X is undocumented on purpose. */\n</code></pre>\n  \n  <p>to avoid future discussion of a resolved issue.</p>\n  \n  <p>Arnold</p>\n</blockquote>\n\n<p>which <a href=\"http://lists.gnu.org/archive/html/bug-grep/2005-02/msg00004.html\" rel=\"noreferrer\">Stepan did shortly thereafter</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/392341/what-is-the-actual-purpose-of-gnu-greps-x-option-and-why-is-it-undocumented"
  },
  {
    "question_title": "Using grep with the --exclude-dir flag to exclude multiple directories",
    "question_body": "<p>I am searching through a Ruby on Rails application for a word using <code>grep</code> on OSX, and I would like to exclude directories that match a certain pattern. </p>\n\n<p>I am using the following command:</p>\n\n<pre><code>grep -inRw -E 'direct' . --exclude-dir -E 'git|log|asset'\n</code></pre>\n\n<p>This command is not doing what I thought it would do. Here is how I <strong>thought</strong> it would work:</p>\n\n<ul>\n<li><strong>i</strong> - case insensitive search</li>\n<li><strong>n</strong> - print line number in which pattern is found</li>\n<li><strong>R</strong> - search recursively</li>\n<li><strong>w</strong> - I only want whole words - i.e., match \"direct\" but not \"directory\"</li>\n<li><strong>-E</strong> - use extended regular expression</li>\n<li><strong>'direct'</strong> - the regular expression I want to match</li>\n<li><strong>.</strong> - search in the current directory</li>\n<li><strong>--exclude-dir -E 'git|log|asset'</strong> - exclude directories that match git or log or asset. </li>\n</ul>\n\n<p>In terms of the exclude directories, the command still ends up searching in the <strong>'./git'</strong> and <strong>'./log'</strong> directories, as well as in <strong>'./app/assets'</strong></p>\n\n<p>I'm obviously lacking a fundamental piece of knowledge, but I do not know what it is.</p>\n",
    "answer": "<p>It's pattern as in <em>globs</em> not pattern as in <em>regex</em>. Per the <a href=\"http://www.gnu.org/software/grep/manual/grep.html#File-and-Directory-Selection\" rel=\"noreferrer\"><code>info</code> page</a>:</p>\n<blockquote>\n<p>--exclude-dir=GLOB</p>\n<p>Skip any command-line directory with a name suffix that matches the\npattern GLOB. When searching recursively, skip any subdirectory whose\nbase name matches GLOB. Ignore any redundant trailing slashes in GLOB.</p>\n</blockquote>\n<p>So, <a href=\"https://unix.stackexchange.com/a/282650\">you either use the switch multiple times</a> or, if your shell supports brace expansion, you could golf it shorter and have the shell expand the list of patterns e.g.:</p>\n<pre><code>grep -inRw -E 'direct' . --exclude-dir={git,log,assets}\n</code></pre>\n<p>to exclude directories named <code>git</code>, <code>log</code> and <code>assets</code> or e.g.</p>\n<pre><code>grep -inRw -E 'direct' . --exclude-dir={\\*git,asset\\*}\n</code></pre>\n<p>to exclude directory names ending in <code>git</code> or starting with <code>asset</code>.</p>\n<p>Note that the shell expands the list only if there are <a href=\"https://unix.stackexchange.com/q/433729\">at least two dirnames/globs inside braces</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/282648/using-grep-with-the-exclude-dir-flag-to-exclude-multiple-directories"
  },
  {
    "question_title": "giving grep output to rm",
    "question_body": "<p>I'm trying to pipe <code>grep</code> output to <code>rm</code>, but it outputs useless stuff. Is any switch required for <code>rm</code>? Or can <code>rm</code> can be provided a regexp directly?</p>\n\n<pre><code>ls | grep '^\\[Daruchini'| rm\n</code></pre>\n\n<p>rm: missing operand\nTry `rm --help' for more information.</p>\n",
    "answer": "<p>You need to use <code>xargs</code> to turn standard input into arguments for <code>rm</code>.</p>\n\n<pre><code>$ ls | grep '^Dar' | xargs rm\n</code></pre>\n\n<p>(Beware of special characters in filenames; with GNU grep, you might prefer</p>\n\n<pre><code>$ ls | grep -Z '^Dar' | xargs -0 rm\n</code></pre>\n\n<p>)</p>\n\n<p>Also, while the shell doesn't use regexps, that's a simple pattern:</p>\n\n<pre><code>$ rm Dar*\n</code></pre>\n\n<p>(meanwhile, I think I need more sleep.)</p>\n",
    "url": "https://unix.stackexchange.com/questions/9597/giving-grep-output-to-rm"
  },
  {
    "question_title": "How to run grep on a single column?",
    "question_body": "<p>I want to grep the output of my <code>ls -l</code> command:</p>\n\n<pre><code>-rw-r--r--   1 root root       1866 Feb 14 07:47 rahmu.file\n-rw-r--r--   1 rahmu user     95653 Feb 14 07:47 foo.file\n-rw-r--r--   1 rahmu user   1073822 Feb 14 21:01 bar.file\n</code></pre>\n\n<p>I want to run <code>grep rahmu</code> on column $3 only, so the output of my <code>grep</code> command should look like this:</p>\n\n<pre><code>-rw-r--r--   1 rahmu user     95653 Feb 14 07:47 foo.file\n-rw-r--r--   1 rahmu user   1073822 Feb 14 21:01 bar.file\n</code></pre>\n\n<p>What's the simplest way to do it? The answer must be portable across many Unices, preferably focusing on Linux and Solaris.</p>\n\n<p><em>NB: I'm not looking for a way to find all the files belonging to a given user. This example was only given to make my question clearer.</em></p>\n",
    "answer": "<p>One more time <code>awk</code> saves the day!</p>\n\n<p>Here's a straightforward way to do it, with a relatively simple syntax:</p>\n\n<pre><code>ls -l | awk '{if ($3 == \"rahmu\") print $0;}'\n</code></pre>\n\n<p>or even simpler: <em>(Thanks to Peter.O in the comments)</em></p>\n\n<pre><code>ls -l | awk '$3 == \"rahmu\"' \n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/31753/how-to-run-grep-on-a-single-column"
  },
  {
    "question_title": "grep and escaping a dollar sign",
    "question_body": "<p>I want to know which files have the string <code>$Id$</code>.</p>\n\n<pre><code>grep \\$Id\\$  my_dir/mylist_of_files\n</code></pre>\n\n<p>returns 0 occurrences.</p>\n\n<p>I discovered that I have to use</p>\n\n<p><code>grep \\$Id$ my_dir/mylist_of_files</code></p>\n\n<p>Then I see that the <code>$Id</code> is colored in the output, i.e. it has been matched.</p>\n\n<p>How could I match the second <code>$</code> and why doesn't <code>\\$Id\\$</code> work.</p>\n\n<p>It doesn't matter if the second <code>$</code> is the last character or not.</p>\n\n<p>I use <code>grep</code> 2.9.</p>\n\n<hr>\n\n<p>Before posting my question, I used google...</p>\n\n<p><a href=\"http://publib.boulder.ibm.com/infocenter/aix/v6r1/index.jsp?topic=/com.ibm.aix.cmds/doc/aixcmds2/grep.htm\">I found an answer</a></p>\n\n<blockquote>\n  <p>To search for a $ (dollar sign) in the file named test2, enter:</p>\n  \n  <p>grep \\\\$ test2</p>\n  \n  <p>The \\\\ (double backslash) characters are necessary in order\n  to force the shell to pass a \\$ (single backslash, dollar sign) to the\n  grep command. The \\ (single backslash) character tells the grep\n  command to treat the following character (in this example the $) as a\n  literal character rather than an expression character. Use the fgrep\n  command to avoid the necessity of using escape characters such as the\n  backslash.</p>\n</blockquote>\n\n<p>but I don't understand why <code>grep \\$Id</code> works and why <code>grep \\\\$Id\\\\$</code> doesn't.</p>\n\n<p>I'm a little bit confused...</p>\n",
    "answer": "<p>There's 2 separate issues here.</p>\n<ol>\n<li><p><code>grep</code> uses <a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03\" rel=\"noreferrer\"><em>Basic Regular Expressions</em></a> (BRE), and <code>$</code> is a special character in BRE's only at the end of an expression.  The consequence of this is that the 2 instances of <code>$</code> in <code>$Id$</code> are not equal.  The first one is a normal character and the second is an anchor that matches the end of the line.  To make the second <code>$</code> match a literal <code>$</code> you'll have to backslash escape it, i.e. <code>$Id\\$</code>.  Escaping the first <code>$</code> also works: <code>\\$Id\\$</code>, and I prefer this since it looks more consistent.¹</p>\n</li>\n<li><p>There are two completely unrelated escaping/quoting mechanisms at work here: shell quoting and regex backslash quoting.  The problem is many characters that regular expressions use are special to the shell as well, and on top of that the regex escape character, the backslash, is also a shell quoting character.  This is why you often see messes involving double backslashes, but I do not recommend using backslashes for shell quoting regular expressions because it is not very readable.</p>\n<p>Instead, the simplest way to do this is to first put your entire regex inside single quotes as in <code>'regex'</code>.  The single quote is the strongest form of quoting the shell has, so as long as your regex does not contain single quotes, you no longer have to worry about shell quoting and can focus on pure BRE syntax.</p>\n</li>\n</ol>\n<p>So, applying this back to your original example, let's throw the correct regex (<code>\\$Id\\$</code>) inside single quotes.  The following should do what you want:</p>\n<pre><code>grep '\\$Id\\$' my_dir/my_file\n</code></pre>\n<p>The reason <code>\\$Id\\$</code> does not work is because after shell quote removal (the more correct way of saying  shell quoting) is applied, the regex that <code>grep</code> sees is <code>$Id$</code>.  As explained in (1.), this regex matches a literal <code>$Id</code> only at the end of a line because the first <code>$</code> is literal while the second is a special anchor character.</p>\n<p><sup>¹ Note also that if you ever switch to Extended Regular Expressions (ERE), e.g. if you decided to use <code>egrep</code> (or <code>grep -E</code>), the <code>$</code> character is always special.  In ERE's <code>$Id$</code> would never match anything because you can't have characters <em>after</em> the end of a line, so <code>\\$Id\\$</code> would be the only way to go.</sup></p>\n",
    "url": "https://unix.stackexchange.com/questions/32018/grep-and-escaping-a-dollar-sign"
  },
  {
    "question_title": "Using grep and looking for unique occurrences",
    "question_body": "<p>I have a text file of this type, and I would look for any lines containing the string <code>Validating Classification</code> and then obtain uniquely the reported errors. I do not know the types of possible errors.</p>\n\n<p>Input file: </p>\n\n<pre><code>201600415 10:40 Error Validating Classification: error1\n201600415 10:41 Error Validating Classification: error1\n201600415 10:42 Error Validating Classification: error2\n201600415 10:43 Error Validating Classification: error3\n201600415 10:44 Error Validating Classification: error3\n</code></pre>\n\n<p>Output file</p>\n\n<pre><code>201600415 10:40 Error Validating Classification: error1\n201600415 10:42 Error Validating Classification: error2\n201600415 10:43 Error Validating Classification: error3\n</code></pre>\n\n<p>Can I achieve this using grep, pipes and other commands?</p>\n",
    "answer": "<p>You will need to discard the timestamps, but 'grep' and 'sort --unique' together can do it for you.</p>\n\n<pre><code>grep --only-matching 'Validating Classification.*' | sort --unique\n</code></pre>\n\n<p>So <code>grep -o</code> will only show the parts of the line that match your regex (which is why you need to include the <code>.*</code> to include everything <em>after</em> the \"Validating Classification\" match). Then once you have just the list of errors, you can use <a href=\"https://unix.stackexchange.com/questions/76049/what-is-the-difference-between-sort-u-and-sort-uniq\"><code>sort -u</code></a> to get just the unique list of errors.</p>\n",
    "url": "https://unix.stackexchange.com/questions/276741/using-grep-and-looking-for-unique-occurrences"
  },
  {
    "question_title": "How to &quot;grep&quot; for line length in a given range?",
    "question_body": "<p><strong><em>NOTE:</strong> This question is the complement of this Q&amp;A: <a href=\"https://unix.stackexchange.com/questions/249224/how-to-grep-for-lines-shorter-or-longer-than-specific-thresholds\">How to &quot;grep&quot; for line length *not* in a given range?</a></em></p>\n\n<hr>\n\n<p>I need to get only the lines from a textfile (a wordlist, separated with newline) that has a length range of minimum or equal than 3 characters, but not longer or equal than 10. </p>\n\n<p>Example: </p>\n\n<p>INPUT: </p>\n\n<pre><code>egyezményét\nmegkíván\nki\nalma\nkevesen\nmeghatározó\n</code></pre>\n\n<p>OUTPUT: </p>\n\n<pre><code>megkíván\nalma\nkevesen\n</code></pre>\n\n<p><strong>Question:</strong> How can I do this in <code>bash</code>?</p>\n",
    "answer": "<pre><code>grep -x '.\\{3,10\\}'\n</code></pre>\n<p>where</p>\n<ul>\n<li><code>-x</code> (also <code>--line-regexp</code> with GNU <code>grep</code>) match pattern to whole line</li>\n<li><code>.</code> any single character</li>\n<li><code>\\{3,10\\}</code> quantify from 3 to 10 times previous symbol (in the case any ones)</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/184519/how-to-grep-for-line-length-in-a-given-range"
  },
  {
    "question_title": "How can I find all files that do NOT contain a text string?",
    "question_body": "<p>What <strong>concise</strong> command can I use to find all files that do NOT contain a text string?</p>\n\n<p>I tried this (using -v to invert grep's parameters) with no luck:</p>\n\n<pre><code>find . -exec grep -v -l shared.php {} \\;\n</code></pre>\n\n<p>Someone said this would work:</p>\n\n<pre><code>find . ! -exec grep -l shared.php {} \\;\n</code></pre>\n\n<p>But it does not seem to work for me.</p>\n\n<p><a href=\"http://www.unix.com/unix-dummies-questions-answers/13861-search-files-dont-contain-string.html\">This page</a> has this example:</p>\n\n<pre><code>find ./logs -size +1c  &gt; t._tmp\nwhile read filename\ndo\n     grep -q \"Process Complete\" $filename\n     if [ $? -ne 0 ] ; then\n             echo $filename\n     fi\ndone &lt; t._tmp\nrm -f t_tmp\n</code></pre>\n\n<p>But that's cumbersome and not at all concise.</p>\n\n<p>ps: I know that <code>grep -L *</code> will do this, but how can I use the find command in combination with grep to excluded files is what I really want to know.</p>\n\n<p>pss: Also I'm not sure how to have grep include subdirectories with the <code>grep -L *</code> syntax, but I still want to know how to use it with <code>find</code> :)</p>\n",
    "answer": "<p>Your find should work if you change <code>-v -l</code> (files that have any line not matching) to <code>-L</code> (files with no lines matching), but you could also use <code>grep</code>'s recursive (<code>-r</code>) option:</p>\n\n<pre><code>grep -rL shared.php .\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/26836/how-can-i-find-all-files-that-do-not-contain-a-text-string"
  },
  {
    "question_title": "How can I &quot;grep&quot; patterns across multiple lines?",
    "question_body": "<p>It seems I am misusing <code>grep</code>/<code>egrep</code>. </p>\n\n<p>I was trying to search for strings in multiple line and could not find a match while I know that what I'm looking for should match. Originally I thought that my regexes were wrong but I eventually read that these tools operate per line (also my regexes were so trivial it could not be the issue).</p>\n\n<p>So which tool would one use to search patterns across multiple lines?</p>\n",
    "answer": "<p>Here's a <code>sed</code> one that will give you <code>grep</code>-like behavior across multiple lines:</p>\n\n<pre><code>sed -n '/foo/{:start /bar/!{N;b start};/your_regex/p}' your_file\n</code></pre>\n\n<p><strong>How it works</strong></p>\n\n<ul>\n<li><code>-n</code> suppresses the default behavior of printing every line</li>\n<li><code>/foo/{}</code> instructs it to match <code>foo</code> and do what comes inside the squigglies to the matching lines. Replace <code>foo</code> with the starting part of the pattern.</li>\n<li><code>:start</code> is a branching label to help us keep looping until we find the end to our regex.</li>\n<li><code>/bar/!{}</code> will execute what's in the squigglies to the lines that don't match <code>bar</code>. Replace <code>bar</code> with the ending part of the pattern.</li>\n<li><code>N</code> appends the next line to the active buffer (<code>sed</code> calls this the pattern space)</li>\n<li><code>b start</code> will unconditionally branch to the <code>start</code> label we created earlier so as to keep appending the next line as long as the pattern space doesn't contain <code>bar</code>.</li>\n<li><code>/your_regex/p</code> prints the pattern space if it matches <code>your_regex</code>. You should replace <code>your_regex</code> by the whole expression you want to match across multiple lines.</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/112132/how-can-i-grep-patterns-across-multiple-lines"
  },
  {
    "question_title": "How to kill all process with given name?",
    "question_body": "<p>I run the command <code>grep &lt;application_name&gt;</code> and get a list of processes.</p>\n<p>I want to kill all process from the list: <code>19440</code>, <code>21630</code>, <code>22694</code>.</p>\n<p>I have tried, but it works with errors.</p>\n<pre><code>kill: illegal pid ?\nkill: illegal pid 00:00:00\nkill: illegal pid &lt;application_name&gt;\n</code></pre>\n<p>How can I do this gracefully?</p>\n",
    "answer": "<pre><code>pkill -f 'PATTERN'\n</code></pre>\n<p>Will kill all the processes that the pattern <code>PATTERN</code> matches.  With the <code>-f</code> option, the whole command line (i.e. including arguments) will be taken into account. Without the <code>-f</code> option, only the command name will be taken into account.</p>\n<p>See also <code>man pkill</code> on your system.</p>\n",
    "url": "https://unix.stackexchange.com/questions/316065/how-to-kill-all-process-with-given-name"
  },
  {
    "question_title": "How to start multi-threaded grep in terminal?",
    "question_body": "<p>I have a folder which has 250+ files of 2 GB each. I need to search for a string/pattern in those files and output the result in an <code>output</code> file. I know I can run the following command, but it is too slow!!</p>\n\n<pre><code>grep mypattern * &gt; output\n</code></pre>\n\n<p>I want to speed it up. Being a programmer in Java, I know multi-threading can be used for speeding up the process. I'm stuck on how to start <code>grep</code> in \"multi-threaded mode\" and write the output into a single <code>output</code> file.</p>\n",
    "answer": "<p>There are two easy solutions for this. Basically, using <code>xargs</code> or <code>parallel</code>.</p>\n\n<p><strong>xargs Approach:</strong></p>\n\n<p>You can use <code>xargs</code> with <code>find</code> as follows:</p>\n\n<pre><code>find . -type f -print0  | xargs -0 -P number_of_processes grep mypattern &gt; output\n</code></pre>\n\n<p>Where you will replace <code>number_of_processes</code> by the maximum number of processes you want to be launched.\nHowever, this is not guaranteed to give you a significant performance in case your performance is I/O limited. In which case you might try to start more processes to compensate for the time lost waiting for I/Os.</p>\n\n<p>Also, with the inclusion of find, you can specify more advanced options instead of just file patterns, like modification time, etc ...</p>\n\n<p>One possible issue with this approach as explained by Stéphane's comments, if there are few files, <code>xargs</code> may not start sufficiently many processes for them. One solution will be to use the <code>-n</code> option for <code>xargs</code> to specify how many arguments should it take from the pipe at a time. Setting <code>-n1</code> will force <code>xargs</code> to start a new process for each single file. This might be a desired behavior if the files are very large (like in the case of this question) and there is a relatively small number of files. However, if the files themselves are small, the overhead of starting a new process may undermine the advantage of parallelism, in which case a greater <code>-n</code> value will be better. Thus, the <code>-n</code> option might be fine tuned according to the file sizes and number.</p>\n\n<p><strong>Parallel Approach:</strong></p>\n\n<p>Another way to do it is to use Ole Tange GNU Parallel tool <code>parallel</code>, (available <a href=\"http://www.gnu.org/software/parallel/\">here</a>). This offers greater fine grain control over parallelism and can even be distributed over multiple hosts (would be beneficial if your directory is shared for example). \n Simplest syntax using parallel will be:</p>\n\n<p><code>find . -type f | parallel -j+1 grep mypattern</code></p>\n\n<p>where the option <code>-j+1</code> instructs parallel to start one process in excess of the number of cores on your machine (This can be helpful for I/O limited tasks, you may even try to go higher in number).</p>\n\n<p>Parallel also has the advantage over <code>xargs</code> of actually retaining the order of the output from each process and generating a contiguous output. For example, with <code>xargs</code>, if process 1 generates a line say <code>p1L1</code>, process 2 generates a line <code>p2L1</code>, process 1 generates another line <code>p1L2</code>, the output will be:</p>\n\n<pre><code>p1L1\np2L1\np1L2\n</code></pre>\n\n<p>whereas with <code>parallel</code> the output should be:</p>\n\n<pre><code>p1L1\np1L2\np2L1\n</code></pre>\n\n<p>This is usually more useful than <code>xargs</code> output.</p>\n",
    "url": "https://unix.stackexchange.com/questions/197352/how-to-start-multi-threaded-grep-in-terminal"
  },
  {
    "question_title": "grep search + next line",
    "question_body": "<p>With the <code>grep</code> command, I found the text I need as follows:</p>\n<pre><code>grep 'C02' ~/temp/log.txt\n</code></pre>\n<p>Now, wherever I find the desired string, I would like to print the line following the found string.</p>\n<p>For example, let's say that the desired text is <code>abc</code>, and <code>abc</code> is found on line 12, I would like to print line 13 too.</p>\n",
    "answer": "<p>If you are using Linux system, you can try:</p>\n\n<pre><code>grep -A1 \"C02\" ~/temp/log.txt\n\n\nOPTIONS\n       -A NUM, --after-context=NUM\n              Print NUM lines of trailing context after matching lines.  Places a line containing -- between contiguous groups of matches.\n       -B NUM, --before-context=NUM\n              Print NUM lines of leading context before matching lines.  Places a line containing -- between contiguous groups of matches.\n       -C NUM, --context=NUM\n              Print NUM lines of output context.  Places a line containing -- between contiguous groups of matches.\n</code></pre>\n\n<p>You can use awk also as:</p>\n\n<pre><code>awk '/C02/{print;getline;print}' ~/temp/log.txt\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/320706/grep-search-next-line"
  },
  {
    "question_title": "&#39;ls -1&#39; : how to list filenames without extension",
    "question_body": "<p><code>ls -1</code> lists my elements like so:</p>\n\n<pre><code>foo.png\nbar.png\nfoobar.png\n...\n</code></pre>\n\n<p>I want it listed without the <code>.png</code> like so:</p>\n\n<pre><code>foo\nbar\nfoobar\n...\n</code></pre>\n\n<p>(the dir only contains <code>.png</code> files)</p>\n\n<p>Can somebody tell me how to use <code>grep</code> in this case?</p>\n\n<p>Purpose:\nI have a text file where all the names are listed without the extension. I want to make a script that compares the text file with the folder to see which file is missing.</p>\n",
    "answer": "<pre><code>ls -1 | sed -e 's/\\.png$//'\n</code></pre>\n\n<p>The <code>sed</code> command removes (that is, it replaces with the empty string) any string <code>.png</code> found at the <strong>end</strong> of a filename.</p>\n\n<p>The <code>.</code> is escaped as <code>\\.</code> so that it is interpreted by <code>sed</code> as a literal <code>.</code> character rather than the regexp <code>.</code> (which means match any character).  The <code>$</code> is the end-of-line anchor, so it doesn't match <code>.png</code> in the middle of a filename.</p>\n",
    "url": "https://unix.stackexchange.com/questions/283886/ls-1-how-to-list-filenames-without-extension"
  },
  {
    "question_title": "How do I list every file in a directory except those with specified extensions?",
    "question_body": "<p>Suppose that I have a folder containing <strong>.txt</strong>, <strong>.pdf</strong>, and other files.  I would like to list the \"other\" files (i.e., files not having the extensions <strong>.txt</strong> or <strong>.pdf</strong>). Do you have any advice on how to do this?</p>\n\n<p>I know how to list files not having a given extension.  For example, if I want to list all files except the <strong>.txt</strong> files, then either </p>\n\n<pre><code>find -not -iname \"*.txt\"\n</code></pre>\n\n<p>or </p>\n\n<pre><code>ls | grep -v '\\.txt$' | column\n</code></pre>\n\n<p>seem to work.  But, how can I list everything except <strong>.txt</strong> files <em>or</em> <strong>.pdf</strong> files?  It seems that I need to use some sort of logical \"or\" in <code>find</code> or <code>grep</code>.</p>\n",
    "answer": "<p>Assuming one has GNU <code>ls</code>, this is possibly the simplest way:</p>\n\n<pre><code>ls -I \"*.txt\" -I \"*.pdf\"\n</code></pre>\n\n<p>If you want to iterate across all the subdirectories:</p>\n\n<pre><code>ls -I \"*.txt\" -I \"*.pdf\" -R\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/47151/how-do-i-list-every-file-in-a-directory-except-those-with-specified-extensions"
  },
  {
    "question_title": "How to display lines 2-4 after each grep result?",
    "question_body": "<p>I'm parsing a mailbox file that stores e-mail server reports for unsuccessfully delivered e-mail. I wish to extract bad e-mail addresses, so that I remove them from the system. The log file looks like this:</p>\n\n<pre><code>...some content...\n                   The mail system\n\n&lt;slavicatomic118@hotmail.com&gt;: host mx1.hotmail.com[65.54.188.94] said: 550\n    Requested action not taken: mailbox unavailable (in reply to RCPT TO\n    command)\n\n...some content...\n                   The mail system\n\n&lt;oki88@optimumpro.net&gt;: host viking.optimumpro.net[79.101.51.82] said: 550\n    Unknown user (in reply to RCPT TO command)\n\n...some content...\n                   The mail system\n\n&lt;sigirna_luka@yahoo.com&gt;: host mta5.am0.yahoodns.net[74.6.140.64] said: 554\n    delivery error: dd This user doesn't have a yahoo.com account\n    (sigirna_luka@yahoo.com) [0] - mta1172.mail.sk1.yahoo.com (in reply to end\n    of DATA command)\n\n...etc.\n</code></pre>\n\n<p>E-mail address comes 2 lines after a line with \"The mail system\". Using grep like this gives me the \"The mail system\" line and the next two lines:</p>\n\n<pre><code>grep -A 2 \"The mail system\" mbox_file\n</code></pre>\n\n<p>However, I don't know how to remove the \"The mail system\" line and the second empty line from this output. I guess I could write PHP/Perl/Python script to do it, but I wonder if this is possible with grep or some other standard tool. I tried to give negative offset to -B parameter:</p>\n\n<pre><code>grep -A 2 -B -2 \"The mail system\" mbox_file\n</code></pre>\n\n<p>But grep complains:</p>\n\n<pre><code>grep: -2: invalid context length argument\n</code></pre>\n\n<p>Is there a way to do this with grep?</p>\n",
    "answer": "<p>The simplest way to solve it using <code>grep</code> only, is to pipe one more inverted <code>grep</code> at the end.\nFor example:</p>\n\n<pre><code>grep -A 4 \"The mail system\" temp.txt | grep -v \"The mail system\" | grep -v '^\\d*$'\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/31531/how-to-display-lines-2-4-after-each-grep-result"
  },
  {
    "question_title": "grep: memory exhausted",
    "question_body": "<p>I was doing a very simple search:</p>\n\n<pre><code>grep -R Milledgeville ~/Documents\n</code></pre>\n\n<p>And after some time this error appeared:</p>\n\n<pre><code>grep: memory exhausted\n</code></pre>\n\n<p>How can I avoid this?</p>\n\n<p>I have 10GB of RAM on my system and few applications running, so I am really surprised a simple grep runs out of memory. <code>~/Documents</code> is about 100GB and contains all kinds of files.</p>\n\n<p><code>grep -RI</code> might not have this problem, but I want to search in binary files too.</p>\n",
    "answer": "<p>Two potential problems:</p>\n\n<ul>\n<li><p><code>grep -R</code> (except for the modified GNU <code>grep</code> found on OS/X 10.8 and above) follows symlinks, so even if there's only 100GB of files in <code>~/Documents</code>, there might still be a symlink to <code>/</code> for instance and you'll end up scanning the whole file system including files like <code>/dev/zero</code>. Use <code>grep -r</code> with newer GNU <code>grep</code>, or use the standard syntax:</p>\n\n<pre><code>find ~/Documents -type f -exec grep Milledgeville /dev/null {} +\n</code></pre>\n\n<p>(however note that the exit status won't reflect the fact that the pattern is matched or not).</p></li>\n<li><p><code>grep</code> finds the lines that match the pattern. For that, it has to load one line at a time in memory. GNU <code>grep</code> as opposed to many other <code>grep</code> implementations  doesn't have a limit on the size of the lines it reads and supports search in binary files. So, if you've got a file with a very big line (that is, with two newline characters very far appart), bigger than the available memory, it will fail.</p>\n\n<p>That would typically happen with a sparse file. You can reproduce it with:</p>\n\n<pre><code>truncate -s200G some-file\ngrep foo some-file\n</code></pre>\n\n<p>That one is difficult to work around. You could do it as (still with GNU <code>grep</code>):</p>\n\n<pre><code>find ~/Documents -type f -exec sh -c 'for i do\n  tr -s \"\\0\" \"\\n\" &lt; \"$i\" | grep --label=\"$i\" -He \"$0\"\n  done' Milledgeville {} +\n</code></pre>\n\n<p>That converts sequences of NUL characters into one newline character prior to feeding the input to <code>grep</code>. That would cover for cases where the problem is due to sparse files.</p>\n\n<p>You could optimise it by doing it only for large files:</p>\n\n<pre><code>find ~/Documents -type f \\( -size -100M -exec \\\n  grep -He Milledgeville {} + -o -exec sh -c 'for i do\n  tr -s \"\\0\" \"\\n\" &lt; \"$i\" | grep --label=\"$i\" -He \"$0\"\n  done' Milledgeville {} + \\)\n</code></pre>\n\n<p>If the files are <em>not</em> sparse and you have a version of GNU <code>grep</code> prior to <code>2.6</code>, you can use the <code>--mmap</code> option. The lines will be mmapped in memory as opposed to copied there, which means the system can always reclaim the memory by paging out the pages to the file. That option was removed in GNU <code>grep</code> 2.6</p></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/90036/grep-memory-exhausted"
  },
  {
    "question_title": "How to use grep and cut in script to obtain website URLs from an HTML file",
    "question_body": "<p>I am trying to use grep and cut to extract URLs from an HTML file. The links look like:</p>\n\n<pre><code>&lt;a href=\"http://examplewebsite.com/\"&gt;\n</code></pre>\n\n<p>Other websites have <code>.net</code>, <code>.gov</code>, but I assume I could make the cut off point right before <code>&gt;</code>. So I know I can use grep and cut somehow to cut off everything before http and after .com, but I have been stuck on it for a while. </p>\n",
    "answer": "<p>Not sure if you are limited on tools:</p>\n\n<p>But regex might not be the best way to go as mentioned, but here is an example that I put together:</p>\n\n<pre><code>cat urls.html | grep -Eo \"(http|https)://[a-zA-Z0-9./?=_%:-]*\" | sort -u\n</code></pre>\n\n<ul>\n<li><code>grep -E</code> : is the same as egrep</li>\n<li><code>grep -o</code> : only outputs what has been grepped</li>\n<li><code>(http|https)</code> : is an either / or</li>\n<li><code>a-z</code> : is all lower case</li>\n<li><code>A-Z</code> : is all upper case</li>\n<li><code>.</code> : is dot</li>\n<li><code>/</code> : is the slash</li>\n<li><code>?</code> : is ?</li>\n<li><code>=</code> : is equal sign</li>\n<li><code>_</code> : is underscore</li>\n<li><code>%</code> : is percentage sign</li>\n<li><code>:</code> : is colon</li>\n<li><code>-</code> : is dash</li>\n<li><code>*</code>: is repeat the [...] group</li>\n<li><code>sort -u</code> : will sort &amp; remove any duplicates</li>\n</ul>\n\n<p>Output:</p>\n\n<pre><code>bob@bob-NE722:~s$  wget -qO- https://stackoverflow.com/ | grep -Eo \"(http|https)://[a-zA-Z0-9./?=_-]*\" | sort -u\nhttps://stackauth.com\nhttps://meta.stackoverflow.com\nhttps://cdn.sstatic.net/Img/svg-icons\nhttps://stackoverflow.com\nhttps://www.stackoverflowbusiness.com/talent\nhttps://www.stackoverflowbusiness.com/advertising\nhttps://stackoverflow.com/users/login?ssrc=head\nhttps://stackoverflow.com/users/signup?ssrc=head\nhttps://stackoverflow.com\nhttps://stackoverflow.com/help\nhttps://chat.stackoverflow.com\nhttps://meta.stackoverflow.com\n...\n</code></pre>\n\n<p>You can also add in <code>\\d</code> to catch other numeral types.</p>\n",
    "url": "https://unix.stackexchange.com/questions/181254/how-to-use-grep-and-cut-in-script-to-obtain-website-urls-from-an-html-file"
  },
  {
    "question_title": "How do I delete the first n lines and last line of a file using shell commands?",
    "question_body": "<p>I have a file named <code>Element_query</code> containing the result of a query :</p>\n\n<pre><code>SQL&gt; select count (*) from element;\n\n[Output of the query which I want to keep in my file]\n\nSQL&gt; spool off;\n</code></pre>\n\n<p>I want to delete 1st line and last line using shell command.</p>\n",
    "answer": "<p>Using GNU <code>sed</code>:</p>\n\n<pre><code>sed -i '1d;$d' Element_query\n</code></pre>\n\n<p><strong>How it works :</strong></p>\n\n<ul>\n<li><code>-i</code> option edit the file itself. You could also remove that option and redirect the output to a new file or another command if you want.</li>\n<li><code>1d</code> deletes the first line (<code>1</code> to only act on the first line, <code>d</code> to delete it)</li>\n<li><code>$d</code> deletes the last line (<code>$</code> to only act on the last line, <code>d</code> to delete it)</li>\n</ul>\n\n<p><strong>Going further :</strong></p>\n\n<ul>\n<li>You can also delete a range. For example, <code>1,5d</code> would delete the first 5 lines.</li>\n<li>You can also delete every line that begins with <code>SQL&gt;</code> using the statement <code>/^SQL&gt; /d</code></li>\n<li>You could delete every blank line with <code>/^$/d</code></li>\n<li>Finally, you can combine any of the statement by separating them with a semi-colon (<code>statement1;statement2;satement3;...</code>) or by specifying them separately on the command line (<code>-e 'statement1' -e 'statement 2' ...</code>)</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/209068/how-do-i-delete-the-first-n-lines-and-last-line-of-a-file-using-shell-commands"
  },
  {
    "question_title": "How do I count the number of occurrences of a word in a text file with the command line?",
    "question_body": "<p>I have a large JSON file that is on one line, and I want to use the command line to be able to count the number of occurrences of a word in the file. How can I do that?</p>\n",
    "answer": "<pre><code>$ tr ' ' '\\n' &lt; FILE | grep WORD | wc -l\n</code></pre>\n\n<p>Where <code>tr</code> replaces spaces with newlines, <code>grep</code> filters all resulting lines matching WORD and <code>wc</code> counts the remaining ones.</p>\n\n<p>One can even save the <code>wc</code> part using the <code>-c</code> option of grep:</p>\n\n<pre><code>$ tr ' ' '\\n' &lt; FILE | grep -c WORD\n</code></pre>\n\n<p>The <code>-c</code> option is defined by POSIX.</p>\n\n<p>If it is not guaranteed that there are spaces between the words, you have to use some other character (as delimiter) to replace. For example alternative <code>tr</code> parts are</p>\n\n<pre><code>tr '\"' '\\n'\n</code></pre>\n\n<p>or</p>\n\n<pre><code>tr \"'\" '\\n'\n</code></pre>\n\n<p>if you want to replace double or single quotes. Of course, you can also use <code>tr</code> to replace multiple characters at once (think different kinds of whitespace and punctuation).</p>\n\n<p>In case you need to count WORD but not prefixWORD, WORDsuffix or  prefixWORDsuffix, you can enclose the WORD pattern in begin/end-of-line markers:</p>\n\n<pre><code>grep -c '^WORD$'\n</code></pre>\n\n<p>Which is equivalent to word-begin/end markers, in our context:</p>\n\n<pre><code>grep -c '\\&lt;WORD\\&gt;'\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/2244/how-do-i-count-the-number-of-occurrences-of-a-word-in-a-text-file-with-the-comma"
  },
  {
    "question_title": "How can I use grep to search multiple unnested directories?",
    "question_body": "<p>It may sound like I'm asking the same thing as <a href=\"https://unix.stackexchange.com/q/7383/22172\">this question</a>, but I have different requirements. This is an example of my filesystem:</p>\n\n<ul>\n<li>/code/\n\n<ul>\n<li>internal/\n\n<ul>\n<li>dev/</li>\n<li>main/</li>\n</ul></li>\n<li>public/\n\n<ul>\n<li>dev/</li>\n<li>main/</li>\n<li>release/</li>\n</ul></li>\n<li>tools/</li>\n</ul></li>\n</ul>\n\n<p><code>/code/internal/dev/</code>, <code>/code/public/dev/</code> and <code>/code/tools/</code> contain subdirectories for multiple projects. I work almost exclusively in the dev branches of <code>/code/internal/</code> and <code>/code/public/</code>, and often I want to search for a text string in those directories along with <code>/code/tools/</code> (which has no branches). In these instances I have to do three separate commands:</p>\n\n<pre><code>$ grep -r \"some string\" /code/internal/dev/\n$ grep -r \"some string\" /code/public/dev/\n$ grep -r \"some string\" /code/tools/\n</code></pre>\n\n<p>I'd like to know if there's a single command to do this. If not, I would most likely need to write a simple bash script.</p>\n",
    "answer": "<p>You can concatenate several paths for grep to look for:</p>\n\n<pre><code>grep -r \"some string\" /code/internal/dev/ /code/public/dev/ /code/tools/\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/126514/how-can-i-use-grep-to-search-multiple-unnested-directories"
  },
  {
    "question_title": "Why is egrep [wW][oO][rR][dD] faster than grep -i word?",
    "question_body": "<p>I've been using <code>grep -i</code> more often and I found out that it is slower than its <code>egrep</code> equivalent, where I match against the upper or lower case of each letter:</p>\n\n<pre><code>$ time grep -iq \"thats\" testfile\n\nreal    0m0.041s\nuser    0m0.038s\nsys     0m0.003s\n$ time egrep -q \"[tT][hH][aA][tT][sS]\" testfile\n\nreal    0m0.010s\nuser    0m0.003s\nsys     0m0.006s\n</code></pre>\n\n<p>Does <code>grep -i</code> do additional tests that <code>egrep</code> doesn't?</p>\n",
    "answer": "<p><code>grep -i 'a'</code> is equivalent to <code>grep '[Aa]'</code> in an ASCII-only locale. In a Unicode locale, character equivalences and conversions can be complex, so <code>grep</code> may have to do extra work to determine which characters are equivalent. The relevant locale setting is <code>LC_CTYPE</code>, which determines how bytes are interpreted as characters.</p>\n\n<p>In my experience, GNU <code>grep</code> can be slow when invoked in a UTF-8 locale. If you know that you're searching for ASCII characters only, invoking it in an ASCII-only locale may be faster. I expect that</p>\n\n<pre><code>time LC_ALL=C grep -iq \"thats\" testfile\ntime LC_ALL=C egrep -q \"[tT][hH][aA][tT][sS]\" testfile\n</code></pre>\n\n<p>would produce indistinguishable timings.</p>\n\n<p>That being said, I can't reproduce your finding with GNU <code>grep</code> on Debian jessie (but you didn't specify your test file). If I set an ASCII locale (<code>LC_ALL=C</code>), <code>grep -i</code> is faster. The effects depend on the exact nature of the string, for example a string with repeated characters reduces the performance (<a href=\"https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm\">which is to be expected</a>).</p>\n",
    "url": "https://unix.stackexchange.com/questions/269587/why-is-egrep-wwoorrdd-faster-than-grep-i-word"
  },
  {
    "question_title": "Grep word boundaries",
    "question_body": "<p>Accorging to GNU documentation:</p>\n\n<pre><code>‘\\&lt;’ Match the empty string at the beginning of word.\n‘\\&gt;’ Match the empty string at the end of word.\n</code></pre>\n\n<p>My /etc/fstab looks like this:</p>\n\n<pre><code>/dev/sdb1       /media/fresh      ext2   defaults     0 0\n</code></pre>\n\n<p>I want grep to return TRUE/FALSE for the existence of /media/fresh. I tried to use <code>\\&lt;</code> and <code>\\&gt;</code> but it didn't work. Why?</p>\n\n<pre><code>egrep '\\&lt;/media/fresh\\&gt;' /etc/fstab\n</code></pre>\n\n<h1>Workaround:</h1>\n\n<pre><code>egrep '[[:blank:]]/media/fresh[[:blank:]]' /etc/fstab\n</code></pre>\n\n<p>But it looks uglier.</p>\n\n<p>My grep is 2.5.1</p>\n",
    "answer": "<p><code>\\&lt;</code> and <code>\\&gt;</code> match empty string at the begin and end of a word respectively and only word constituent characters are:</p>\n\n<pre><code>[[:alnum:]_]\n</code></pre>\n\n<p>From <code>man grep</code>:</p>\n\n<pre><code>Word-constituent characters are letters, digits, and the underscore.\n</code></pre>\n\n<p>So, your Regex is failing because <code>/</code> is not a valid word constituent character.</p>\n\n<p>Instead as you have spaces around, you can use <code>-w</code> option of <code>grep</code> to match a word:</p>\n\n<pre><code>grep -wo '/media/fresh' /etc/fstab\n</code></pre>\n\n<p><strong>Example:</strong></p>\n\n<pre><code>$ grep -wo '/media/fresh' &lt;&lt;&lt; '/dev/sdb1       /media/fresh      ext2   defaults     0 0'\n/media/fresh\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/281938/grep-word-boundaries"
  },
  {
    "question_title": "Find files containing one string but not the other",
    "question_body": "<p>I am in a folder with lots of <code>.txt</code> files, I would like to find all the files which contain <code>stringA</code> but don't contain <code>stringB</code> (they are not necessarily in the same line). Does anyone know how to do this?</p>\n",
    "answer": "<p>As long as your filenames do not contain spaces, tabs, newline (assuming an unmodified <code>$IFS</code>) or wildcard characters and don't start with <code>-</code>, and if your <code>grep</code> supports the <code>-L</code> option, you can do it as follows:</p>\n<pre><code>$ cat file1\nstringA\nstringC\n$ cat file2\nstringA\nstringB\n$ grep -L stringB $(grep -l stringA file?)\nfile1\n</code></pre>\n<p>The <code>grep</code> executed in the subshell <code>$()</code>, will print all filenames which contain <code>stringA</code>. This filelist is input for the main <code>grep</code> command, which lists all files that do not contain <code>stringB</code>.</p>\n<p>From <code>man grep</code></p>\n<blockquote>\n<pre><code>  -v, --invert-match\n</code></pre>\n</blockquote>\n<pre><code>          Invert the sense of matching, to select non-matching lines.  (-v is specified by POSIX.)\n</code></pre>\n<blockquote>\n<pre><code>  -L, --files-without-match\n</code></pre>\n</blockquote>\n<pre><code>          Suppress normal output; instead print the name of each input file from which no output would normally have been printed.  The scanning will stop on the first match.\n</code></pre>\n<blockquote>\n<pre><code>  -l, --files-with-matches\n</code></pre>\n</blockquote>\n<pre><code>          Suppress normal output; instead print the name of each input file from which output would normally have been printed.  The scanning will stop on the first match.  (-l is specified by POSIX.)\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/128434/find-files-containing-one-string-but-not-the-other"
  },
  {
    "question_title": "How to print the first line using grep command?",
    "question_body": "<p>I have a file called <code>file.txt</code>. How can I print the first line only using the <code>grep</code> command?</p>\n",
    "answer": "<p>Although it's an unconventional application of grep, you can do it in GNU grep using</p>\n<pre><code>grep -m1 &quot;&quot; file.txt\n</code></pre>\n<p>It works because the empty expression matches anything, while <code>-m1</code> causes grep to exit after the first match</p>\n<pre><code>-m NUM, --max-count=NUM\n       Stop reading a file after NUM matching lines.\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/294486/how-to-print-the-first-line-using-grep-command"
  },
  {
    "question_title": "Is there a &#39;rc&#39; configuration file for grep/egrep? (~/.egreprc?)",
    "question_body": "<p>I usually use <code>grep</code> when developing, and there are some extensions that I always want to exclude (like <code>*.pyc</code>).</p>\n\n<p>Is it possible to create a <code>~/.egreprc</code> or something like that, and add filtering to exclude <code>pyc</code> files from all results?</p>\n\n<p>Is this possible, or will I have to create an alias for using <code>grep</code> in this manner, and call the alias instead of <code>grep</code>?</p>\n",
    "answer": "<p>No, there's no rc file for grep. </p>\n\n<p>GNU grep 2.4 through 2.21 applied options from the environment variable <code>GREP_OPTIONS</code>, but more recent versions no longer honor it.</p>\n\n<p>For interactive use, define an alias in your shell initialization file (<code>.bashrc</code> or <code>.zshrc</code>). I use a variant of the following:</p>\n\n<pre><code>alias regrep='grep -Er --exclude=*~ --exclude=*.pyc --exclude-dir=.bzr --exclude-dir=.git --exclude-dir=.svn'\n</code></pre>\n\n<p>If you call the alias <code>grep</code>, and you occasionally want to call <code>grep</code> without the options, type <code>\\grep</code>. The backslash bypasses the alias.</p>\n",
    "url": "https://unix.stackexchange.com/questions/8214/is-there-a-rc-configuration-file-for-grep-egrep-egreprc"
  },
  {
    "question_title": "Return owner of process given PID",
    "question_body": "<p>I'm trying to grab the owner of a process from a list.</p>\n<p>I have the command <code>pidof nmap</code> to get the  then <code>ps -u &lt;PID&gt; | grep USER</code> that I'm currently playing around with. But when I run it, it ends up just printing the titles (top line):</p>\n<p><a href=\"https://i.sstatic.net/IGhUN.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/IGhUN.png\" alt=\"screenshot of ps\" /></a></p>\n<p>How can I grab the name of the owner given the process ID?</p>\n",
    "answer": "<pre><code>ps -o user= -p PIDHERE\n</code></pre>\n\n<p>This selects the process <code>PIDHERE</code> with <code>-p</code>, then instructs <code>ps</code> to format the output by printing only the column named <code>user</code>; the <code>=</code> sign means \"rename the column <code>user</code> to (nothing)\", effectively removing the header line.</p>\n",
    "url": "https://unix.stackexchange.com/questions/284934/return-owner-of-process-given-pid"
  },
  {
    "question_title": "tr complains of “Illegal byte sequence”",
    "question_body": "<p>I'm brand new to UNIX and I am using Kirk McElhearn's \"The Mac OS X Command Line\" to teach myself some commands. </p>\n\n<p>I am attempting to use <code>tr</code> and <code>grep</code> so that I can search for text strings in a regular MS-Office Word Document. </p>\n\n<pre><code>$ tr '\\r' '\\n' &lt; target-file | grep search-string\n</code></pre>\n\n<p>But all it returns is:</p>\n\n<pre><code>Illegal byte sequence.\n</code></pre>\n\n<p></p>\n\n<pre><code>robomechanoid:Position-Paper-Final-Draft robertjralph$ tr '\\r' '\\n' &lt; Position-Paper-Final-Version.docx | grep DeCSS\ntr: Illegal byte sequence\nrobomechanoid:Position-Paper-Final-Draft robertjralph$ \n</code></pre>\n\n<p>I've actually run the same line on a script that I created in <code>vi</code> and it does the search correctly.</p>\n",
    "answer": "<p><code>grep</code> is a text processing tool. It expects their input to be <a href=\"http://en.wikipedia.org/wiki/Text_file\" rel=\"noreferrer\">text files</a>. It seems that the same goes for <code>tr</code> on macOS (even though <code>tr</code> is supposed to support binary files).</p>\n\n<p>Computers store data as sequences of <a href=\"http://en.wikipedia.org/wiki/Byte\" rel=\"noreferrer\">bytes</a>. A text is a sequence of characters. There are several ways to encode characters as bytes, called <a href=\"http://en.wikipedia.org/wiki/Character_encoding\" rel=\"noreferrer\">character encodings</a>. The de facto standard character encoding in most of the world, especially on OSX, is <a href=\"http://en.wikipedia.org/wiki/UTF-8\" rel=\"noreferrer\">UTF-8</a>, which is an encoding for the <a href=\"http://en.wikipedia.org/wiki/Unicode\" rel=\"noreferrer\">Unicode</a> character set. There are only 256 possible bytes, but over a million possible Unicode characters, so most characters are encoded as multiple bytes. UTF-8 is a variable-length encoding: depending on the character, it can take from one to four bytes to encode a character. Some sequences of bytes do not represent any character in UTF-8. Therefore, there are sequences of bytes which are not valid UTF-8 text files.</p>\n\n<p><code>tr</code> is complaining because it encountered such a byte sequence. It expects to see a text file encoded in UTF-8, but it sees binary data which is not valid UTF-8.</p>\n\n<p>A Microsoft Word document is not a text file: it's a word processing document. Word processing document formats encode not only text, but also formatting, embedded images, etc. The Word format, like most word processing formats, is not a text file.</p>\n\n<p>You can instruct text processing tools to operate on bytes by changing the <a href=\"http://en.wikipedia.org/wiki/Locale\" rel=\"noreferrer\">locale</a>. Specifically, select the “C” locale, which basically means means “nothing fancy”. On the command line, you can choose locale settings with <a href=\"http://en.wikipedia.org/wiki/Environment_variable\" rel=\"noreferrer\">environment variables</a>.</p>\n\n<pre><code>export LC_CTYPE=C\ntr '\\r' '\\n' &lt; target-file | grep search-string\n</code></pre>\n\n<p>This will not emit any error, but it won't do anything useful either since <code>target-file</code> is still a binary file which is unlikely to contain most search strings that you'll specify.</p>\n\n<p>Incidentally, <code>tr '\\r' '\\n'</code> is not a very useful command unless you have text files left over from Mac OS 9 or older. <code>\\r</code> (carriage return) was the newline separator in Mac OS before Mac OS X. Since OSX, the newline separator is <code>\\n</code> (line feed, the unix standard) and text files do not contain carriage returns. Windows uses the two-character sequence CR-LF to represent line breaks; <code>tr -d '\\r'</code> would convert a Windows text file into a Unix/Linux/OSX text file.</p>\n\n<p>So how can you search in a Word document from the command line? A <code>.docx</code> Word document is actually a <a href=\"http://en.wikipedia.org/wiki/Zip_(file_format)\" rel=\"noreferrer\">zip archive</a> containing several files, the main ones being in <a href=\"http://en.wikipedia.org/wiki/XML\" rel=\"noreferrer\">XML</a>. </p>\n\n<pre><code>unzip -l Position-Paper-Final-Version.docx\n</code></pre>\n\n<p>Mac OS X includes the <a href=\"https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/zipgrep.1.html\" rel=\"noreferrer\">zipgrep</a> utility to search inside zip files.</p>\n\n<pre><code>zipgrep DeCSS Position-Paper-Final-Version.docx\n</code></pre>\n\n<p>The result is not going to be very readable because XML files in the docx format mostly consist of one huge line. If you want to search inside the main body text of the document, extract the file <code>word/document.xml</code> from the archive. Note that in addition to the document text, this file contains XML markup which represents the structure of the document. You can massage the XML markup a bit with <a href=\"https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/sed.1.html\" rel=\"noreferrer\"><code>sed</code></a> to split it into manageable lines.</p>\n\n<pre><code>unzip -p Position-Paper-Final-Version.docx word/document.xml |\nsed -e 's/&gt;&lt;/&gt;\\n&lt;/g' |\ngrep DeCSS\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/141420/tr-complains-of-illegal-byte-sequence"
  },
  {
    "question_title": "List the files containing a particular word in their text",
    "question_body": "<p>I would like to list the files recursively and uniquely that contain the given word. </p>\n\n<p><strong>Example</strong>: Checking for word 'check', I normal do is a grep</p>\n\n<pre><code>$ grep check * -R\n</code></pre>\n\n<p>But as there are many occurrence of this word, I get a lot of output. So I just need to list the filenames that contain the given search word. I guess some trick with <code>find</code> and <code>xargs</code> would suffice here, but not sure.</p>\n\n<p>Any ideas?</p>\n",
    "answer": "<p>Use the <code>-l</code> or <code>--files-with-matches</code> option which is documented as follows:</p>\n\n<blockquote>\n  <p>Suppress  normal  output;  instead  print the name of each input file\n  from which output would normally have  been  printed.   The scanning \n  will  stop  on  the  first match.  (<code>-l</code> is specified by POSIX.)</p>\n</blockquote>\n\n<p>So, for you example you can use the following:</p>\n\n<pre><code>$ grep check * -lR\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/65181/list-the-files-containing-a-particular-word-in-their-text"
  },
  {
    "question_title": "How can I update to a newer version of Git using apt-get?",
    "question_body": "<p>I've just set up a new machine with Ubuntu Oneiric 11.10 and then run</p>\n\n<pre><code>apt-get update\napt-get upgrade\napt-get install git\n</code></pre>\n\n<p>Now if I run <code>git --version</code> it tells me I have <code>git version 1.7.5.4</code> but on my local machine I have the much newer <code>git version 1.7.9.2</code></p>\n\n<p>I know I can install from source to get the newest version, but I thought that it was a good idea to use the package manager as much as possible to keep everything standardized.</p>\n\n<p>So is it possible to use <code>apt-get</code> to get a newer version of <code>git</code>, and what is the right way to do it?</p>\n",
    "answer": "<p>Here are the commands you need to run, if you just want to get it done:</p>\n<pre><code>sudo add-apt-repository ppa:git-core/ppa -y\nsudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com A1715D88E1DF1F24 40976EAF437D05B5 3B4FE6ACC0B21F32 A6616109451BBBF2\nsudo apt-get update\nsudo apt-get install git -y\ngit --version\n</code></pre>\n<p>As of Dec 2018, I got git 2.20.1 that way, while the version in the Ubuntu Xenial repositories was 2.7.4.</p>\n<p>If your system doesn't have <code>add-apt-repository</code>, you can install it via:</p>\n<pre><code>sudo apt-get install python-software-properties software-properties-common\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/33617/how-can-i-update-to-a-newer-version-of-git-using-apt-get"
  },
  {
    "question_title": "Determine if Git working directory is clean from a script",
    "question_body": "<p>I have a script which runs <code>rsync</code> with a Git working directory as destination. I want the script to have different behavior depending on if the working directory is clean (no changes to commit), or not. For instance, if the output of <code>git status</code> is as below, I want the script to exit:</p>\n\n<pre><code>git status\nAlready up-to-date.\n# On branch master\nnothing to commit (working directory clean)\nEverything up-to-date\n</code></pre>\n\n<p>If the directory is not clean then I would like it to execute some more commands.</p>\n\n<p>How can I check for output like the above in a shell script?</p>\n",
    "answer": "<p>Parsing the output of <code>git status</code> is a bad idea because the output is intended to be human readable, not machine-readable. There's no guarantee that the output will remain the same in future versions of Git or in differently configured environments.</p>\n\n<p><a href=\"https://unix.stackexchange.com/questions/155046/if-then-in-shell-script#comment253463_155046\" title=\"If / then in shell script\">UVVs comment</a> is on the right track, but unfortunately the return code of <code>git status</code> doesn't change when there are uncommitted changes. It does, however, provide the <a href=\"https://www.kernel.org/pub/software/scm/git/docs/git-status.html\"><code>--porcelain</code></a> option, which causes the output of <code>git status --porcelain</code> to be formatted in an easy-to-parse format for scripts, <em>and</em> will remain stable across Git versions and regardless of user configuration.</p>\n\n<p>We can use empty output of <code>git status --porcelain</code> as an indicator that there are no changes to be committed:</p>\n\n<pre><code>if [ -z \"$(git status --porcelain)\" ]; then \n  # Working directory clean\nelse \n  # Uncommitted changes\nfi\n</code></pre>\n\n<p>If we do not care about untracked files in the working directory, we can use the <a href=\"https://www.kernel.org/pub/software/scm/git/docs/git-status.html\"><code>--untracked-files=no</code></a> option to disregard those:</p>\n\n<pre><code>if [ -z \"$(git status --untracked-files=no --porcelain)\" ]; then \n  # Working directory clean excluding untracked files\nelse \n  # Uncommitted changes in tracked files\nfi\n</code></pre>\n\n<p>To make this more robust against conditions which <em>actually</em> cause <code>git status</code> to fail without output to <code>stdout</code>, we can refine the check to:</p>\n\n<pre><code>if output=$(git status --porcelain) &amp;&amp; [ -z \"$output\" ]; then\n  # Working directory clean\nelse \n  # Uncommitted changes\nfi\n</code></pre>\n\n<p>It's also worth noting that, although <code>git status</code> does not give meaningful exit code when the working directory is unclean, <code>git diff</code> provides the <a href=\"https://www.kernel.org/pub/software/scm/git/docs/git-diff.html\"><code>--exit-code</code></a> option, which makes it behave similar to the <a href=\"http://man7.org/linux/man-pages/man1/diff.1.html\">diff</a> utility, that is, exiting with status <code>1</code> when there were differences and <code>0</code> when none were found.</p>\n\n<p>Using this, we can check for unstaged changes with:</p>\n\n<pre><code>git diff --exit-code\n</code></pre>\n\n<p>and staged, but not committed changes with:</p>\n\n<pre><code>git diff --cached --exit-code\n</code></pre>\n\n<p>Although <code>git diff</code> can report on untracked files in submodules via appropriate arguments to <a href=\"https://www.kernel.org/pub/software/scm/git/docs/git-diff.html\"><code>--ignore-submodules</code></a>, unfortunately it seems that there is no way to have it report on untracked files in the actual working directory. If untracked files in the working directory are relevant, <code>git status --porcelain</code> is probably the best bet.</p>\n",
    "url": "https://unix.stackexchange.com/questions/155046/determine-if-git-working-directory-is-clean-from-a-script"
  },
  {
    "question_title": "GUI for GIT similar to SourceTree",
    "question_body": "<p>Is there a similar piece of software to <a href=\"http://www.sourcetreeapp.com/\">SourceTree</a>, a GUI for git, for Linux? I know about Giggle, git cola, etc. I'm looking for a beautiful, easy to use GUI for git.</p>\n",
    "answer": "<p>A nice alternative is <a href=\"https://www.syntevo.com/smartgit/\" rel=\"noreferrer\">SmartGit</a>. It has very similar features to SourceTree and has built in 3-column conflict resolution, visual logs, pulling, pushing, merging, syncing, tagging and all things git :)</p>\n",
    "url": "https://unix.stackexchange.com/questions/48469/gui-for-git-similar-to-sourcetree"
  },
  {
    "question_title": "Creating a user without a password",
    "question_body": "<p>I'm trying to create a user without password like this:</p>\n\n<pre><code>sudo adduser \\\n   --system \\\n   --shell /bin/bash \\\n   --gecos ‘User for managing of git version control’ \\\n   --group \\\n   --disabled-password \\\n   --home /home/git \\\n   git\n</code></pre>\n\n<p>It's created fine. But when I try to login under the git user I'm getting the password entering:</p>\n\n<pre><code>su git\nPassword:...\n</code></pre>\n\n<p>When I leave it empty I get an error:</p>\n\n<pre><code>su: Authentication failed\n</code></pre>\n\n<p>What's wrong?  </p>\n",
    "answer": "<p>The <code>--disabled-password</code> option will not set a password, meaning no password is legal, but  login  is still possible (for example with SSH RSA keys).</p>\n\n<p>To create an user without a password, use <code>passwd -d $username</code> after the user is created to make the password empty. Note not all systems allow users with empty password to log in.</p>\n",
    "url": "https://unix.stackexchange.com/questions/56765/creating-a-user-without-a-password"
  },
  {
    "question_title": "Git submodule shows new commits, submodule status says nothing to commit",
    "question_body": "<p>In a git repository, I have set up my .gitmodules file to reference a github repository:</p>\n\n<pre><code>[submodule \"src/repo\"]\n    path = src/repo\n    url = repourl\n</code></pre>\n\n<p>when I 'git status' on this repo, it shows:</p>\n\n<pre><code>On branch master\nYour branch is up-to-date with 'origin/master'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\nmodified:   src/repo (new commits)\n</code></pre>\n\n<p>If I cd into src/repo and git status on repo, it says that there is nothing to commit.</p>\n\n<p>Why is my top-level git repo complaining?</p>\n",
    "answer": "<p>I just ran into this same class of problem, and I was able to use the solution offered by <strong>@AugustinAmenabar</strong> in the comments section of the accepted answer. My setup was a bit more complex, so I added the <code>--recursive</code> flag to bring all dependencies up to date.</p>\n\n<p><code>git submodule update --recursive src/repo</code></p>\n",
    "url": "https://unix.stackexchange.com/questions/214879/git-submodule-shows-new-commits-submodule-status-says-nothing-to-commit"
  },
  {
    "question_title": "git pull from remote but no such ref was fetched?",
    "question_body": "<p>I have a git mirror on my disk and when I want to update my repo with git pull it gives me error message:</p>\n\n<pre><code> Your configuration specifies to merge with the ref '3.5/master' from the remote, but no such ref was fetched.\n</code></pre>\n\n<p>It also gives me: </p>\n\n<pre><code>  1ce6dac..a5ab7de  3.4/bfq    -&gt; origin/3.4/bfq\n  fa52ab1..f5d387e  3.4/master -&gt; origin/3.4/master\n  398cc33..1c3000a  3.4/upstream-updates -&gt; origin/3.4/upstream-updates\n  d01630e..6b612f7  3.7/master -&gt; origin/3.7/master\n  491e78a..f49f47f  3.7/misc   -&gt; origin/3.7/misc\n  5b7be63..356d8c6  3.7/upstream-updates -&gt; origin/3.7/upstream-updates\n  636753a..027c1f3  3.8/master -&gt; origin/3.8/master\n  b8e524c..cfcf7b5  3.8/misc   -&gt; origin/3.8/misc\n  * [neuer Zweig]     3.8/upstream-updates -&gt; origin/3.8/upstream-updates\n</code></pre>\n\n<p>When I run make menuconfig it gives me Linux version 3.5.7? What does this mean? How can I update my repo?</p>\n",
    "answer": "<p>Check the branch you are on (<code>git branch</code>), check the configuration for that branch (in <code>.../.git/config</code>), you probably are on the wrong branch or your configuration for it tells to merge with a (now?) non-existent remote branch.</p>\n",
    "url": "https://unix.stackexchange.com/questions/66548/git-pull-from-remote-but-no-such-ref-was-fetched"
  },
  {
    "question_title": "Tips for putting ~ under source control",
    "question_body": "<p>I want to put my home directory (~) under source control (git, in this case), as I have many setting files (.gitconfig, .gitignore, .emacs, etc.) in there I would like to carry across machines, and having them in Git would make it nice for retrieving them.  </p>\n\n<p>My main machine is my MacBook, and the way that OS X is set up, there are many folders I want to ignore (Documents, Downloads, .ssh).  There are also folders which are already using Git (.emacs.d).  </p>\n\n<p>My thought was to just add all these directories to my .gitignore file, but that seems kind of tiresome, and could potentially lead to some unforeseen consequences.  My next thought was to periodically copy the files I want to store into some folder in home, then commit that folder.  The problem with that will be that I have to remember to move them before committing.</p>\n\n<p>Is there a clean way to do this?</p>\n",
    "answer": "<p>I have <code>$HOME</code> under git. The first line of my .gitignore file is</p>\n\n<pre><code>/*\n</code></pre>\n\n<p>The rest are patterns to not ignore using the <code>!</code> modifier. This first line means the default is to ignore all files in my home directory. Those files that I want to version control go into <code>.gitignore</code> like this:</p>\n\n<pre><code>!/.gitignore\n!/.profile\n[...]\n</code></pre>\n\n<p>A trickier pattern I have is:</p>\n\n<pre><code>!/.ssh\n/.ssh/*\n!/.ssh/config\n</code></pre>\n\n<p>That is, I only want to version <code>.ssh/config</code> - I don't want my keys and other files in .ssh to go into git. The above is how I achieve that.</p>\n\n<p>Edit: Added slashes to start of all paths. This makes the ignore patterns match from the top of the repository ($HOME) instead of anywhere. For example, if <code>!lib/</code> was a pattern (don't ignore everything in the lib directory) and you add a file <code>.gitignore</code>, previously the pattern (<code>!.gitignore</code>) was matching that. With the leading slash (<code>!/.gitignore</code>), it will only match <code>.gitignore</code> in my home directory and not in any subdirectories.</p>\n\n<p>I haven't seen a case where this makes a practical difference with my ignore list, but it appears to me to be more technically accurate.</p>\n",
    "url": "https://unix.stackexchange.com/questions/1875/tips-for-putting-under-source-control"
  },
  {
    "question_title": "How to use rsync to backup a directory without git subdirectory",
    "question_body": "<p>I want to copy my <code>c</code> directory with all subdirectories excluding <code>./git</code> subdirectory. I do it using <code>rsync</code> :</p>\n\n<pre><code>echo \"copy c and sh files \"\nrsync -a --include='*.c' --include='*.sh' --include='*/' --exclude='*' ~/c/ ~/Dropbox/Public/c\n# remove .git directory = do not send it to dropbox. Thx to Tomasz Sowa\nrm -rf ~/Dropbox/Public/c/.git\n</code></pre>\n\n<p>Can I do it better?</p>\n",
    "answer": "<p>Just add an explicit exclude for .git:</p>\n\n<p><code>rsync -a --exclude='.git/' --include='*.c' --include='*.sh' --include='*/' --exclude='*' ~/c/ ~/Dropbox/Public/c</code></p>\n\n<p>Another option is to create <code>~/.cvsignore</code> containing the following line along with any other directories you'd like to exclude:</p>\n\n<p><code>.git/</code></p>\n",
    "url": "https://unix.stackexchange.com/questions/100660/how-to-use-rsync-to-backup-a-directory-without-git-subdirectory"
  },
  {
    "question_title": "File permission with six octal digits in git. What does it mean?",
    "question_body": "<p>I performed a <code>git commit</code> command and it gave me the following reply:</p>\n<pre><code>7 files changed, 93 insertions(+), 15 deletions(-)\nmode change 100644 =&gt; 100755 assets/internal/fonts/icomoon.svg\nmode change 100644 =&gt; 100755 assets/internal/fonts/icomoon.ttf\nmode change 100644 =&gt; 100755 assets/internal/fonts/icomoon.woff\n</code></pre>\n<p>I know files can have user / group / other rwx permissions and those can be expressed as three octal digits, like &quot;644&quot; or &quot;755&quot;. But why is git showing six digits here?</p>\n<p>I've read the following articles but didn't find an answer:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/File_system_permissions\" rel=\"nofollow noreferrer\">Wikipedia's article on &quot;File system permissions&quot;</a></li>\n<li><a href=\"https://stackoverflow.com/questions/1257592/how-do-i-remove-files-saying-old-mode-100755-new-mode-100644-from-unstaged-cha?noredirect=1&amp;lq=1\">How do I remove files saying “old mode 100755 new mode 100644” from unstaged changes in Git?</a></li>\n<li><a href=\"https://www.techrepublic.com/article/unix-permissions-made-easy/\" rel=\"nofollow noreferrer\">Unix permissions made easy</a></li>\n<li><a href=\"http://www.thinkplexx.com/learn/article/unix/command/chmod-permissions-flags-explained-600-0600-700-777-100-etc\" rel=\"nofollow noreferrer\">Chmod permissions (flags) explained: 600, 0600, 700, 777, 100 etc..</a></li>\n</ul>\n",
    "answer": "<p>The values shown are the 16-bit file modes <a href=\"https://github.com/git/git/blob/master/Documentation/technical/index-format.txt\" rel=\"noreferrer\">as stored by Git</a>, following the layout of <a href=\"https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/sys_stat.h.html\" rel=\"noreferrer\">POSIX types and modes</a>:</p>\n<pre><code>  32-bit mode, split into (high to low bits)\n\n    4-bit object type\n      valid values in binary are 1000 (regular file), 1010 (symbolic link)\n      and 1110 (gitlink)\n\n    3-bit unused\n\n    9-bit unix permission. Only 0755 and 0644 are valid for regular files.\n    Symbolic links and gitlinks have value 0 in this field.\n</code></pre>\n<p>That file doesn’t mention directories; they are represented using object type 0100. Gitlinks are used for <a href=\"https://github.com/git/git/blob/master/Documentation/gitsubmodules.txt\" rel=\"noreferrer\">submodules</a>.</p>\n<p>Each digit in the six-digit value is in octal, representing three bits; 16 bits thus need six digits, the first of which only represents one bit:</p>\n<pre><code>Type|---|Perm bits\n\n1000 000 111101101\n1 0   0   7  5  5\n\n1000 000 110100100\n1 0   0   6  4  4\n</code></pre>\n<p>Git doesn’t store arbitrary modes, only a subset of the values are allowed, from the usual POSIX types and modes (in octal, 12 for a symbolic link, 10 for a regular file, 04 for a directory) to which git adds 16 for Git links. The mode is appended, using four octal digits. For files, you’ll only ever see 100755 or 100644 (although 100664 is also technically possible); directories are 040000 (permissions are ignored), symbolic links 120000. The set-user-ID, set-group-ID and sticky bits aren’t supported at all (they would be stored in the unused bits).</p>\n<p>See also <a href=\"https://unix.stackexchange.com/a/193468/86440\">this related answer</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/450480/file-permission-with-six-octal-digits-in-git-what-does-it-mean"
  },
  {
    "question_title": "Storing username and password in Git",
    "question_body": "<p>When I do </p>\n\n<pre><code>git push\n</code></pre>\n\n<p>I get the command prompt like </p>\n\n<pre><code>Username for 'https://github.com':\n</code></pre>\n\n<p>then I enter my username manually like</p>\n\n<pre><code>Username for 'https://github.com': myusername\n</code></pre>\n\n<p>and then I hit <kbd>Enter</kbd> and I get prompt for my password</p>\n\n<pre><code>Password for 'https://myusername@github.com':\n</code></pre>\n\n<p>I want the username to be written automatically instead of manually having to type it all the time.</p>\n\n<p>I tried doing it with <code>xdotool</code> but it didn't work out.</p>\n\n<p>I have already done</p>\n\n<pre><code>git config --global user.name myusername\ngit config --global user.email myemail@gmail.com\n</code></pre>\n\n<p>but still it always asks for me to type manually</p>\n",
    "answer": "<p>In Terminal, enter the following to enable credential memory:</p>\n\n<pre><code>$ git config --global credential.helper cache\n</code></pre>\n\n<p>You may update the default password cache timeout (in seconds):</p>\n\n<pre><code># This cache timeout is in seconds\n$ git config --global credential.helper 'cache --timeout=3600' \n</code></pre>\n\n<p>You may also use (but please use the <strong>single</strong> quotes, else double quotes may break for some characters):</p>\n\n<pre><code>$ git config --global user.name 'your user name'\n$ git config --global user.password 'your password'\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/379272/storing-username-and-password-in-git"
  },
  {
    "question_title": "Is it possible to clone only part of a git project?",
    "question_body": "<p>I found a collection of slackbuilds, some i need\nthere are on GitHub.\n<a href=\"https://github.com/PhantomX/slackbuilds/\">https://github.com/PhantomX/slackbuilds/</a>\nI don't want to get all git.</p>\n\n<pre><code>git clone https://github.com/PhantomX/slackbuilds.git\n</code></pre>\n\n<p>But only get a slackbuild, for <a href=\"https://github.com/PhantomX/slackbuilds/tree/master/dcmtk\">this one</a>.</p>\n\n<p>How to do this? Is it possible?</p>\n",
    "answer": "<p>You will end up downloading the entire history, so I don't see much benefit in it, but you can checkout specific parts using a &quot;sparse&quot; checkout. Quoting <a href=\"https://stackoverflow.com/a/13738951/2072269\">this Stack Overflow post</a>:</p>\n<blockquote>\n<p>The steps to do a sparse <em>clone</em> are as follows:</p>\n<pre><code>mkdir &lt;repo&gt;\ncd &lt;repo&gt;\ngit init\ngit remote add -f origin &lt;url&gt;\n</code></pre>\n</blockquote>\n<p>I'm going to interrupt here. Since I'm quoting another post, I don't want to edit the quoted parts, but <em>do <strong>not</strong></em> use <code>-f</code> with <code>git remote add</code>. It will do a fetch, which will pull in the entire history. Just add the remote without a fetch:</p>\n<pre><code>git remote add origin &lt;url&gt;\n</code></pre>\n<p>And then do a shallow fetch like described later.</p>\n<blockquote>\n<p>This creates an empty repository with your remote, and fetches all\nobjects but doesn't check them out. Then do:</p>\n<pre><code>git config core.sparseCheckout true\n</code></pre>\n<p>Now you need to define which files/folders you want to actually check\nout. This is done by listing them in <code>.git/info/sparse-checkout</code>, eg:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;some/dir/&quot; &gt;&gt; .git/info/sparse-checkout\necho &quot;another/sub/tree&quot; &gt;&gt; .git/info/sparse-checkout\n</code></pre>\n<p>[...]</p>\n<p>You might want to have a look at the <a href=\"https://jasonkarns.wordpress.com/2011/11/15/subdirectory-checkouts-with-git-sparse-checkout/\" rel=\"nofollow noreferrer\">extended tutorial</a> and you\nshould probably read the official <a href=\"https://schacon.github.io/git/git-read-tree.html#_sparse_checkout\" rel=\"nofollow noreferrer\">documentation for sparse\ncheckout</a>.</p>\n</blockquote>\n<p>You might be better off using <a href=\"https://stackoverflow.com/a/28039894/2072269\">a shallow clone</a>. Instead of a normal <code>git pull</code>, try:</p>\n<pre><code>git pull --depth=1 origin master\n</code></pre>\n<hr />\n<p>I had an occasion to test this again recently, trying to get only the <a href=\"https://github.com/powerline/fonts\" rel=\"nofollow noreferrer\">Ubuntu Mono Powerline fonts</a>. The steps above ended up downloading some 11 MB, where the Ubuntu Fonts themselves are ~900 KB:</p>\n<pre><code>% git pull --depth=1 origin master\nremote: Enumerating objects: 310, done.\nremote: Counting objects: 100% (310/310), done.\nremote: Compressing objects: 100% (236/236), done.\nremote: Total 310 (delta 75), reused 260 (delta 71), pack-reused 0\nReceiving objects: 100% (310/310), 10.40 MiB | 3.25 MiB/s, done.\nResolving deltas: 100% (75/75), done.\nFrom https://github.com/powerline/fonts\n * branch            master     -&gt; FETCH_HEAD\n * [new branch]      master     -&gt; origin/master\n% du -hxd1 .\n11M     ./.git\n824K    ./UbuntuMono\n12M     .\n</code></pre>\n<p>A normal <code>clone</code> took about 20 MB. There's some savings, but not enough.</p>\n<p>Using <a href=\"https://unix.stackexchange.com/a/468182/70524\">the <code>--filter</code> + checkout method in Ciro Santilli's answer</a> really cuts down the size, but as mentioned there, downloads each blob one by one, which is slow:</p>\n<pre><code>% git fetch --depth=1 --filter=blob:none\nremote: Enumerating objects: 52, done.\nremote: Counting objects: 100% (52/52), done.\nremote: Compressing objects: 100% (49/49), done.\nremote: Total 52 (delta 1), reused 35 (delta 1), pack-reused 0\nReceiving objects: 100% (52/52), 14.55 KiB | 1.32 MiB/s, done.\nResolving deltas: 100% (1/1), done.\nFrom https://github.com/powerline/fonts\n * [new branch]      master     -&gt; origin/master\n * [new branch]      terminus   -&gt; origin/terminus\n% git checkout origin/master -- UbuntuMono\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 1.98 KiB | 1.98 MiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 581 bytes | 581.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 121.43 KiB | 609.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 100.66 KiB | 512.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 107.62 KiB | 583.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 112.15 KiB | 791.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 454 bytes | 454.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 468 bytes | 468.00 KiB/s, done.\n% du -hxd1 .\n692K    ./.git\n824K    ./UbuntuMono\n1.5M    .\n</code></pre>\n<hr />\n<p>TL;DR: Use all of <code>--filter</code>, sparse checkout and shallow clone to reduce the total download, or only use sparse checkout + shallow clone if you don't care about the total download and just want that one directory however it may be obtained.</p>\n",
    "url": "https://unix.stackexchange.com/questions/233327/is-it-possible-to-clone-only-part-of-a-git-project"
  },
  {
    "question_title": "git diff displays colors incorrectly",
    "question_body": "<p>In order to get coloured output from all git commands, I set the following:</p>\n\n<pre><code>git config --global color.ui true\n</code></pre>\n\n<p>However, this produces an output like this for <code>git diff</code>, <code>git log</code></p>\n\n<p><img src=\"https://i.sstatic.net/ZWmbt.png\" alt=\"git diff screenshot\"></p>\n\n<p>whereas commands like <code>git status</code> display fine</p>\n\n<p><img src=\"https://i.sstatic.net/iekDY.png\" alt=\"git status screenshot\"></p>\n\n<p>Why is it not recognizing the escaped color codes in only some of the commands and how can I fix it?</p>\n\n<p>I'm using iTerm 2 (terminal type <code>xterm-256color</code>) on OS X 10.8.2 and zsh as my shell</p>\n\n<pre><code>zsh --version\nzsh 5.0.0 (x86_64-apple-darwin12.0.0)\n\ngit --version                                                                                                                      \ngit version 1.7.9.6 (Apple Git-31.1)\n</code></pre>\n",
    "answer": "<p>You're seeing the escape sequences that tell the terminal to change colors displayed with the escape character shown as <code>ESC</code>, whereas the desired behavior would be that the escape sequences have their intended effect.</p>\n\n<p>Commands such as <code>git diff</code> and <code>git log</code> pipe their output into a <a href=\"http://en.wikipedia.org/wiki/Terminal_pager\">pager</a>, <a href=\"http://en.wikipedia.org/wiki/Less_%28Unix%29\"><code>less</code></a> by default. Git tries to tell <code>less</code> to allow control characters to have their control effect, but this isn't working for you.</p>\n\n<p>If <code>less</code> is your pager but you have the environment variable <code>LESS</code> set to a value that doesn't include <code>-r</code> or <code>-R</code>, git is unable to tell <code>less</code> to display colors. It normally passes <code>LESS=-FRSX</code>, but not if <code>LESS</code> is already set in the environment. A fix is to explicitly pass the <code>-R</code> option to tell <code>less</code> to display colors when invoked by git:</p>\n\n<pre><code>git config --global core.pager 'less -R'\n</code></pre>\n\n<p>If <code>less</code> isn't your pager, either switch to <code>less</code> or figure out how to make your pager display colors.</p>\n\n<p>If you don't want git to display colors when it's invoking a pager, set <code>color.ui</code> to <code>auto</code> instead of <code>true</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/64927/git-diff-displays-colors-incorrectly"
  },
  {
    "question_title": "How did `git pull` eat my homework?",
    "question_body": "<p>I feel like a kid in the principal's office explaining that the dog ate my homework the night before it was due, but I'm staring some crazy data loss bug in the face and I can't figure out how it happened. I would like to know how git could eat my repository whole! I've put git through the wringer many times and it's never blinked. I've used it to split a 20 Gig Subversion repo into 27 git repos and filter-branched the foo out of them to untangle the mess and it's never lost a byte on me. The reflog is always there to fall back on. This time the carpet is gone!</p>\n\n<p>From my perspective, all I did is run <code>git pull</code> and it nuked my entire local repository. I don't mean it \"messed up the checked out version\" or \"the branch I was on\" or anything like that. I mean <em>the entire thing is gone</em>.</p>\n\n<p>Here is a screen-shot of my terminal at the incident:</p>\n\n<p><img src=\"https://i.sstatic.net/KZEiL.png\" alt=\"incident screen shot\"></p>\n\n<p>Let me walk you through that. My command prompt includes data about the current git repo (using prezto's vcs_info implementation) so you can see when the git repo disappeared. The first command is normal enough:</p>\n\n<pre><code>  » caleb » jaguar » ~/p/w/incil.info » ◼  zend ★ »\n❯❯❯ git co master\nSwitched to branch 'master'\nYour branch is up-to-date with 'origin/master'.\n</code></pre>\n\n<p>There you can see I was on the 'zend' branch, and checked out master. So far so good. You'll see in the prompt before my next command that it successfully switched branches:</p>\n\n<pre><code>  » caleb » jaguar » ~/p/w/incil.info » ◼  master ★ »\n❯❯❯ git pull\nremote: Counting objects: 37, done.\nremote: Compressing objects: 100% (37/37), done.\nremote: Total 37 (delta 25), reused 0 (delta 0)\nUnpacking objects: 100% (37/37), done.\nFrom gitlab.alerque.com:ipk/incil.info\n + 7412a21...eca4d26 master     -&gt; origin/master  (forced update)\n   f03fa5d..c8ea00b  devel      -&gt; origin/devel\n + 2af282c...009b8ec verse-spinner -&gt; origin/verse-spinner  (forced update)\nFirst, rewinding head to replay your work on top of it...\n&gt;&gt;&gt; elapsed time 11s\n</code></pre>\n\n<p>And just like that it's gone. The elapsed time marker outputs before the next prompt if more than 10 seconds have elapsed. Git did not give any output beyond the notice that it was rewinding to replay. No indication that it finished.</p>\n\n<p>The next prompt includes no data about what branch we are on or the state of git.</p>\n\n<p>Not noticing it had failed I obliviously tried to run another git command only to be told I wasn't in a git repo. Note the PWD has not changed:</p>\n\n<pre><code>  » caleb » jaguar » ~/p/w/incil.info »\n❯❯❯ git fetch --all\nfatal: Not a git repository (or any parent up to mount point /home)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n</code></pre>\n\n<p>After this a look around showed that I was in a completely empty directory. Nothing. No '.git' directory, nothing. Empty.</p>\n\n<p>My local git is at version 2.0.2. Here are a couple tidbits from my git config that might be relevant to making out what happened:</p>\n\n<pre><code>[branch]\n        autosetuprebase = always\n        rebase = preserve\n[pull]\n        rebase = true\n[rebase]\n        autosquash = true\n        autostash = true\n[alias]\n        co = checkout\n</code></pre>\n\n<p>For example I have <code>git pull</code> set to always do a rebase instead of a merge, so that part of the output above is normal.</p>\n\n<p>I can recover the data. I don't think there were any git objects other than some unimportant stashes that hadn't been pushed to other repos, but <strong>I'd like to know what happened</strong>.</p>\n\n<p>I have checked for:</p>\n\n<ul>\n<li>Messages in dmesg or the systemd journal. Nothing even remotely relevant.</li>\n<li>There is no indication of drive or file system failure (LVM + LUKS + EXT4 all look normal). There is nothing in lost+found.</li>\n<li>I didn't run anything else. There is nothing in the history I'm not showing above, and no other terminals were used during this time. There are no <code>rm</code> commands floating around that might have executed in the wrong CWD, etc.</li>\n<li>Poking at another git repo in another directory shows no apparent abnormality executing <code>git pull</code>s.</li>\n</ul>\n\n<p>What else should I be looking for here?</p>\n",
    "answer": "<p>Yes, <code>git</code> ate my homework. All of it.</p>\n\n<p>I made a <code>dd</code> image of this disk after the incident and messed around with it later. Reconstructing the series of events from system logs, I deduce what happened was something like this:</p>\n\n<ol>\n<li>A system update command (<code>pacman -Syu</code>) had been issued days before this incident.</li>\n<li>An extended network outage meant that it was left re-trying to download packages. Frustrated at the lack of internet, I'd put the system to sleep and gone to bed.</li>\n<li>Days later the system was woken up and it started finding and downloading packages again.</li>\n<li>Package download finished sometime just before I happened to be messing around with this repository.</li>\n<li>The system <em>glibc</em> installation got updated after the <code>git checkout</code> and before the <code>git pull</code>.</li>\n<li>The <code>git</code> binary got replaced after the <code>git pull</code> started and before it finished.</li>\n<li>And on the seventh day, <code>git</code> rested from all its labors. And deleted the world so everybody else had to rest too.</li>\n</ol>\n\n<p>I don't know <em>exactly</em> what race condition occurred that made this happen, but swapping out binaries in the middle of an operation is certainly not nice nor a testable / repeatable condition. Usually a copy of a running binary is stored in memory, but <code>git</code> is weird and something about the way it re-spawns versions of itself I'm sure led to this mess. Obviously it should have died rather than destroying everything, but that's what happened.</p>\n",
    "url": "https://unix.stackexchange.com/questions/146299/how-did-git-pull-eat-my-homework"
  },
  {
    "question_title": "Tar a folder without .git files?",
    "question_body": "<p>If I tar a folder that is a git repository, can I do so without including the <code>.git</code> related files? If not, how would I go about doing that via a command?</p>\n",
    "answer": "<p>Simplest answer: Add <code>--exclude-vcs</code>. This excludes all version control system directories</p>\n\n<p>Personally I use</p>\n\n<pre><code>tar --exclude-vcs -zcvf foo.tar.gz ./FOLDER_NAME\n</code></pre>\n\n<p>so all you need to do is add the <code>--exclude-vcs</code> at the end of the command.</p>\n",
    "url": "https://unix.stackexchange.com/questions/18657/tar-a-folder-without-git-files"
  },
  {
    "question_title": "How to fix &quot;Hunk #1 FAILED at 1 (different line endings)&quot; message?",
    "question_body": "<p>I am trying to create a patch with the command</p>\n\n<pre><code>git diff sourcefile &gt;/var/lib/laymab/overlay/category/ebuild/files/thepatch.patch\n</code></pre>\n\n<p>when I apply the patch, it gives me </p>\n\n<pre><code>$ patch -v\nGNU patch 2.7.5\n\n$ /usr/bin/patch -p1 &lt;/var/lib/laymab/overlay/category/ebuild/files/thepatch.patch\npatching file sourcefile\nHunk #1 FAILED at 1 (different line endings).\nHunk #2 FAILED at 23 (different line endings).\nHunk #3 FAILED at 47 (different line endings).\nHunk #4 FAILED at 65 (different line endings).\nHunk #5 FAILED at 361 (different line endings).\n5 out of 5 hunks FAILED -- saving rejects to file sourcefile.rej\n</code></pre>\n\n<p>I tried to apply dos2unix to both src file and patch file, but the message don't gone...</p>\n\n<p>UPD: --ignore-whitespace doesn't help too</p>\n\n<pre><code>PATCH COMMAND:  patch -p1 -g0 -E --no-backup-if-mismatch --ignore-whitespace --dry-run -f &lt; '/var/lib/layman/dotnet/dev-dotnet/slntools/files/remove-wix-project-from-sln-file-v2.patch'\n\n=====================================================\nchecking file Main/SLNTools.sln\nHunk #1 FAILED at 14 (different line endings).\nHunk #2 FAILED at 49 (different line endings).\nHunk #3 FAILED at 69 (different line endings).\nHunk #4 FAILED at 102 (different line endings).\n4 out of 4 hunks FAILED\n</code></pre>\n\n<p>UPD: found a very good article: <a href=\"https://stackoverflow.com/a/4425433/1709408\">https://stackoverflow.com/a/4425433/1709408</a></p>\n",
    "answer": "<p>I had the same problem using the <code>patch</code>command that comes with MSYS2 on Windows. In my case both the source file and the patch had CRLF line-ending, and converting both to LF didn't work either. What worked was the following: </p>\n\n<pre><code>$ dos2unix patch-file.patch\n$ patch -p1 &lt; patch-file.patch\n$ unix2dos modified-files...\n</code></pre>\n\n<p><code>patch</code> will convert the line-endings to LF on all the patched files, so it's necessary to convert them back to CRLF.</p>\n\n<p>Obs: the <code>patch</code> version I'm using is 2.7.5</p>\n",
    "url": "https://unix.stackexchange.com/questions/239364/how-to-fix-hunk-1-failed-at-1-different-line-endings-message"
  },
  {
    "question_title": "How to exit a git merge asking for commit message?",
    "question_body": "<p>I'm using git. I did a normal merge, but it keeps asking this:</p>\n\n<pre><code># Please enter a commit message to explain why this merge is necessary,\n# especially if it merges an updated upstream into a topic branch.\n</code></pre>\n\n<p>And even if I write something, I can't exit from here. I can't find docs explaining this. How should I do?</p>\n",
    "answer": "<p>This is depend on the editor you're using.</p>\n\n<p>If <a href=\"/questions/tagged/vim\" class=\"post-tag\" title=\"show questions tagged &#39;vim&#39;\" rel=\"tag\">vim</a> you can use <kbd>ESC</kbd> and <kbd>:wq</kbd> or <kbd>ESC</kbd> and <kbd>Shift</kbd>+<kbd>zz</kbd>. Both command save file and exit.</p>\n\n<p>You also can check <code>~/.gitconfig</code> for editor, in my case (<code>cat ~/.gitconfig</code>):</p>\n\n<pre><code>[user]\n    name = somename\n    email = somemail@gmail.com\n[core]\n    editor = vim\n    excludesfile = /home/mypath/.gitignore_global\n[color]\n  ui = auto\n  # other settings here\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/181280/how-to-exit-a-git-merge-asking-for-commit-message"
  },
  {
    "question_title": "gitk crashes when viewing commit containing emoji: X Error of failed request: BadLength (poly request too large or internal Xlib length error)",
    "question_body": "<p>I'm able to open <code>gitk</code> but it crashes as soon as I open a commit whom changes contains an emoji (not the commit message).</p>\n<h3>Error</h3>\n<pre><code>❯ gitk --all\nX Error of failed request:  BadLength (poly request too large or internal Xlib length error)\n  Major opcode of failed request:  139 (RENDER)\n  Minor opcode of failed request:  20 (RenderAddGlyphs)\n  Serial number of failed request:  6687\n  Current serial number in output stream:  6706\n</code></pre>\n<h3>Env</h3>\n<pre><code>❯ cat /etc/os-release --plain\nNAME=&quot;Linux Mint&quot;\nVERSION=&quot;20 (Ulyana)&quot;\nID=linuxmint\nID_LIKE=ubuntu\nPRETTY_NAME=&quot;Linux Mint 20&quot;\nVERSION_ID=&quot;20&quot;\nHOME_URL=&quot;https://www.linuxmint.com/&quot;\nSUPPORT_URL=&quot;https://forums.linuxmint.com/&quot;\nBUG_REPORT_URL=&quot;http://linuxmint-troubleshooting-guide.readthedocs.io/en/latest/&quot;\nPRIVACY_POLICY_URL=&quot;https://www.linuxmint.com/&quot;\nVERSION_CODENAME=ulyana\nUBUNTU_CODENAME=focal\n</code></pre>\n<h3>Git</h3>\n<p>❯ git --version\ngit version 2.25.1</p>\n<h3>Examples</h3>\n<blockquote>\n<p>6e05ecd add v3-&gt;v4 migration script to update variables\n<a href=\"https://github.com/rafaelrinaldi/pure/pull/271/commits/6e05ecdad0e4f623050e154e16c0af0315767940\" rel=\"noreferrer\">https://github.com/rafaelrinaldi/pure/pull/271/commits/6e05ecdad0e4f623050e154e16c0af0315767940</a></p>\n</blockquote>\n<h3>Questions</h3>\n<p>I tried various things:</p>\n<ul>\n<li>removing <code>~/.Xresources</code> config related to fonts</li>\n<li>editing then removing <code>~/.config/fontconfig/conf.d/30-icons.conf</code></li>\n</ul>\n<p>Without success, most of the <a href=\"https://github.com/LukeSmithxyz/st/issues/130\" rel=\"noreferrer\">issues</a> I found were related to <a href=\"https://github.com/lukesmithxyz/st\" rel=\"noreferrer\"><code>st</code> terminal</a>. However, I'm not using it but <code>guake</code>, and the issue also happened with <code>yakuake</code>, <code>gnome-terminal</code> and <code>hyper</code></p>\n<p>How could I fix this?</p>\n",
    "answer": "<p>As suggested in <a href=\"https://bugs.launchpad.net/ubuntu/+source/git/+bug/1852985/comments/11\" rel=\"noreferrer\">https://bugs.launchpad.net/ubuntu/+source/git/+bug/1852985/comments/11</a>, I am able to stop gitk from crashing by installing unifont:</p>\n<pre><code>sudo apt install unifont\n</code></pre>\n<p>Like in the comment, the emoji is displayed in gitk as an empty square, but gitk no longer crashes.</p>\n",
    "url": "https://unix.stackexchange.com/questions/629281/gitk-crashes-when-viewing-commit-containing-emoji-x-error-of-failed-request-ba"
  },
  {
    "question_title": "Installing git &quot;sudo: apt-get: command not found&quot;",
    "question_body": "<p>I am trying to install git. I run the following command: </p>\n\n<p><code>sudo apt-get install git-core git-gui git-doc</code> </p>\n\n<p>But receive the following error: \n<code>sudo: apt-get: command not found</code></p>\n\n<p>What should I do?</p>\n",
    "answer": "<p>Since you're using CentOS 5, the default <em>package manager</em> is <code>yum</code>, not <code>apt-get</code>. To install a program using it, you'd normally use the following command:</p>\n\n<pre><code>$ sudo yum install &lt;packagename&gt;\n</code></pre>\n\n<p>However, when trying to install git this way, you'll encounter the following error on CentOS 5:</p>\n\n<pre><code>$ sudo yum install git\nSetting up Install Process\nParsing package install arguments\nNo package git available.\nNothing to do\n</code></pre>\n\n<p>This tells you that the package repositories that <code>yum</code> knows about don't contain the required rpms (RPM Package Manager files) to install <code>git</code>. This is presumably because CentOS 5 is based on RHEL 5, which was released in 2007, before <code>git</code> was considered a mature version control system. To get around this problem, we need to add additional repositories to the list that <code>yum</code> uses (We're going to add the RPMforge repository, as per <a href=\"http://wiki.centos.org/AdditionalResources/Repositories/RPMForge?action=show&amp;redirect=Repositories%2FRPMForge#head-b06dd43af4eb366c28879a551701b1b5e4aefccd\">these instructions</a>).</p>\n\n<p><strong>This assumes you want the i386 packages. Test by running <code>uname -i</code>. If you want the x86_64 packages, replace all occurrences of i386 with x86_64 in the following commands</strong></p>\n\n<p>First, download the <code>rpmforge-release</code> package:</p>\n\n<pre><code>$ wget http://packages.sw.be/rpmforge-release/rpmforge-release-0.5.3-1.el5.rf.i386.rpm\n</code></pre>\n\n<p>Next, verify and install the package:</p>\n\n<pre><code>$ sudo rpm --import http://apt.sw.be/RPM-GPG-KEY.dag.txt\n$ rpm -K rpmforge-release-0.5.3-1.el5.rf.i386.rpm\n$ sudo rpm -i rpmforge-release-0.5.3-1.el5.rf.i386.rpm\n</code></pre>\n\n<p>And now we should be able to install <code>git</code>:</p>\n\n<pre><code>$ sudo yum install git-gui\n</code></pre>\n\n<p><code>yum</code> will work out the dependencies, and ask you at relevant points if you want to proceed. Press <kbd>y</kbd> for Yes, and <kbd>n</kbd> or <kbd>return</kbd> for No.</p>\n",
    "url": "https://unix.stackexchange.com/questions/33688/installing-git-sudo-apt-get-command-not-found"
  },
  {
    "question_title": "Are there pitfalls to putting $HOME in git instead of symlinking dotfiles?",
    "question_body": "<p>I have for many years had my entire <code>$HOME</code> directory checked into subversion. This has included all my dotfiles and application profiles, many scripts, tools and hacks, my preferred basic home directory structure, not a few oddball projects and a warehouse worth of random data. This was a good thing. While it lasted.</p>\n\n<p>But it's gotten out of hand. The basic checkout is the same across dozens of systems, but not all that stuff is appropriate for all my machines. It doesn't even all play nicely with different distros.</p>\n\n<p>I'm in the process of cleaning house -- separating the data out where it belongs, splitting out some scripts as separate projects, fixing some broken links in stuff that should be automated, etc.</p>\n\n<p>My intent is to replace <code>subversion</code> with <code>git</code> for the toplevel checkout of <code>$HOME</code>, but I'd like to pare this down to just the things I'd like to have on ALL my systems, meaning dotfiles, a few directories and some basic custom scripts.</p>\n\n<p><strong>In reading up online a lot of people seem to be doing this using the symlink approach: clone into a subdirectory then create symlinks from <code>$HOME</code> into the repository.</strong> Having had my <code>$HOME</code> under full version control for over a decade, I don't like the idea of this approach and I can't figure out why people seem so averse to the straight checkout method. <strong>Are there pitfalls I need to know about specific to <code>git</code> as a top level checkout for <code>$HOME</code>?</strong></p>\n\n<p><sub>P.S. Partly as an exercise in good coding, I'm also planning on making my root checkout public on GitHub. It's scary how much security sensitive information I've allowed to collect in files that ought to be sharable without a second thought! WiFi password, un-passphrased RSA keys, etc. Eeek!</sub></p>\n",
    "answer": "<p><strong>Yes</strong>, there is at least one major pitfall when considering <code>git</code> to manage a home directory that is not a concern with <code>subversion</code>.</p>\n\n<p><em>Git is both greedy and recursive by default</em>.</p>\n\n<p>Subversion will naively ignore anything it doesn't know about and it stops processing folders either up or down from your checkout when it reaches one that it doesn't know about (or that belongs to a different repository). Git, on the other hand, keeps recursing into all child directories making nested checkouts very complicated due to namespace issues. Since your home directory is likely also the place where you checkout and work on various other git repositories, having your home directory in git is almost certainly going to make your life an impossible mess.</p>\n\n<p>As it turns out, this is the main reason people checkout their dotfiles into an isolated folder and then symlink into it. It keeps git out of the way when doing anything else in any child directory of your <code>$HOME</code>. While this is purely a matter of preference if checking your home into subversion, it becomes a matter of necessity if using git.</p>\n\n<p><strong>However</strong>, there is an alternate solution. Git allows for something called a \"fake root\" where all the repository machinery is hidden in an alternate folder that can be physically separated from the checkout working directory. The result is that the git toolkit won't get confused: it won't even SEE your repository, only the working copy. By setting a couple environment variables you can tip off git where to find the goods for those moments when you are managing your home directory. Without the environment variables set nobody is the wiser and your home looks like it's classic file-y self.</p>\n\n<p>To make this trick flow a little smoother, there are some great tools out there. The <a href=\"http://vcs-home.branchable.com/\">vcs-home mailing list</a> seems like the defacto place to start, and the about page has a convenient wrap up of howtos and people's experiences. Along the way are some nifty little tools like <a href=\"https://github.com/RichiH/vcsh\">vcsh</a>, <a href=\"http://joeyh.name/code/mr/\">mr</a>. If you want to keep your home directory directly in git, vcsh is almost a must have tool. If you end up splitting your home directory into several repostories behind the scenes, combine <code>vcsh</code> with <code>mr</code> for quick and not very dirty way to manage it all at once.</p>\n",
    "url": "https://unix.stackexchange.com/questions/46538/are-there-pitfalls-to-putting-home-in-git-instead-of-symlinking-dotfiles"
  },
  {
    "question_title": "Why is less being run unnecessarily by git?",
    "question_body": "<p>When I run <code>git branch</code> (from bash or csh), it automagically pipes the output through <code>less</code>. However, with only a few branches in the repository this is beyond unnecessary, it is annoying, as the branch listing disappears once I quit less. </p>\n\n<p>Checking <code>~/.gitconfig</code> file and the local <code>.git/config</code> files finds nothing about a pager or any thing else that would cause this. Otherwise, nothing I've found in web searches has been helpful or promising.</p>\n\n<p>Why is this happening, and what (if anything) can I do to make <code>less</code> run when needed (e.g. when doing a <code>git log</code> when there's a lot of history) but not otherwise (like a <code>git branch</code> with only 2 or 3 branches)?</p>\n",
    "answer": "<p>You can set the following:</p>\n\n<pre><code>git config --global core.pager 'less -FRX'\n</code></pre>\n\n<p>This will ensure that <code>less</code> will</p>\n\n<ul>\n<li>Exit if the entire file can be displayed on the first screen (<code>F</code>)</li>\n<li>Output the raw control characters for terminal formatting (<code>R</code>)</li>\n<li><s>Chop long lines (<code>S</code>)</s></li>\n<li>Don't send the init/de-init strings to the terminal - avoids clearing the screen on exit (<code>X</code>)</li>\n</ul>\n\n<p>Edit: Removed the <code>S</code> option based on Peter A. Scheider's comment</p>\n",
    "url": "https://unix.stackexchange.com/questions/536727/why-is-less-being-run-unnecessarily-by-git"
  },
  {
    "question_title": "How to remove a file from the git index",
    "question_body": "<p>How do I remove a file from a git repositorie's <strong>index</strong> without removing the file from the working tree?</p>\n\n<p>If I had a file <code>./notes.txt</code> that was being tracked by git, I could run <code>git rm notes.txt</code>. But that would remove the file. I'd rather want git just to stop tracking the file.</p>\n",
    "answer": "<p>You could just use <code>git rm --cached notes.txt</code>. This will keep the file but remove it from the index.</p>\n",
    "url": "https://unix.stackexchange.com/questions/1818/how-to-remove-a-file-from-the-git-index"
  },
  {
    "question_title": "What is the .gitignore pattern equivalent of the regex (Big|Small)(State|City)-[0-9]*\\.csv",
    "question_body": "<p>I have a regex I stuck in my <code>.gitignore</code> similar to:</p>\n\n<pre><code>(Big|Small)(State|City)-[0-9]*\\.csv\n</code></pre>\n\n<p>It didn't work so I tested it against <a href=\"http://regexlab.codeplex.com/\">RegexLab.NET</a>. </p>\n\n<p>I then found the <a href=\"http://linux.die.net/man/5/gitignore\">gitignore</a> man page which led me to learn that gitignore doesn't use regexes, but rather <a href=\"http://linux.die.net/man/3/fnmatch\">fnmatch(3)</a>. </p>\n\n<p>However, fnmatch it doesn't seem to have an equivalent of the capture groups. Is this feasible or do I need to break this into three lines?</p>\n",
    "answer": "<p>There's no way to express this regular expression with the patterns that gitignore supports. The problem is not the lack of capture groups (in fact, you are not using capture groups as such), the problem is the lack of a <code>|</code> operator. You need to break this into four lines.</p>\n\n<pre><code>BigState-[0-9]*.csv\nSmallState-[0-9]*.csv\nBigCity-[0-9]*.csv\nSmallCity-[0-9]*.csv\n</code></pre>\n\n<p>Note that the patterns match e.g. <code>BigState-4foo.csv</code>, since <code>*</code> matches any sequence of characters. You can't do better than this with glob patterns, unless you're willing to match only a fixed number of digits.</p>\n",
    "url": "https://unix.stackexchange.com/questions/31790/what-is-the-gitignore-pattern-equivalent-of-the-regex-bigsmallstatecity"
  },
  {
    "question_title": "GPG Hangs When Private Keys are Accessed",
    "question_body": "<p>I like to sign my git commits with my PGP key, so I was quite alarmed when I went to <code>git commit -S</code> but instead of prompting for my PGP key passphrase, git just started hanging. I haven't made a change to my GPG setup in several months and have made many commits since then with no problem. Additionally, when I attempt to view my private keys with <code>gpg -K</code>, gpg hangs. However, when I run <code>gpg -k</code> to view my public keys, it returns the list like normal. Hopefully someone will have some idea of what is causing this problem and how to fix it.</p>\n",
    "answer": "<p>I came across this exact issue (OSX Sierra 10.12.6, gpg/GnuPG 2.2.5)</p>\n\n<p>Commands that would hang:</p>\n\n<pre><code>gpg -K # --list-secret-keys\ngpg -d # --decrypt\ngpg --edit-key\ngpgconf --kill gpg-agent\n</code></pre>\n\n<p>My solution was the same as mentioned by <a href=\"https://unix.stackexchange.com/users/177537/john-leuenhagen\">John</a> above (ie. kill gpg-agent) as most other methods on <a href=\"https://superuser.com/questions/1075404/how-can-i-restart-gpg-agent\">how-can-i-restart-gpg-agent</a> would also hang.</p>\n\n<pre><code># Solution    \npkill -9 gpg-agent\n</code></pre>\n\n<p>Then for signing git commits I set the tty env as mentioned by <a href=\"https://unix.stackexchange.com/users/7696/cas\">cas</a> above and also at <a href=\"https://stackoverflow.com/questions/39494631/gpg-failed-to-sign-the-data-fatal-failed-to-write-commit-object-git-2-10-0/42265848#42265848\">gpg-failed-to-sign-commit-object</a>.</p>\n\n<pre><code>export GPG_TTY=$(tty)\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/382279/gpg-hangs-when-private-keys-are-accessed"
  },
  {
    "question_title": "&quot;X11 forwarding request failed&quot; when connecting to github.com",
    "question_body": "<p>I'm getting a bizarre error message while using git:</p>\n\n<pre><code>$ git clone git@github.com:Itseez/opencv.git\nCloning into 'opencv'\nWarning: Permanently added the RSA host key for IP address '192.30.252.128' to the list of known hosts.\nX11 forwarding request failed on channel 0\n(...)\n</code></pre>\n\n<p>I was under the impression that X11 wasn't required for git, so this seemed strange. This clone worked successfully, so this is more of a \"warning\" issue than an \"error\" issue, but it seem unsettling. After all, git shouldn't <em>need</em> X11. Any suggestions?</p>\n",
    "answer": "<p>Note that to disable <code>ForwardX11</code> just for github.com you need something like the following in your <code>~/.ssh/config</code></p>\n\n<pre><code>Host github.com\n    ForwardX11 no\n\nHost *\n    ForwardX11 yes\n</code></pre>\n\n<p>The last two lines assume that in general you /do/ want to forward your X connection. This can cause confusion because the following is WRONG:</p>\n\n<pre><code>ForwardX11 yes\n\nHost github.com\n    ForwardX11 no\n</code></pre>\n\n<p>Which is what I had (and caused me no end of confusion). This is because in .ssh/config, the first setting wins, and isn't overwritten by subsequent customizations.</p>\n\n<p>HTH, Dan.</p>\n",
    "url": "https://unix.stackexchange.com/questions/240013/x11-forwarding-request-failed-when-connecting-to-github-com"
  },
  {
    "question_title": "how to set up username and passwords for different git repos?",
    "question_body": "<pre><code>─[$] cat ~/.gitconfig\n\n[user]\n    name = Shirish Agarwal\n    email = xxxx@xxxx.xxx\n[core]\n    editor = leafpad\n    excludesfiles = /home/shirish/.gitignore\n    gitproxy = \\\"ssh\\\" for gitorious.org\n[merge]\n    tool = meld\n[push]\n    default = simple\n[color]\n    ui = true\n    status = auto\n    branch = auto\n</code></pre>\n\n<p>Now I want to put my git credentials for github, gitlab and gitorious so each time I do not have to lookup the credentials on the browser. How can this be done so it's automated ?</p>\n\n<p>I am running zsh </p>\n",
    "answer": "<h1>Using SSH</h1>\n<p>The common approach for handling git authentication is to delegate it to SSH. Typically you set your SSH public key in the remote repository (<em>e.g.</em> <a href=\"https://github.com/settings/keys\" rel=\"nofollow noreferrer\">on GitHub</a>), and then you use that whenever you need to authenticate. You can use a key agent of course, either handled by your desktop environment or manually with <code>ssh-agent</code> and <code>ssh-add</code>.</p>\n<p>To avoid having to specify the username, you can configure that in SSH too, in <code>~/.ssh/config</code>; for example I have</p>\n<pre><code>Host git.opendaylight.org\n  User skitt\n</code></pre>\n<p>and then I can clone using</p>\n<pre><code>git clone ssh://git.opendaylight.org:29418/aaa\n</code></pre>\n<p>(note the absence of a username there).</p>\n<h1>Using <code>gitcredentials</code></h1>\n<p>If the SSH approach doesn't apply (<em>e.g.</em> you're using a repository accessed over HTTPS), git does have its own way of handling credentials, using <a href=\"https://git-scm.com/docs/gitcredentials\" rel=\"nofollow noreferrer\"><code>gitcredentials</code></a> (and typically <a href=\"https://git-scm.com/docs/git-credential-store\" rel=\"nofollow noreferrer\"><code>git-credential-store</code></a>). You specify your username using</p>\n<pre><code>git config credential.${remote}.username yourusername\n</code></pre>\n<p>and the credential helper using</p>\n<pre><code>git config credential.helper store\n</code></pre>\n<p>(specify <code>--global</code> if you want to use this setup everywhere).</p>\n<p>Then the first time you access a repository, git will ask for your password, and it will be stored (by default in <code>~/.git-credentials</code>). Subsequent accesses to the repository will use the stored password instead of asking you.</p>\n<p><em><strong>Warning</strong></em>: This does store your credentials plaintext in your home directory. So it is inadvisable unless you understand what this means and are happy with the risk.</p>\n",
    "url": "https://unix.stackexchange.com/questions/335704/how-to-set-up-username-and-passwords-for-different-git-repos"
  },
  {
    "question_title": "Have tree hide gitignored files",
    "question_body": "<p>Is there a way to make <code>tree</code> not show files that are ignored in <code>.gitignore</code>?</p>\n",
    "answer": "<p>Another way is possible if you're using <a href=\"http://manpages.ubuntu.com/manpages/impish/man1/tree.1.html\" rel=\"noreferrer\">tree 1.8.0</a>\nsince it supports the <code>--fromfile</code> flag:</p>\n<blockquote>\n<p><strong><code>--fromfile</code></strong>  <ul>Reads a directory listing from a file\nrather than the file-system. \nPaths provided on the command\nline are files to read from rather than directories to search. \nThe dot (<strong><code>.</code></strong>)  directory  indicates  that  tree\nshould read paths from standard input.</ul></p>\n</blockquote>\n<p>We could use <code>git ls-tree</code> to get all non-git-ignored files in a project, and pipe the output to <code>tree</code>.</p>\n<p>Assuming we have a git repository, where <code>ignored</code> files are ignored in <code>.gitignore</code>:</p>\n<pre><code>git_repo\n├── .gitignore\n├── bar\n│   ├── b.txt\n│   └── ignored\n├── foo\n│   ├── a.txt\n│   └── ignored\n└── ignored\n</code></pre>\n<p>The following command:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git ls-tree -r --name-only HEAD | tree --fromfile\n</code></pre>\n<p>Gives:</p>\n<pre><code>.\n├── .gitignore\n├── bar\n│   └── b.txt\n└── foo\n    └── a.txt\n\n2 directories, 3 files\n</code></pre>\n<p>Or if you need a specific path:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>git ls-tree -r --name-only HEAD foo | tree --fromfile\n</code></pre>\n<p>Gives:</p>\n<pre><code>.\n└── a.txt\n\n0 directories, 1 file\n</code></pre>\n<h2>Caveats</h2>\n<ul>\n<li><strong>Beware</strong> that changes such as deleted or renamed files that haven't already been committed can cause <code>git ls-tree</code> to appear out of sync.</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/291282/have-tree-hide-gitignored-files"
  },
  {
    "question_title": "How to keep track of changes in /etc/",
    "question_body": "<p>I would like to keep track of changes in /etc/</p>\n\n<p>Basically I'd like to know if a file was changed, by <code>yum update</code> or by a user and roll it back if I don't like the chage.\nI thought of using a VCS like git, LVM or btrfs snapshots or a backup program for this. </p>\n\n<p>What would you recommend?</p>\n",
    "answer": "<p>It sounds like you want <a href=\"http://etckeeper.branchable.com/\" rel=\"noreferrer\">etckeeper</a> from Joey Hess of Debian, which manages files under <code>/etc</code> using version control.  It supports git, mercurial, darcs and bazaar.</p>\n\n<blockquote>\n  <p>git is the VCS best supported by etckeeper and the VCS users are most likely to know.  It's possible that your distribution has chosen to modify etckeeper so its default VCS is not git. You should only be using etckeeper with a VCS other than git if you're in love with the other VCS.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/29117/how-to-keep-track-of-changes-in-etc"
  },
  {
    "question_title": "How to find the first tag that contains a git commit?",
    "question_body": "<p>I have the SHA ID of a commit that I am interested in and would like to know how to find the first tag that contains it.</p>\n",
    "answer": "<p>As previously stated, this can be done with <code>git describe</code>.  In your particular case, however, you may find it more convenient to run  <code>git name-rev --tags --name-only &lt;SHA&gt;</code>, which outputs exactly what you want.  See <a href=\"https://www.kernel.org/pub/software/scm/git/docs/git-name-rev.html\" rel=\"noreferrer\">git-name-rev(1)</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/47659/how-to-find-the-first-tag-that-contains-a-git-commit"
  },
  {
    "question_title": "Given a git commit hash, how to find out which kernel release contains it?",
    "question_body": "<p>Assume I have some issue that was fixed by a recent patch to the official Linux git repository. I have a work around, but I’d like to undo it when a release happens that contains my the fix. I know the exact git commit hash, e.g. <a href=\"http://repo.or.cz/w/linux-2.6.git/commit/f3a1ef9cee4812e2d08c855eb373f0d83433e34c\">f3a1ef9cee4812e2d08c855eb373f0d83433e34c</a>.</p>\n\n<p>What is the easiest way to answer the question: What kernel releases so far contain this patch? Bonus points if no local Linux git repository is needed.</p>\n\n<p>(<a href=\"http://lwn.net/Articles/392293/\">LWM</a> discusses some ideas, but these do require a local repository.)</p>\n",
    "answer": "<p>As <a href=\"https://lwn.net/Articles/392293/\" rel=\"noreferrer\">mentioned on LWN</a>, the easiest is:</p>\n\n<pre><code>git describe --contains f3a1ef9cee4812e2d08c855eb373f0d83433e34c\n</code></pre>\n\n<p>If you don't want a local clone, gitweb's \"plain\" formatted commit contains the same info in the <code>X-Git-Tag</code> header. <em>Unfortunately kernel.org switched over to cgit which apparently does not disclose this information.</em> Previously it was possible to find it out like this:</p>\n\n<p><a href=\"http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff_plain;h=f3a1ef9cee4812e2d08c855eb373f0d83433e34c\" rel=\"noreferrer\">http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff_plain;h=f3a1ef9cee4812e2d08c855eb373f0d83433e34c</a></p>\n\n<p>Here, <code>X-Git-Tag</code> is actually missing at the moment because that commit isn't in a tagged release in that repository.  But you can look at an earlier commit, like:</p>\n\n<p><a href=\"http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff_plain;h=dc0827c128c0ee5a58b822b99d662b59f4b8e970\" rel=\"noreferrer\">http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff_plain;h=dc0827c128c0ee5a58b822b99d662b59f4b8e970</a></p>\n\n<p>Here, you see:</p>\n\n<pre><code>X-Git-Tag: v3.4-rc1~184^2~10\n</code></pre>\n\n<p>which tells me that the tag \"v3.4-rc1\" was the first tag to follow my patch, so I'd expect to see it in v3.4.</p>\n",
    "url": "https://unix.stackexchange.com/questions/45120/given-a-git-commit-hash-how-to-find-out-which-kernel-release-contains-it"
  },
  {
    "question_title": "git-upload-pack hangs indefinitely",
    "question_body": "<p>I have the following call structure:</p>\n\n<ol>\n<li>Jenkins runs <code>fab -Huser@host set_repository_commit_hash:123abc</code>.</li>\n<li><code>set_repository_commit_hash</code> runs <code>git fetch</code> with <code>pty = False</code>.</li>\n<li>The child process <code>ssh git@github.com git-upload-pack 'user/repository.git'</code> never finishes.</li>\n</ol>\n\n<p>I've tried running <code>git fetch</code> in a local clone and that succeeds, but running <code>ssh git@github.com git-upload-pack 'user/repository.git'</code> just returns the following and hangs:</p>\n\n<pre><code>00ab84249d3bb20930c185c08848c60b71f7b28990d6 HEADmulti_ack thin-pack side-band side-band-64k ofs-delta shallow no-progress include-tag multi_ack_detailed agent=git/1.8.4\n0041cb34b1c8ca75d478df38c794fc15c5f01cc6377e refs/heads/branch_name\n004012577068adf47015001bfa0cff9386d6cdf497ce refs/heads/[...]\n003f84249d3bb20930c185c08848c60b71f7b28990d6 refs/heads/master\n[a couple more lines like the ones above, then:]\n0000\n</code></pre>\n\n<p>Is this a known SSH/Git/Fabric/Jenkins problem?</p>\n\n<p>I did <code>strace</code> it, but I haven't recorded the session. I believe it was stuck on a <code>read</code>.</p>\n\n<p>Possibly relevant links:</p>\n\n<ol>\n<li><a href=\"https://issues.jenkins-ci.org/browse/JENKINS-14752?page=com.atlassian.jira.plugin.system.issuetabpanels:changehistory-tabpanel\" rel=\"noreferrer\">Jenkins issue 14752: SCM Polling / Max # of concurrent polling = 1\nhangs github polling</a></li>\n<li><a href=\"https://stackoverflow.com/q/17072484\">Why would git-upload-pack (during git clone) hang?</a></li>\n<li><a href=\"http://code.google.com/p/tortoisegit/issues/detail?id=1880\" rel=\"noreferrer\">tortoisegit issue 1880: tortoisegit fetch hangs due to running/never-exiting tortoisegitplink</a> (especially <a href=\"http://code.google.com/p/tortoisegit/issues/detail?id=1880#c7\" rel=\"noreferrer\">comment #7</a>)</li>\n<li><a href=\"https://bbs.archlinux.org/viewtopic.php?pid=1010538\" rel=\"noreferrer\">What is this random never-ending 'git-upload-pack' process?</a></li>\n</ol>\n",
    "answer": "<p>This problem appears to have gone away on its own, as can be expected by a rapidly evolving piece of software. Since I have not observed this issue for probably a couple years now I'd like to extend my thanks to whoever fixed it and consider this question obsolete.</p>\n\n<p>If you are experiencing this issue with recent Git versions please consider <a href=\"https://unix.stackexchange.com/questions/ask\">asking a separate question</a>, since it is likely not the exact same issue.</p>\n",
    "url": "https://unix.stackexchange.com/questions/98959/git-upload-pack-hangs-indefinitely"
  },
  {
    "question_title": "Answer yes in a bash script",
    "question_body": "<p>I'm trying to do a <code>git clone</code> trough a bash script, but the first time that I run the script and the server is not known yet the script fails. I have something like this:</p>\n\n<pre><code>yes | git clone git@github.com:repo/repoo.git\n</code></pre>\n\n<pre class=\"lang-none prettyprint-override\"><code>The authenticity of host 'github.com (207.97.227.239)' can't be established.\nRSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.\nAre you sure you want to continue connecting (yes/no)? \n</code></pre>\n\n<p>But it's ignoring the <code>yes</code>. Do you know how to force <code>git clone</code> to add the key to the known hosts? </p>\n",
    "answer": "<p>Add the following to your <code>~/.ssh/config</code> file:</p>\n\n<pre><code>Host github.com\n    StrictHostKeyChecking no\n</code></pre>\n\n<p>Anything using the open-ssh client to establish a remote shell (with the git client does) should skip the key checks to github.com.</p>\n\n<p>This is actually a bad idea since any form of skipping the checks (whether you automatically hit yes or skip the check in the first place) creates room for a man in the middle security compromise. A better way would be to retrieve and validate the fingerprint and store it in the <code>known_hosts</code> file before needing to run some script that automatically connects.</p>\n",
    "url": "https://unix.stackexchange.com/questions/18853/answer-yes-in-a-bash-script"
  },
  {
    "question_title": "Ignore whitespaces changes in all git commands",
    "question_body": "<p>I've found tons of sites that explain how to have git warn you when you're changing line endings, or miscellaneous other techniques to prevent you from messing up an entire file. Assume it's too late for that -- the tree already has commits that toggle the line endings of files, so <code>git diff</code> shows the subtraction of the old file followed by the addition of a new file with the same content</p>\n\n<p>I'm looking for a git configuration option or command-line flag that tells <code>diff</code> to just ignore those -- if two lines differ only by whitespace, pretend they're the same. I need this config option/flag to work for anything that relies on file differences -- <code>diff</code>, <code>blame</code>, even <code>merge</code>/<code>rebase</code> ideally -- I want <code>git</code> to completely ignore trailing whitespace, particularly line endings. How can I do that?</p>\n",
    "answer": "<p>For diff, there's <code>git diff --ignore-space-at-eol</code>, which should be good enough. For diff and blame, you can ignore all whitespace changes with <code>-w</code>: <code>git diff -w</code>, <code>git blame -w</code>.</p>\n\n<p>For <code>git apply</code> and <code>git rebase</code>, the documentation mentions <code>--ignore-whitespace</code>.</p>\n\n<p>For merge, it looks like you need to use an external merge tool. You can use this wrapper script (untested), where <code>favorite-mergetool</code> is your favorite <a href=\"http://www.kernel.org/pub/software/scm/git/docs/git-mergetool.html\" rel=\"noreferrer\">merge tool</a>; run <code>git -c mergetool.nocr.cmd=/path/to/wrapper/script merge</code>. The result of the merge will be in unix format; if you prefer another format, convert everything to that different format, or convert <code>$MERGED</code> after the merge.</p>\n\n<pre><code>#!/bin/sh\nset -e\nTEMP=$(mktemp)\ntr -d '\\013' &lt;\"$BASE\" &gt;\"$TEMP\"\nmv -f \"$TEMP\" \"$BASE\"\nTEMP=$(mktemp)\ntr -d '\\013' &lt;\"$LOCAL\" &gt;\"$TEMP\"\nmv -f \"$TEMP\" \"$LOCAL\"\nTEMP=$(mktemp)\ntr -d '\\013' &lt;\"$REMOTE\" &gt;\"$TEMP\"\nmv -f \"$TEMP\" \"$REMOTE\"\nfavorite-mergetool \"$@\"\n</code></pre>\n\n<p>To minimize trouble with mixed line endings, make sure <a href=\"http://www.kernel.org/pub/software/scm/git/docs/gitattributes.html#_tt_text_tt\" rel=\"noreferrer\">text files are declared as such</a>.</p>\n\n<p>See also <a href=\"https://stackoverflow.com/questions/861995/is-it-possible-for-git-merge-to-ignore-line-ending-differences\">Is it possible for git-merge to ignore line-ending differences?</a> on Stack Overflow.</p>\n",
    "url": "https://unix.stackexchange.com/questions/10277/ignore-whitespaces-changes-in-all-git-commands"
  },
  {
    "question_title": "How did the Linux Kernel project track bugs in the Early Days?",
    "question_body": "<p>We all know that Linus Torvalds created Git because of issues with Bitkeeper. What is not known (at least to me) is, how were issues/tickets/bugs tracked up until then? I tried but didn't get anything interesting. The only discussion I was able to get on the subject was this one where <a href=\"http://yarchive.net/comp/linux/bug_tracking.html\" rel=\"nofollow noreferrer\">Linus shared concerns with about using Bugzilla</a>. </p>\n\n<p><strong>Speculation:</strong> - The easiest way for people to track bugs in the initial phase would have been to put tickets in a branch of its own but am sure that pretty quickly that wouldn't have scaled with the noise over-taking the good bugs. </p>\n\n<p>I've seen and used Bugzilla and unless you know the right 'keywords' at times you would be stumped. <strong>NOTE:</strong> I'm specifically interested in the early years (1991-1995) as to how they <strong><em>used to</em></strong> track issues. </p>\n\n<p>I did look at two threads, \"<a href=\"https://marc.info/?l=linux-kernel&amp;m=111280216717070\" rel=\"nofollow noreferrer\">Kernel SCM saga</a>\", and \"<a href=\"https://marc.info/?l=git&amp;m=117254154130732\" rel=\"nofollow noreferrer\">Trivia: When did git self-host?</a>\" but none of these made mention about bug-tracking of the kernel in the early days. </p>\n\n<p>I searched around and wasn't able to get of any FOSS bug-tracking software which was there in 1991-1992. Bugzilla, Request-tracker, and others came much later, so they appear to be out. </p>\n\n<h3>Key questions</h3>\n\n<ol>\n<li>How did then Linus, the subsystem-maintainers, and users report and track bugs in those days? </li>\n<li>Did they use some bug-tracking software, made a branch of bugs and manually committed questions and discussions on the bug therein (would be expensive and painful to do that) or just use e-mail. </li>\n<li>Much later, Bugzilla came along (first release 1998) and that seems to be the <a href=\"https://www.kernel.org/doc/man-pages/reporting_code_bugs.html\" rel=\"nofollow noreferrer\">primary way to report bugs afterwards</a>.</li>\n</ol>\n\n<p>Looking forward to have a clearer picture of how things were done in the older days.</p>\n",
    "answer": "<p>In the beginning, if you had something to contribute (a patch or a bug report), you mailed it to Linus. This evolved into mailing it to the list (which was <code>linux-kernel@vger.rutgers.edu</code> before <code>kernel.org</code> was created).</p>\n\n<p>There was no version control. From time to time, Linus put a tarball on the FTP server. This was the equivalent of a \"tag\". The available tools at the beginning were RCS and CVS, and Linus hates those, so everybody just mailed patches. (There is an <a href=\"https://git.wiki.kernel.org/index.php/LinusTalk200705Transcript\">explanation from Linus</a> about why he didn't want to use CVS.)</p>\n\n<p>There were other pre-Bitkeeper proprietary version control systems, but the decentralized, volunteer-based development of Linux made it impossible to use them. A random person who just found a bug will never send a patch if it has to go through a proprietary version control system with licenses starting in the thousands of dollars.</p>\n\n<p>Bitkeeper got around both of those problems: it wasn't centralized like CVS, and while it was not Free Software, kernel contributors were allowed to use it without paying. That made it good enough for a while.</p>\n\n<p>Even with today's git-based development, the mailing lists are still where the action is. When you want to contribute something, you'll prepare it with git of course, but you'll have to discuss it on the relevant mailing list before it gets merged. Bugzilla is there to look \"professional\" and soak up half-baked bug reports from people who don't <em>really</em> want to get involved.</p>\n\n<p>To see some of the old bug-reporting instructions, get the <a href=\"https://archive.org/details/git-history-of-linux\">historical Linux repository</a>. (This is a git repository containing all the versions from before git existed; mostly it contains one commit per release since it was reconstructed from the tarballs). Files of interest include <code>README</code>, <code>MAINTAINERS</code>, and <code>REPORTING-BUGS</code>.</p>\n\n<p>One of the interesting things you can find there is this from the Linux-0.99.12 README:</p>\n\n<pre><code> - if you have problems that seem to be due to kernel bugs, please mail\n   them to me (Linus.Torvalds@Helsinki.FI), and possibly to any other\n   relevant mailing-list or to the newsgroup.  The mailing-lists are\n   useful especially for SCSI and NETworking problems, as I can't test\n   either of those personally anyway.\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/163639/how-did-the-linux-kernel-project-track-bugs-in-the-early-days"
  },
  {
    "question_title": "How can I create an alias for a git [action] command (which includes spaces)?",
    "question_body": "<p>Most of my my aliases are of this form: <code>alias p='pwd'</code></p>\n\n<p>I want to alias <code>git commit</code> so that it does <code>git commit -v</code></p>\n\n<p>But trying to create an alias with a space gives an error:</p>\n\n<pre><code>$ alias 'git commit'='git commit -v'\n-bash: alias: `git commit': invalid alias name\n</code></pre>\n",
    "answer": "<p>Not a direct answer to your question (since aliases can only be one word), but you should be using <code>git-config</code> instead:</p>\n\n<pre><code>git config --global alias.civ commit -v\n</code></pre>\n\n<p>This creates a git alias so that <code>git civ</code> runs <code>git commit -v</code>.  Unfortunately, AFAIK <a href=\"https://stackoverflow.com/questions/5916565/define-git-alias-with-the-same-name-to-shadow-original-command\">there is no way to override existing git commands with aliases</a>.  However, you can always pick a suitable alias name to live with as an alternative.</p>\n",
    "url": "https://unix.stackexchange.com/questions/48862/how-can-i-create-an-alias-for-a-git-action-command-which-includes-spaces"
  },
  {
    "question_title": "Find files that are not in .gitignore",
    "question_body": "<p>I have find command that display files in my project:</p>\n\n<pre><code>find . -type f -not -path './node_modules*' -a -not -path '*.git*' \\\n       -a -not -path './coverage*' -a -not -path './bower_components*' \\\n       -a -not -name '*~'\n</code></pre>\n\n<p>How can I filter the files so it don't show the ones that are in .gitignore?</p>\n\n<p>I thought that I use:</p>\n\n<pre><code>while read file; do\n    grep $file .gitignore &gt; /dev/null &amp;&amp; echo $file;\ndone\n</code></pre>\n\n<p>but .gitignore file can have glob patterns (also it will not work with paths if file is in .gitignore), How can I filter files based on patterns that may have globs?</p>\n",
    "answer": "<p><code>git</code> provides <a href=\"https://git-scm.com/docs/git-check-ignore\" rel=\"noreferrer\"><code>git-check-ignore</code></a> to check whether a file is excluded by <code>.gitignore</code>.</p>\n\n<p>So you could use:</p>\n\n<pre><code>find . -type f -not -path './node_modules*' \\\n       -a -not -path '*.git*'               \\\n       -a -not -path './coverage*'          \\\n       -a -not -path './bower_components*'  \\\n       -a -not -name '*~'                   \\\n       -exec sh -c '\n         for f do\n           git check-ignore -q \"$f\" ||\n           printf '%s\\n' \"$f\"\n         done\n       ' find-sh {} +\n</code></pre>\n\n<p>Note that you would pay big cost for this because the check was performed for each file.</p>\n",
    "url": "https://unix.stackexchange.com/questions/358270/find-files-that-are-not-in-gitignore"
  },
  {
    "question_title": "command line method or programmatically add ssh key to github.com user account",
    "question_body": "<p>Is there a way to identify with a username and password to github.com servers for the purpose of adding an ssh key to the github user account? So far everything I've read suggests that a user's ssh key must be added via the web GUI. I'm looking for the method or process of adding a key via a command line interface or else a bash/ansible/something script.</p>\n",
    "answer": "<h2><strong>Update 2020</strong></h2>\n<p>As stated in <a href=\"https://developer.github.com/changes/2020-02-14-deprecating-password-auth/\" rel=\"noreferrer\">developer changes</a>, Password authentication is going to be deprecated at:</p>\n<p><strong>November 13, 2020 at 16:00 UTC</strong></p>\n<p>Additionally, as @trysis asked in the comments, we need a solution for 2FA.</p>\n<p>The new way is to use a <a href=\"https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token\" rel=\"noreferrer\">personal access token</a>:\n<a href=\"https://i.sstatic.net/u6dLi.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/u6dLi.png\" alt=\"enter image description here\" /></a></p>\n<p>For our specific example (adding a ssh key), we only need write permissions (read permissions are added automatically on using write permissions):\n<a href=\"https://i.sstatic.net/XoVSt.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/XoVSt.png\" alt=\"enter image description here\" /></a></p>\n<p>The updated command (via curl):</p>\n<pre><code>curl -H &quot;Authorization: token YourGeneratedToken&quot; --data '{&quot;title&quot;:&quot;test-key&quot;,&quot;key&quot;:&quot;ssh-rsa AAA...&quot;}' https://api.github.com/user/keys\n</code></pre>\n<p>This does also work when 2FA is enabled.</p>\n<hr />\n<h2><strong>OLD</strong></h2>\n<p>Auth with username and password is <a href=\"https://developer.github.com/v3/#authentication\" rel=\"noreferrer\">supported by github api</a>:</p>\n<blockquote>\n<p>There are three ways to authenticate through GitHub API v3.\n...<br />\nBasic Authentication<br />\n$ curl -u &quot;username&quot; <a href=\"https://api.github.com\" rel=\"noreferrer\">https://api.github.com</a><br />\n...</p>\n</blockquote>\n<p>So just <a href=\"https://developer.github.com/libraries/\" rel=\"noreferrer\">choose a lib</a> in the language you prefer\nand use the implemented version of the <em><a href=\"https://developer.github.com/v3/users/keys/#create-a-public-key\" rel=\"noreferrer\">Create a Public Key</a></em> &quot;Public Key&quot; API Section:</p>\n<blockquote>\n<p>Creates a public key. Requires that you are authenticated via Basic Auth, or OAuth with at least [write:public_key] scope.</p>\n</blockquote>\n<p><strong>INPUT</strong><br />\n<code>POST /user/keys</code></p>\n<pre><code>{\n    &quot;title&quot;: &quot;octocat@octomac&quot;,\n    &quot;key&quot;: &quot;ssh-rsa AAA...&quot;\n}\n</code></pre>\n<p>If you want to use it from command line (via curl):</p>\n<pre><code>curl -u &quot;username&quot; --data '{&quot;title&quot;:&quot;test-key&quot;,&quot;key&quot;:&quot;ssh-rsa AAA...&quot;}' https://api.github.com/user/keys\n</code></pre>\n<p>or even without prompting for password:</p>\n<pre><code>curl -u &quot;username:password&quot; --data '{&quot;title&quot;:&quot;test-key&quot;,&quot;key&quot;:&quot;ssh-rsa AAA...&quot;}' https://api.github.com/user/keys\n</code></pre>\n<p>here is a nice little tutorial for <a href=\"https://gist.github.com/btoone/2288960\" rel=\"noreferrer\">using curl to interact with github API</a></p>\n",
    "url": "https://unix.stackexchange.com/questions/136894/command-line-method-or-programmatically-add-ssh-key-to-github-com-user-account"
  },
  {
    "question_title": "How do I get a linux kernel patch set from the mailing list?",
    "question_body": "<p>I don't subscribe to the linux-kernel mailing list, but I want to get a set of patches that were posted a few weeks ago and apply them to my kernel for testing.  I'm very familiar with patching, building, etc.  My question is, what's the best way to get a copy of this patch set?  It's not applied to any Git repo that I'm aware of, it's just been posted to the mailing list for discussion.</p>\n\n<p>I find a number of sites that archive the linux-kernel mailing list and I can see the set of patches there, but none of these sites have any method (that I can find) of downloading the raw email so I can use \"git apply\" or \"patch\" or whatever.  Just copy/pasting the content from my web browser seems like it will not be very successful due to whitespace differences etc.</p>\n\n<p>How do people manage this?</p>\n",
    "answer": "<p><a href=\"http://marc.info/\">http://marc.info/</a> has a link for each message to get the raw body, and <a href=\"https://lkml.org/\">https://lkml.org/</a> has (in the sidebar) links to download any contained diffs.</p>\n\n<p>There are also archives with NNTP access that may provide raw messages, though I haven't tried this.</p>\n",
    "url": "https://unix.stackexchange.com/questions/80519/how-do-i-get-a-linux-kernel-patch-set-from-the-mailing-list"
  },
  {
    "question_title": "Why can I checkout a branch that was removed on GitHub?",
    "question_body": "<p>In our GitHub repository, a coworker removed a branch named <code>release</code>. But when I run <code>git checkout release</code> locally, I always get the removed branch <code>release</code>. Same, even when I checked out another branch, deleted the <code>release</code> branch with <code>git branch -D release</code> and ran again <code>git checkout release</code>. </p>\n\n<p>Is there something to fix on the GitHub repository, or shall I fix something locally?</p>\n",
    "answer": "<p>After deleting a branch on the remote side you may still see this formerly fetched remote branch locally, see:</p>\n\n<pre><code>$ git branch -a\n[...]\nrelease\nremotes/origin/release\n[...]\n</code></pre>\n\n<p>You only removed the \"release\" but not \"remotes/origin/release\". Delete it like this:</p>\n\n<pre><code>$ git branch -rd origin/release\n</code></pre>\n\n<p>Or remove <em>all</em> fetched branches which do not exist on the remote side anymore:</p>\n\n<pre><code>$ git remote prune origin \n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/365859/why-can-i-checkout-a-branch-that-was-removed-on-github"
  },
  {
    "question_title": "How to modify a PKGBUILD which uses git sources to pull only a shallow clone?",
    "question_body": "<p>The other day I tried installing <a href=\"https://aur.archlinux.org/packages/opencv-git/\"><code>opencv-git</code></a> from the <em>AUR</em> with <code>makepkg</code> on Arch Linux. Of course it pulls from the <em>git</em> repository as the name indicates. This pulls 1Gb. I am reading about making a <a href=\"https://git.wiki.kernel.org/index.php/GitFaq#How_do_I_do_a_quick_clone_without_history_revisions.3F\"><em>shallow clone</em></a> with <a href=\"http://git-scm.com/book\"><code>git</code></a>. When I look at the <code>PKGBUILD</code> file, using <code>grep git PKGBUILD</code>, I see:</p>\n\n<pre class=\"lang-bsh prettyprint-override\"><code>pkgname=\"opencv-git\"\nmakedepends=('git' 'cmake' 'python2-numpy' 'mesa' 'eigen2')\nprovides=(\"${pkgname%-git}\")\nconflicts=(\"${pkgname%-git}\")\nsource=(\"${pkgname%-git}::git+http://github.com/Itseez/opencv.git\"\n    cd \"${srcdir}/${pkgname%-git}\"\n    git describe --long | sed -r 's/([^-]*-g)/r\\1/;s/-/./g'\n    cd \"${srcdir}/${pkgname%-git}\"\n    cd \"${srcdir}/${pkgname%-git}\"\n    cd \"${srcdir}/${pkgname%-git}\"\n    install -Dm644 \"LICENSE\" \"${pkgdir}/usr/share/licenses/${pkgname%-git}/LICENSE\"\n</code></pre>\n\n<p>Is there a way to modify the recipe or the <code>makepkg</code> command to pull only a shallow clone (the latest version of the source is what I want) and not the full repository to save space and bandwidth? Reading <a href=\"https://www.archlinux.org/pacman/PKGBUILD.5.html\"><code>man 5 PKGBUILD</code></a> doesn't provide the insight I'm looking for. Also looked quickly through the <code>makepkg</code> and <code>pacman</code> <a href=\"https://www.archlinux.org/pacman/\">manpages</a> - can't seem to find how to do that.</p>\n",
    "answer": "<p>This can be done by using a custom <a href=\"https://www.archlinux.org/pacman/makepkg.conf.5.html\" rel=\"nofollow noreferrer\">dlagent</a>. I do not really understand Arch packaging or how the dlagents work, so I only have a hack answer, but it gets the job done.</p>\n<p>The idea is to modify the PKGBUILD to use a custom download agent. I modified the source</p>\n<pre class=\"lang-bash prettyprint-override\"><code>&quot;${pkgname%-git}::git+http://github.com/Itseez/opencv.git&quot;\n</code></pre>\n<p>into</p>\n<pre class=\"lang-bash prettyprint-override\"><code>&quot;${pkgname%-git}::mygit://opencv.git&quot;\n</code></pre>\n<p>and then defined a new dlagent called <code>mygit</code> which does a shallow clone by</p>\n<pre class=\"lang-bash prettyprint-override\"><code>makepkg DLAGENTS='mygit::/usr/bin/git clone --depth 1 http://github.com/Itseez/opencv.git'\n</code></pre>\n<p>Also notice that the repository that is being cloned is hard-coded into the command. Again, this can probably be avoided. Finally, the download location is not what the PKGBUILD expects. To work around this, I simply move the repository after downloading it. I do this by adding</p>\n<pre class=\"lang-bash prettyprint-override\"><code>mv &quot;${srcdir}/../mygit:/opencv.git&quot; &quot;${srcdir}/../${pkgname%-git}&quot;\n</code></pre>\n<p>at the beginning of the <code>pkgver</code> function.</p>\n<p>I think the cleaner solution would be to figure out what the <code>git+http</code> dlagent is doing and redfine that temporarily. This should avoid all the hack aspects of the solution.</p>\n",
    "url": "https://unix.stackexchange.com/questions/154919/how-to-modify-a-pkgbuild-which-uses-git-sources-to-pull-only-a-shallow-clone"
  },
  {
    "question_title": "Is there a usable gui front-end to git on Linux?",
    "question_body": "<p>I'm a former windows user and just started using ubuntu. On windows, we had two great softwares: <code>TortoiseSVN</code> and <code>TortoiseGit</code>. Both are so good programs that they allow us to do everything like commit, rollback, merge, view history, browse repos, etc. without knowing a SINGLE cli command.</p>\n\n<p>Now on linux, I'm finding it difficult to do memorize git commands for everything. <code>git push</code> and <code>git remote add</code> is fine. But merging is tedious especially conflict-resolution. In TortoiseGit, it is simply a matter of a few right-clicks!</p>\n\n<p>I've found some crap like git-gui, etc. but the features are nothing comparable to TortoiseGit. Why isn't there any fully-fledged TortoiseGit port on linux systems?</p>\n",
    "answer": "<p>The GIT project maintains a page with all the GUIs available for all platforms both free and commercial. I'd list them all here but it's a pretty extensive list with screenshots and descriptions.</p>\n\n<ul>\n<li><a href=\"http://git-scm.com/downloads/guis\" rel=\"noreferrer\">GUI Clients</a></li>\n</ul>\n\n<p>GIT also comes, typically with 2 GUIs. You can run them as follows:</p>\n\n<pre><code>$ git gui\n</code></pre>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://i.sstatic.net/tN7pd.png\" alt=\"ss #1\"></p>\n\n<p>If you're in a GIT workspace that you've cloned locally you can use <code>gitk</code> to browse it:</p>\n\n<pre><code>$ gitk\n</code></pre>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://i.sstatic.net/ijWLF.png\" alt=\"ss #2\"></p>\n\n<p><strong>NOTE:</strong> If they aren't installed you can install them, typically, with these package names: </p>\n\n<pre><code>- git-gui\n- gitk\n</code></pre>\n\n<h3>Other options</h3>\n\n<p>From the GIT page there are these options that are free for Linux.</p>\n\n<ul>\n<li><a href=\"http://git-cola.github.com/\" rel=\"noreferrer\">git-cola</a></li>\n</ul>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://i.sstatic.net/wiroS.png\" alt=\"ss #3\"></p>\n\n<ul>\n<li><a href=\"http://www.syntevo.com/smartgit/index.html\" rel=\"noreferrer\">SmartGit</a></li>\n</ul>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://i.sstatic.net/yKRLV.png\" alt=\"ss #4\"></p>\n\n<ul>\n<li><a href=\"http://www.giteyeapp.com/\" rel=\"noreferrer\">GitEye</a></li>\n</ul>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://i.sstatic.net/1sr8I.png\" alt=\"ss #5\"></p>\n\n<ul>\n<li><a href=\"https://wiki.gnome.org/Apps/giggle/\" rel=\"noreferrer\">giggle</a></li>\n</ul>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://i.sstatic.net/bYypy.png\" alt=\"ss #6\"></p>\n\n<ul>\n<li><a href=\"https://wiki.gnome.org/Apps/Gitg/\" rel=\"noreferrer\">gitg</a></li>\n</ul>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://i.sstatic.net/KBceF.png\" alt=\"ss #7\"></p>\n",
    "url": "https://unix.stackexchange.com/questions/144100/is-there-a-usable-gui-front-end-to-git-on-linux"
  },
  {
    "question_title": "How to set multiple `core.excludesfile` in `.gitconfig`?",
    "question_body": "<p>I'm syncing <code>~/.gitconfig</code> and <code>~/.gitignore</code> files in ubuntu and Mac by using dropbox and created symlink for it.</p>\n\n<p>And <code>excludesfile</code> is declared like this.</p>\n\n<pre><code>[core]\n        editor = /usr/bin/vim\n        excludesfile = /Users/username/.gitignore\n</code></pre>\n\n<p>The problem is home directory differs by os, therefore I need multiple setting for excludesfile.</p>\n\n<p>Is it possible to define multiple <code>core.excludesfile</code>?</p>\n",
    "answer": "<p>You can only have a single <code>core.excludesfile</code>; the last setting is the one that's used. However, you don't need multiple files: git supports <code>~</code> as an abbreviation for your home directory.</p>\n\n<pre><code>[core]\n    excludesfile = ~/.gitignore\n</code></pre>\n\n<p>In general, if you really needed to have multiple excludes files, the simplest solution would be to generate a single file that's the concatenation of the others, and update it whenever one of the files changes.</p>\n",
    "url": "https://unix.stackexchange.com/questions/116408/how-to-set-multiple-core-excludesfile-in-gitconfig"
  },
  {
    "question_title": "Display only relevant hunks of a diff/patch based on a regexp",
    "question_body": "<p><code>git log -G&lt;regex&gt; -p</code> is a wonderful tool to search a codebase's history for changes that match the specified pattern. However, it can be overwhelming to locate the relevant hunk in the diff/patch output in a sea of mostly irrelevant hunks.</p>\n\n<p>It’s of course possible to search the output of <code>git log</code> for the original string/regex, but that does little to reduce the visual noise and distraction of many unrelated changes.</p>\n\n<p>Reading up on <code>git log</code>, I see there's the <code>--pickaxe-all</code>, which is the exact opposite of what I want: it broadens the output (to the entire changeset), whereas I want to limit it (to the specific hunk).</p>\n\n<p><strong>Essentially, I’m looking for a way to \"intelligently\" parse the diff/patch into individual hunks and then execute a search against each hunk (targeting just the changed lines), discard the hunks that don’t match, and output the ones that do.</strong></p>\n\n<p><strong>Does a tool such as I describe exist? Is there a better approach to get the matched/affected hunks?</strong></p>\n\n<p>Some initial research I've done...</p>\n\n<ul>\n<li><p>If it were possible to <code>grep</code> the diff/patch output and make the context option values dynamic—say, via regexps instead of line counts—that might suffice. But <code>grep</code> isn't exactly built that way (nor am I necessarily requesting that feature).</p></li>\n<li><p>I found the <a href=\"https://github.com/twaugh/patchutils\">patchutils</a> suite, which initially sounded like it might suit my needs. But after reading its <code>man</code> pages, the tools doesn't appear to handle matching hunks based on regexps. (They can accept a list of hunks, though...)</p></li>\n<li><p>I finally came across <a href=\"https://github.com/jaalto/splitpatch\">splitpatch.rb</a>, which seems to handle the parsing of the patch well, but it would need to be significantly augmented to handle reading patches via <code>stdin</code>, matching desired hunks, and then outputting the hunks.</p></li>\n</ul>\n",
    "answer": "<p>here <a href=\"https://stackoverflow.com/a/35434714/5305907\">https://stackoverflow.com/a/35434714/5305907</a> is described a way to do what you are looking for. effectively:</p>\n\n<p><code>git diff -U1 | grepdiff 'console' --output-matching=hunk</code></p>\n\n<p>It shows only the hunks that match with the given string \"console\".</p>\n",
    "url": "https://unix.stackexchange.com/questions/216066/display-only-relevant-hunks-of-a-diff-patch-based-on-a-regexp"
  },
  {
    "question_title": "Github adding a repository as a fork from an existing clone",
    "question_body": "<p>So I have a git repository that I cloned from an upstream source on github. I made a few changes to it (that are uncommitted in the master branch). What I want to do is push my changes onto my github page as a new branch and have github still see it as a fork.</p>\n<p>Is that possible? I'm fairly new to git and github. Did my question even make sense?</p>\n<p>The easiest way that I can think of (which I'm sure is the most aroundabout way), is to fork the repo on github. Clone it locally to a different directory. Add the upstream origin repo. Create a branch in that new forked repo. Copy my code changes by hand into the new local repo. And then push it back up to my github.</p>\n<p>Is this a common use case that there's a simpler way of doing it without duplicating directories?</p>\n<p>I guess I'm asking here as opposed to SO since I'm on linux using command line git and the people here give better answers imo =]</p>\n",
    "answer": "<p>You can do it all from your existing repository (no need to clone the fork into a new (local) repository, create your branch, copy your commits/changes, etc.).</p>\n\n<ol>\n<li><p>Get your commits ready to be published.</p>\n\n<p>Refine any existing local commits (e.g. with <code>git commit --amend</code> and/or <code>git rebase --interactive</code>).</p>\n\n<p>Commit any of your uncommitted changes that you want to publish (I am not sure if you meant to imply that you have some commits on your local <em>master</em> and some uncommitted changes, or just some uncommitted changes; incidentally, uncommitted changes are not “on a branch”, they are strictly in your working tree).</p>\n\n<p>Rename your <em>master</em> branch to give it the name you want for your “new branch”. This is not strictly necessary (you can push from any branch to any other branch), but it will probably reduce confusion in the long run if your local branch and the branch in your GitHub fork have the same name.</p>\n\n<pre><code>git branch -m master my-feature\n</code></pre></li>\n<li><p>Fork the upstream GitHub repository<br>\n(e.g.) <code>github.com:UpstreamOwner/repostory_name.git</code> as<br>\n(e.g.) <code>github.com:YourUser/repository_name.git</code>.</p>\n\n<p>This is done on the GitHub website (or a “client” that uses the GitHub APIs), there are no local Git commands involved.</p></li>\n<li><p>In your local repository (the one that was originally cloned from the upstream GitHub repository and has your changes in its <em>master</em>), add your fork repository as a remote:</p>\n\n<pre><code>git remote add -f github github.com:YourUser/repository_name.git\n</code></pre></li>\n<li><p>Push your branch to your fork repository on GitHub.</p>\n\n<pre><code>git push github my-feature\n</code></pre></li>\n<li><p>Optionally, rename the remotes so that your fork is known as “origin” and the upstream as “upstream”.</p>\n\n<pre><code>git remote rename origin upstream\ngit remote rename github origin\n</code></pre>\n\n<p>One reason to rename the remotes would be because you want to be able to use <code>git push</code> without specifying a repository (it defaults to “origin”).</p></li>\n</ol>\n",
    "url": "https://unix.stackexchange.com/questions/16743/github-adding-a-repository-as-a-fork-from-an-existing-clone"
  },
  {
    "question_title": "Using git to manage /etc?",
    "question_body": "<p>I am thinking on a system, where <code>/etc</code> were tracked on a remote git repository. I am thinking on a git workflow, where every host machine where a different branch.</p>\n\n<p>Every previous versions on every machine could be easily tracked, compared, merged.</p>\n\n<p>If a <code>/etc</code> modification had to be committed on many machines, it could be easily done by some merging script.</p>\n\n<p>In case of an \"unwanted\" <code>/etc</code> change, this could be good visible (even alarm scripts could be tuned to watch that).</p>\n\n<p>Anybody used already a such configuration? Are there any security problems with it?</p>\n",
    "answer": "<p>The program <code>etckeeper</code> does manage <code>/etc</code> in <code>git</code>, you just need to change the default vcs backend from <code>bzr</code> to <code>git</code> in <code>/etc/etckeeper/etckeeper.conf</code>.</p>\n\n<p>It is installed by default in Ubuntu Linux, and handles the common cases of when to commit automatically.<br>\nIt commits before installing packages in case there are uncomitted manual changes, and after installing. </p>\n",
    "url": "https://unix.stackexchange.com/questions/138665/using-git-to-manage-etc"
  },
  {
    "question_title": "What&#39;s the difference between `Commit hash`, `Parent Hash` and `Tree hash` in git?",
    "question_body": "<p>Today I'm learning some basic git knowledge via reading this doc online:</p>\n\n<p><a href=\"http://git-scm.com/book/en/v2/Git-Basics-Viewing-the-Commit-Hi\" rel=\"noreferrer\">http://git-scm.com/book/en/v2/Git-Basics-Viewing-the-Commit-Hi</a></p>\n\n<p>And at that chapter, I'm start to learn using <code>git log --pretty=format:\" \"</code> to show log info at my taste.</p>\n\n<p>But some how, I saw in the format table two similar options, <code>%H</code> for <code>Commit Hash</code>, <code>%P</code> for <code>Parent Hash</code> and <code>%T</code> for <code>Tree Hash</code>.</p>\n\n<p>I experimented them on my command line, it comes out they are all hash values of same length with different value.</p>\n\n<p>I googled and stackoverflowed, no obvious hints so far.</p>\n\n<p>I have idea about this <code>Hash value</code>, it's a checksum of that git commit.</p>\n\n<p>But what does <code>Parent Hash</code> and <code>Tree hash</code> do?</p>\n\n<ul>\n<li>PS: Ah, I got some ideas now, did the <code>Parent Hash</code> mean the hash value of the directly origin of a branch?</li>\n</ul>\n",
    "answer": "<h2>Parent hashes:</h2>\n\n<pre><code>$ git log --graph\n*   commit c06c4c912dbd9ee377d14ec8ebe2847cf1a3ec7e\n|\\  Merge: 79e6924 3113760\n| | Author: linjie &lt;linjielig@gmail.com&gt;\n| | Date:   Mon Mar 14 16:02:09 2016 +0800\n| |\n| |     commit5\n| |\n| |     Merge branch 'dev'\n| |\n| * commit 31137606f85d8960fa1640d0881682a081ffa9d0\n| | Author: linjie &lt;linjielig@gmail.com&gt;\n| | Date:   Mon Mar 14 16:01:26 2016 +0800\n| |\n| |     commit3\n| |\n* | commit 79e69240ccd218d49d78a72f33002fd6bc62f407\n|/  Author: linjie &lt;linjielig@gmail.com&gt;\n|   Date:   Mon Mar 14 16:01:59 2016 +0800\n|\n|       commit4\n|\n* commit 7fd4e3fdddb89858d925a89767ec62985ba07f3d\n| Author: linjie &lt;linjielig@gmail.com&gt;\n| Date:   Mon Mar 14 16:01:00 2016 +0800\n|\n|     commit2\n|\n* commit 316dd3fb3c7b501bc9974676adcf558a18508dd4\n  Author: linjie &lt;linjielig@gmail.com&gt;\n  Date:   Mon Mar 14 16:00:34 2016 +0800\n\n     commit1\n\n$ git log --pretty=format:'%&lt;(82)%P %s'\n79e69240ccd218d49d78a72f33002fd6bc62f407 31137606f85d8960fa1640d0881682a081ffa9d0  commit5\n7fd4e3fdddb89858d925a89767ec62985ba07f3d                                           commit4\n7fd4e3fdddb89858d925a89767ec62985ba07f3d                                           commit3\n316dd3fb3c7b501bc9974676adcf558a18508dd4                                           commit2\n                                                                                   commit1\n</code></pre>\n\n<p>You can see <strong>commit4</strong> and <strong>commit3</strong> is parent of <strong>commit5</strong>, <strong>commit2</strong> is parent of <strong>commit3</strong> and <strong>commit4</strong>, <strong>commit1</strong> is parent of <strong>commit2</strong>.</p>\n\n<h2>Tree hash:</h2>\n\n<pre><code>$ git log --pretty=format:'%T %s'\nf3c7cee96f33938631a9b023ccf5d8743b00db0e commit5\ne0ecb42ae45ddc91c947289f928ea5085c70b208 commit4\nd466aea17dc07516c449c58a73b2dc3faa9d11a1 commit3\nb39f2e707050e0c5bbb3b48680f416ef05b179ba commit2\n5706ec2b32605e27fa04cbef37d582325d14dda9 commit1\n\n$ git cat-file -p f3c7ce\n100644 blob 8bb2e871e94c486a867f5cfcbc6f30d004f6a9e5    dev\n100644 blob 47f16c8e00adba77ec5c176876e99c8e9f05d69b    master\n\n$ git cat-file -p 5706ec\n100644 blob fc0bfde0d44bb4d6c7d27b6e587ebedd34ba5911    master\n</code></pre>\n\n<p>The command's function: Pretty-print the contents of <code>&lt;object&gt;</code> based on it's type.</p>\n\n<pre><code>git cat-file -p \n</code></pre>\n\n<p>In git,all the content is stored as tree and blob objects, with trees corresponding to UNIX directory entries and blobs corresponding more or less to inodes or file contents. A single tree object contains one or more tree entries, each of which contains a SHA-1 pointer to a blob or subtree with its associated mode, type, and filename. Git normally creates a tree by taking the state of your staging area or index and writing a series of tree objects from it. Commit objects have the information about who saved the tree object, when they saved or why they were saved. This is the basic information that the commit object stores for you.</p>\n\n<h2>Conclusion:</h2>\n\n<p>Commit hash, Parent hash, Tree hash are all SHA-1. Commit hash and Parent hash is identical except Parent hash has child. Tree hash is represent a Tree object. Commit hash and Parent hash represent a commit object.</p>\n\n<h2>Reference:</h2>\n\n<ol>\n<li><p><a href=\"https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\" rel=\"noreferrer\">Git Internals - Git Objects</a></p></li>\n<li><p><a href=\"https://git-scm.com/docs/git-cat-file\" rel=\"noreferrer\">git-cat-file - Provide content or type and size information for repository objects</a></p></li>\n</ol>\n",
    "url": "https://unix.stackexchange.com/questions/165369/whats-the-difference-between-commit-hash-parent-hash-and-tree-hash-in-gi"
  },
  {
    "question_title": "Can git configuration be set across multiple repositories?",
    "question_body": "<p>Git seems to support configuration values at three levels:</p>\n\n<ul>\n<li>Per-system global settings (stored in <code>/etc/git-core</code>)</li>\n<li>Per-user global settings (stored in <code>~/.gitconfig</code>)</li>\n<li>Per-repository local settings (stored in <code>$REPO/.git/config</code>)</li>\n</ul>\n\n<p>These options cover most of the basis but I'm looking for a way to handle a fourth level. I have a (very) large collection of repositories for which I need to use a different value for <code>user.email</code> than my usual. These repositories are often created and manipulated through automated scripts, and setting up per repository local settings is cumbersome.</p>\n\n<p>All of the repositories in question are located under a certain path prefix on my local system. Is there a way to set a configuration value somewhere that will be inherited by all repositories under that path? (Sort of like <code>.htaccess</code> settings inherit all the way up the file system.) Perhaps there would be a way to set conditional values in the global config file? What other arrangements could be made in a UNIX environment to cope with a set of repositories like mine?</p>\n",
    "answer": "<p>I have found no way to configure git at this fourth level. The only way seems to be per-command configuration value overrides using <code>git -c key=value</code>.</p>\n\n<p>My current hacky solution is to define a shell function that serves as a wrapper for git. When called, it passes the arguments onto the system git command, but not before checking on the present working directory and adding an extra argument to the command if applicable.</p>\n\n<pre><code>function git () {\n    case \"$PWD\" in\n        /path/to/repos/*)\n            command git -c user.email=alternate@credentials.org \"$@\"\n            ;;\n        *)\n            command git \"$@\"\n            ;;\n    esac\n}\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/42785/can-git-configuration-be-set-across-multiple-repositories"
  },
  {
    "question_title": "Remember password for ssh key for some time",
    "question_body": "<p>I created ssh key. I use it to connect with git repositories. When creating the key, I noticed the prompt that said the password should be hard to guess. Therefore, I came up with 40+ characters-long password. Now, every time I do <code>git clone</code>, <code>push</code> or anything similar, I need to input the password(which takes some time, especially when I don't get it right).</p>\n\n<p>I certainly am glad that I'm enjoying security features; however, I'd prefer for ssh password to cache for 5-15 minutes(or any other arbitrary amount); sometimes I do many operations on repository in small time frame, and typing password is taking too much time. How can I do this?</p>\n",
    "answer": "<p>You can do this using an SSH agent. Most desktop environments start one for you; you can add your key to it by running</p>\n\n<pre><code>ssh-add\n</code></pre>\n\n<p>If you need to start the agent, run</p>\n\n<pre><code>eval $(ssh-agent)\n</code></pre>\n\n<p>(this sets up a number of environment variables).</p>\n\n<p>The <code>-t</code> option to <code>ssh-agent</code> will allow you to specify the timeout. See <a href=\"https://unix.stackexchange.com/questions/122511/configuring-the-default-timeout-for-the-ssh-agent#122529\">Configuring the default timeout for the SSH agent</a> for more details.</p>\n",
    "url": "https://unix.stackexchange.com/questions/271430/remember-password-for-ssh-key-for-some-time"
  },
  {
    "question_title": "watch command not showing colors for &#39;git status&#39;",
    "question_body": "<p>I'm trying to get <code>watch</code> to display colors from 'git status'.</p>\n\n<p>I've tried running watch with the <code>--color</code> option, as suggested elsewhere here, but, still, <code>watch --color 'git status'</code> doesn't display colors.</p>\n",
    "answer": "<p>When <code>git status</code> is run under <code>watch</code>, it is able to detect that its standard output is not a terminal, meaning it will not output colors if the <code>color.status</code> setting is set to <code>auto</code>.  To force <code>git status</code> to always output colors (even under <code>watch</code>), set <code>color.stats</code> to <code>always</code>, e.g. </p>\n\n<pre><code>git config color.status always\n</code></pre>\n\n<p>to set the setting permanently, or as <a href=\"https://unix.stackexchange.com/questions/46814/watch-command-not-showing-colors-for-git-status/47065#comment65142_46814\">@ChrisJonsen points out</a>, use <code>git -c color.status=always status</code> to run <code>git status</code> with a one-time override.</p>\n",
    "url": "https://unix.stackexchange.com/questions/46814/watch-command-not-showing-colors-for-git-status"
  },
  {
    "question_title": "git completion with zsh: filenames with spaces aren&#39;t being escaped properly",
    "question_body": "<p>Git completion:</p>\n\n<p>I'm having difficulty with git's filename autocompletions on my system. I'm using <code>zsh</code> (5.0.5) with <code>git</code> (1.9.3) on OS X (10.9.3). Both <code>zsh</code> and <code>git</code> have been installed via homebrew. (Full version output are at the bottom of the post.)</p>\n\n<p><code>git</code>'s filename completion isn't inserting spaces like I expect. When I type the name of a file with a space in the name, the shell inserts the filename without spaces escaped. <code>zsh</code>'s built-in completion doesn't do this, but <code>git</code>'s does.</p>\n\n<p>Here's an example of what I'm seeing.</p>\n\n<p>I have a repository with a few files with spaces in their names.</p>\n\n<pre><code>% ls -la\ntest\ntest four - latest.txt\ntest three.txt\ntest two\n</code></pre>\n\n<p>The shell backslash escapes the filenames as expected when I use tab completion to insert the file name.</p>\n\n<pre><code>% echo \"testing\" &gt;&gt; test&lt;tab&gt;\n</code></pre>\n\n<p>autocompletes to this after hitting tab three times.</p>\n\n<pre><code>% echo \"testing\" &gt;&gt; test\\ four\\ -\\ latest.txt\n––– file\ntest                       test\\ four\\ -\\ latest.txt  test\\ three.txt            test\\ two                \n</code></pre>\n\n<p><code>git status</code> shows these filenames in quotes (it totally understands what's up):</p>\n\n<pre><code>% git status --short\n M test\n M \"test four - latest.txt\"\n M \"test three.txt\"\n M \"test two\"\n</code></pre>\n\n<p>but when I try to <code>git add</code> with tab autocompletion, it goes sideways.</p>\n\n<pre><code>% git add test&lt;tab&gt;\n</code></pre>\n\n<p>results in this after hitting tab three times:</p>\n\n<pre><code>% git add test four - latest.txt\ntest                    test four - latest.txt  test three.txt          test two\n</code></pre>\n\n<p>I've tried regressing this a bit: my dotfiles are in version control, so I've tried <code>zsh 4.3.15</code>, <code>git 1.8.3</code>, and my dotfiles from a year ago, when I'm nearly certain this worked. Weirdly, this setup was still broken.</p>\n\n<p>I <em>have</em> narrowed it down to the <code>_git</code> completion file that is being sourced from <code>/usr/local/share/zsh/site-functions</code>:</p>\n\n<pre><code>% echo $FPATH\n/usr/local/share/zsh/site-functions:/usr/local/Cellar/zsh/5.0.5/share/zsh/functions\n% ls -l /usr/local/share/zsh/site-functions\n_git@ -&gt; ../../../Cellar/git/1.9.3/share/zsh/site-functions/_git\n_hg@ -&gt; ../../../Cellar/mercurial/3.0/share/zsh/site-functions/_hg\n_j@ -&gt; ../../../Cellar/autojump/21.7.1/share/zsh/site-functions/_j\ngit-completion.bash@ -&gt; ../../../Cellar/git/1.9.3/share/zsh/site-functions/git-completion.bash\ngo@ -&gt; ../../../Cellar/go/HEAD/share/zsh/site-functions/go\n</code></pre>\n\n<p>If I manually change <code>$FPATH</code> before my <code>.zshrc</code> runs <code>compinit</code> (or simply remove the <code>/usr/local/share/zsh/site-functions/_git</code> symbolic link), then completions fall back to <code>zsh</code> and work as expected.</p>\n\n<p>The <code>zsh</code> completion without <code>_git</code>:</p>\n\n<pre><code>% git add test&lt;tab&gt;\n</code></pre>\n\n<p>hitting tab three times produces correct results:</p>\n\n<pre><code>% git add test\\ four\\ -\\ latest.txt\n––– modified file\ntest                       test\\ four\\ -\\ latest.txt  test\\ three.txt            test\\ two                \n</code></pre>\n\n<p>Side note: I've tried removing the <code>git-completion.bash</code> link, and it just totally breaks things:</p>\n\n<pre><code>% git add test&lt;tab&gt;\n</code></pre>\n\n<p>produces this busted-ness:</p>\n\n<pre><code>% git add test__git_zsh_bash_func:9: command not found: __git_aliased_command\n    git add test\n––– file\ntest                       test\\ four\\ -\\ latest.txt  test\\ three.txt            test\\ two                \n</code></pre>\n\n<hr>\n\n<p>I <strong>really</strong> want to get this working properly: the rest of the <code>_git</code> completions were great because they're more repo-aware than the <code>zsh</code> ones, but I need filenames with spaces or other special characters to be properly escaped.</p>\n\n<hr>\n\n<p>Software versions:</p>\n\n<pre><code>% zsh --version\nzsh 5.0.5 (x86_64-apple-darwin13.0.0)\n\n% git --version\ngit version 1.9.3\n\n% sw_vers\nProductName:    Mac OS X\nProductVersion: 10.9.3\nBuildVersion:   13D65\n</code></pre>\n\n<hr>\n\n<p>I've uploaded the <code>_git</code> and <code>git-completion.bash</code> files: <a href=\"http://cl.ly/code/153v2t142i2G\">git-completion.bash</a> and <a href=\"http://cl.ly/code/423e0i1X1p29\">_git</a> (renamed to <code>_git.sh</code> so CloudApp will make it viewable in the browser.)</p>\n",
    "answer": "<p>This bug is mentioned on the <a href=\"https://marc.info/?l=git&amp;m=147265392409883&amp;w=2\" rel=\"noreferrer\">mailing list</a>.</p>\n\n<p>The fix is to edit the file <code>git-completion.zsh</code> and remove the <code>-Q</code> option from <code>compadd</code>, in in <code>__gitcomp_file</code>.</p>\n\n<pre><code>--- i/contrib/completion/git-completion.zsh\n+++ w/contrib/completion/git-completion.zsh\n@@ -90,7 +90,7 @@ __gitcomp_file ()\n\n    local IFS=$'\\n'\n    compset -P '*[=:]'\n-   compadd -Q -p \"${2-}\" -f -- ${=1} &amp;&amp; _ret=0\n+   compadd -p \"${2-}\" -f -- ${=1} &amp;&amp; _ret=0\n }\n\n __git_zsh_bash_func ()\n</code></pre>\n\n<p>This file is installed from the <code>contrib/completion</code> directory, and its path may vary with your package manager. If you installed with homebrew on macOS, it's located in <code>/usr/local/Cellar/git/2.10.2/share/zsh/site-functions</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/132290/git-completion-with-zsh-filenames-with-spaces-arent-being-escaped-properly"
  },
  {
    "question_title": "Git pager is less, but what is causing the output coloring?",
    "question_body": "<p><code>less</code> itself isn't capable of doing syntax highlighting, according to <a href=\"https://unix.stackexchange.com/questions/90990/less-command-and-syntax-highlighting\">this thread</a>. </p>\n\n<p>However, <code>git diff</code> nicely shows colored output in less, its default pager. When I redirect the output of <code>git diff</code> into a file, no color escape sequences are visible. </p>\n\n<p>Does <code>git diff</code> know where it's being sent, and formats the output accordingly? How would one do that?</p>\n\n<hr>\n\n<p>I just noticed that git colors the <code>diff</code> output (e.g. <code>git diff</code>), however, it doesn't know how to syntax highlighting in general. e.g.</p>\n\n<pre><code>git show 415fec6:log.tex\n</code></pre>\n\n<p>doesn't enable any TeX-like syntax. </p>\n\n<hr>\n\n<p>Reading the <code>git</code> sources, I found the following hints</p>\n\n<p>in <code>diff.h</code>:</p>\n\n<pre><code>int use_color;\n</code></pre>\n\n<hr>\n\n<p>I was previously referring to <em>syntax highlighting</em>, but that was not correct. What I mean is output coloring, see e.g.</p>\n\n<p><img src=\"https://i.sstatic.net/cbBQu.png\" alt=\"example color output\"></p>\n",
    "answer": "<p>Git uses <code>isatty()</code> to check whether stdout is a tty: this is used to see if a pager must be used (<a href=\"https://github.com/git/git/blob/398dd4bd039680ba98497fbedffa415a43583c16/pager.c\">pager.c</a>) as well as colors (<a href=\"https://github.com/git/git/blob/398dd4bd039680ba98497fbedffa415a43583c16/color.c\">color.c</a>).</p>\n",
    "url": "https://unix.stackexchange.com/questions/153943/git-pager-is-less-but-what-is-causing-the-output-coloring"
  },
  {
    "question_title": "Fetch a specific branch using git",
    "question_body": "<p>I want to download source code from the <a href=\"https://github.com/Xilinx/u-boot-xlnx/tree/master-next\">master-next branch</a> using git as described in the <a href=\"http://www.wiki.xilinx.com/Fetch+Sources\">Xilinx wiki</a>.</p>\n\n<p>I tried this:</p>\n\n<pre><code>#git clone git://github.com/Xilinx/u-boot-xlnx/tree/master-next.git\n\nInitialized empty Git repository in /home/Hannan/master-next/.git/\nfatal: remote error:\nXilinx/u-boot-xlnx/tree/master-next is not a valid repository name\nEmail support@github.com for help\n</code></pre>\n\n<p>Even this failed:</p>\n\n<pre><code># git clone git://github.com/Xilinx/tree/master-next/u-boot-xlnx.git\nInitialized empty Git repository in /home/Hannan/u-boot-xlnx/.git/\nfatal: remote error:\nXilinx/tree/master-next/u-boot-xlnx is not a valid repository name\nEmail support@github.com for help\n</code></pre>\n\n<p>The command that works is:</p>\n\n<pre><code> git clone git://github.com/Xilinx/u-boot-xlnx.git\n</code></pre>\n\n<p>But how do I know that this will indeed fetch the <a href=\"https://github.com/Xilinx/u-boot-xlnx/tree/master-next\">master-next branch</a> and not the <a href=\"https://github.com/Xilinx/u-boot-xlnx/tree/master\">master branch</a>? How do I correctly fetch a specific branch using git?</p>\n\n<p>I am using RHEL 6, accessed via PuTTY.</p>\n",
    "answer": "<p>Like the error message tells you, <code>git clone</code> expects a git repository. You can't \"add\" the name of a branch in the path like that. See <a href=\"http://git-scm.com/book/en/Git-Branching\">branches in git</a>.</p>\n\n<p>You can clone a single branch (and it's history) :\n<code>git clone &lt;url&gt; --branch &lt;branch&gt; --single-branch</code> </p>\n\n<p>See <code>git help clone</code>.</p>\n\n<p>But the clone command you ran gave you a copy of the whole repository, you can see the existing branches with <code>git branch</code> and more details about them with <code>git show-branch</code>.</p>\n\n<p>Switch to the branch you want with <code>git checkout branch-name</code>.</p>\n\n<p>The first chapters of the <a href=\"http://git-scm.com/book\">Pro Git</a> book (available online) give more details on the basic commands.</p>\n",
    "url": "https://unix.stackexchange.com/questions/127066/fetch-a-specific-branch-using-git"
  },
  {
    "question_title": "How to find all git repositories within given folders (fast)",
    "question_body": "<p>Naive approach is <code>find dir1 dir2 dir3 -type d -name .git | xargs -I {} dirname {}\n</code>, but it's too slow for me, because I have a lot deep folder structures inside git repositories (at least I think that this is the reason). I've read about that I can use <code>prune</code> to prevent find to recurse into directories once it found something, but there's two things. I'm not sure how this works (I mean I don't understand what <code>prune</code> does although I've read man page) and the second it wouldn't work in my case, because it would prevent <code>find</code> to recurse into <code>.git</code> folder but not into all other folders.</p>\n\n<p>So what I actually need is:</p>\n\n<p>for all subdirectories check if they contain a <code>.git</code> folder and if it is then stop searching in this filesystem branch and report result. It would be perfect if this would also exclude any hidden directories from search.</p>\n",
    "answer": "<p>Okay, I still don't totally sure how this works, but I've tested it and it works.</p>\n\n<pre><code>.\n├── a\n│   ├── .git\n│   └── a\n│       └── .git\n└── b\n    └── .git\n\n6 directories, 0 files\n\n% find . -type d -exec test -e '{}/.git' ';' -print -prune\n./a\n./b\n</code></pre>\n\n<p>I'm looking forward into making the same faster.</p>\n",
    "url": "https://unix.stackexchange.com/questions/333862/how-to-find-all-git-repositories-within-given-folders-fast"
  },
  {
    "question_title": "Make vim stop splitting my Git commit messages",
    "question_body": "<p>Vim has this awful annoying habit of splitting my commit messages and automatically line-breaking them, rendering the second line with a red background for some reason.</p>\n\n<p><img src=\"https://i.sstatic.net/cF0yU.png\" alt=\"enter image description here\"></p>\n\n<p>How can I make vim <em>stop</em> doing this?</p>\n",
    "answer": "<p>I found the setting I needed in <code>~/.vim/after/ftplugin/gitcommit.vim</code> was:</p>\n\n<p><code>setlocal textwidth=0</code></p>\n",
    "url": "https://unix.stackexchange.com/questions/138148/make-vim-stop-splitting-my-git-commit-messages"
  },
  {
    "question_title": "Excluding files in etckeeper with .gitignore doesn&#39;t work",
    "question_body": "<p>I use etckeeper for changes in my configfiles (on Debian squeeze)</p>\n\n<p>Since I also have an ircdeamon running, there are some files, that change every minute in the folder</p>\n\n<pre><code>/etc/hybserv/\n</code></pre>\n\n<p>I don't want to version control them anymore, so I added </p>\n\n<pre><code>hybserv/*\n</code></pre>\n\n<p>to the end of</p>\n\n<pre><code>/etc/.gitignore\n</code></pre>\n\n<p>but they are not ignored! They keep showing up every hour in the hourly commit.</p>\n\n<p>What am I doing wrong?</p>\n",
    "answer": "<p>You need to delete (=unregister) them from git.</p>\n\n<p>Use something like </p>\n\n<pre><code>cd /etc\ngit rm --cached hybserv/*\ngit commit -m \"Remove hybserv/* files from git\"\n</code></pre>\n\n<p>Note the <code>--cached</code> option. With it, the files are <em>only</em> removed from git and are not deleted from the disk.</p>\n",
    "url": "https://unix.stackexchange.com/questions/63627/excluding-files-in-etckeeper-with-gitignore-doesnt-work"
  },
  {
    "question_title": "Git pull in a Linux environment changes the owner of the files",
    "question_body": "<p>Why will every time I execute a <code>git pull</code> on my live server, the owner of the files be changed? I keep on manually doing a <code>chown john:john index.php</code>.</p>\n\n<p>What do I need to change so that, when I do a <code>git pull</code>, the owner of the files will be automatically be or stay <code>john</code>?</p>\n",
    "answer": "<p>When you use <code>git pull</code> (which is equivalent to <code>git fetch; git merge</code>), git create new files and does not care about previous ownership (git does not store that information).</p>\n<p>There is different solution to solve this. The easier is probably to add a <code>post-update</code> hooks (in your <code>.git/hooks</code> directory) to automatically call <code>chown john:john . -R</code> after merge/pull  (<a href=\"https://stackoverflow.com/questions/9613545/how-to-control-ownership-of-files-auto-pushed-to-a-git-target-repo-by-commit-hoo\">see that SO solution for an example</a>).</p>\n<p>Alternate solutions are:</p>\n<ul>\n<li>Run <code>git pull</code> as the john user instead of root (that will require write permissions to john for .git directory).</li>\n<li>Create a hook to make the deployment, which will either use john as user, either do the chown after update (so you will have to do <code>git pull</code> inside the <code>post-receive</code> hook of the bare repo).</li>\n</ul>\n<p>In linux, it might be another solution with <code>setuid</code> / <code>setgid</code></p>\n",
    "url": "https://unix.stackexchange.com/questions/154859/git-pull-in-a-linux-environment-changes-the-owner-of-the-files"
  },
  {
    "question_title": "How can I solve this ssh-agent problem?",
    "question_body": "<p>I'm using Linux Mint, and have not been able to get gnome-keyring to unlock automatically at login, it seems.</p>\n\n<p>A symptom of my problem is as follows:</p>\n\n<pre><code>$ ssh-add\nIdentity added: /home/me/.ssh/id_rsa (/home/me/.ssh/id_rsa)\n\n$ git pull\nWARNING: gnome-keyring:: couldn't connect to: /tmp/keyring-Nmf3J3/pkcs11: No such file or directory\n</code></pre>\n\n<p>How can I make it that git can push/pull without any passphrase input from me?</p>\n\n<p>I realize there's several things here with gnome-keyring, and ssh-agent, but have not been able to nail it down.</p>\n\n<p>Running <code>ssh-add</code> during a session means that I am no longer asked for my passphrase for SSH/git.</p>\n\n<p>The problem is that I would need to run <code>ssh-add</code> during each session - I must be missing how to have Gnome's keyring unlock at login.</p>\n\n<pre><code>$ export | grep GNOME          \nGNOME_KEYRING_CONTROL=/tmp/keyring-hjMM4V\nGNOME_KEYRING_PID=1961\n</code></pre>\n\n<p>It happened again during the same session as the first edit. I did <code>git pull</code> and got <code>WARNING: gnome-keyring:: couldn't connect to: /tmp/keyring-hjMM4V/pkcs11: No such file or directory</code>.</p>\n\n<pre><code>$ env | grep SSH\nSSH_AGENT_PID=2116\nSSH_AUTH_SOCK=/tmp/ssh-OACxJMBY2038/agent.2038\n\n$ ps -fp $SSH_AGENT_PID\nUID        PID  PPID  C STIME TTY          TIME CMD\neoin      2116  2038  0 09:47 ?        00:00:00 /usr/bin/ssh-agent /usr/bin/dbus-launch --exit-with-session x-session-manager\n</code></pre>\n",
    "answer": "<p>What is meant to happen is:</p>\n\n<p>You start a gnome session, part of that a gnome-keyring daemon (which also acts as a ssh agent) starts and the environment of anything started during that gnome session is updated with information on how to contact that ssh agent. The password you issue upon graphically logging in is used to unlock the default keyring.</p>\n\n<p>When you use gnome-keyring as a ssh-agent, you don't want to use another agent like <code>ssh-agent</code>.</p>\n\n<p>When your X session terminates, so does gnome-keyring. But your tmux session remains. Then, even if you start another gnome-keyring or ssh-agent, the environment of the processes already started by <code>tmux</code> won't be able to talk to it unless you update their environment with the path of the new socket.</p>\n\n<p>What you could do is:</p>\n\n<pre><code>gnome-keyring-daemon -r &gt; ~/.gkr\n</code></pre>\n\n<p>And do <code>. ~/.gkr</code> in all the shells you want to use the new gnome-keyring</p>\n\n<p>Beware though of which DISPLAY that gnome-keyring-daemon is going to connect to.</p>\n",
    "url": "https://unix.stackexchange.com/questions/52551/how-can-i-solve-this-ssh-agent-problem"
  },
  {
    "question_title": "Read lines into array, one element per line using bash",
    "question_body": "<p>I am trying to get a bash array of all the unstaged modifications of files in a directory (using Git). The following code works to print out all the modified files in a directory:</p>\n\n<pre><code>git -C $dir/.. status --porcelain | grep \"^.\\w\" | cut -c 4-\n</code></pre>\n\n<p>This prints</p>\n\n<pre><code>\"Directory Name/File B.txt\"\n\"File A.txt\"\n</code></pre>\n\n<hr>\n\n<p>I tried using</p>\n\n<pre><code>arr1=($(git status --porcelain | grep \"^.\\w\" | cut -c 4-))\n</code></pre>\n\n<p>but then</p>\n\n<pre><code>for a in \"${arr1[@]}\"; do echo \"$a\"; done\n</code></pre>\n\n<p>(both with and without the quotes around <code>${arr1[@]}</code> prints</p>\n\n<pre><code>\"Directory\nName/File\nB.txt\"\n\"File\nA.txt\"\n</code></pre>\n\n<hr>\n\n<p>I also tried </p>\n\n<pre><code>git -C $dir/.. status --porcelain | grep \"^.\\w\" | cut -c 4- | readarray arr2\n</code></pre>\n\n<p>but then</p>\n\n<pre><code>for a in \"${arr2[@]}\"; do echo \"$a\"; done\n</code></pre>\n\n<p>(both with and without the quotes around <code>${arr2[@]}</code>) prints nothing. Using <code>declare -a arr2</code> beforehand does absolutely nothing either.</p>\n\n<hr>\n\n<p>My question is this: How can I read in these values into an array? (This is being used for my argos plugin <a href=\"https://github.com/arjvik/gitbar\" rel=\"noreferrer\">gitbar</a>, in case it matters, so you can see all my code).</p>\n",
    "answer": "<h1>TL;DR</h1>\n\n<p>In bash:</p>\n\n<pre><code>readarray -t arr2 &lt; &lt;(git … )\nprintf '%s\\n' \"${arr2[@]}\"\n</code></pre>\n\n<hr>\n\n<p>There are two distinct problems on your question</p>\n\n<ol>\n<li><p>Shell splitting.<br>\nWhen you did:</p>\n\n<pre><code>arr1=($(git … ))\n</code></pre>\n\n<p>the \"command expansion\" is unquoted, and so: it is subject to shell split and glob.</p>\n\n<p>The exactly see what that shell splitting do, use printf:</p>\n\n<pre><code>$ printf '&lt;%s&gt;  '  $(echo word '\"one simple sentence\"')\n&lt;word&gt;  &lt;\"one&gt;  &lt;simple&gt;  &lt;sentence\"&gt;\n</code></pre>\n\n<p>That would be avoided by <strong>quoting</strong>:</p>\n\n<pre><code>$ printf '&lt;%s&gt;  '  \"$(echo word '\"one simple sentence\"')\"\n&lt;word \"one simple sentence\"&gt;\n</code></pre>\n\n<p>But that, also, would avoid the splitting on newlines that you want.</p></li>\n<li><p>Pipe<br>\nWhen you executed:</p>\n\n<pre><code>git … | … | … | readarray arr2\n</code></pre>\n\n<p>The array variable <code>arr2</code> got set but it went away when the pipe (<code>|</code>) was closed.</p>\n\n<p>You could use the value if you stay inside the last subshell:</p>\n\n<pre><code>$ printf '%s\\n' \"First value.\" \"Second value.\" | \n        { readarray -t arr2; printf '%s\\n' \"${arr2[@]}\"; }\nFirst value.\nSecond value.\n</code></pre>\n\n<p>But the value of <code>arr2</code> will not survive out of the pipe.</p></li>\n</ol>\n\n<h1>Solution(s)</h1>\n\n<p>You need to use read to split on newlines but not with a pipe.<br>\n From older to newer:</p>\n\n<ol>\n<li><p>Loop.<br>\nFor old shells without arrays (using positional arguments, the only quasi-array):</p>\n\n<pre><code>set --\nwhile IFS='' read -r value; do\n    set -- \"$@\" \"$value\"\ndone &lt;&lt;-EOT\n$(printf '%s\\n' \"First value.\" \"Second value.\")\nEOT\n\nprintf '%s\\n' \"$@\"\n</code></pre>\n\n<p>To set an array (ksh, zsh, bash)</p>\n\n<pre><code>i=0; arr1=()\nwhile IFS='' read -r value; do\n    arr1+=(\"$value\")\ndone &lt;&lt;-EOT\n$(printf '%s\\n' \"First value.\" \"Second value.\")\nEOT\n\nprintf '%s\\n' \"${arr1[@]}\"\n</code></pre></li>\n<li><p>Here-string<br>\nInstead of the here document (<code>&lt;&lt;</code>) we can use a here-string (<code>&lt;&lt;&lt;</code>):</p>\n\n<pre><code>i=0; arr1=()\nwhile IFS='' read -r value; do\n    arr1+=(\"$value\")\ndone &lt;&lt;&lt;\"$(printf '%s\\n' \"First value.\" \"Second value.\")\"\n\nprintf '%s\\n' \"${arr1[@]}\"\n</code></pre></li>\n<li><p>Process substitution<br>\nIn shells that support it (ksh, zsh, bash) you can use <code>&lt;( … )</code> to replace the here-string:</p>\n\n<pre><code>i=0; arr1=()\nwhile IFS='' read -r value; do\n    arr1+=(\"$value\")\ndone &lt; &lt;(printf '%s\\n' \"First value.\" \"Second value.\")\n\nprintf '%s\\n' \"${arr1[@]}\"\n</code></pre>\n\n<p>With differences: <code>&lt;( )</code> is able to emit NUL bytes while a here-string might remove (or emit a warning) the NULs. A here-string adds a trailing newline by default. There may be others AFAIK.</p></li>\n<li><p>readarray<br>\nUse <code>readarray</code> in <code>bash</code><sup><strong>[a]</strong></sup> (a.k.a <code>mapfile</code>) to avoid the loop:</p>\n\n<pre><code>readarray -t arr2 &lt; &lt;(printf '%s\\n' \"First value.\" \"Second value.\")\nprintf '%s\\n' \"${arr2[@]}\"\n</code></pre></li>\n</ol>\n\n<hr>\n\n<p><sup><strong>[a]</strong></sup>In ksh you will need to use <code>read -A</code>, which clears the variable before use, but needs some \"magic\" to split on newlines and read the whole input at once.</p>\n\n<pre><code>IFS=$'\\n' read -d '' -A arr2 &lt; &lt;(printf '%s\\n' \"First value.\" \"Second value.\")\n</code></pre>\n\n<p>You will need to <a href=\"https://stackoverflow.com/a/12652267/9110606\">load a <code>mapfile</code> module in zsh</a> to do something similar.</p>\n",
    "url": "https://unix.stackexchange.com/questions/485221/read-lines-into-array-one-element-per-line-using-bash"
  },
  {
    "question_title": "Delete any file mentioned in .gitignore",
    "question_body": "<p>I have the following entries in a <code>.gitignore</code> file and I want to remove them. The reason why is because these files are temporary-junk files created during a TeX compilation and I want to be able to remove them.</p>\n\n<p>How I can do that?</p>\n",
    "answer": "<pre><code>git clean -Xdf\n</code></pre>\n\n<p>With capital <code>-X</code>, <code>git clean</code> <em>only</em> removes files that match <code>.gitignore</code>.</p>\n\n<p>With <code>-d</code>, <code>git clean</code> also deletes entire directories that match <code>.gitignore</code>.</p>\n\n<p>With <code>-f</code>, <code>git clean</code> will actually delete things instead of warning you. Be careful though. There might be other things (like personal <code>direnv</code> configuration) that are <code>.gitignored</code> for other reasons, so be sure it is always understood what this command will do before it is run.</p>\n\n<p>A special thanks to @Kusalananda for sharing <code>git clean</code>.</p>\n\n<hr>\n\n<p>As an alternative, if you have, say, a folder for dependencies that includes symlinked directories and you don't want <code>git clean</code> to recursively delete everything <em>inside</em> those directories, you could use this longer form:</p>\n\n<pre><code>git ls-files --others --ignored --exclude-standard --directory | head -c -1 | tr '\\n' '\\0' | xargs -0 -r rm -r\n</code></pre>\n\n<p><code>git ls-files</code> with these options lists the same directories as <code>git clean -Xdn</code>, but more machine-readable.</p>\n\n<p><code>head -c -1</code> trims the trailing newline byte.</p>\n\n<p><code>tr '\\n' '\\0'</code> translates all newlines to <code>NUL</code>s.</p>\n\n<p><code>xargs -0 -r rm -r</code> deletes the files and directories. <code>-0</code> makes <code>NUL</code> the delimiter. <code>-r</code> causes it to skip empty input (warning: this is a non-portable GNU extension).</p>\n",
    "url": "https://unix.stackexchange.com/questions/435960/delete-any-file-mentioned-in-gitignore"
  },
  {
    "question_title": "Pipe filelist into &#39;git add&#39;",
    "question_body": "<p>I performed a fresh clone and copied/pasted a working directory into the cloned directory. Now have a list of changed files:</p>\n\n<pre><code>$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 \nGNUmakefile\nReadme.txt\nbase32.h\nbase64.h\n...\n</code></pre>\n\n<p>When I try to get Git to add them, it results in an error (I don't care about adding 1 at a time):</p>\n\n<pre><code>$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add\nNothing specified, nothing added.\nMaybe you wanted to say 'git add .'?\n</code></pre>\n\n<p>Adding the <code>-</code>:</p>\n\n<pre><code>$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add -\nfatal: pathspec '-' did not match any files\n</code></pre>\n\n<p>And <code>--</code>:</p>\n\n<pre><code>$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add --\nNothing specified, nothing added.\nMaybe you wanted to say 'git add .'?\n</code></pre>\n\n<p>Trying to use <em>interactive</em> from the man page appears to have made a greater mess of things:</p>\n\n<pre><code>$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add -i\n           staged     unstaged path\n  1:    unchanged        +1/-1 GNUmakefile\n  2:    unchanged      +11/-11 Readme.txt\n  ...\n\n*** Commands ***\n  1: status   2: update   3: revert   4: add untracked\n  5: patch    6: diff     7: quit     8: help\nHuh (GNUmakefile)?\nWhat now&gt; *** Commands ***\n  1: status   2: update   3: revert   4: add untracked\n  5: patch    6: diff     7: quit     8: help\nHuh (Readme.txt)?\n</code></pre>\n\n<p>(I've already deleted the directory Git made a mess of, so I'm not trying to solve that issue).</p>\n\n<p>How do I tell Git to add the files piped into it?</p>\n",
    "answer": "<p><code>git add</code> is expecting the files to be listed as arguments, not piped into <code>stdin</code>.  Try either</p>\n\n<pre><code>git status --short | grep -v \"??\" | cut -d \" \" -f 3 | xargs git add\n</code></pre>\n\n<p>or</p>\n\n<pre><code>for file in $(git status --short | grep -v \"??\" | cut -d \" \" -f 3); do\n    git add $file;\ndone\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/244169/pipe-filelist-into-git-add"
  },
  {
    "question_title": "How to sort (by whatever key) BibTex entries in `.bib` file?",
    "question_body": "<p>I work with LaTeX and do versioning with Git. For bibliography management I use Mendeley.</p>\n\n<p>The problem is that each time Mendeley synchronizes it's <code>.bib</code> exports,\nthey are in different order, what makes bibliography versioning much harder.</p>\n\n<p>My idea is to sort BibTex entries in <code>.bib</code> file, each time before commit.</p>\n\n<p>Could you help me, how to do this in smart (short&amp;sweet) way ? :)</p>\n\n<p>P.S. I can run this routine manually. I do not need git integration. I just want program/script to sort <code>.bib</code> file.</p>\n",
    "answer": "<p>How about <a href=\"ftp://ftp.mpi-sb.mpg.de/pub/tex/mirror/ftp.dante.de/pub/tex/biblio/bibtex/utils/bibsort/\"><code>bibsort</code></a>?</p>\n\n<pre><code>NAME\n     bibsort - sort a BibTeX bibliography file\n\nSYNOPSIS\n     bibsort [optional sort(1) switches] &lt; infile &gt;outfile\n\nDESCRIPTION\n     bibsort filters a BibTeX bibliography, or bibliography frag-\n     ment,  on  its standard input, printing on standard output a\n     sorted bibliography.\n</code></pre>\n\n<p><a href=\"ftp://ftp.mpi-sb.mpg.de/pub/tex/mirror/ftp.dante.de/pub/tex/biblio/bibtex/utils/bibsort/bibsort.sh\">It's a shell script</a> wrapping <code>nawk</code> (and <code>tr</code>, <code>sort</code> and <code>grep</code>) and includes two warnings you might have to pay attention to (see the source).</p>\n\n<p>(<strong>Edit</strong> There're also a lot of <a href=\"http://search.cpan.org/search?mode=all&amp;query=bibtex\">bibtex-related Perl modules</a>...)</p>\n\n<p><strong>Edit2</strong> I just recognized you'd like to sort for <em>any</em> key, while <code>bibsort</code> apparently sorts by <em>the bibtex tags</em> only -- but maybe its source (it's not too long) is still helpful...?</p>\n",
    "url": "https://unix.stackexchange.com/questions/31266/how-to-sort-by-whatever-key-bibtex-entries-in-bib-file"
  },
  {
    "question_title": "diff to show only the additions in a changed file",
    "question_body": "<p>Normally <code>diff</code> and <code>git diff</code> show both the original and the modified line with <code>-</code> and <code>+</code> respectively. Is there any way, I can filter only to see the modified line? This would reduce the number of lines to read by a factor of 2 instantly.</p>\n\n<p>I was assuming </p>\n\n<pre><code>git diff test.yml | grep '^+' | less -R\n</code></pre>\n\n<p>and </p>\n\n<pre><code>git diff test.yml | egrep '^+' | less -R\n</code></pre>\n\n<p>to have the same result. ie they would show any new additions in a file. However <code>egrep</code> shows me the entire file. Why is that so?</p>\n\n<p>With the above method anyways, I lose the color. Is there any way to retain the color?</p>\n",
    "answer": "<p>You can use <code>--word-diff</code> to condense the <code>+</code> and <code>-</code> lines together with the changes highlighted using red/green text and stop using grep all together.</p>\n\n<p><a href=\"https://i.sstatic.net/0fbcr.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/0fbcr.png\" alt=\"enter image description here\"></a></p>\n\n<p>You can combine this with <code>-U0</code> to remove all context around the diffs if you really want to condense it down further.</p>\n\n<p><a href=\"https://i.sstatic.net/CcUEu.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/CcUEu.png\" alt=\"enter image description here\"></a></p>\n\n<p>This approch is better than using grep as you don't lose output, you can tell when a line was added or simply changed and you don't completely lose removals while still condensing the output down into something that is easy to read. </p>\n\n<p>The answer to the question regarding egrep is already answered by @Stephen Kitt <a href=\"https://unix.stackexchange.com/a/392195/119007\">here</a> </p>\n",
    "url": "https://unix.stackexchange.com/questions/392191/diff-to-show-only-the-additions-in-a-changed-file"
  },
  {
    "question_title": "What does &quot;--&quot; mean in Linux/Unix command line?",
    "question_body": "<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"https://unix.stackexchange.com/questions/11376/what-does-double-dash-mean\">What does “--” (double-dash) mean?</a>  </p>\n</blockquote>\n\n\n\n<p><code>git diff [options] [&lt;commit&gt;] [--] [&lt;path&gt;…]</code></p>\n\n<p>In here, how should I understand what [--] means? And when should I use it.</p>\n",
    "answer": "<p>The <code>--</code> is commonly used in command to indicate the end of options. This is useful if your filename begins with a \"-\" or your input is unknown. Here is an example of its use:</p>\n\n<p><code>git diff --stat -- --file1 --file2</code></p>\n\n<p><code>--file1</code> is treated as a filename rather than another option.</p>\n",
    "url": "https://unix.stackexchange.com/questions/52167/what-does-mean-in-linux-unix-command-line"
  },
  {
    "question_title": "Is `git diff` related to `diff`?",
    "question_body": "<p>Is <code>git diff</code> related to <code>diff</code>?</p>\n\n<ol>\n<li><p>Is <code>git diff</code> implemented based on <code>diff</code>?</p></li>\n<li><p>Is <a href=\"https://git-scm.com/docs/git-diff\" rel=\"noreferrer\">the command line syntax of <code>git diff</code></a> similar to <a href=\"http://man7.org/linux/man-pages/man1/diff.1.html\" rel=\"noreferrer\">that of <code>diff</code></a>? Does learning one help  using the other much?</p></li>\n<li><p>are their output files following the same format? Can they be both used by <code>git patch</code> and <code>patch</code>? (Is there <code>git patch</code>? How is it related to <code>patch</code>?)</p></li>\n</ol>\n\n<p>Thanks.</p>\n",
    "answer": "<p>The file format is interoperable.  Git uses the best format, <code>diff -u</code>.  It also extends it to represent additional types of changes.</p>\n\n<p>The equivalent to <code>patch</code> is <code>git apply</code>.  It stages the changes in the index as well as applying them to the working tree.</p>\n\n<p>I remember <code>git apply</code> being stricter than <code>patch</code>, although the reference documentation doesn't seem to make an explicit comparison.  It does mention several tests / errors which can be enabled or disabled.</p>\n\n<p>The reference documentation also suggests that it could be used as \"a replacement for GNU patch\" - even outside of a git repository, if you use a certain option.</p>\n",
    "url": "https://unix.stackexchange.com/questions/356652/is-git-diff-related-to-diff"
  },
  {
    "question_title": "git grep colors differ from grep custom colors",
    "question_body": "<p>I've setup grep colors in my <code>~/.bashrc</code> :</p>\n\n<pre><code>export GREP_COLORS='ms=01;34:mc=01;34:sl=:cx=:fn=35:ln=32:bn=32:se=36'\n</code></pre>\n\n<p>They work for </p>\n\n<ul>\n<li><code>grep --color=auto</code></li>\n<li><code>grep --color=always</code></li>\n</ul>\n\n<p>Unfortunately, those custom colors are ignored by:</p>\n\n<ul>\n<li><code>git grep --color=auto</code></li>\n<li><code>git grep --color=always</code></li>\n</ul>\n\n<p>How to make <code>git grep</code> to use above <code>$GREP_COLORS</code> colors ?</p>\n",
    "answer": "<p>Git grep is not using the <code>GREP_COLORS</code> environment variable. Instead you should add custom entries in you <code>~/.gitconfig</code></p>\n\n<p>For example: </p>\n\n<pre><code>[color \"grep\"]\n    linenumber = yellow bold\n    match = red\n    filename = magenta     \n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/30591/git-grep-colors-differ-from-grep-custom-colors"
  },
  {
    "question_title": "adding SSL certificate for Github only (not all certificates from ca-certificates package)",
    "question_body": "<p>I get the following error when accessing Github over HTTPS:</p>\n\n<pre><code>error: server certificate verification failed. \nCAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none\n</code></pre>\n\n<p>This is because I don't have any certificates in <code>/etc/ssl/certs/</code>. I know how to fix this problem. I can install the package <code>ca-certificates</code> from Debian repository. The problem is, however, that this will install all certificates (thousands) which I don't necessarily want to accept/trust.</p>\n\n<p>How can I install certificate for Github only?</p>\n\n<h3>a Subproblem/Subquestion</h3>\n\n<p>On another machine, where the package <code>ca-certificates</code> is already installed and git works, I have noticed that some certificates in <code>/etc/ssl/certs/</code> are one-certificate-per-file and other are many-certificates-in-one-file. The particular file containing Github certificate, <code>/etc/ssl/certs/ca-certificates.crt</code> contains over 150 other certificates:</p>\n\n<pre><code>$ grep 'BEGIN CERTIFICATE' /etc/ssl/certs/ca-certificates.crt | wc -l\n159\n</code></pre>\n\n<p>How can I find which one out of these 159 certificate is the one I need? (other than brute force - slicing the file in halves and checking both halves, repeating <code>while n &gt; 1</code>).</p>\n",
    "answer": "<p>In order to access your Github you need to do it via ssh. So you need to add your ssh public key to github. After that you are able to access github via ssh i.e.:</p>\n\n<pre><code>git init git@github.com:yourname/yourrepo.git\n</code></pre>\n\n<p>See also: <a href=\"https://help.github.com/articles/generating-ssh-keys\">Github: generating ssh keys</a>, <a href=\"http://www.wikihow.com/Add-Ssh-Public-Keys-on-Github\">WikiHow</a></p>\n\n<p>[Edit #1]</p>\n\n<p>without certificate checks:</p>\n\n<pre><code>GIT_SSL_NO_VERIFY=true git clone https://github.com/p/repo.git\n</code></pre>\n\n<p>or authenticated</p>\n\n<pre><code>GIT_SSL_NO_VERIFY=true git clone https://user@pass:github.com/p/repo.git\n</code></pre>\n\n<p>For me it is still not clear what are you asking for, because you know that installing ca-certificates will fix the problem.</p>\n\n<p>[Edit #2]</p>\n\n<p>Ok, the other question was </p>\n\n<blockquote>\n  <p>how to have only the certificate which is needed to access github.com via https</p>\n</blockquote>\n\n<ol>\n<li><p>Open your browser and navigate to <a href=\"https://github.com/\">https://github.com/</a>.\nKlick on the green name on the left from <code>https://</code> and klick on\n<code>Certificates</code>. On the <code>Details</code> tab, you'll see the \ncertificate chain, which is:</p>\n\n<pre><code>DigiCert ...\n  DigiCert ...\n   github.com ...\n</code></pre></li>\n<li><p>Export each of the DigiCert certicates to a file.</p></li>\n<li>copy the files to <code>/etc/ssl/certs/</code></li>\n<li>run <code>c_rehash</code> which cat all certificates to <code>ca-certificates.crt</code></li>\n<li>you are done.</li>\n</ol>\n\n<p>As I said, I am not a friend of such actions because github can change the CA's anytime, \nso it will always result in additional work.</p>\n",
    "url": "https://unix.stackexchange.com/questions/109880/adding-ssl-certificate-for-github-only-not-all-certificates-from-ca-certificate"
  },
  {
    "question_title": "git status with submodule shows &quot;new commits&quot;",
    "question_body": "<p>If I have a project with submodules, make changes in the submodule, and then commit those changes, the main project will show \"new commits\" on that submodule.  </p>\n\n<p>However, if I \"git pull\" and update my local project, and in the pull comes a change in the submodule commit, \"git status\" will also show \"new commits\".</p>\n\n<p>I'm confused that \"new commits\" can either mean \"you have local changes you need to commit\", or \"you have been updated with a new reference that you need to update\".</p>\n\n<p>It seems like \"new commits\" can tell you two totally opposite things.</p>\n\n<p>Is there a better way to know from the top level if you have changes you need to push, vs you have a new reference you need to update to?</p>\n",
    "answer": "<p>Comment from mfurseman is the best solution, as far as I know:</p>\n<pre><code>git submodule update --init --recursive\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/537893/git-status-with-submodule-shows-new-commits"
  },
  {
    "question_title": "How to use &quot;less -F&quot; without &quot;-X&quot;, but still display output if only one page?",
    "question_body": "<p>I'm tweaking the pager of Git, but I've got some issues with it.</p>\n\n<p>What I want is:</p>\n\n<ol>\n<li>Always colored output</li>\n<li>Scrolling by touchpad or mouse</li>\n<li>Quit-if-one-screen</li>\n</ol>\n\n<p>And my current configuration is:</p>\n\n<pre><code>$ git config --global core.pager\nless -+F -+X -+S\n</code></pre>\n\n<p>This does everything except the last one.</p>\n\n<p>But, if I remove <code>-+F</code>, there will be no output in case of one-screen. If I remove <code>-+X</code> as well, the output is back but I cannot scroll by touchpad in <code>less</code>.</p>\n\n<p>Is there a workaround which can meet all the requirements above?</p>\n",
    "answer": "<p><strong><em>UPDATE</em></strong></p>\n\n<p>tl;dr Solution: upgrade to less 530</p>\n\n<p>From <a href=\"http://www.greenwoodsoftware.com/less/news.530.html\" rel=\"noreferrer\">http://www.greenwoodsoftware.com/less/news.530.html</a>:</p>\n\n<blockquote>\n  <p>Don't output terminal init sequence if using -F and file fits on one screen.</p>\n</blockquote>\n\n<p>So with this fix we don't even need to bother determining whether to use <code>-X</code> on our own, <code>less -F</code> just takes care of it.</p>\n\n<p>PS. Some other less configs that I use:</p>\n\n<pre><code>export PAGER='less -F -S -R -M -i'\nexport MANPAGER='less -R -M -i +Gg'\ngit config --global core.pager 'less -F -S -R -i'\n#alias less='less -F -S -R -M -i'\n</code></pre>\n\n<hr>\n\n<p>I eventually ended up with writing a wrapper on my own.</p>\n\n<pre><code>#!/usr/local/bin/bash\n\n# BSD/OSX compatibility\n[[ $(type -p gsed) ]] &amp;&amp; SED=$(type -p gsed) || SED=$(type -p sed)\nCONTEXT=$(expand &lt;&amp;0)\n[[ ${#CONTEXT} -eq 0 ]] &amp;&amp; exit 0\nCONTEXT_NONCOLOR=$( $SED -r \"s/\\x1B\\[([0-9]{1,2}(;[0-9]{1,2})?)?[mGK]//g\" &lt;&lt;&lt; \"$CONTEXT\")\nLINE_COUNT=$( (fold -w $(tput cols) | wc -l) &lt;&lt;&lt; \"$CONTEXT_NONCOLOR\" )\n\n[[ $LINE_COUNT -ge $(tput lines) ]] &amp;&amp; less -+X -+S -R &lt;&lt;&lt; \"$CONTEXT\" || echo \"$CONTEXT\"\n</code></pre>\n\n<p>BSD/OSX users should manually install <code>gnu-sed</code>. The amazing regexp, which helps remove color codes, is from <a href=\"https://stackoverflow.com/a/18000433/2487227\">https://stackoverflow.com/a/18000433/2487227</a></p>\n\n<p>I've saved this script to <code>/usr/local/bin/pager</code> and then <code>git config --global core.pager /usr/local/bin/pager</code></p>\n\n<p><s>The treatment for OCD patients, hooray!</s></p>\n",
    "url": "https://unix.stackexchange.com/questions/329093/how-to-use-less-f-without-x-but-still-display-output-if-only-one-page"
  },
  {
    "question_title": "what does &quot; gbp:error: upstream/1.5.13 is not a valid treeish&quot; mean?",
    "question_body": "<p>I want to build a debian package with git build package.(gbp)\nI passed all steps, and at least, when I entered <code>gbp buildpackage</code>, This error appeared.</p>\n\n<p>what does it mean?\nand what should I do?</p>\n\n<pre><code>gbp:error: upstream/1.5.13 is not a valid treeish\n</code></pre>\n",
    "answer": "<p>This can be caused by the tarball not being present in the parent directory. I get this (highly crypric) message even with a debian/ folder present. </p>\n\n<p>My solution was to use uscan to get the watch file to download a fresh copy of the tarball</p>\n\n<pre><code>uscan --force-download\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/167533/what-does-gbperror-upstream-1-5-13-is-not-a-valid-treeish-mean"
  },
  {
    "question_title": "Colored word-diff just like git&#39;s?",
    "question_body": "<p>I've been for long using git's (colored) <code>--word-diff</code> which I find awesome. I found myself today in a situation where I had two strings I needed to compare word by word and craving for something just like git's diff.</p>\n\n<p>I am aware of <code>wdiff</code> + <code>colordiff</code>, but its results certainly aren't stellar:</p>\n\n<pre><code>me@me:~$ wdiff &lt;(echo -e \"abc\\ndef\") &lt;(echo -e \"dbcx\\ndef\") | colordiff\n[-abc-]{+dbcx+}\ndef\n</code></pre>\n\n<p>Is there any better option around? I don't particularly like those <code>[-</code>, <code>-]</code> and <code>{+</code>, <code>+}</code>.</p>\n",
    "answer": "<p>You can use <code>git</code>'s own <a href=\"https://github.com/git/git/tree/master/contrib/diff-highlight\" rel=\"noreferrer\">diff-highlight</a>.  It can highlight word differences in unified <code>diff</code> output, and it can also cope with ANSI colors on input.  So you can do something like this:</p>\n\n<pre><code>colordiff -u &lt;(echo -e \"foo abc\\ndef\") &lt;(echo -e \"foo dbcx\\ndef\") | diff-highlight\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/307095/colored-word-diff-just-like-gits"
  },
  {
    "question_title": "Git: edit previous commits&#39; messages only",
    "question_body": "<p>For lazy reasons I pushed a bunch of commits with default messages and now it has become cumbersome, as I don't really know what I've changed in each commit.</p>\n\n<p>How do I edit just the messages of previous commits and (if possible) keep the commit tree?</p>\n",
    "answer": "<p>To edit the commit messages of a series of commits, I run</p>\n<pre><code>git rebase -i firstsha\n</code></pre>\n<p>where <code>firstsha</code> is an identifier for the parent commit of the first commit I want to edit. (You can use any valid reference here, so <code>git rebase -i HEAD~4</code> will show the last four commits.)</p>\n<p>In the editor that opens, change all the “pick” entries to “reword” on commits you wish to modify, then close the editor; you will then be asked to enter commit messages for all the commits you chose.</p>\n<p>Note that this <em>will</em> change the commit tree, because the hashes of the commits will change. You will have to force-push your new tree, or push it to a new branch. Take care when rebasing merges; you’ll need to use <code>-r</code> (<code>--rebase-merges</code>), and read <a href=\"https://git-scm.com/docs/git-rebase#_rebasing_merges\" rel=\"nofollow noreferrer\">the “Rebasing merges” section of the <code>git rebase</code> manpage</a>.</p>\n<p>To quickly edit only the last commit, run</p>\n<pre><code>git commit --amend\n</code></pre>\n<p>(but beware of anything already staged for commit).</p>\n",
    "url": "https://unix.stackexchange.com/questions/485918/git-edit-previous-commits-messages-only"
  },
  {
    "question_title": "Unmet dependencies while installing Git on Debian",
    "question_body": "<p>I am attempting to install git on <code>Debian 8.6 Jessie</code> and have run into some dependency issues. What's odd is that I didn't have any issues the few times I recently installed <code>Git</code> in a VM while I was getting used to Linux.</p>\n\n<p><code>apt-get install git</code> </p>\n\n<p><strong>Results in</strong>:</p>\n\n<pre><code>The following packages have unmet dependencies:\n  git : Depends: liberror-perl but is not installable\n        Recommends: rsync but it is not installable\nE: Unable to correct problems, you have held broken packages.\n</code></pre>\n\n<p><strong>UPDATE</strong></p>\n\n<p>my <code>sources.list</code></p>\n\n<p><a href=\"https://i.sstatic.net/xqXdc.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xqXdc.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Seems to be an issue with my system. I can no longer properly install anything. I'm getting dependency issues installing things like <code>Pulseaudio</code> which I've previously installed successfully a few days ago.</p>\n",
    "answer": "<p>You should edit your sources.list , by adding the following line:</p>\n\n<pre><code>deb http://ftp.ca.debian.org/debian/ jessie main contrib\n</code></pre>\n\n<p>Then upgrade your package and install <code>git</code>:</p>\n\n<pre><code>apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get dist-upgrade\napt-get -f install\napt-get install git\n</code></pre>\n\n<p><strong>Edit</strong></p>\n\n<p>the following package <a href=\"https://packages.debian.org/jessie/amd64/git/download\" rel=\"noreferrer\"><code>git</code></a> , <a href=\"https://packages.debian.org/jessie/all/liberror-perl/download\" rel=\"noreferrer\"><code>liberror-perl</code></a> and <code>[rsync</code>]<a href=\"https://packages.debian.org/jessie/amd64/rsync/download\" rel=\"noreferrer\">3</a> can be downloaded from the <code>main</code> repo , because you don't have the <code>main</code> repo on your <code>sources.list</code> you cannot install <code>git</code> and its dependencies .</p>\n\n<p>Your <code>sources.list</code> should be (with <code>non-free</code> packages):</p>\n\n<pre><code>deb http://ftp.ca.debian.org/debian/ jessie main contrib non-free\ndeb-src http://ftp.ca.debian.org/debian/ jessie main contrib non-free\n\ndeb http://security.debian.org/ jessie/updates main contrib non-free\ndeb-src http://security.debian.org/ jessie/updates main contrib non-free\n\ndeb http://ftp.ca.debian.org/debian/ jessie-updates main contrib non-free\ndeb-src http://ftp.ca.debian.org/debian/ jessie-updates main contrib non-free\n\ndeb http://ftp.ca.debian.org/debian/ jessie-backports main contrib non-free\n</code></pre>\n\n<p>On debian Stretch your <code>/etc/apt/sources.list</code> should be (at least):</p>\n\n<pre><code>deb http://deb.debian.org/debian stretch main\ndeb http://security.debian.org/ stretch/updates main \ndeb http://deb.debian.org/debian/ stretch-updates main\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/332862/unmet-dependencies-while-installing-git-on-debian"
  },
  {
    "question_title": "Does git contain information about used id / group id changes?",
    "question_body": "<p>I have googling awhile, but can not find such information. Looks like git doesn't contain users and groups, only permissions. Am I right?</p>\n",
    "answer": "<p>See the <a href=\"https://git.wiki.kernel.org/index.php/ContentLimitations\">Content Limitations</a> section of the git Wiki: git does not track file ownership, group membership, doesn't track most permission bits, ACLs, access and modification times, etc.</p>\n\n<p>Git tracks contents, and doesn't care much about pretty much everything else.</p>\n",
    "url": "https://unix.stackexchange.com/questions/101510/does-git-contain-information-about-used-id-group-id-changes"
  },
  {
    "question_title": "Tool for searching across branches and through history in a Git repository",
    "question_body": "<p>I'm looking for a tool, or for suggestions towards a script, that would be able to search a Git repository for files based on both filenames and file contents (<code>find</code>/<code>grep</code>-like).  It would need to be able to search not just in the currently checked out branch, but through that branch's history as well as in other branches.</p>\n\n<p>The very specific example of a Git repository that I have is a checkout of <a href=\"https://github.com/dspinellis/unix-history-repo/\" rel=\"noreferrer\">dspinellis' <code>unix-history-repo</code></a>.  I often look for historical implementations of things in there, but it's a pain to track down files as I often need to <em>guess</em> what branch I need to be looking at (there are 165 branches in that repository).</p>\n\n<p><strong>Example of thing I would like to do with this tool</strong> is to <a href=\"https://unix.stackexchange.com/questions/493709/does-anyone-remember-wow\">find the <code>wow</code> command</a>, which may have been an external command or a built-in command in <code>sh</code> or <code>csh</code> at some point, if it existed as part of some early BSD Unix (if it existed at all).  To do this, I would want to search for <code>wow.*</code> as a filename pattern, and also for files that may at some point have included a <code>wow</code> C function, across the 165 branches of dspinellis' Git repository.</p>\n",
    "answer": "<p>Heh, guess what I’ve been doing too...</p>\n\n<p>To look for a file name across all branches, I use</p>\n\n<pre><code>git log --all --name-only --pretty=format:%H -- wow\\*\n</code></pre>\n\n<p><code>wow</code> can be replaced by any glob. This runs quite quickly on the Unix history repository. The format shows the hash leading to the creation of the matching file, so you can then check the tree out at that point and explore further.</p>\n\n<p>To search file contents across all branches, I use</p>\n\n<pre><code>git rev-list --all | xargs git grep \"excited too\"\n</code></pre>\n\n<p>which lists all commit objects and searches through them. This is very, very slow on the Unix history repository; listing all the branches and grepping there is quicker:</p>\n\n<pre><code>git grep \"excited too\" $(git branch -r | awk '{print $1}')\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/493747/tool-for-searching-across-branches-and-through-history-in-a-git-repository"
  },
  {
    "question_title": "Git based package manager/installer for Linux",
    "question_body": "<p>Is there any linux distro that has a git based package manager/installer. I want something similar to FreeBSD Ports (which is CVS based, I think) or Mac OS X <a href=\"https://brew.sh/\" rel=\"nofollow noreferrer\">Homebrew</a> (git based).</p>\n",
    "answer": "<p>There's <a href=\"http://exherbo.org/\" rel=\"nofollow\">Exherbo</a>, which uses multiple Git repositories to store its exheres (its term for build recipes similar to Gentoo's ebuilds or BSD's ports).  It's still a pretty young distribution, though.</p>\n\n<p><strong>Update:</strong> <a href=\"https://www.gentoo.org/\" rel=\"nofollow\">Gentoo</a> recently <a href=\"https://www.gentoo.org/news/2015/08/12/git-migration.html\" rel=\"nofollow\">moved to Git for its package repository</a>.  However, I don't think it's set up yet to have normal users get updates via Git (although I believe it's planned to allow that).</p>\n",
    "url": "https://unix.stackexchange.com/questions/36357/git-based-package-manager-installer-for-linux"
  },
  {
    "question_title": "How do I encrypt git on my server?",
    "question_body": "<p>Heres the closest I've gotten: I installed gitolite in the <code>/Private</code> folder using <code>ecryptfs-utils</code>  (<code>sudo apt-get install ecryptfs-utils</code> <code>adduser git</code> <code>ecryptfs-setup-private</code> then the rest was configuring gitolite using a root install).</p>\n\n<p>It worked just fine as long as someone was logged in as the user <code>git</code> using a password (<code>su git</code> using root does not work). Since the private folder activates through logging in with a password and gitolite uses RSA keys (required) the private folder is hidden thus error occurs.</p>\n\n<p>Is there a way I can log into my server after a reboot, type in the password and have the git user private folder available until next time the machine restarts?</p>\n\n<p>Or maybe theres an easy way to encrypt a folder for git repositories?</p>\n",
    "answer": "<p>You simply need to remove the file <code>~/.ecryptfs/auto-umount</code>.</p>\n\n<p>This file is a flag that pam_ecryptfs checks on logout.  This file exists by default at setup, along with <code>~/.ecryptfs/auto-mount</code>, such that your private directory is automatically mounted and unmounted at login/logout.  But each can be removed independently to change that behavior.  Enjoy!</p>\n",
    "url": "https://unix.stackexchange.com/questions/10378/how-do-i-encrypt-git-on-my-server"
  },
  {
    "question_title": "Is there a way to prevent git from changing permissions and ownership on pull?",
    "question_body": "<p>Every time I do <code>git pull</code> or <code>git reset</code>, <code>git</code> resets changes to permissions and ownership I made. See for yourself:</p>\n\n<pre><code>#!/usr/bin/env bash\nrm -rf 1 2\n\nmkdir 1\ncd 1\ngit init\necho 1 &gt; 1 &amp;&amp; git add 1 &amp;&amp; git ci -m 1\n\ngit clone . ../2\ncd $_\nchmod 0640 1\nchgrp http 1\n\ncd ../1\necho 12 &gt; 1 &amp;&amp; git ci -am 2\n\ncd ../2\nstat 1\ngit pull\nstat 1\n</code></pre>\n\n<p>The output:</p>\n\n<pre><code>$ ./1.sh 2&gt;/dev/null | grep -F 'Access: ('\nAccess: (0640/-rw-r-----)  Uid: ( 1000/    yuri)   Gid: (   33/    http)\nAccess: (0664/-rw-rw-r--)  Uid: ( 1000/    yuri)   Gid: ( 1000/    yuri)\n</code></pre>\n\n<p>Is there a way to work around it?</p>\n\n<p>I want to make some files/directories accessible for writing by the web server.</p>\n",
    "answer": "<p>This sounds like the user you're running has the default group set to <code>yuri</code>. You can confirm this like so:</p>\n\n<pre><code>$ id -a\nuid=1000(saml) gid=1000(saml) groups=1000(saml),10(wheel),989(wireshark)\n</code></pre>\n\n<p>The UID of your account is this: <code>uid=1000(saml)</code> whereas the default group is <code>git=1000(saml)</code> and any secondary groups are thereafter.</p>\n\n<p><strong>NOTE:</strong> If you want the git clone to have specific ownership, then you have at least 2 options.</p>\n\n<h3>Option #1</h3>\n\n<p>Set a parent directory with the permissions as you want like so:</p>\n\n<pre><code>$ mkdir topdir\n$ chgrp http topdir\n$ chmod g+s topdir\n\n$ cd topdir\n$ git clone ....\n</code></pre>\n\n<p>This forced the directory <code>topdir</code> to enforce any child directories underneath it to have the group <code>http</code> applied. This will work by in large but can lead to problems, since if you move files into this git clone workspace, those files will not have their groups enforced by the changes made above.</p>\n\n<h3>Option #2</h3>\n\n<p>Prior to doing work, change your default group to <code>http</code> like so:</p>\n\n<pre><code>$ newgrp http\n$ git clone ...\n</code></pre>\n\n<p>This method will force any new files created to have their group set to <code>http</code> instead of your normal default group of <code>yuri</code>, but this will only work so long as you remember to do a <code>newgrp</code> prior to working in this workspace.</p>\n\n<h3>Other options</h3>\n\n<p>If neither of these seem acceptable you can try using ACLs instead on the git workspace directory. These are discussed in multiple Q&amp;A's on this site, such as in this Q&amp;A titled: <a href=\"https://unix.stackexchange.com/questions/115631/getting-new-files-to-inherit-group-permissions-on-linux/115632#115632\">Getting new files to inherit group permissions on Linux</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/160527/is-there-a-way-to-prevent-git-from-changing-permissions-and-ownership-on-pull"
  },
  {
    "question_title": "Zsh git filename completion with &quot;--git-dir=... --work-tree=...&quot;: not a git repository",
    "question_body": "<p>I track my dotfiles using this method:</p>\n\n<ul>\n<li>A bare repository resides in <code>$HOME/repos/dotfiles</code>.</li>\n<li>All my dotfiles reside in their normal location, e.g. <code>$HOME/.vim/vimrc</code>, not <code>$HOME/repos/dotfiles/vimrc</code>.</li>\n<li>I run <code>git --git-dir=$HOME/repos/dotfiles --work-tree=$HOME ...</code> to manage things.</li>\n</ul>\n\n<p>(Actually, I have a function <code>g()</code> that expands to the above command when I'm in <code>$HOME</code>, and to just <code>git</code> otherwise.)</p>\n\n<p>Everything works great, except...</p>\n\n<p><strong>The problem:</strong> Zsh git filename completion doesn't work.</p>\n\n<p><strong>Example:</strong></p>\n\n<pre><code>% pwd\n/home/brian\n% g status                                                                                                                                         ~\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n\n        modified:   .vim/vimrc\n        modified:   .xmonad/xmonad.hs\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n% g add &lt;Tab&gt;\nCompleting not a git repository\n</code></pre>\n\n<p>(The \"Completing ...\" stuff is due to <code>zstyle ':completion:*' format $'%{\\e[0;31m%}Completing %B%d%b%{\\e[0m%}'</code>.)</p>\n\n<p>Notably, it would not suffice to somehow tell zsh's git completion to \"move to/follow\" the given value of <code>--work-tree</code>, as if <code>git</code> were being called from that directory, because explicitly doing that doesn't work either:</p>\n\n<pre><code>% cd repos/dotfiles\n% g status\nfatal: This operation must be run in a work tree\n% g add &lt;Tab&gt;\nCompleting not a git repository\n</code></pre>\n\n<p><strong>The question:</strong> Is there an easy way to get extend zsh's git completion to this kind of a case?</p>\n",
    "answer": "<p>Sadly, its a bug in <code>zsh</code> completion for git. You can find discussion on 'zsh' mailing list <a href=\"http://www.zsh.org/mla/workers/2017/msg01186.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n\n<p>Daniel Shahaf did provided a patch for '_git':</p>\n\n<pre><code>diff --git a/Completion/Unix/Command/_git b/Completion/Unix/Command/_git\nindex 518e6d198..45a0fa622 100644\n--- a/Completion/Unix/Command/_git\n+++ b/Completion/Unix/Command/_git\n@@ -6609,20 +6609,33 @@ __git_files_relative () {\n (( $+functions[__git_files] )) ||\n __git_files () {\n   local compadd_opts opts tag description gitcdup gitprefix files expl\n+  local pref\n\n   zparseopts -D -E -a compadd_opts V: J: 1 2 n f X: M: P: S: r: R: q F:\n   zparseopts -D -E -a opts -- -cached -deleted -modified -others -ignored -unmerged -killed x+: --exclude+:\n   tag=$1 description=$2; shift 2\n\n-  gitcdup=$(_call_program gitcdup git rev-parse --show-cdup 2&gt;/dev/null)\n-  __git_command_successful $pipestatus || return 1\n+  case $(_call_program gitinworktree git rev-parse --is-inside-work-tree 2&gt;/dev/null) in\n+    (true)\n+      gitcdup=$(_call_program gitcdup git rev-parse --show-cdup 2&gt;/dev/null)\n+      __git_command_successful $pipestatus || return 1\n\n-  gitprefix=$(_call_program gitprefix git rev-parse --show-prefix 2&gt;/dev/null)\n-  __git_command_successful $pipestatus || return 1\n+      gitprefix=$(_call_program gitprefix git rev-parse --show-prefix 2&gt;/dev/null)\n+      __git_command_successful $pipestatus || return 1\n+\n+      local pref=$gitcdup$gitprefix$PREFIX\n+      ;;\n+    (false)\n+      local pref=\n+      ;;\n+    (*)\n+      # XXX what to do?\n+      return 1\n+      ;;\n+  esac\n\n   # TODO: --directory should probably be added to $opts when --others is given.\n\n-  local pref=$gitcdup$gitprefix$PREFIX\n\n   # First allow ls-files to pattern-match in case of remote repository\n   files=(${(0)\"$(_call_program files git ls-files -z --exclude-standard ${(q)opts} -- ${(q)${pref:+$pref\\\\\\*}} 2&gt;/dev/null)\"})\n@@ -7585,7 +7598,8 @@ _git() {\n         ;;\n       (option-or-argument)\n         curcontext=${curcontext%:*:*}:git-$words[1]:\n-       (( $+opt_args[--git-dir] )) &amp;&amp; local -x GIT_DIR=$opt_args[--git-dir]\n+        (( $+opt_args[--git-dir] )) &amp;&amp; local -x GIT_DIR=${(e)opt_args[--git-dir]}\n+        (( $+opt_args[--work-tree] )) &amp;&amp; local -x GIT_WORK_TREE=${(e)opt_args[--work-tree]}\n        if ! _call_function ret _git-$words[1]; then\n          if zstyle -T :completion:$curcontext: use-fallback; then\n            _default &amp;&amp; ret=0\n</code></pre>\n\n<p>It applies clearly to <code>zsh 5.4.1</code> but it didn't work form me,YMMV.</p>\n\n<p>I'll update this answer as the issue is being worked on.</p>\n\n<p>EDIT:</p>\n\n<p>With the above patch it does work, but where you put <code>add</code> is important - it has to be at the end:</p>\n\n<pre><code>git --git-dir=$HOME/.dotfiles --work-tree=$HOME/ add\n</code></pre>\n\n<p>One more thing worth noticing - its somewhat slow.</p>\n",
    "url": "https://unix.stackexchange.com/questions/350797/zsh-git-filename-completion-with-git-dir-work-tree-not-a-git-repo"
  },
  {
    "question_title": "Use git submodule foreach with function",
    "question_body": "<p>My indention is to have a script that updates all git submodules according to which branch is given. If there's no such branch for a submodule, master is used.</p>\n\n<p>This is what I have now:</p>\n\n<pre><code>#!/bin/bash -x\n\nif [ -z $1 ]; then\n    echo \"Branch name required.\"\n    exit\nfi\n\nfunction pbranch {\n    exists=`git show-ref refs/heads/$branch`\n\n    if [ -z $exists ]; then\n        branch=\"master\"\n    fi\n\n    git co $branch\n    git pull origin $branch\n}\n\nbranch=$1\n\ngit submodule foreach pbranch\n</code></pre>\n\n<p>But when running this script, the error is thrown:</p>\n\n<pre><code>oleq@pc ~/project&gt; git-fetchmodules major\n+ '[' -z major ']'\n+ branch=major\n+ git submodule foreach pbranch\nEntering 'submodule'\n/usr/lib/git-core/git-submodule: 1: eval: pbranch: not found\nStopping at 'submodule'; script returned non-zero status.\n</code></pre>\n\n<p>My guess is that <code>git submodule foreach</code> utilizes eval (according to the <a href=\"https://www.kernel.org/pub/software/scm/git/docs/git-submodule.html\" rel=\"noreferrer\">documentation</a>), which I don't use correctly in this context. </p>\n\n<p>There are billions of <a href=\"https://stackoverflow.com/questions/1030169/git-easy-way-pull-latest-of-all-submodules\">examples</a> of how to use this command with \"inline callback\" but I couldn't found a single one with the callback in form of the function. Any idea how to solve this?</p>\n",
    "answer": "<p>I resolved my issue by putting the function inside of the quotes as a callback:</p>\n\n<pre><code>#!/bin/bash\n\nif [ -z $1 ]; then\n    echo \"Branch name required.\"\n    exit\nfi\n\ngit submodule foreach \"\n    branch=$1;\n    exists=\\$(git show-ref refs/heads/\\$branch | cut -d ' ' -f1);\n\n    if [ -z \\$exists ]; then\n        branch='master';\n    fi;\n\n    echo Checking branch \\$branch for submodule \\$name.;\n\n    git fetch --all -p;\n    git co \\$branch;\n    git reset --hard origin/\\$branch;\n\"\n</code></pre>\n\n<p>Note that variables like <code>$1</code> are those from the script's namespace. The \"escaped ones\" like <code>$\\(bar)</code>, <code>\\$branch</code> are evaluated within \"the callback\". It was pretty easy.</p>\n",
    "url": "https://unix.stackexchange.com/questions/83468/use-git-submodule-foreach-with-function"
  },
  {
    "question_title": "diff --git unknown option",
    "question_body": "<p>when I try to run </p>\n\n<pre><code>diff --git a/drivers/cpufreq/intel_pstate.c b/drivers/cpufreq/intel_pstate.c\n</code></pre>\n\n<p>I get unknown option (I have installed git via <code>apt-get install git</code>)</p>\n",
    "answer": "<p>Either use </p>\n\n<pre><code>diff -u file1 file2\n</code></pre>\n\n<p>or</p>\n\n<pre><code>git diff branch/commit1 branch/commit2\n</code></pre>\n\n<p>More on git diff at <a href=\"https://www.kernel.org/pub/software/scm/git/docs/git-diff.html\">https://www.kernel.org/pub/software/scm/git/docs/git-diff.html</a></p>\n\n<p>I am not aware of any <code>--git</code> option however for <code>diff</code> and the man page doesn't show it.</p>\n",
    "url": "https://unix.stackexchange.com/questions/102287/diff-git-unknown-option"
  },
  {
    "question_title": "Compress a folder with tar?",
    "question_body": "<p>I'm trying to compress a folder (<code>/var/www/</code>) to <code>~/www_backups/$time.tar</code> where <code>$time</code> is the current date.</p>\n\n<p>This is what I have:</p>\n\n<pre><code>cd /var/www &amp;&amp; sudo tar -czf ~/www_backups $time\"\n</code></pre>\n\n<p>I am completely lost and I've been at this for hours now. Not sure if <code>-czf</code> is correct. I simply want to copy all of the content in <code>/var/www</code> into a <code>$time.tar</code> file, and I want to maintain the file permissions for all of the files. Can anyone help me out?</p>\n",
    "answer": "<p>To <code>tar</code> and <code>gzip</code> a folder, the syntax is:</p>\n<pre><code>tar czf name_of_archive_file.tar.gz name_of_directory_to_tar\n</code></pre>\n<p>Adding <code>-</code> before the options (<code>czf</code>) is optional with <code>tar</code>. The effect of <code>czf</code> is as follows:</p>\n<ul>\n<li><code>c</code> — create an archive file (as opposed to extract, which is <code>x</code>)</li>\n<li><code>f</code> — filename of the archive file</li>\n<li><code>z</code> — filter archive through <code>gzip</code> (remove this option to create a <code>.tar</code> file)</li>\n</ul>\n<p>If you want to <code>tar</code> the current directory, use <code>.</code> to designate that.</p>\n<p>To construct filenames dynamically, use the <code>date</code> utility (look at its man page for the available format options). For example:</p>\n<pre><code>cd /var/www &amp;&amp;\ntar czf ~/www_backups/$(date +%Y%m%d-%H%M%S).tar.gz .\n</code></pre>\n<p>This will create a file named something like <code>20120902-185558.tar.gz</code>.</p>\n<p>On Linux, chances are your <code>tar</code> also supports BZip2 compression with the <code>j</code> rather than <code>z</code> option. And possibly others. Check the man page on your local system.</p>\n",
    "url": "https://unix.stackexchange.com/questions/46969/compress-a-folder-with-tar"
  },
  {
    "question_title": "How to extract specific file(s) from tar.gz",
    "question_body": "<p>How can we extract specific files from a large tar.gz file? I found <a href=\"https://unix.stackexchange.com/questions/42198/untar-only-a-certain-number-of-files-from-a-large-tarball/42199#42199\">the process of extracting files from a tar in this question</a> but, when I tried the mentioned command there, I got the error:</p>\n\n<pre><code>$ tar --extract --file={test.tar.gz} {extract11}\ntar: {test.tar.gz}: Cannot open: No such file or directory\ntar: Error is not recoverable: exiting now\n</code></pre>\n\n<p>How do I then extract a file from <code>tar.gz</code>?</p>\n",
    "answer": "<p>List the paths in tar:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>tar -ztf test.tar.gz\n</code></pre>\n<p>Note the file name / path exactly as it is listed. e.g. If it says e.g. <code>./extract11</code>, or <code>some/bunch/of/dirs/extract11</code> that's what you have to use.</p>\n<p>You can extract that file in this format:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>tar -zxvf &lt;tar filename&gt; &lt;file you want to extract&gt;\n</code></pre>\n<p>The file the file will show up under exactly that name, needed directories are created automatically.</p>\n<hr />\n<p>Options used:</p>\n<ul>\n<li><code>-z</code>: filter archive through gzip, use to decompress .gz files.</li>\n<li><code>-t</code>: List the contents of an archive</li>\n<li><code>-x</code>: instructs tar to extract files.</li>\n<li><code>-v</code>: Verbose (show progress while extracting files).</li>\n<li><code>-f</code>: specifies filename / tarball name.</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/61461/how-to-extract-specific-files-from-tar-gz"
  },
  {
    "question_title": "tar: Removing leading `/&#39; from member names",
    "question_body": "<pre><code>root@server # tar fcz bkup.tar.gz /home/foo/\ntar: Removing leading `/' from member names\n</code></pre>\n\n<p>How can I solve this problem and keep the <code>/</code> on file names ?</p>\n",
    "answer": "<p>If you want to get rid of &quot;Removing leading `/' from member names&quot; being printed to STDERR, but still want to leave off those leading slashes as tar wisely does by default, I saw an excellent solution <a href=\"http://www.linuxquestions.org/questions/linux-general-1/bin-tar-removing-leading-%60-from-member-names-269508/\" rel=\"nofollow noreferrer\">here</a> by commenter timsoft.</p>\n<p>The solution involves using -C option to change directory to the root (/), then specifying the file tree to archive <strong>without</strong> a leading slash, because now you only need a relative path.  This does the same thing as a normal tar create command, but no stripping is needed:</p>\n<pre><code>tar fcz bkup.tar.gz -C / home/foo/\n</code></pre>\n<p>or</p>\n<pre><code>cd / &amp;&amp; tar fcz $HOME/bkup.tar.gz home/foo/*\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/59243/tar-removing-leading-from-member-names"
  },
  {
    "question_title": "How do you extract a single folder from a large tar.gz archive?",
    "question_body": "<p>I am using this command on a <strong>5GB</strong> archive</p>\n\n<pre><code>tar -zxvf archive.tar.gz /folder/in/archive\n</code></pre>\n\n<p>is this the correct way to do this?  It seems to be taking forever with no command line output...</p>\n",
    "answer": "<p><code>tar</code> stores relative paths <em>by default</em>. GNU tar even says so if you try to store an absolute path:</p>\n\n<pre><code>tar -cf foo.tar /home/foo\ntar: Removing leading `/' from member names\n</code></pre>\n\n<p>If you need to extract a particular folder, have a look at what's in the tar file:</p>\n\n<pre><code>tar -tvf foo.tar\n</code></pre>\n\n<p>And note the exact filename. In the case of my <code>foo.tar</code> file, I could extract <code>/home/foo/bar</code> by saying:</p>\n\n<pre><code>tar -xvf foo.tar home/foo/bar # Note: no leading slash\n</code></pre>\n\n<p>So no, the way you posted isn't (necessarily) the correct way to do it. You have to leave out the leading slash. If you want to simulate absolute paths, do <code>cd /</code> first and make sure you're the superuser. Also, this does the same:</p>\n\n<pre><code>tar -C / -xvf foo.tar home/foo/bar # -C is the ‘change directory’ option\n</code></pre>\n\n<p>There are very obvious, good reasons why <code>tar</code> converts paths to relative ones. One is the ability to restore an archive in places other than its original source. The other is security. You could extract an archive, expect its files to appear in your current working directory, and instead overwrite system files (or your own work) elsewhere by mistake.</p>\n\n<p>Note: if you use the <code>-P</code> option, <code>tar</code> <strong>will</strong> archive absolute paths. So it always pays to check the contents of big archives before extracting.</p>\n",
    "url": "https://unix.stackexchange.com/questions/35311/how-do-you-extract-a-single-folder-from-a-large-tar-gz-archive"
  },
  {
    "question_title": "How to choose directory name during untarring",
    "question_body": "<p>Say I have a file named <code>ugly_name.tar</code>, which when extracted, becomes <code>ugly_name</code> directory. What command can I use so that the resulting directory name is <code>pretty_name</code> instead?</p>\n",
    "answer": "<p>This should work:</p>\n\n<pre><code>mkdir pretty_name &amp;&amp; tar xf ugly_name.tar -C pretty_name --strip-components 1\n</code></pre>\n\n<p><code>-C</code> changes to the specified directory before unpacking (or packing). <code>--strip-components</code> removes the specified number of directories from the filenames stored in the archive.</p>\n\n<p>Note that this is not really portable. GNU tar and at least some of the BSD tars have the <code>--strip-components</code> option, but doesn't seem to exist on other unix-like platforms.</p>\n\n<p>The dumb way of doing this would work pretty much everywhere though.</p>\n\n<pre><code>tar xf ugly_name.tar &amp;&amp; mv ugly_name pretty_name\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/11018/how-to-choose-directory-name-during-untarring"
  },
  {
    "question_title": "How to XZ a directory with TAR using maximum compression?",
    "question_body": "<p>So I need to compress a directory with max compression. </p>\n\n<p>How can I do it with <code>xz</code>? I mean I will need <code>tar</code> too because I can't compress a directory with only <code>xz</code>. Is there a oneliner to produce e.g. <code>foo.tar.xz</code>?</p>\n",
    "answer": "<p>With GNU <code>tar</code> 1.22 (2009) or later (on most shells, like Bash):</p>\n<pre class=\"lang-none prettyprint-override\"><code>XZ_OPT=-9 tar cJf tarfile.tar.xz directory\n</code></pre>\n<p><code>tar</code>'s <strong>uppercase</strong> <code>J</code> switch uses XZ (whereas the lowercase <code>j</code> switch uses bzip2).</p>\n<p><a href=\"https://tukaani.org/xz/man/xz.1.html#XZ_OPT\" rel=\"noreferrer\">The <code>XZ_OPT</code> environment variable</a> lets you set <code>xz</code> options.</p>\n<p>This is now <em><strong>maximal</strong></em>.</p>\n<p>See <a href=\"https://tukaani.org/xz/man/xz.1.html\" rel=\"noreferrer\"><code>man xz</code></a> for other options you can set (<code>-e</code>/<code>--extreme</code> <em>might</em> give you some additional compression benefit for some datasets).</p>\n<pre class=\"lang-none prettyprint-override\"><code>XZ_OPT=-e9 tar cJf tarfile.tar.xz directory\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/28976/how-to-xz-a-directory-with-tar-using-maximum-compression"
  },
  {
    "question_title": "Create a tar archive split into blocks of a maximum size",
    "question_body": "<p>I need to backup a fairly large directory, but I am limited by the size of individual files. I'd like to essentially create a <code>tar.(gz|bz2)</code> archive which is split into 200MB maximum archives. Clonezilla does something similar to this by splitting image backups named like so:</p>\n\n<pre><code>sda1.backup.tar.gz.aa\nsda1.backup.tar.gz.ab\nsda1.backup.tar.gz.ac\n</code></pre>\n\n<p>Is there a way I can do this in one command? I understand how to use the <code>split</code> command, but I'd like to not have to create one giant archive, then split it into smaller archives, as this would double the disk space I'd need in order to initially create the archive.</p>\n",
    "answer": "<p>You can pipe tar to the split command:</p>\n<pre><code>tar cvzf - dir/ | split --bytes=200MB - sda1.backup.tar.gz.\n</code></pre>\n<p>On some *nix systems (like OS X) you may get the following error:</p>\n<pre><code>split: illegal option -- -\n</code></pre>\n<p>In that case try this (note the <code>-b 200m</code>):</p>\n<pre><code>tar cvzf - dir/ | split -b 200m - sda1.backup.tar.gz.\n</code></pre>\n<p>If you happen to be trying to split the file to fit on a FAT32 formatted drive,\nuse a byte limit of 4294967295. For example:</p>\n<pre><code>tar cvzf - /Applications/Install\\ macOS\\ Sierra.app/ | \\\nsplit -b 4294967295 - /Volumes/UNTITLED/install_macos_sierra.tgz.\n</code></pre>\n<p>When you want to extract the files use the following command (as of @Naftuli Kay commented):</p>\n<pre><code>cat sda1.backup.tar.gz.* | tar xzvf -\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/61774/create-a-tar-archive-split-into-blocks-of-a-maximum-size"
  },
  {
    "question_title": "How to download an archive and extract it without saving the archive to disk?",
    "question_body": "<p>I'd like to download, and extract an archive under a given directory. Here is how I've been doing it so far:</p>\n\n<pre><code>wget http://downloads.mysql.com/source/dbt2-0.37.50.3.tar.gz\ntar zxf dbt2-0.37.50.3.tar.gz\nmv dbt2-0.37.50.3 dbt2\n</code></pre>\n\n<p>I'd like instead to download and extract the archive <em>on the fly</em>, without having the <code>tar.gz</code> written to the disk. I think this is possible by piping the output of <code>wget</code> to <code>tar</code>, and giving <code>tar</code> a target, but in practice I don't know how to put the pieces together.</p>\n",
    "answer": "<p>You can do it by telling <code>wget</code> to output its payload to stdout (with the flag <code>-O-</code>) and suppress its own output (with the flag <code>-q</code>). The output is then made the input (via stdin) to the <code>tar</code> command by a <em>pipe</em> (<code>|</code>):</p>\n<pre><code>wget -qO- your_link_here | gunzip | tar xvf -\n</code></pre>\n<p><code>f -</code> tells <code>tar</code> the archive is to be read from stdin. With some <code>tar</code> implementations, that's the default, in others, that's often a tape device.</p>\n<p>Some <code>tar</code> implementations can detect compressions and decompress by themselves in which case you can remove the <code>| gunzip</code>, some support a <code>z</code> option to decompress gzip-compressed archives on the fly by themselves (often by invoking <code>gunzip</code> themselves).</p>\n<p>To specify a target directory, if your <code>tar</code> supports <code>-C</code>:</p>\n<pre><code>wget -qO- your_link_here | gunzip | tar xvf - -C /target/directory\n</code></pre>\n<p>If not:</p>\n<pre><code>(cd /target/directory &amp;&amp; wget -qO- your_link_here | gunzip | tar xvf -)\n</code></pre>\n<p>If you happen to have GNU <code>tar</code>, you can also rename the output dir:</p>\n<pre><code>wget -qO- your_link_here | tar --transform 's/^dbt2-0.37.50.3/dbt2/' -xvzf -\n</code></pre>\n<p>In libarchive's <code>tar</code> (<code>bsdtar</code>), or <code>star</code>, the equivalent is with the <code>-s/pattern/replacement/</code> option like in the standard <code>pax</code> command.</p>\n",
    "url": "https://unix.stackexchange.com/questions/85194/how-to-download-an-archive-and-extract-it-without-saving-the-archive-to-disk"
  },
  {
    "question_title": "How to add/update a file to an existing tar.gz archive?",
    "question_body": "<p>Is there a way to add/update a file in a tar.gz archive? Basically, I have an archive which contains a file at <code>/data/data/com.myapp.backup/./files/settings.txt</code> and I'd like to pull that file from the archive (already done) and push it back into the archive once the edit has been done. How can I accomplish this? Is it problematic because of the <code>.</code> in the path? </p>\n",
    "answer": "<p>To pull your file from your archive, you can use <code>tar xzf archive.tar.gz my/path/to/file.txt</code>.  Note that the directories in the file's path will be created as well.  Use <code>tar t</code> (i.e. <code>tar tzf archive.tar.gz</code>) to list the files in the archive.</p>\n\n<p><code>tar</code> does not support \"in-place\" updating of files.  However, you can add files to the end of an archive, even if they have the same path as a file already in the archive.  In that case, both copies of the file will be in the archive, and the file added later will override the earlier one.  The command to use for this is <code>tar r</code> (or <code>tar u</code> to only add files that are newer than the archive) is the command to use.  The <code>.</code> in the path should not be a problem.</p>\n\n<p>There is a catch, though: you can't add to a compressed archive.  So you would have to do:</p>\n\n<pre><code>gunzip archive.tar.gz\ntar rf archive.tar data/data/com.myapp.backup/./files/settings.txt\ngzip archive.tar\n</code></pre>\n\n<p>Which is probably not what you want to hear, since it means rewriting the entire archive twice over.  If it's not a very large archive, it might be better to untar the whole thing and then re-tar it after editing.  Alternately, you could use an uncompressed archive.</p>\n",
    "url": "https://unix.stackexchange.com/questions/13093/how-to-add-update-a-file-to-an-existing-tar-gz-archive"
  },
  {
    "question_title": "tar --exclude doesn&#39;t exclude. Why?",
    "question_body": "<p>I have this very simple line in a bash script which executes successfully (i.e. producing the <code>_data.tar</code> file), except that it <strong>doesn't</strong> exclude the sub-directories it is told exclude via the <code>--exclude</code> option:</p>\n\n<pre><code>/bin/tar -cf /home/_data.tar  --exclude='/data/sub1/*'  --exclude='/data/sub2/*' --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'  /data\n</code></pre>\n\n<p>Instead, it produces a <code>_data.tar</code> file that contains everything under /data, including the files in the subdirectories I wanted to exclude.</p>\n\n<p>Any idea why? and how to fix this?</p>\n\n<p><strong>Update</strong> I implemented my observations based on the link provided in the first answer below (top level dir first, no whitespace after last exclude):</p>\n\n<pre><code>/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1/*'  --exclude='/data/sub2/*'  --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'\n</code></pre>\n\n<p>But that didn't help. All \"excluded\" sub-directories are present in the resulting <code>_data.tar</code> file.</p>\n\n<p>This is puzzling. Whether this is a bug in current tar (GNU tar 1.23, on a CentOS 6.2, Linux 2.6.32)  or \"extreme sensitivity\" of tar to whitespaces and other easy-to-miss typos, I consider this a bug. For now.</p>\n\n<p><strong>This is horrible</strong>: I tried the insight suggested below (no trailing <code>/*</code>) and it still doesn't work in the production script:</p>\n\n<pre><code>/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1'  --exclude='/data/sub2'  --exclude='/data/sub3'  --exclude='/data/sub4'\n</code></pre>\n\n<p>I can't see any difference between what I tried and what @Richard Perrin tried, except for the quotes and 2 spaces instead of 1. I am going to try this (must wait for the nightly script to run as the directory to be backed up is huge) and report back.</p>\n\n<pre><code>/bin/tar -cf /home/_data.tar  /data --exclude=/data/sub1 --exclude=/data/sub2 --exclude=/data/sub3 --exclude=/data/sub4\n</code></pre>\n\n<p>I am beginning to think that all these <code>tar --exclude</code> sensitivities aren't tar's but something in my environment, but then what could that be?</p>\n\n<p><strong>It worked!</strong> The last variation tried (no single-quotes and single-space instead of double-space between the <code>--exclude</code>s) tested working. Weird but accepting.</p>\n\n<p><strong>Unbelievable!</strong> It turns out that an older version of <code>tar</code> (1.15.1) would only exclude if the top-level dir is <strong>last</strong> on the command line. This is the exact opposite of how version 1.23 requires. FYI.</p>\n",
    "answer": "<p>It may be that your version of <code>tar</code> requires that the <code>--exclude</code> options have to be placed at the beginning of the <code>tar</code> command.</p>\n\n<p>See: <a href=\"https://stackoverflow.com/q/984204\">https://stackoverflow.com/q/984204</a></p>\n\n<pre><code>tar --exclude='./folder' --exclude='./upload/folder2' \\\n    -zcvf /backup/filename.tgz .\n</code></pre>\n\n<p>See: <a href=\"http://mandrivausers.org/index.php?/topic/8585-multiple-exclude-in-tar/\" rel=\"noreferrer\">http://mandrivausers.org/index.php?/topic/8585-multiple-exclude-in-tar/</a></p>\n\n<pre><code>tar --exclude=&lt;first&gt; --exclude=&lt;second&gt; -cjf backupfile.bz2 /home/*\n</code></pre>\n\n<p>Alternative:</p>\n\n<pre><code>EXCLD='first second third'\ntar -X &lt;(for i in ${EXCLD}; do echo $i; done) -cjf backupfile.bz2 /home/*\n</code></pre>\n\n<p>Yet another <code>tar</code> command tip is from <a href=\"http://thibauld.com/2010/02/how-to-exclude-multiple-directories-while-creating-an-archive-with-tar/\" rel=\"noreferrer\">here</a>:</p>\n\n<pre><code>tar cvfz myproject.tgz --exclude='path/dir_to_exclude1' \\\n                       --exclude='path/dir_to_exclude2' myproject\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/32845/tar-exclude-doesnt-exclude-why"
  },
  {
    "question_title": "creating a tar archive without including parent directory",
    "question_body": "<p>I am trying to create a graphical program for my script.</p>\n\n<p>Inside the script I use tar to create a tar archive.</p>\n\n<p>From the graphical program I get the full name of file that I want to create a tar archive.</p>\n\n<pre><code>tar -cvf temp.tar /home/username/dir1/dir2/selecteddir \n</code></pre>\n\n<p>My tar archive includes home, username, dir1, dir2 and selecteddir while i want tar to create archive only including selecteddir.</p>\n",
    "answer": "<p>You can use the <code>-C</code> option of <code>tar</code> to accomplish this:</p>\n\n<pre><code>tar -C /home/username/dir1/dir2 -cvf temp.tar selecteddir\n</code></pre>\n\n<p>From the man page of <code>tar</code>:</p>\n\n<pre><code>-C directory\n         In c and r mode, this changes the directory before adding the following files.  \n         In x mode, change directories after opening the archive but before extracting \n         entries from the archive.\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/168357/creating-a-tar-archive-without-including-parent-directory"
  },
  {
    "question_title": "Why would I tar a single file?",
    "question_body": "<p>At my company, we download a local development database snapshot as a <code>db.dump.tar.gz</code> file. The compression makes sense, but the tarball only contains a single file (<code>db.dump</code>).</p>\n\n<p>Is there any point to archiving a single file, or is <code>.tar.gz</code> just such a common idiom? Why not just <code>.gz</code>?</p>\n",
    "answer": "<p>Advantages of using <code>.tar.gz</code> instead of <code>.gz</code> are that</p>\n\n<ul>\n<li><code>tar</code> stores more meta-data (UNIX permissions etc.) than <code>gzip</code>.</li>\n<li>the setup can more easily be expanded to store multiple files</li>\n<li>.tar.gz files are very common, <em>only-gzipped</em> files may puzzle some users.\n(cf.  <a href=\"https://unix.stackexchange.com/questions/277793/is-there-any-point-in-tarring-a-single-file#comment482881_277793\">MelBurslans comment</a>)</li>\n</ul>\n\n<p>The overhead of using <code>tar</code> is also very small.</p>\n\n<p>If not really needed, I still do not recommend to tar a single file.\nThere are many useful tools which can access compressed single files directly (such as <code>zcat</code>, <code>zgrep</code> etc. - also existing for <code>bzip2</code> and <code>xz</code>).</p>\n",
    "url": "https://unix.stackexchange.com/questions/277793/why-would-i-tar-a-single-file"
  },
  {
    "question_title": "tar exits on &quot;Cannot stat: No such file of directory&quot;, why?",
    "question_body": "<p>I'm trying to create tar.gz file using the following command:</p>\n\n<pre><code>sudo tar -vcfz dvr_rdk_v1.tar.gz dvr_rdk/\n</code></pre>\n\n<p>It then start to create files (many files in folder), but then I get the following error:</p>\n\n<pre><code>tar: dvr_rdk_v1.tar.gz: Cannot stat: No such file or directory\ntar: Exiting with failure status due to previous errors\n</code></pre>\n\n<p>I don't see any description of this error, what does it mean?</p>\n",
    "answer": "<p>Remove <code>-</code> from <code>vcfz</code> options. <code>tar</code> does not need hyphen for options.</p>\n\n<p>With a hyphen, the argument for the <code>-f</code> option is <code>z</code>. So the command is in effect trying to archive <code>dvr_rdk_v1.tar.gz</code> and <code>dvr_rdk</code> into an archive called <code>z</code>. Without the hyphen, the semantics of the options changes, so that the next argument on the command line, i.e. your archive's filename, becomes the argument to the <code>f</code> flag.</p>\n\n<p>Also check your write permission to the directory from which you are executing the command.</p>\n",
    "url": "https://unix.stackexchange.com/questions/149494/tar-exits-on-cannot-stat-no-such-file-of-directory-why"
  },
  {
    "question_title": "How can I best copy large numbers of small files over scp?",
    "question_body": "<p>I have a directory that's got several gigabytes and several thousand small files. I want to copy it over the network with scp more than once. CPU time on the source and destination machines is cheap, but the network overhead added by copying each file individually is huge. I would tar/gzip it up and ship it over, but the source machine is short on disk.</p>\n\n<p>Is there a way for me to pipe the output of <code>tar -czf &lt;output&gt; &lt;directory&gt;</code> to scp? If not, is there another easy solution? My source machine is ancient (SunOS) so I'd rather not go installing things on it.</p>\n",
    "answer": "<p>You can pipe tar across an ssh session:</p>\n\n<pre><code>$ tar czf - &lt;files&gt; | ssh user@host \"cd /wherever &amp;&amp; tar xvzf -\"\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/10026/how-can-i-best-copy-large-numbers-of-small-files-over-scp"
  },
  {
    "question_title": "scp and compress at the same time, no intermediate save",
    "question_body": "<p>What is the canonical way to:</p>\n\n<ul>\n<li><code>scp</code> a file to a remote location</li>\n<li>compress the file in transit (<code>tar</code> or not, single file or whole folder, <code>7za</code> or something else even more efficient)</li>\n<li>do the above without saving intermediate files</li>\n</ul>\n\n<p>I am familiar with shell pipes like this:</p>\n\n<pre><code>tar cf - MyBackups | 7za a -si -mx=9 -ms=on MyBackups.tar.7z\n</code></pre>\n\n<p>essentially:</p>\n\n<ul>\n<li>rolling a whole folder into a single <code>tar</code></li>\n<li>pass data through <code>stdout</code> to <code>stdin</code> of the compressing program</li>\n<li>apply <em>aggressive</em> compression</li>\n</ul>\n\n<p>What's the best way to do this over an <code>ssh</code> link, with the file landing on the remote filesystem?</p>\n\n<hr>\n\n<p>I prefer not to <code>sshfs</code> mount.</p>\n\n<hr>\n\n<p>This, does not work:</p>\n\n<pre><code>scp &lt;(tar cvf - MyBackups | 7za a -si -mx=9 -so) localhost:/tmp/tmp.tar.7z\n</code></pre>\n\n<p>because:</p>\n\n<pre><code>/dev/fd/63: not a regular file\n</code></pre>\n",
    "answer": "<p>There are many ways to do what you want. The simplest is to use a pìpe:</p>\n\n<pre><code>tar zcvf -  MyBackups | ssh user@server \"cat &gt; /path/to/backup/foo.tgz\"\n</code></pre>\n\n<p>Here, the compression is being handled by <code>tar</code> which calls <code>gzip</code> (<code>z</code> flag). You can also use <code>compress</code> (<code>Z</code>) and <code>bzip</code> (<code>j</code>). For <code>7z</code>, do this:</p>\n\n<pre><code>tar cf - MyBackups | 7za a -si -mx=9 -ms=on MyBackups.tar.7z | \n   ssh user@server \"cat &gt; /path/to/backup/foo.7z\"\n</code></pre>\n\n<p>The <em>best</em> way, however, is probably <code>rsync</code>.</p>\n\n<pre><code>   Rsync is a fast and extraordinarily versatile  file  copying  tool.   It  can  copy\n   locally, to/from another host over any remote shell, or to/from a remote rsync dae‐\n   mon.  It offers a large number of options that control every aspect of its behavior\n   and  permit  very  flexible  specification of the set of files to be copied.  It is\n   famous for its delta-transfer algorithm, which reduces the amount of data sent over\n   the network by sending only the differences between the source files and the exist‐\n   ing files in the destination.  Rsync is widely used for backups and  mirroring  and\n   as an improved copy command for everyday use.\n</code></pre>\n\n<p><code>rsync</code> has <em>way</em> too many options. It really is worth reading through them but they are scary at first sight. The ones you care about in this context though are:</p>\n\n<pre><code>    -z, --compress              compress file data during the transfer\n        --compress-level=NUM    explicitly set compression level\n\n   -z, --compress\n          With this option, rsync compresses the file data as it is sent to the desti‐\n          nation machine, which reduces the amount of data being transmitted --  \n          something that is useful over a slow connection.\n\n          Note  that this option typically achieves better compression ratios than can\n          be achieved by using a compressing remote shell or a  compressing  transport\n          because  it takes advantage of the implicit information in the matching data\n          blocks that are not explicitly sent over the connection.\n</code></pre>\n\n<p>So, in your case, you would want something like this:</p>\n\n<pre><code>rsync -z MyBackups user@server:/path/to/backup/\n</code></pre>\n\n<p>The files would be compressed while in transit and arrive decompressed at the destination. </p>\n\n<hr>\n\n<p>Some more choices:</p>\n\n<ul>\n<li><p><code>scp</code> itself can compress the data</p>\n\n<pre><code> -C      Compression enable.  Passes the -C flag to ssh(1) to\n         enable compression.\n\n$ scp -C source user@server:/path/to/backup\n</code></pre></li>\n<li><p>There may be a way to get <code>rsync</code> and <code>7za</code> to play nice but there is no point in doing so. The benefit of <code>rsync</code> is that it will only copy the bits that have changed between the local and remote files. However, a small local change can result in a very different compressed file so there is no point in using <code>rsync</code> for this. It just complicates matters with no benefit. Just use direct <code>ssh</code> as shown above. If you <em>really</em> want to do this, you can try by giving a subshell as an argument to <code>rsync</code>. On my system, I could not get this to work with <code>7za</code> because it does not allow you to write compressed data to a terminal.  Perhaps your implementation is different. Try something like (<strong>this does not work for me</strong>):</p>\n\n<pre><code>rsync $(tar cf - MyBackups | 7za a -an -txz -si -so) \\\n  user@server:/path/to/backup\n</code></pre></li>\n<li><p>Another point is that <code>7z</code> <strong>should not be used for backups on Linux</strong>. As stated on the <code>7z</code> man page:</p>\n\n<blockquote>\n  <p>DO NOT USE the 7-zip format for backup purpose  on  Linux/Unix\n     because :<br>\n      - 7-zip does not store the owner/group of the file.</p>\n</blockquote></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/70581/scp-and-compress-at-the-same-time-no-intermediate-save"
  },
  {
    "question_title": "How to create backup from symbolic links?",
    "question_body": "<p>I have a directory with symbolic links to other directories. I want to archive the symbolic links, not as links but as archives containing the files of directories they refer to, using tar command. How can I do this?</p>\n",
    "answer": "<p>Use the <code>-h</code> tar option. From the man page:</p>\n\n<pre><code>-h, --dereference\n    don't archive symlinks; archive the files they point to\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/32633/how-to-create-backup-from-symbolic-links"
  },
  {
    "question_title": "What are the differences between bsdtar and GNU tar?",
    "question_body": "<p>I've always used GNU <code>tar</code>. However, all GNU/Linux distributions that I've seen ship <code>bsdtar</code> in their repositories. I've even seen it installed by default in some, IIRC. I know for sure that Arch GNU/Linux requires it as a part of <code>basedevel</code> (maybe <code>base</code>, but I'm not sure), as I've seen it in PKGBUILDs.</p>\n\n<p>Why would you want to use <code>bsdtar</code> instead of GNU <code>tar</code>? What are the advantages?</p>\n\n<p>Note that I am the person who asked <a href=\"https://unix.stackexchange.com/questions/79355/what-are-the-main-differences-between-bsd-and-gnu-linux-userland\">What are the main differences between BSD and GNU/Linux userland?</a>.</p>\n",
    "answer": "<p>The Ubuntu <code>bsdtar</code> is actually the tar implementation bundled with <code>libarchive</code>; and that should be differentiated from classical <code>bsdtar</code>. Some BSD variants do use <code>libarchive</code> for their tar implementation, eg FreeBSD.</p>\n\n<p><code>GNUtar</code> does support the <a href=\"http://www.gnu.org/software/tar/manual/html_section/Formats.html#SEC132\" rel=\"noreferrer\">other tar variants</a> and automatic compression detection.</p>\n\n<p>As <em>visualication</em> pasted the blurb from Ubuntu, there are a few things in there that are specific to <code>libarchive</code>:</p>\n\n<ol>\n<li><code>libarchive</code> is by definition a library, and different from both classical <code>bsdtar</code> and <code>GNUtar</code> in that way.</li>\n<li><code>libarchive</code> cannot read some older obscure GNU tar variations, most notable was encoding of some headers in base64, so that the tar file would be 7-bit clean ASCII (this was the case for 1.13.6-1.13.11 and changed in 1.13.12, that code was only officially in tar for 2 weeks)</li>\n<li><code>libarchive</code>'s <code>bsdtar</code> will read non-tar files (eg zip, iso9660, cpio), but classical bsdtar will not.</li>\n</ol>\n\n<p>Now that we've gotten <code>libarchive</code> out of the way, it mostly comes down to what is supported in classical <code>bsdtar</code>.</p>\n\n<p>You can see the manpages yourself here:</p>\n\n<ul>\n<li><a href=\"http://www.gnu.org/software/tar/manual/\" rel=\"noreferrer\">GNU tar(1)</a></li>\n<li><a href=\"http://www.freebsd.org/cgi/man.cgi?query=tar&amp;sektion=1\" rel=\"noreferrer\">FreeBSD tar(1)</a> - libarchive-based</li>\n<li><a href=\"http://netbsd.gw.com/cgi-bin/man-cgi?tar++NetBSD-current\" rel=\"noreferrer\">NetBSD tar(1)</a></li>\n<li><a href=\"https://man.openbsd.org/tar\" rel=\"noreferrer\">OpenBSD tar(1)</a></li>\n<li><a href=\"http://schilytools.sourceforge.net/man/man1/star.1.html\" rel=\"noreferrer\">Standard/Schily tar(1)</a> - the oldest free tar implementation, no heritage to any other</li>\n<li><a href=\"https://busybox.net/downloads/BusyBox.html#tar\" rel=\"noreferrer\">busybox (1)</a> - Mini tar implementation for BusyBox, common in embedded systems</li>\n</ul>\n\n<p>In your original question, you asked what are the advantages to the classical <code>bsdtar</code>, and I'm not sure there are really any. The only time it really matters is if you're trying to writing shell scripts that need to work on all systems; you need to make sure what you pass to <code>tar</code> is actually valid in all variants.</p>\n\n<p><code>GNUtar</code>, <code>libarchive</code>'s <code>bsdtar</code>, classical <code>bsdtar</code>, <code>star</code> and <code>BusyBox</code>'s <code>tar</code> are certainly the tar implementations that you'll run into most of the time, but I'm certain there are others out there (early QNX for example). <code>libarchive</code>/<code>GNUtar</code>/<code>star</code> are the most feature-packed, but in many ways they have long deviated from the original standards (possibly for the better).</p>\n",
    "url": "https://unix.stackexchange.com/questions/101561/what-are-the-differences-between-bsdtar-and-gnu-tar"
  },
  {
    "question_title": "Create target directory when extracting tarball",
    "question_body": "<p>Is it possible to create a target directory, similar to <code>mkdir -p</code>, where I can define a non-existent target directory within my tar command, and tar will create the directory for me?</p>\n\n<p>I know I can redirect the output to a directory using <code>tar -C /target/dir</code>, but this doesn't work if the target directory is non-existent.</p>\n",
    "answer": "<pre><code>mkdir -p /target/dir &amp;&amp; tar -C /target/dir\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/25311/create-target-directory-when-extracting-tarball"
  },
  {
    "question_title": "How to create tar archive in a different directory?",
    "question_body": "<p>I want to create a tar archive in a different directory rather than the current directory.</p>\n\n<p>I tried this command:</p>\n\n<pre><code>tar czf file.tar.gz file1 -C /var/www/\n</code></pre>\n\n<p>but it creates the archive in the current directory. Why?</p>\n",
    "answer": "<p>The easy way, if you don't particularly <em>need</em> to use <code>-C</code> to tell <code>tar</code> to change to some other directory, is to simply specify the full path to the archive on the command line. Then you can be in whatever directory you prefer to create the directory structure that you want inside the archive.</p>\n\n<p>The following will create the archive <code>/var/www/file.tar.gz</code> and put <code>file1</code> from the current directory (whatever that happens to be) in it, with no in-archive path information.</p>\n\n<pre><code>tar czf /var/www/file.tar.gz file1\n</code></pre>\n\n<p>The path (to either the archive, the constituent files, or both) can of course also be relative. If <code>file1</code> is in <code>/tmp</code>, you are in <code>/var/spool</code> and want to create the archive in <code>/var/www</code>, you could use something like:</p>\n\n<pre><code>tar czf ../www/file1.tar.gz /tmp/file1\n</code></pre>\n\n<p>There's a million variations on the theme, but this should get you started. Add the <code>v</code> flag if you want to see what <code>tar</code> actually does.</p>\n",
    "url": "https://unix.stackexchange.com/questions/64895/how-to-create-tar-archive-in-a-different-directory"
  },
  {
    "question_title": "Why can&#39;t tar extract .zip files?",
    "question_body": "<p>I tried a majority of the formats (gzip, etc.) to extract a zip file with <code>tar</code>, and when I became frustrated enough to Google for it, I found no way to extract a zip file with <code>tar</code> and only recommendations to use <code>zip</code> or <code>unzip</code>. As a matter of fact, my Linux system doesn't even have a <code>zip</code> utility, but only <code>unzip</code> (leaving me to wonder why this is the main recommended option). Of course <code>unzip</code> worked, solving my problem, but why can't <code>tar</code> extract zip files? Perhaps I should instead be asking, what is the difference between zip and the compression methods supported by <code>tar</code>?</p>\n",
    "answer": "<p>The UNIX philosophy is to have small tools. One tool is doing exactly one thing, but this especially well.</p>\n\n<p>The <code>tar</code> tool is just combining several files into a single file without any compression.</p>\n\n<p>The <code>gzip</code> tool is just compressing a single file.</p>\n\n<p>If you want to have both, you just combine both tools resulting in a <code>.tar.gz</code> file.</p>\n\n<hr>\n\n<p>The <code>zip</code> tool is a completely different thing. It takes a bunch of files and combines them into a single compressed file. With totally different algorithms.</p>\n\n<hr>\n\n<p>If you want one tool to rule them all use <code>atool</code>. It will support a <a href=\"http://manpages.ubuntu.com/manpages/natty/man1/als.1.html#contenttoc4\">whole bunch</a> of different formats simply by detecting the format and calling the correct tool.</p>\n",
    "url": "https://unix.stackexchange.com/questions/146206/why-cant-tar-extract-zip-files"
  },
  {
    "question_title": "Extract multiple .tar.gz files with a single tar call",
    "question_body": "<p>I was wondering whether (and, of course, how) it’s possible to tell <code>tar</code> to extract multiple files in a single run.</p>\n\n<p>I’m an experienced Unix user for several years and of course I know that you can use <code>for</code> or <code>find</code> or things like that to call <code>tar</code> once for each archive you want to extract, but I couldn’t come up with a working command line that caused my <code>tar</code> to extract two .tar.gz files at once. (And no, there’s nothing wrong with <code>for</code>, I’m merely asking whether it’s possible to do without.)</p>\n\n<p>I’m asking this question rather out of curiosity, maybe</p>\n\n<ul>\n<li>there’s a strange fork of <code>tar</code> somewhere that supports this</li>\n<li>someone knows how to use the <code>-M</code> parameter that <code>tar</code> suggested to me when I tried <code>tar -zxv -f a.tgz -f b.tgz</code></li>\n<li>we’re all blind and it’s totally easy to do — but I couldn’t find any hint in the web that didn’t utilize <code>for</code> or <code>find</code> or <code>xargs</code> or the like.</li>\n</ul>\n\n<p>Please don’t reply with <code>tar -zxvf *.tar.gz</code> (because that does <strong>not</strong> work) and only reply with “doesn’t work” if you’re absolutely sure about it (and maybe have a good explanation <em>why</em>, too).</p>\n\n<p><strong>Edit:</strong> I was pointed to <a href=\"https://stackoverflow.com/questions/583889/how-can-you-untar-more-than-one-file-at-a-time/583891#583891\">an answer to this question on Stack Overflow</a> which says in great detail that it’s not possible without breaking current <code>tar</code> syntax, but I don’t think that’s true. Using <code>tar -zxv -f a.tgz -f b.tgz</code> or <code>tar -zxv --all-args-are-archives *.tar.gz</code> would break no existing syntax, imho.</p>\n",
    "answer": "<p>This is possible, the syntax is pretty easy:</p>\n\n<pre><code>$ cat *.tar | tar -xvf - -i\n</code></pre>\n\n<p>The -i option ignores the EOF at the end of the tar archives, from the man page:</p>\n\n<pre><code>-i, --ignore-zeros\nignore blocks of zeros in archive (normally mean EOF)\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/19840/extract-multiple-tar-gz-files-with-a-single-tar-call"
  },
  {
    "question_title": "What does --strip-components -C mean in tar?",
    "question_body": "<p>I want to make sure I understand the following code:</p>\n\n<pre><code>tar xvzf FILE --exclude-from=exclude.me --strip-components 1 -C DESTINATION\n</code></pre>\n\n<p>which was posted in <a href=\"https://unix.stackexchange.com/a/535763/149572\">this answer</a>.</p>\n\n<p>From <code>man tar</code>:</p>\n\n<blockquote>\n  <p><code>--strip-components=NUMBER</code>\n  <br>\n  strip NUMBER leading components from file names on extraction</p>\n  \n  <p><code>-C</code>, <code>--directory=DIR</code>\n  <br>\n  change to directory DIR</p>\n</blockquote>\n\n<p>I didn't understand the manual explanation for <code>--strip-components</code>.</p>\n\n<p>About <code>-C</code>, I understood that it means something like \"put stripped components in a noted directory.\"</p>\n\n<p>What does <code>--strip-components -C</code> mean?</p>\n",
    "answer": "<p>The fragment of manpage you included in your question comes from man\nfor GNU tar. GNU is a software project that prefers <em>info</em> manuals\nover manpages.  In fact, tar manpage <a href=\"https://git.savannah.gnu.org/cgit/tar.git/commit/?id=f5048215892c1bf6cf602c41146c556f60a63a51\" rel=\"noreferrer\">has been added to the GNU tar\nsource code tree only in\n2014</a>\nand it still is just a reference, not a full-blown manual with\nexamples.  You can invoke a full info manual with <code>info tar</code>, it's\nalso available online\n<a href=\"https://www.gnu.org/software/tar/manual/tar.html\" rel=\"noreferrer\">here</a>.  It contains\nseveral examples of <code>--strip-components</code> usage, the relevant fragments\nare:</p>\n\n<blockquote>\n  <p><code>--strip-components=number</code></p>\n  \n  <p>Strip given number of leading components from file names before extraction. </p>\n  \n  <p>For example, if archive `archive.tar' contained `some/file/name', then running</p>\n  \n  <p><code>tar --extract --file archive.tar --strip-components=2</code></p>\n  \n  <p>would extract this file to file `name'.</p>\n</blockquote>\n\n<p>and:</p>\n\n<blockquote>\n  <p><code>--strip-components=number</code></p>\n  \n  <p>Strip given number of leading components from file names before extraction.</p>\n  \n  <p>For example, suppose you have archived whole `/usr' hierarchy to a tar archive named `usr.tar'. Among other files, this archive contains `usr/include/stdlib.h', which you wish to extract to the current working directory. To do so, you type:</p>\n  \n  <p><code>$ tar -xf usr.tar --strip=2 usr/include/stdlib.h</code></p>\n  \n  <p>The option `--strip=2' instructs tar to strip the two leading components (`usr/' and `include/') off the file name.</p>\n</blockquote>\n\n<hr>\n\n<p><strong>That said;</strong></p>\n\n<p>There are other implementations of tar out there, for example <a href=\"https://www.freebsd.org/cgi/man.cgi?tar(1)\" rel=\"noreferrer\">FreeBSD\ntar manpage</a> has a\ndifferent explanation of this command:</p>\n\n<blockquote>\n  <p><code>--strip-components count</code></p>\n  \n  <p>Remove the specified number of leading path elements. Pathnames\n           with fewer elements will be silently skipped. Note that the\n           pathname is edited after checking inclusion/exclusion patterns\n           but before security checks.</p>\n</blockquote>\n\n<p>In other words, you should understand a Unix path as a sequence of\nelements separated by <code>/</code> (unless there is only one <code>/</code>).</p>\n\n<p><strong>Here is my own example</strong> (other examples are available in the info manual I linked to above):</p>\n\n<p>Let's create a new directory structure:</p>\n\n<pre><code>mkdir -p a/b/c\n</code></pre>\n\n<p>Path <code>a/b/c</code> is composed of 3 elements: <code>a</code>, <code>b</code>, and <code>c</code>.</p>\n\n<p>Create an empty file in this directory and put it into .tar archive:</p>\n\n<pre><code>$ touch a/b/c/FILE\n$ tar -cf archive.tar a/b/c/FILE\n</code></pre>\n\n<p><code>FILE</code> is a 4th element of <code>a/b/c/FILE</code> path.</p>\n\n<p>List contents of archive.tar:</p>\n\n<pre><code>$ tar tf archive.tar\na/b/c/FILE\n</code></pre>\n\n<p>You can now extract archive.tar with <code>--strip-components</code> and an\nargument that will tell it how many path elements you want to be removed from the <code>a/b/c/FILE</code> when extracted. Remove an original <code>a</code> directory:</p>\n\n<pre><code>rm -r a\n</code></pre>\n\n<p>Extract with <code>--strip-components=1</code> - only <code>a</code> has not been recreated:</p>\n\n<pre><code>$ tar xf archive.tar --strip-components=1\n$ ls -Al\ntotal 16\n-rw-r--r-- 1 ja users 10240 Mar 26 15:41 archive.tar\ndrwxr-xr-x 3 ja users  4096 Mar 26 15:43 b\n$ tree b\nb\n└── c\n    └── FILE\n\n1 directory, 1 file\n</code></pre>\n\n<p>With <code>--strip-components=2</code> you see that <code>a/b</code> - 2 elements have not\nbeen recreated:</p>\n\n<pre><code>$ rm -r b\n$ tar xf archive.tar --strip-components=2\n$ ls -Al\ntotal 16\n-rw-r--r-- 1 ja users 10240 Mar 26 15:41 archive.tar\ndrwxr-xr-x 2 ja users  4096 Mar 26 15:46 c\n$ tree c\nc\n└── FILE\n\n0 directories, 1 file\n</code></pre>\n\n<p>With <code>--strip-components=3</code> 3 elements <code>a/b/c</code> have not been recreated\nand we got <code>FILE</code> in the same level directory in which we run <code>tar</code>:</p>\n\n<pre><code>$ rm -r c\n$ tar xf archive.tar --strip-components=3\n$ ls -Al\ntotal 12\n-rw-r--r-- 1 ja users     0 Mar 26 15:39 FILE\n-rw-r--r-- 1 ja users 10240 Mar 26 15:41 archive.tar\n</code></pre>\n\n<hr>\n\n<p><code>-C</code> option tells tar to change to a given directory before running a\nrequested operation, extracting but also archiving.  In <a href=\"https://unix.stackexchange.com/questions/535772/what-does-strip-components-c-means#comment1070678_535775\">this\ncomment</a>\nyou asked:</p>\n\n<blockquote>\n  <p>Asking tar to do cd: why cd? I mean to ask, why it's not just mv?</p>\n</blockquote>\n\n<p>Why do you think that <code>mv</code> is better? To what directory would you like\nto extract tar archive first:</p>\n\n<ul>\n<li><p>/tmp - what if it's missing or full?</p></li>\n<li><p>\"$TMPDIR\" - what if it's unset, missing or full?</p></li>\n<li><p>current directory - what if user has no <code>w</code> permission, just <code>r</code> and\n<code>x</code>?</p></li>\n<li><p>what if a temporary directory, whatever it is already contained\nfiles with the same names as in <code>tar</code> archive and extracting would\noverwrite them?</p></li>\n<li><p>what if a temporary directory, whatever it is didn't support Unix\nfilesystems and all info about ownership, executable bits etc. would\nbe lost?</p></li>\n</ul>\n\n<p>Also notice that <code>-C</code> is a common <code>change directory</code> option in other\nprograms as well, <a href=\"https://git-scm.com/docs/git\" rel=\"noreferrer\">Git</a> and\n<a href=\"https://linux.die.net/man/1/make\" rel=\"noreferrer\">make</a> are first that come to my\nmind.</p>\n",
    "url": "https://unix.stackexchange.com/questions/535772/what-does-strip-components-c-mean-in-tar"
  },
  {
    "question_title": "ansible extract without first directory",
    "question_body": "<p>When extracting a tar.gz file in ansible I end up with a first directory</p>\n\n<pre><code>- name: Extract archive\n  unarchive: src=file.tar.gz\n             dest=/foo/bar\n</code></pre>\n\n<p>which results in <code>/foo/bar/bar-version-someFirstLevelFolder/contentOfArchive</code>\nHow can I prevent creating this extra level of hierarchy?</p>\n",
    "answer": "<p>In order to strip the <code>bar-version-someFirstLevelFolder</code> you need to use the <code>--strip-components=1</code> option in <code>tar</code>. So your playbook should look like</p>\n<pre><code>- name: Extract archive\n  unarchive:\n    src: file.tar.gz\n    dest: /foo/bar\n    extra_opts: ['--strip-components=1', '--show-stored-names']\n</code></pre>\n<p>The <code>show-stored-names</code> option, available since Ansible 2.1, will <a href=\"https://github.com/ansible/ansible-modules-core/issues/2480#issuecomment-257082347\" rel=\"noreferrer\">fix the idempotency</a> problems.</p>\n",
    "url": "https://unix.stackexchange.com/questions/346346/ansible-extract-without-first-directory"
  },
  {
    "question_title": "Does tar actually compress files, or just group them together?",
    "question_body": "<p>I usually assumed that <code>tar</code> was a compression utility, but I am unsure, does it actually compress files, or is it just like an ISO file, a file to hold files?</p>\n",
    "answer": "<p>Tar is an archiving tool (Tape ARchive), it only collects files and their metadata together and produces one file. If you want to compress that file later you can use gzip/bzip2/xz. For convenience, many modern implementations of <code>tar</code> provide arguments to compress the archive automatically for you. Checkout the <a href=\"https://man7.org/linux/man-pages/man1/tar.1.html\" rel=\"nofollow noreferrer\">tar man page</a> for more details.</p>\n",
    "url": "https://unix.stackexchange.com/questions/127169/does-tar-actually-compress-files-or-just-group-them-together"
  },
  {
    "question_title": "Is there a parallel file archiver (like tar)?",
    "question_body": "<p>Is there something out there for parallel archiving of files?</p>\n\n<p>Tar is great, but I don't use tape archives, and it's more important to me that the archiving happens quickly (with compression like bzip2) since I have smp.</p>\n",
    "answer": "<p>I think you are looking for pbzip2:</p>\n\n<blockquote>\n  <p>PBZIP2 is a parallel implementation of\n  the bzip2 block-sorting file\n  compressor that uses pthreads and\n  achieves near-linear speedup on SMP\n  machines.</p>\n</blockquote>\n\n<p>Have a look at the <a href=\"https://launchpad.net/pbzip2/\" rel=\"noreferrer\">project homepage</a> or check your favorite package repository.</p>\n",
    "url": "https://unix.stackexchange.com/questions/2983/is-there-a-parallel-file-archiver-like-tar"
  },
  {
    "question_title": "How to solve “tar: invalid magic” error on Linux Alpine",
    "question_body": "<p>I'm installing sqlite on Alpine Linux. I download <code>sqlite-autoconf-3130000.tar.gz</code> but <code>tar</code> could not open it. I tried this <a href=\"https://unix.stackexchange.com/questions/249353/invalid-tar-magic-on-openwrt\">answer</a> but it's not working. <code>tar</code> gives this message:</p>\n\n<pre><code>tar: invalid magic\ntar: short read\n</code></pre>\n\n<p>I wrote these commands.</p>\n\n<pre><code>wget https://www.sqlite.org/2015/sqlite-autoconf-3090100.tar.gz\ntar -zxvf sqlite-autoconf-3090100.tar.gz\n</code></pre>\n",
    "answer": "<p>Try to install the tar package (apk add tar). Busybox tar (default) doesn't support all features.</p>\n",
    "url": "https://unix.stackexchange.com/questions/302192/how-to-solve-tar-invalid-magic-error-on-linux-alpine"
  },
  {
    "question_title": "Why use superflous dash (-) to pass option flags to tar?",
    "question_body": "<p>To create a tar file for a directory, the <code>tar</code> command with <code>compress</code>, <code>verbose</code> and <code>file</code> options can be typed thus:</p>\n\n<pre><code>$ tar -cvf my.tar my_directory/\n</code></pre>\n\n<p>But it also works to do it this way:</p>\n\n<pre><code> $ tar cvf my.tar my_directory/\n</code></pre>\n\n<p>That is, without the dash (-) preceding the options. Why would you ever pass a dash (-) to the option list?</p>\n",
    "answer": "<p>There are several different patterns for options that have been used historically in UNIX applications.  Several old ones, like <em>tar</em>, use a positional scheme:</p>\n\n<blockquote>\n  <p><em>command options arguments</em></p>\n</blockquote>\n\n<p>as for example tar uses</p>\n\n<blockquote>\n  <p>tar *something*f <em>\"file operated on\"</em> *\"paths of files to manipulate\"*</p>\n</blockquote>\n\n<p>In a first attempt to avoid the confusion, <em>tar</em> and a few other programs with the old flags-arguments style allowed delimiting the flags with dashes, but most of us old guys simply ignored that.</p>\n\n<p>Some other commands have a more complicated command line syntax, like <em>dd(1)</em> which uses flags, equal signs, pathnames, arguments and a partridge in a pear tree, all with wild abandon.</p>\n\n<p>In BSD and later versions of unix, this had more or less converged to single-character flags marked with '-', but this began to present a couple of problems:</p>\n\n<ul>\n<li>the flags could be hard to remember</li>\n<li>sometimes you actually wanted to use a name with '-'</li>\n<li>and especially with GNU tools, there began to be limitations imposed by the number of possible flags.  So GNU tools added GNU long options like <code>--output</code>.</li>\n</ul>\n\n<p>Then Sun decided that the extra '-' was redundant and started using long-style flags with single '-'s.</p>\n\n<p>And that's how it came to be the mess it is now.</p>\n",
    "url": "https://unix.stackexchange.com/questions/13573/why-use-superflous-dash-to-pass-option-flags-to-tar"
  },
  {
    "question_title": "View a file in a tar archive without extracting it",
    "question_body": "<p>I want to view the content of the tarred file without extracting it, \nScenario: I have a.tar and inside there is a file called <code>./x/y.txt</code>. I want to view the content of <code>y.txt</code> without actually extracting the <code>a.tar</code>.</p>\n",
    "answer": "<p>It's probably a GNU specific option, but you could use the <code>-O</code> or <code>--to-stdout</code> to extract files to standard output</p>\n\n<pre><code>$ tar -axf file.tgz foo/bar -O\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/208482/view-a-file-in-a-tar-archive-without-extracting-it"
  },
  {
    "question_title": "Is there a way to tar multiple files in a directory (Linux/Unix)?",
    "question_body": "<p>Is there a fairly simple way to <code>tar</code> several (specific) files in a directory? For example let's say the following is a directory:</p>\n\n<pre><code>-rw-r--r-- 1 allend bin     98 Jul 20 15:50 scriptlog.log\n-rw-r--r-- 1 allend bin  19533 Jul 29 21:47 serveralert.log\n-rwxr--r-- 1 allend bin   1625 Jul 29 21:47 orion\n-rw-r--r-- 1 allend bin  24064 Jul 29 21:49 orion_files.tar\n-rwxr--r-- 1 allend bin    156 Aug  4 21:22 htmltest\n-rw-r--r-- 1 allend bin    131 Aug  4 21:23 page.html\n</code></pre>\n\n<p>What if I only want to <code>tar</code> the files <code>serveralert.log</code> and <code>page.html</code>?</p>\n",
    "answer": "<p>Is there something wrong with listing the files you'd like to add to the .tar file?</p>\n\n<pre><code>$ tar cvf some.tar file1 file2 file3\n</code></pre>\n\n<h3>Example</h3>\n\n<pre><code>$ tar cvf some.tar serveralert.log page.html\nserveralert.log\npage.html\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/85517/is-there-a-way-to-tar-multiple-files-in-a-directory-linux-unix"
  },
  {
    "question_title": "tar extraction depends on filename?",
    "question_body": "<p>I often download tarballs with wget from sourceforge.net.</p>\n\n<p>The downloaded files then are named, e.g <code>SQliteManager-1.2.4.tar.gz?r=http:%2F%2Fsourceforge.net%2Fprojects%2Fsqlitemanager%2Ffiles%2F&amp;ts=1305711521&amp;use_mirror=switch</code></p>\n\n<p>When I try to</p>\n\n<pre><code>tar xzf SQliteManager-1.2.4.tar.gz\\?r\\=http\\:%2F%2Fsourceforge.net%2Fprojects%2Fsqlitemanager%2Ffiles%2F\\&amp;ts\\=1305711521\\&amp;use_mirror\\=switch\n</code></pre>\n\n<p>I receive the following error message:</p>\n\n<pre><code>tar (child): Cannot connect to SQliteManager-1.2.4.tar.gz?r=http: resolve failed\n\ngzip: stdin: unexpected end of file\ntar: Child returned status 128\ntar: Error is not recoverable: exiting now\n</code></pre>\n\n<p>After renaming the file to <code>foo.tar.gz</code> the extraction works perfect.</p>\n\n<p>Is there a way, that i am not forced to rename each time the target file before extracting?</p>\n",
    "answer": "<p>The reason for the error you are seeing can be found in the <a href=\"http://www.gnu.org/software/tar/manual/html_section/file.html#SEC104\">GNU tar documentation</a>:</p>\n\n<blockquote>\n  <p>If the archive file name includes a\n  colon (‘:’), then it is assumed to be\n  a file on another machine[...]</p>\n</blockquote>\n\n<p>That is, it is interpretting <code>SQliteManager-1.2.4.tar.gz?r=http</code> as a host name and trying to resolve it to an IP address, hence the \"resolve failed\" error.</p>\n\n<p>That same documentation goes on to say:</p>\n\n<blockquote>\n  <p>If you need to use a file whose name\n  includes a colon, then the remote tape\n  drive behavior can be inhibited by\n  using the ‘--force-local’ option.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/13377/tar-extraction-depends-on-filename"
  },
  {
    "question_title": "Fastest way combine many files into one (tar czf is too slow)",
    "question_body": "<p>Currently I'm running <code>tar czf</code> to combine backup files. The files are in a specific directory.</p>\n\n<p>But the number of files is growing. Using <code>tzr czf</code> takes too much time (more than 20 minutes and counting).</p>\n\n<p>I need to combine the files more quickly and in a scalable fashion.</p>\n\n<p>I've found <code>genisoimage</code>, <code>readom</code> and <code>mkisofs</code>. But I don't know which is fastest and what the limitations are for each of them.</p>\n",
    "answer": "<p>You should check if most of your time are being spent on CPU or in I/O. Either way, there are ways to improve it:</p>\n\n<p><strong>A: don't compress</strong></p>\n\n<p>You didn't mention \"compression\" in your list of requirements so try dropping the \"z\" from your arguments list: <code>tar cf</code>. This might be speed up things a bit.</p>\n\n<p>There are other techniques to speed-up the process, like using \"-N \" to skip files you already backed up before.</p>\n\n<p><strong>B: backup the whole partition with dd</strong></p>\n\n<p>Alternatively, if you're backing up an entire partition, take a copy of the whole disk image instead. This would save processing and a <em>lot</em> of disk head seek time. <code>tar</code> and any other program working at a higher level have a  overhead of having to read and process directory entries and inodes to find where the file content is and to do more head <strong>disk seeks</strong>, reading each file from a different place from the disk.</p>\n\n<p>To backup the underlying data much faster, use:</p>\n\n<p><code>dd bs=16M if=/dev/sda1 of=/another/filesystem</code></p>\n\n<p>(This assumes you're not using RAID, which may change things a bit)</p>\n",
    "url": "https://unix.stackexchange.com/questions/19129/fastest-way-combine-many-files-into-one-tar-czf-is-too-slow"
  },
  {
    "question_title": "Will tar -cvzf packed.tar.gz mydir take hidden files into account?",
    "question_body": "<p>I need to create a tarball of a given directory. However, I need to make sure hidden files are included too (such as those beginning with <code>.</code>).</p>\n\n<p>Will the following command automatically take the hidden files into account?</p>\n\n<pre><code>tar -cvzf packed.tar.gz mydir\n</code></pre>\n\n<p>If not, how can I make sure I include hidden files?</p>\n",
    "answer": "<p>Yes, it will.</p>\n\n<p>Files starting with <code>.</code> are not \"hidden\" in all contexts.  They aren't expanded by <code>*</code>, and <code>ls</code> doesn't list them by default, but <code>tar</code> doesn't care about the leading <code>.</code>.  (<code>find</code> doesn't care either.)</p>\n\n<p>(Of course, this is one of those things that's easy to find out by experiment.)</p>\n",
    "url": "https://unix.stackexchange.com/questions/31975/will-tar-cvzf-packed-tar-gz-mydir-take-hidden-files-into-account"
  },
  {
    "question_title": "tar without preserving user",
    "question_body": "<p>I'd like to compress some files for http distribution, but found that <code>.tar.gz</code> keeps the user name and user ID and there doesn't seem to be any way to not do that? (There is a <code>--numeric-owner</code> option for tar which seems to ignore the user name, but still keeps the user ID.)</p>\n\n<p>Doesn't that mean that <code>.tar.gz</code> is a poor choice for file distribution as my system probably is the only one with my user ID and my user name? Is <code>.7z</code> a better format for file distribution, or do you have any other recommendation?</p>\n",
    "answer": "<p>Generally .tar.gz is a usable file distribution format. GNU tar allows you not to preserve the owner and permissions.</p>\n<pre><code>$ tar -c -f archive.tar --owner=0 --group=0 --no-same-owner --no-same-permissions .\n</code></pre>\n<p><a href=\"https://www.gnu.org/software/tar/manual/html_section/tar_33.html#SEC69\" rel=\"noreferrer\">https://www.gnu.org/software/tar/manual/html_section/tar_33.html#SEC69</a></p>\n<p>If your version of tar does not support the GNU options you can copy your source files to another directory tree and update group and ownership there, prior to creating your tar.gz file for distribution.</p>\n<p><code>--owner=0</code> and <code>--group=0</code> works only in compression phase of the file while in decompression phase it has no effect.<br />\n<code>--no-same-owner</code> <code>--no-same-permissions</code> works only in decompression phase while in compression phase it has no effect.<br />\nPut together they can constitute a default function in which tar assumes the characteristics of not remembering the user who compressed or decompressed the files.<br />\nWhen during compression the files are stored with user and group 0, during the decompression via GUI, they assume the permissions of the user who extracts the files, so it is a valid solution to forget the user in the compression phase.</p>\n",
    "url": "https://unix.stackexchange.com/questions/285237/tar-without-preserving-user"
  },
  {
    "question_title": "How to untar safely, without polluting the current directory in case of a tarbomb?",
    "question_body": "<p>Respectable projects release tar archives that contain a single directory, for instance <code>zyrgus-3.18.tar.gz</code> contains a <code>zyrgus-3.18</code> folder which in turn contains <code>src</code>, <code>build</code>, <code>dist</code>, etc.</p>\n\n<p>But some punk projects put everything at the root :'-( This results in a <a href=\"https://unix.stackexchange.com/questions/5123/how-to-de-unzip-de-tar-xvf-de-unarchive-in-a-messy-folder\">total mess</a> when unarchiving. Creating a folder manually every time is a pain, and unnecessary most of the time.</p>\n\n<ul>\n<li>Is there a super-fast way to tell whether a .tar or .tar.gz file contains more than a single directory at its root? Even for a big archive.</li>\n<li>Or even better, is there a tool that in such cases would create a directory (name of the archive without the extension) and put everything inside?</li>\n</ul>\n",
    "answer": "<p><a href=\"http://wummel.github.io/patool/\">patool</a> handles different kinds of archives and creates a subdirectory in case the archive contains multiple files to prevent cluttering the working directory with the extracted files.</p>\n\n<h3>Extract archive</h3>\n\n<pre><code>patool extract archive.tar\n</code></pre>\n\n<p>To obtain a list of the supported formats, use <code>patool formats</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/242298/how-to-untar-safely-without-polluting-the-current-directory-in-case-of-a-tarbom"
  },
  {
    "question_title": "Why is tar archive so much bigger than text file, 10240 bytes?",
    "question_body": "<p>I've checked these two questions (<a href=\"https://unix.stackexchange.com/questions/81424/why-is-my-tar-file-bigger-than-the-directory-backed-up\">question one</a>, <a href=\"https://unix.stackexchange.com/questions/77899/why-is-my-tar-file-bigger-than-its-contents\">question two</a>), but they were not helpful for me to understand. I have a file <code>file.txt</code> with 40 lines of <code>Hello World!</code> string. <code>ls -l</code> shows that its size is 520 bytes. Now I archive this file with <code>tar -cvf file.tar file.txt</code> and when I do <code>ls -l</code> again I see that <code>file.tar</code> is 10240 bytes. Why?</p>\n<p>I've read some manuals and have understood that archiving and compressing are different things. But can someone please explain how it is working?</p>\n",
    "answer": "<p><code>tar</code> archives have a minimum size of 10240 bytes by default; see <a href=\"https://www.gnu.org/software/tar/manual/html_node/Blocking-Factor.html\" rel=\"noreferrer\">the GNU <code>tar</code> manual</a> for details (but this is not GNU-specific).</p>\n<p>With GNU <code>tar</code>, you can reduce this by specifying either a different block size, or different block factor, or both:</p>\n<pre><code>tar -cv -b 1 -f file.tar file.txt\n</code></pre>\n<p>The result will still be bigger than <code>file.txt</code>, because <code>file.tar</code> stores metadata about <code>file.txt</code> in addition to <code>file.txt</code> itself. In most cases you’ll see one block for the file’s metadata (name, size, timestamps, ownership, permissions), then the file content, then two blocks for the end-of-archive entry, so the smallest archive containing a non-zero-length file is four blocks in size (2,048 bytes with a 512-byte block).</p>\n",
    "url": "https://unix.stackexchange.com/questions/685766/why-is-tar-archive-so-much-bigger-than-text-file-10240-bytes"
  },
  {
    "question_title": "Prevent showing time stamp message when running &quot;tar xzf&quot;",
    "question_body": "<p>When running <code>tar xzf</code>, how to prevent this message (Prevent showing time stamp message):</p>\n\n<pre><code>tar: node: time stamp 2011-06-07 02:02:30 is 8309 s in the future\ntar: user/Node: time stamp 2011-06-07 01:56:05 is 7924 s in the future\n</code></pre>\n",
    "answer": "<p>tar has an option to suppress this message [1]:</p>\n\n<blockquote> -m, --touch<br />\ndon't extract file modified time</blockquote>\n\n<p>However, you should probably also check that you don't have an issue with your system clock.</p>\n\n<p>[1] <a href=\"http://unixhelp.ed.ac.uk/CGI/man-cgi?tar\">http://unixhelp.ed.ac.uk/CGI/man-cgi?tar</a></p>\n",
    "url": "https://unix.stackexchange.com/questions/14528/prevent-showing-time-stamp-message-when-running-tar-xzf"
  },
  {
    "question_title": "Test tar file integrity in bash",
    "question_body": "<p>I have a bash script that creates a '.tar' file. Once the file is created, I would like to test its integrity and send an email to the root user if the integrity is bad.</p>\n\n<p>I know I would need to use the command <code>tar -tf /root/archive.tar</code> to check the integrity of the file, but how would I implement this in a bash if statement and check for errors?</p>\n",
    "answer": "<p>If <code>tar</code> finds errors in its input it will <code>exit(3)</code>¹ with a non-zero exit value.  This — with most <code>tar</code> implementations — is also done when listing archive contents with <code>t</code>.  So you could simply check for the exit value of <code>tar</code> to determine if something has gone wrong:</p>\n\n<pre><code>if ! tar tf /root/archive.tar &amp;&gt; /dev/null; then\n    write_an_email_to_root\nfi\n</code></pre>\n\n<p>If your <code>tar</code> does not find all errors with <code>t</code>, you could still extract the archive to <code>stdout</code> and redirect <code>stdout</code> to <code>/dev/null</code>, which would be the slower but more reliable approach:</p>\n\n<pre><code>if ! tar xOf /root/archive.tar &amp;&gt; /dev/null; then\n    write_an_email_to_root\nfi\n</code></pre>\n\n<p>¹ This notation denotes the manpage, not the actual call.  See <a href=\"http://man7.org/linux/man-pages/man3/exit.3.html\"><code>man 3 exit</code></a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/129599/test-tar-file-integrity-in-bash"
  },
  {
    "question_title": "tar with relative paths",
    "question_body": "<p>I try to create an archive with tar using relative paths. I use the following command:</p>\n\n<pre><code>tar czf ~/files/wp/my-page-order.tar.gz -C ~/webapps/zers/wp-content/plugins/ ~/webapps/zers/wp-content/plugins/my-page-order\n</code></pre>\n\n<p>But the archived files still have absolute paths. How can I use tar with relative paths? </p>\n",
    "answer": "<p><code>~</code> is expanded by the shell. Don't use <code>~</code> with -C:</p>\n\n<pre><code>tar czf ~/files/wp/my-page-order.tar.gz \\\n      -C ~ \\\n       webapps/zers/wp-content/plugins/my-page-order\n</code></pre>\n\n<p>(tar will include <code>webapps/zers/wp-content/plugins/my-page-order</code> path)\nor</p>\n\n<pre><code>tar czf ~/files/wp/my-page-order.tar.gz \\\n      -C ~/webapps/zers/wp-content/plugins \\\n       my-page-order\n</code></pre>\n\n<p>(tar will include <code>my-page-order</code> path)</p>\n\n<p>Or just <code>cd</code> first....</p>\n\n<pre><code>cd ~/webapps/zers/wp-content/plugins\ntar czf ~/files/wp/my-page-order.tar.gz my-page-order\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/22842/tar-with-relative-paths"
  },
  {
    "question_title": "How do I securely extract an untrusted tar file?",
    "question_body": "<p>I would like to be able to extract a tar file, such that all extracted files are placed under a certain prefix directory.  Any attempt by the tar files to write to outside directories should cause the extraction to fail.</p>\n\n<p>As you might imagine, this is so that I can securely extract an untrusted tar file.</p>\n\n<p>How can I do this with GNU <code>tar</code>?</p>\n\n<p>I came up with:\n</p>\n\n<pre><code>tar --exclude='/*' --exclude='*/../*' --exclude='../*' -xvf untrusted_file.tar\n</code></pre>\n\n<p>but I am not sure that this is paranoid enough.</p>\n",
    "answer": "<p>You don't need the paranoia at all. GNU <code>tar</code> — and in fact <em>any</em> well-written <code>tar</code> program produced in the past 30 years or so — will refuse to extract files in the tarball that begin with a slash or that contain <code>..</code> elements, by default.</p>\n\n<p>You have to go out of your way to force modern <code>tar</code> programs to extract such potentially-malicious tarballs: both GNU and BSD <code>tar</code> need the <code>-P</code> option to make them disable this protection. See the section <a href=\"https://www.gnu.org/software/tar/manual/html_section/tar_55.html#SEC119\">Absolute File Names</a> in the GNU tar manual.</p>\n\n<p>The <code>-P</code> flag isn't specified by POSIX,¹ though, so other <code>tar</code> programs may have different ways of coping with this. For example, the Schily Tools' <a href=\"http://linuxcommand.org/man_pages/star1.html\"><code>star</code> program</a> uses <code>-/</code> and <code>-..</code> to disable these protections.</p>\n\n<p>The only thing you might consider adding to a naïve <code>tar</code> command is a <code>-C</code> flag to force it to extract things in a safe temporary directory, so you don't have to <code>cd</code> there first.</p>\n\n<hr>\n\n<p><strong>Asides</strong>:</p>\n\n<ol>\n<li><p>Technically, <code>tar</code> isn't specified by POSIX any more at all. They tried to tell the Unix computing world that we should be using <a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/utilities/pax.html\"><code>pax</code></a> now instead of <code>tar</code> and <code>cpio</code>, but the computing world largely ignored them. </p>\n\n<p>It's relevant here to note that the POSIX specification for <code>pax</code> doesn't say how it should handle leading slashes or embedded <code>..</code> elements. There's a nonstandard <code>--insecure</code> flag for <a href=\"https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/pax.1.html\">BSD <code>pax</code></a> to suppress protections against embedded <code>..</code> path elements, but there is apparently no default protection against leading slashes; the BSD <code>pax</code> man page indirectly recommends writing <code>-s</code> substitution rules to deal with the absolute path risk.</p>\n\n<p>That's the sort of thing that happens when a de facto standard remains in active use while the de jure standard is largely ignored.</p></li>\n</ol>\n",
    "url": "https://unix.stackexchange.com/questions/276959/how-do-i-securely-extract-an-untrusted-tar-file"
  },
  {
    "question_title": "Remove files from tar archive",
    "question_body": "<p>I have a large file <code>foo.tar.xz</code> that contains a lot (say 200000) of files. I figured out that this archive contains some (around 5000) files I don't want. I don't have sufficient disk space to decompress the whole thing onto my disk; additionally, I fear attributes / rights might get lost if I do so. I have enough space to host two copies of the <em>compressed</em> archive though. Is there a tool to remove some of the files from the archive (specified with a regex on the filename) on-the-fly, i.e. without unpacking the archive into individual files?</p>\n",
    "answer": "<p><a href=\"https://www.gnu.org/software/tar/manual/html_node/delete.html\" rel=\"noreferrer\">GNU tar has a <code>--delete</code> option that works with archives too nowadays.</a></p>\n\n<p>Use it like this, for example:</p>\n\n<pre><code>tar -vf yourArchive.tar --delete your/path/to/delete\n</code></pre>\n\n<p><strong>Beware:</strong> It will most likely <em>not</em> work on any kind of magnetic tape medium. But <code>tar</code> has no problems working in a pipe, so you can just use a temporary tar file and overwrite the tape with that afterwards. It also won't work on compressed files, so you would need to uncompress the file.</p>\n\n<p>Also, the operation will be rather slow in any case, due to the (by design) packed linear nature of tar archives.</p>\n",
    "url": "https://unix.stackexchange.com/questions/68732/remove-files-from-tar-archive"
  },
  {
    "question_title": "tar extract into directory with same base name?",
    "question_body": "<p>I have a zipped file like <code>myArchive123.tar.gz</code>. Inside, it contains a folder like <code>helloWorld</code></p>\n<p>If I extract it with <code>tar -xf myArchive123.tar.gz</code>, I get the <code>helloWorld</code> folder:</p>\n<pre><code>ls \nmyArchive123.tar.gz\nhelloWorld \n</code></pre>\n<p>I want the output to be the same as the file name minus the <code>.tar.gz</code> extension. I.e.:</p>\n<pre><code>tar &lt;magic paramaters&gt; myArchive123.tar.gz \nls \n myArchive123.tar.gz\n myArchive123\ncd myArchive123\nls \n  helloWorld\n</code></pre>\n<p>Can this be done?</p>\n<ul>\n<li>I never know what's inside the archive. It could be a folder, could be many files.</li>\n<li>I'd be ok with using another tool if tar can't do it.</li>\n<li>I'd be ok with a longer form that can be turned into a script</li>\n</ul>\n<p><strong>EDIT</strong><br />\nIn the meantime, I wrote myself a script that seems to get the job done (see my posted answer below).\nThe main thing is that it should be packageable into a one-liner like:</p>\n<pre><code>extract &lt;file&gt;\n</code></pre>\n",
    "answer": "<pre class=\"lang-console prettyprint-override\"><code>$ tar -xf myArchive123.tar.gz --one-top-level\n</code></pre>\n<p><strong>--one-top-level</strong>[=DIR] <br/>\nExtract all files into DIR, or, if used without argument, into a subdirectory named by the base name of the archive (minus standard compression suffixes recognizable by --auto-compress).</p>\n",
    "url": "https://unix.stackexchange.com/questions/198151/tar-extract-into-directory-with-same-base-name"
  },
  {
    "question_title": "tar + rsync + untar. Any speed benefit over just rsync?",
    "question_body": "<p>I often find myself sending folders with 10K - 100K of files to a remote machine (within the same network on-campus). </p>\n\n<p>I was just wondering if there are reasons to believe that, </p>\n\n<pre><code> tar + rsync + untar\n</code></pre>\n\n<p>Or simply</p>\n\n<pre><code> tar (from src to dest) + untar\n</code></pre>\n\n<p>could be faster in practice than</p>\n\n<pre><code>rsync \n</code></pre>\n\n<p>when transferring the files <strong>for the first time</strong>. </p>\n\n<p>I am interested in an answer that addresses the above in two scenarios: using compression and not using it.</p>\n\n<h1>Update</h1>\n\n<p>I have just run some experiments moving 10,000 small files (total size = 50 MB), and <code>tar+rsync+untar</code> was consistently faster than running <code>rsync</code> directly (both without compression).</p>\n",
    "answer": "<p>When you send the same set of files, <code>rsync</code> is better suited because it will only send differences. <code>tar</code> will always send everything and this is a waste of resources when a lot of the data are already there. The <code>tar + rsync + untar</code> loses this advantage in this case, as well as the advantage of keeping the folders in-sync with <code>rsync --delete</code>.</p>\n\n<p>If you copy the files for the first time, first packeting, then sending, then unpacking (AFAIK <code>rsync</code> doesn't take piped input) is cumbersome and always worse than just rsyncing, because <code>rsync</code> won't have to do any task more than <code>tar</code> anyway.</p>\n\n<p>Tip: rsync version 3 or later does incremental recursion, meaning it starts copying almost immediately before it counts all files.</p>\n\n<p>Tip2: If you use <code>rsync</code> over <code>ssh</code>, you may also use either <code>tar+ssh</code></p>\n\n<pre><code>tar -C /src/dir -jcf - ./ | ssh user@server 'tar -C /dest/dir -jxf -'\n</code></pre>\n\n<p>or just <code>scp</code></p>\n\n<pre><code>scp -Cr srcdir user@server:destdir\n</code></pre>\n\n<p>General rule, keep it simple.</p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>I've created 59M demo data</p>\n\n<pre><code>mkdir tmp; cd tmp\nfor i in {1..5000}; do dd if=/dev/urandom of=file$i count=1 bs=10k; done\n</code></pre>\n\n<p>and tested several times the file transfer to a remote server (not in the same lan), using both methods</p>\n\n<pre><code>time rsync -r  tmp server:tmp2\n\nreal    0m11.520s\nuser    0m0.940s\nsys     0m0.472s\n\ntime (tar cf demo.tar tmp; rsync demo.tar server: ; ssh server 'tar xf demo.tar; rm demo.tar'; rm demo.tar)\n\nreal    0m15.026s\nuser    0m0.944s\nsys     0m0.700s\n</code></pre>\n\n<p>while keeping separate logs from the ssh traffic packets sent</p>\n\n<pre><code>wc -l rsync.log rsync+tar.log \n   36730 rsync.log\n   37962 rsync+tar.log\n   74692 total\n</code></pre>\n\n<p>In this case, I can't see any advantage in less network traffic by using rsync+tar, which is expected when the default mtu is 1500 and while the files are 10k size. rsync+tar had more traffic generated, was slower for 2-3 seconds and left two garbage files that had to be cleaned up.</p>\n\n<p>I did the same tests on two machines on the same lan, and there the rsync+tar did much better times and much much less network traffic. I assume cause of jumbo frames.</p>\n\n<p>Maybe rsync+tar would be better than just rsync on a much larger data set. But frankly I don't think it's worth the trouble, you need double space in each side for packing and unpacking, and there are a couple of other options as I've already mentioned above.</p>\n",
    "url": "https://unix.stackexchange.com/questions/30953/tar-rsync-untar-any-speed-benefit-over-just-rsync"
  },
  {
    "question_title": "Encrypting and compressing",
    "question_body": "<p>Is there a better way to compress and then encrypt files other than <code>tar</code> followed by <code>openssl</code> or <code>gpg</code>?</p>\n",
    "answer": "<p><code>tar</code> is the usual tool to bundle files. Plain <code>tar</code> itself doesn't compress. There are separate tools such as <a href=\"http://en.wikipedia.org/wiki/Gzip\">gzip</a>, <a href=\"http://en.wikipedia.org/wiki/Bzip2\">bzip2</a> and <a href=\"http://en.wikipedia.org/wiki/Xz\">xz</a> (in increasing order of compression ratio on typical files) that compress one file. Many <code>tar</code> implementation, including GNU tar (the normal implementation on Linux), can automatically compress with an option (<code>-z</code> for gzip, <code>-j</code> for bzip2, <code>-J</code> for xz):</p>\n\n<pre><code>tar -cJf myarchive.tar.xz file1 file2 file3\n</code></pre>\n\n<p>To encrypt a file, use <a href=\"http://en.wikipedia.org/wiki/GNU_Privacy_Guard\">gpg</a>. Create a key and associate it with your email address (GPG/PGP key identifiers usually contain an email address, though it is not necessary ). Encrypt your files, specifying your email as the recipient. To decrypt a file, you'll need to enter the passphrase to unlock your private key.</p>\n\n<p>GPG also lets you encrypt a file with a password. This is less secure and less flexible. It's less flexible because you need to specify the password when encrypting (so for example you can't make unattended backups). It's less secure because the only security is the password, whereas key-based encryption splits the security between the password and the key.</p>\n\n<p>Don't use the <code>openssl</code> command line tool. It's a showcase for the OpenSSL library, not designed for production use. Although you can do some things with it (in particular, it does have all the primitives needed for a basic certification authority), it's hard to use correctly and it doesn't have all you need to do things right. Where GPG gives you a bicycle, OpenSSL gives you some metal rods of various sizes and a couple of rubber chambers (screws and pump not included). Use GPG.</p>\n",
    "url": "https://unix.stackexchange.com/questions/144391/encrypting-and-compressing"
  },
  {
    "question_title": "How do I compress files in-place?",
    "question_body": "<p>I have a machine with 90% hard-disk usage.  I want to compress its 500+ log files into a smaller new file. However, the hard disk is too small to keep both the original files and the compressed ones.</p>\n\n<p>So what I need is to compress all log files into a single new file one by one, deleting each original once compressed.</p>\n\n<p>How can I do that in Linux?</p>\n",
    "answer": "<p><code>gzip</code> or <code>bzip2</code> will compress the file and remove the non-compressed one automatically (this is their default behaviour).</p>\n\n<p>However, keep in mind that while the compressing process, both files will exists.</p>\n\n<p>If you want to compress log files (ie: files containing text), you may prefer <code>bzip2</code>, since it has a better ratio for text files.</p>\n\n<pre><code>bzip2 -9 myfile       # will produce myfile.bz2\n</code></pre>\n\n<p>Comparison and examples:</p>\n\n<pre><code>$ ls -l myfile\n-rw-rw-r-- 1 apaul apaul 585999 29 april 10:09 myfile\n\n$ bzip2 -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 115780 29 april 10:09 myfile.bz2\n\n$ bunzip2 myfile.bz2\n\n$ gzip -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 146234 29 april 10:09 myfile.gz\n</code></pre>\n\n<p><strong>UPDATE</strong> as @Jjoao told me in a comment, interestingly, <code>xz</code> seems to have a best ratio on plain files with its default options:</p>\n\n<pre><code>$ xz -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 109384 29 april 10:09 myfile.xz\n</code></pre>\n\n<p>For more informations, here is an interesting benchmark for different tools: <a href=\"http://binfalse.de/2011/04/04/comparison-of-compression/\">http://binfalse.de/2011/04/04/comparison-of-compression/</a></p>\n\n<p>For the example above, I use <code>-9</code> for a best compression ratio, but if the time needed to compress data is more important than the ratio, you'd better not use it (use a lower option, ie <code>-1</code>, or something between).</p>\n",
    "url": "https://unix.stackexchange.com/questions/199328/how-do-i-compress-files-in-place"
  },
  {
    "question_title": "Is there a way to convert a zip to a tar without extracting it to the filesystem?",
    "question_body": "<p>Is there a way to convert a <code>zip</code> archive to a <code>tar</code> archive without extracting to a temporary directory first? (and without writing my own implementation of <code>tar</code> or <code>unzip</code>)</p>\n",
    "answer": "<p>This is now available as installable command from PyPI, see the end of this post.</p>\n<hr />\n<p>I don't know of any &quot;standard&quot; utility that does so, but when I needed this functionality I wrote the following Python script to go from ZIP to Bzip2 compressed tar archives without extracting anything to disk first:</p>\n<pre><code>#! /usr/bin/env python\n    \n&quot;&quot;&quot;zip2tar &quot;&quot;&quot;\n\nimport sys\nimport os\nfrom zipfile import ZipFile\nimport tarfile\nimport time\n\ndef main(ifn, ofn):\n    with ZipFile(ifn) as zipf:\n        with tarfile.open(ofn, 'w:bz2') as tarf:\n            for zip_info in zipf.infolist():\n                #print zip_info.filename, zip_info.file_size\n                tar_info = tarfile.TarInfo(name=zip_info.filename)\n                tar_info.size = zip_info.file_size\n                tar_info.mtime = time.mktime(tuple(zip_info.date_time) +\n                                         (-1, -1, -1))\n                tarf.addfile(\n                    tarinfo=tar_info,\n                    fileobj=zipf.open(zip_info.filename)\n                )\n\ninput_file_name = sys.argv[1]\noutput_file_name = os.path.splitext(input_file_name)[0] + '.tar.bz2'\n\nmain(input_file_name, output_file_name)\n</code></pre>\n<p>Just save it to <code>zip2tar</code> and make it executable or save it to <code>zip2tar.py</code> and  run <code>python zip2tar.py</code>. Provide the ZIP filename as an argument to the script, the output filename for <code>xyz.zip</code> will be <code>xyz.tar.bz2</code>.</p>\n<p>The Bzip2 compressed output is normally much smaller than the zip file because the latter doesn't use compression patterns over multiple files, but there is also less chance of recovering later file if something in the Bzip2 file is wrong.</p>\n<p>If you don't want the output compressed, remove <code>:bz2</code> and <code>.bz2</code> from the code.</p>\n<hr />\n<p>If you have <code>pip</code> installed in a python3 environment, you can do:</p>\n<pre><code>pip3 install ruamel.zip2tar\n</code></pre>\n<p>to get a <code>zip2tar</code> commandline utility doing the above (disclaimer: I am the author of that package).</p>\n",
    "url": "https://unix.stackexchange.com/questions/146264/is-there-a-way-to-convert-a-zip-to-a-tar-without-extracting-it-to-the-filesystem"
  },
  {
    "question_title": "How to use multi-threading for creating and extracting tar.xz",
    "question_body": "<p>I use</p>\n<pre>\ntar -cJvf <i>resultfile.tar.xz</i> <i>files_to_compress</i>\n</pre>\n<p>to create <code>tar.xz</code> and</p>\n<pre>\ntar -xzvf <i>resultfile.tar.xz</i>\n</pre>\n<p>to extract the archive in current directory. How to use multi threading in both cases?  I don't want to install any utilities.</p>\n",
    "answer": "<pre class=\"lang-bash prettyprint-override\"><code>tar -c -I 'xz -9 -T0' -f archive.tar.xz [list of files and folders]\n</code></pre>\n<p>This compresses a list of files and directories into an <code>.tar.xz</code> archive. It does so by specifying the arguments to be passed to the <code>xz</code> subprocess, which compresses the tar archive.</p>\n<p>This is done using the <code>-I</code> argument to tar, which tells <code>tar</code> what program to use to compress the tar archive, and what arguments to pass to it. The <code>-9</code> tells <code>xz</code> to use maximum compression. The <code>-T0</code> tells <code>xz</code> to use as many threads as you have CPUs. Note that if you are on MacOS or a BSD distro, the <code>-I</code> option is available only on GNU tar, not on BSD tar. For MacOS users, this can be resolved by installing <a href=\"https://formulae.brew.sh/formula/gnu-tar\" rel=\"nofollow noreferrer\"><code>gnu-tar</code></a> through Homebrew and using the <code>gtar</code> command instead of <code>tar</code>.</p>\n<hr />\n<p><strong>An update from January, 2024:</strong></p>\n<p>Even when using the multithreading option, <code>xz</code> barely scales beyond two threads, besides its compression/decompression performance is relatively low.</p>\n<p>I highly recommend using <code>ZSTD</code> instead. The command will be:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>tar -c -I 'zstd -22 --ultra --long -T0' -f archive.tar.xz [list of files and folders]\n</code></pre>\n<p>Caveats:</p>\n<ul>\n<li>This commands needs a lot of RAM, at the very least 5GB. You may want to reduce the compression level, or remove <code>ultra/long</code> options to decrease RAM consumption.</li>\n<li>Threading might not be used if you don't have enough source data (less than 1GB). If you still want to use threading for a small amount of data, decrease the compression level from 22 to say 20.</li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/608207/how-to-use-multi-threading-for-creating-and-extracting-tar-xz"
  },
  {
    "question_title": "How do I extract with tar to a different directory?",
    "question_body": "<p>This doesn't work:</p>\n\n<pre><code>tar xf /tmp/foo.tar.gz foo/bar\ntar: foo/bar: Not found in archive\n</code></pre>\n\n<p>It's not obvious to me what would do this beyond extracting it in place and moving the files over.</p>\n",
    "answer": "<p>From <code>man tar</code>:</p>\n\n<pre><code>     -C directory\n         In c and r mode, this changes the directory before adding the\n         following files.  In x mode, change directories after opening the\n         archive but before extracting entries from the archive.\n</code></pre>\n\n<p>i.e, <code>tar xC /foo/bar -f /tmp/foo.tar.gz</code> should do the job.\n(on FreeBSD, <a href=\"http://www.gnu.org/software/tar/manual/tar.html#SEC117\">but GNU tar is basically the same in this respect, see \"Changing the Working Directory\" in its manual</a>)</p>\n",
    "url": "https://unix.stackexchange.com/questions/23744/how-do-i-extract-with-tar-to-a-different-directory"
  },
  {
    "question_title": "What does the -f parameter do in the tar command",
    "question_body": "<p>When using <code>tar</code> I always include <code>-f</code> in the parameters but I have no idea why.</p>\n\n<p>I looked up the man and it said;</p>\n\n<pre><code>-f, --file [HOSTNAME:]F\n\nuse archive file or device F (default\n\"-\", meaning stdin/stdout)\n</code></pre>\n\n<p>But to be honest I have no idea what that means.  Can anyone shed any light on it?</p>\n",
    "answer": "<p>The <code>-f</code> option tells <code>tar</code> that the next argument is the file name of the archive, or standard output if it is <code>-</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/1280/what-does-the-f-parameter-do-in-the-tar-command"
  },
  {
    "question_title": "Is it true that a tarball can change where files are extracted to regardless of the commands used to extract it?",
    "question_body": "<p>I have a coworker who says you need to be careful extracting tarballs because they can make changes you don't know about. I always thought a tarball was just a hierarchy of compressed files, so if you extract it to /tmp/example/ it can't possibly sneak a file into /etc/ or anything like that.</p>\n",
    "answer": "<p>Different tar utilities behave differently in this regard, so it's good to be careful. For a tar file that you didn't create, always list the table of contents before extracting it.</p>\n\n<p><strong><a href=\"https://docs.oracle.com/cd/E26502_01/html/E29030/tar-1.html\" rel=\"noreferrer\">Solaris tar</a>:</strong></p>\n\n<blockquote>\n  <p>The named files are extracted from the tarfile and written to the directory specified in the tarfile, relative to the current directory. Use the relative path names of files and directories to be extracted.</p>\n  \n  <p>Absolute path names contained in the tar archive are unpacked using the absolute path names, that is, the leading forward slash (/) is not stripped off.</p>\n</blockquote>\n\n<p>In the case of a tar file with full (absolute) path names, such as:</p>\n\n<pre><code>/tmp/real-file\n/etc/sneaky-file-here\n</code></pre>\n\n<p>... if you extract such a file, you'll end up with both files.</p>\n\n<p><strong><a href=\"http://www.gnu.org/software/tar/manual/html_node/absolute.html\" rel=\"noreferrer\">GNU tar</a>:</strong></p>\n\n<blockquote>\n  <p>By default, GNU tar drops a leading <code>/</code> on input or output, and complains about file names containing a <code>..</code> component. There is an option that turns off this behavior:</p>\n  \n  <p><code>--absolute-names</code></p>\n  \n  <p><code>-P</code></p>\n  \n  <p>Do not strip leading slashes from file names, and permit file names containing a <code>..</code> file name component.</p>\n</blockquote>\n\n<p>... if you extract a fully-pathed tar file using GNU tar <em>without</em> using the <code>-P</code> option, it will tell you:</p>\n\n<blockquote>\n  <p>tar: Removing leading <code>/</code> from member names</p>\n</blockquote>\n\n<p>and will extract the file into subdirectories of your current directory.</p>\n\n<p><strong><a href=\"https://www.ibm.com/support/knowledgecenter/en/ssw_aix_72/com.ibm.aix.cmds5/tar.htm\" rel=\"noreferrer\">AIX tar</a>:</strong></p>\n\n<p>says nothing about it, and behaves as the Solaris tar -- it will create and extract tar files with full/absolute path names.</p>\n\n<p><strong><a href=\"https://docstore.mik.ua/manuals/hp-ux/en/B2355-60130/tar.1.html\" rel=\"noreferrer\">HP-UX tar</a>:</strong>\n<sup>(better online reference welcomed)</sup></p>\n\n<blockquote>\n  <p><strong>WARNINGS</strong></p>\n  \n  <p>There is no way to restore an absolute path name to a relative position.</p>\n</blockquote>\n\n<p><strong><a href=\"https://man.openbsd.org/tar\" rel=\"noreferrer\">OpenBSD tar</a>:</strong></p>\n\n<blockquote>\n  <p><code>-P</code></p>\n  \n  <p>Do not strip leading slashes (<code>/</code>) from pathnames.  The default is to strip leading slashes.</p>\n</blockquote>\n\n<p>There are <code>-P</code> options implemented for <code>tar</code> on macOS, FreeBSD and NetBSD as well, with the same semantics, with the addition that <code>tar</code> on <a href=\"https://www.freebsd.org/cgi/man.cgi?tar(1)\" rel=\"noreferrer\">FreeBSD</a> and macOS will \"refuse to extract archive entries whose pathnames contain <code>..</code> or\n         whose target directory would be altered by a symlink\" without <code>-P</code>.</p>\n\n<p><strong><a href=\"http://schilytools.sourceforge.net/man/man1/star.1.html\" rel=\"noreferrer\">schilytools star</a>:</strong></p>\n\n<blockquote>\n  <p><code>-/</code></p>\n  \n  <p>Don't  strip  leading  slashes  from  file  names  when extracting  an  archive.  Tar archives containing absolute pathnames are usually a bad idea. With  other  tar implementations,  they  may possibly never be extracted without clobbering existing files. Star for  that  reason,  by  default strips leading slashes from filenames when in extract mode.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/465758/is-it-true-that-a-tarball-can-change-where-files-are-extracted-to-regardless-of"
  },
  {
    "question_title": "Dereferencing hard links",
    "question_body": "<p>In the manual page of <code>tar</code> command, an option for following hard links is listed.</p>\n\n<pre><code>-h, --dereference\n      follow symlinks; archive and dump the files they point to\n\n--hard-dereference\n      follow hard links; archive and dump the files they refer to\n</code></pre>\n\n<p>How does <code>tar</code> know that a file is a hard link? How does it <em>follow</em> it?</p>\n\n<p>What if I don't choose this option? How does it <strong>not</strong> hard-dereference?</p>\n",
    "answer": "<p>By default, if you tell <code>tar</code> to archive a file with hard links, and more than one such link is included among the files to be archived, it archives the file only once, and records the second (and any additional names) as hard links.  This means that when you extract that archive, the hard links will be restored.</p>\n<p>If you use the <a href=\"https://www.gnu.org/software/tar/manual/html_node/Option-Summary.html#index-hard_002ddereference_002c-summary\" rel=\"nofollow noreferrer\"><code>--hard-dereference</code></a> option, then <code>tar</code> does <em>not</em> preserve hard links.  Instead, it treats them as independent files that just happen to have the same contents and metadata.  When you extract the archive, the files will be independent.</p>\n<p>Note:  It recognizes hard links by first checking the link count of the file.  It records the device number and inode of each file with more than one link, and uses that to detect when the same file is being archived again.  (When you use <code>--hard-dereference</code>, it does not do this.)</p>\n",
    "url": "https://unix.stackexchange.com/questions/43037/dereferencing-hard-links"
  },
  {
    "question_title": "Can I zip an entire folder using gzip?",
    "question_body": "<p>I'm trying to zip a folder in unix.\nCan that be done using the gzip command?</p>\n",
    "answer": "<p>No. </p>\n\n<p>Unlike <code>zip</code>, <code>gzip</code> functions as a compression algorithm <em>only</em>.</p>\n\n<p>Because of various reasons some of which hearken back to the era of tape drives, Unix uses a program named <code>tar</code> to archive data, which can then be compressed with a compression program like <code>gzip</code>, <code>bzip2</code>, <code>7zip</code>, etc.</p>\n\n<p>In order to \"zip\" a directory, the correct command would be</p>\n\n<pre><code>tar -zcvf archive.tar.gz directory/ \n</code></pre>\n\n<p>This will tell <code>tar</code> to</p>\n\n<ul>\n<li><p>compress it using the <strong>z</strong> (gzip) algorithm</p></li>\n<li><p><strong>c</strong> (create) an archive from the files in <em><code>directory</code></em> (<code>tar</code> is recursive by default)</p></li>\n<li><p><strong>v</strong> (verbosely) list (on /dev/stderr so it doesn't affect piped commands) all the files it adds to the archive.</p></li>\n<li><p>and store the output as a <strong>f</strong> (file) named <em><code>archive.tar.gz</code></em></p></li>\n</ul>\n\n<p>The <code>tar</code> command offers <code>gzip</code> support (via the <code>-z</code> flag) purely for your convenience. The <code>gzip</code> command/lib is completely separate. The command above is effectively the same as</p>\n\n<pre><code>tar -cv directory | gzip &gt; archive.tar.gz\n</code></pre>\n\n<p>To decompress and unpack the archive into the current directory you would use</p>\n\n<pre><code>tar -zxvf archive.tar.gz\n</code></pre>\n\n<p>That command is effectively the same as</p>\n\n<pre><code>gunzip &lt; archive.tar.gz | tar -xv\n</code></pre>\n\n<p><code>tar</code> has many, many, MANY other options and uses as well; I heartily recommend reading through its manpage sometime.</p>\n",
    "url": "https://unix.stackexchange.com/questions/93139/can-i-zip-an-entire-folder-using-gzip"
  },
  {
    "question_title": "How to tell gzip to keep original file?",
    "question_body": "<p>I would like to compress a text file using gzip command line tool while <strong>keeping</strong> the original file. By default running the following command</p>\n\n<pre><code>gzip file.txt\n</code></pre>\n\n<p>results in modifying this file and renaming it <code>file.txt.gz</code>. instead of this behavior I would like to have this new compressed file in <strong>addition</strong> to the existing one <code>file.txt</code>. For now I am using the following command to do that</p>\n\n<pre><code>gzip -c file.txt &gt; file.txt.gz\n</code></pre>\n\n<p>It works but I am wondering why there is no easier solution to do such a common task ? Maybe I missed the option doing that ?</p>\n",
    "answer": "<p>For GNU <code>gzip</code> 1.6 or above, FreeBSD and derivatives or recent versions of NetBSD, see <a href=\"/a/79411\">don_cristi's answer</a>.</p>\n<p>With any version, you can use shell redirections as in:</p>\n<pre><code>gzip &lt; file.txt &gt; file.txt.gz\n</code></pre>\n<p>When not given any argument, <code>gzip</code> reads its standard input, compresses it and writes the compressed version to its standard output. As a bonus, when using shell redirections, you don't have to worry about files called <code>&quot;--help&quot;</code> or <code>&quot;-&quot;</code> (that latter one still being a problem for <code>gzip -c --</code>).</p>\n<p>Another benefit over <code>gzip -c file.txt &gt; file.txt.gz</code> is that if <code>file.txt</code> can't be opened, the command will fail without creating an empty <code>file.txt.gz</code> (or overwriting an existing <code>file.txt.gz</code>) and without running <code>gzip</code> at all.</p>\n<p>A significant difference compared to <code>gzip -k</code> though is that there will be no attempt at copying the <code>file.txt</code>'s metadata (ownership, permissions, modification time, name of uncompressed file) to <code>file.txt.gz</code>.</p>\n<p>Also if <code>file.txt.gz</code> already existed, it will silently override it unless you have turned the <code>noclobber</code> option on in your shell (with <code>set -o noclobber</code> for instance in POSIX shells).</p>\n",
    "url": "https://unix.stackexchange.com/questions/46786/how-to-tell-gzip-to-keep-original-file"
  },
  {
    "question_title": "Unzipping a .gz file without removing the gzipped file",
    "question_body": "<p>I have a file <code>file.gz</code>, when I try to unzip this file by using <code>gunzip file.gz</code>, it unzipped the file but only contains extracted and removes the <code>file.gz</code> file.</p>\n\n<p>How can I unzip by keeping both unzipped file and zipped file?</p>\n",
    "answer": "<p>Here are several alternatives:</p>\n\n<ul>\n<li><p>Give <code>gunzip</code> the <code>--keep</code> option (version 1.6 or later)</p>\n\n<blockquote>\n  <p><code>-k</code>   <code>--keep</code><br>\n  &nbsp; &nbsp; &nbsp; &nbsp; Keep (don't delete) input files during compression or decompression.</p>\n</blockquote>\n\n<pre><code>gunzip -k file.gz\n</code></pre></li>\n<li><p>Pass the file to <code>gunzip</code> as stdin</p>\n\n<pre><code>gunzip &lt; file.gz &gt; file\n</code></pre></li>\n<li><p>Use <code>zcat</code> (or, on older systems, <code>gzcat</code>)</p>\n\n<pre><code>zcat file.gz &gt; file\n</code></pre></li>\n</ul>\n",
    "url": "https://unix.stackexchange.com/questions/156261/unzipping-a-gz-file-without-removing-the-gzipped-file"
  },
  {
    "question_title": "Why are tar archive formats switching to xz compression to replace bzip2 and what about gzip?",
    "question_body": "<p>More and more <a href=\"http://www.gnu.org/software/tar/manual/tar.html\"><code>tar</code></a> archives use the <a href=\"http://en.wikipedia.org/wiki/Xz\"><code>xz</code></a> format based on LZMA2 for compression instead of the traditional <a href=\"http://en.wikipedia.org/wiki/Bz2\"><code>bzip2(bz2)</code></a> compression. In fact <em>kernel.org</em> made a late \"<em>Good-bye bzip2</em>\" <a href=\"https://www.kernel.org/happy-new-year-and-good-bye-bzip2.html\">announcement, 27th Dec. 2013</a>, indicating kernel sources would from this point on be released in both tar.gz and tar.xz format - and on the main page of the <a href=\"https://www.kernel.org/\">website</a> what's directly offered is in <code>tar.xz</code>.</p>\n\n<p>Are there any specific reasons explaining why this is happening and what is the relevance of <a href=\"http://en.wikipedia.org/wiki/Gzip\"><code>gzip</code></a> in this context?</p>\n",
    "answer": "<p>For distributing archives over the Internet, the following things are generally a priority:</p>\n\n<ol>\n<li>Compression ratio (i.e., how small the compressor makes the data);</li>\n<li>Decompression time (CPU requirements);</li>\n<li>Decompression memory requirements; and</li>\n<li>Compatibility (how wide-spread the decompression program is)</li>\n</ol>\n\n<p>Compression memory &amp; CPU requirements aren't very important, because you can use a large fast machine for that, and you only have to do it once.</p>\n\n<p>Compared to bzip2, xz has a better compression ratio and lower (better) decompression time. It, however—at the compression settings typically used—requires more memory to decompress<sup>[1]</sup> and is somewhat less widespread. Gzip uses less memory than either.</p>\n\n<p>So, both gzip and xz format archives are posted, allowing you to pick:</p>\n\n<ul>\n<li>Need to decompress on a machine with <em>very</em> limited memory (&lt;32 MB): gzip. Given, not very likely when talking about kernel sources.</li>\n<li>Need to decompress minimal tools available: gzip</li>\n<li>Want to save download time and/or bandwidth: xz</li>\n</ul>\n\n<p>There isn't really a realistic combination of factors that'd get you to pick bzip2. So its being phased out.</p>\n\n<p>I looked at compression comparisons in <a href=\"http://stephane.lesimple.fr/blog/2010-07-20/lzop-vs-compress-vs-gzip-vs-bzip2-vs-lzma-vs-lzma2xz-benchmark-reloaded.html\" rel=\"noreferrer\">a blog post</a>. I didn't attempt to replicate the results, and I suspect some of it has changed (mostly, I expect <code>xz</code> has improved, as its the newest.)</p>\n\n<p>(There are some specific scenarios where a good bzip2 implementation may be preferable to xz: bzip2 can compresses a file with lots of zeros and genome DNA sequences better than xz. Newer versions of xz now have an (optional) block mode which allows data recovery after the point of corruption and parallel compression and [in theory] decompression. Previously, only bzip2 offered these.<sup>[2]</sup> However none of these are relevant for kernel distribution)</p>\n\n<hr>\n\n<p>1: In archive size, <code>xz -3</code> is around <code>bzip -9</code>. Then xz uses less memory to decompress. But <code>xz -9</code> (as, e.g., used for Linux kernel tarballs) uses much more than <code>bzip -9</code>. (And even <code>xz -0</code> needs more than <code>gzip -9</code>).</p>\n\n<p>2: <a href=\"http://markmail.org/message/226x5nlbigycxqzb\" rel=\"noreferrer\">F21 System Wide Change: lbzip2 as default bzip2 implementation</a></p>\n",
    "url": "https://unix.stackexchange.com/questions/108100/why-are-tar-archive-formats-switching-to-xz-compression-to-replace-bzip2-and-wha"
  },
  {
    "question_title": "How to uncompress zlib data in UNIX?",
    "question_body": "<p>I have created zlib-compressed data in Python, like this:</p>\n\n<pre><code>import zlib\ns = '...'\nz = zlib.compress(s)\nwith open('/tmp/data', 'w') as f:\n    f.write(z)\n</code></pre>\n\n<p>(or one-liner in shell: <code>echo -n '...' | python2 -c 'import sys,zlib; sys.stdout.write(zlib.compress(sys.stdin.read()))' &gt; /tmp/data</code>)</p>\n\n<p>Now, I want to uncompress the data in shell. Neither <code>zcat</code> nor <code>uncompress</code> work:</p>\n\n<pre><code>$ cat /tmp/data | gzip -d -\ngzip: stdin: not in gzip format\n\n$ zcat /tmp/data \ngzip: /tmp/data.gz: not in gzip format\n\n$ cat /tmp/data | uncompress -\ngzip: stdin: not in gzip format\n</code></pre>\n\n<p>It seems that I have created gzip-like file, but without any headers. Unfortunately I don't see any option to uncompress such raw data in gzip man page, and the zlib package does not contain any executable utility.</p>\n\n<p>Is there a utility to uncompress raw zlib data?</p>\n",
    "answer": "<p>It is also possible to decompress it using standard <a href=\"/questions/tagged/shell-script\" class=\"post-tag\" title=\"show questions tagged &#39;shell-script&#39;\" rel=\"tag\">shell-script</a> + <a href=\"/questions/tagged/gzip\" class=\"post-tag\" title=\"show questions tagged &#39;gzip&#39;\" rel=\"tag\">gzip</a>, if you don't have, or want to use <a href=\"/questions/tagged/openssl\" class=\"post-tag\" title=\"show questions tagged &#39;openssl&#39;\" rel=\"tag\">openssl</a> or other tools.<br>The trick is to prepend the <a href=\"https://web.archive.org/web/20200220015003/http://www.onicos.com/staff/iz/formats/gzip.html\" rel=\"noreferrer\">gzip magic number and compress method</a> to the actual data from <code>zlib.compress</code>:</p>\n<pre><code>printf &quot;\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00&quot; |cat - /tmp/data |gzip -dc &gt;/tmp/out\n</code></pre>\n<p>Edits:<br>\n@d0sboots commented: For RAW Deflate data, you need to add 2 more null bytes: <br> → <code>&quot;\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00&quot;</code></p>\n<p><a href=\"https://stackoverflow.com/questions/9050260/what-does-a-zlib-header-look-like\">This Q on SO</a> gives more information about this approach. An answer there suggests that there is also an 8 byte footer.</p>\n<p>Users @Vitali-Kushner and @mark-bessey reported success even with truncated files, so a gzip footer does not seem strictly required.</p>\n<p>@tobias-kienzler suggested this function for the <a href=\"/questions/tagged/bashrc\" class=\"post-tag\" title=\"show questions tagged &#39;bashrc&#39;\" rel=\"tag\">bashrc</a>:<br>\n<code>zlibd() (printf &quot;\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00&quot; | cat - &quot;$@&quot; | gzip -dc)</code></p>\n",
    "url": "https://unix.stackexchange.com/questions/22834/how-to-uncompress-zlib-data-in-unix"
  },
  {
    "question_title": "Is there a tool that combines zcat and cat transparently?",
    "question_body": "<p>When handling log files, some end up as gzipped files thanks to <code>logrotate</code> and others not. So when you try something like this:</p>\n\n<pre><code>$ zcat *\n</code></pre>\n\n<p>you end up with a command line like <code>zcat xyz.log xyz.log.1 xyz.log.2.gz xyz.log.3.gz</code> and then with:</p>\n\n<pre><code>gzip: xyz.log: not in gzip format\n</code></pre>\n\n<p>Is there a tool that will take the magic bytes, similar to how <code>file</code> works, and use <code>zcat</code> or <code>cat</code> depending on the outcome so that I can pipe the output to <code>grep</code> for example?</p>\n\n<p>NB: I know I can script it, but I am asking whether there is a tool out there already.</p>\n",
    "answer": "<p>Try it with <code>-f</code> or <code>--force</code>:</p>\n\n<pre><code>zcat -f -- *\n</code></pre>\n\n<p>Since <code>zcat</code> is just a simple script that runs</p>\n\n<pre><code>exec gzip -cd \"$@\"\n</code></pre>\n\n<p>with long options that would translate to</p>\n\n<pre><code>exec gzip --stdout --decompress \"$@\"\n</code></pre>\n\n<p>and, as per the <code>man gzip</code> (emphasize mine):</p>\n\n<pre>\n<i>-f --force</i>\n      Force compression or decompression even if the file has multiple links\n      or the corresponding file already exists, or if the compressed data is\n      read from or written to a terminal. <i>If the input data is not in a format\n      recognized by gzip, and if the option --stdout is also given, copy the\n      input data without change to the standard output: <b>let zcat behave as cat</b>.</i>\n</pre>\n\n<p>Also:</p>\n\n<blockquote>\n  <p>so that I can pipe the output to <code>grep</code> for example</p>\n</blockquote>\n\n<p>You could use <code>zgrep</code> for that:</p>\n\n<pre><code>zgrep -- PATTERN *\n</code></pre>\n\n<p>though see Stéphane's comment below.</p>\n",
    "url": "https://unix.stackexchange.com/questions/77296/is-there-a-tool-that-combines-zcat-and-cat-transparently"
  },
  {
    "question_title": "Check validity of gz file",
    "question_body": "<p>How can I check the validity of a gz file, I do not have the hash of the file, I'm using <code>gzip -t</code> but it is not returning any output.</p>\n",
    "answer": "<p>The <code>gzip -t</code> command only returns an exit code to the shell saying whether the file passed the integrity test or not.</p>\n\n<p>Example (in a script):</p>\n\n<pre><code>if gzip -t file.gz; then\n    echo 'file is ok'\nelse \n    echo 'file is corrupt'\nfi\n</code></pre>\n\n<p>Adding <code>-v</code> will make it actually report the result with a message.</p>\n\n<p>Example:</p>\n\n<pre><code>$ gzip -v -t file.gz\nfile.gz:        OK\n</code></pre>\n\n<p>So the file is ok.  Let's corrupt the file (by writing the character <code>0</code> at byte 40 in the file) and try again.</p>\n\n<pre><code>$ dd seek=40 bs=1 count=1 of=file.gz &lt;&lt;&lt;\"0\"\n1+0 records in\n1+0 records out\n1 bytes transferred in 0.000 secs (2028 bytes/sec)\n</code></pre>\n\n\n\n<pre><code>$ gzip -v -t file.gz\nfile.gz:        gzip: file.gz: Inappropriate file type or format\n</code></pre>\n\n<p>The integrity of a file with respect to its compression does <em>not</em> guarantee that the file <em>contents</em> is what you believe it is.  If you have an MD5 checksum (or some similar checksum) of the file from whomever provided it, then you would be able to get an additional confirmation that the file not only is a valid <code>gzip</code> archive, but also that its contents is what you expect it to be.</p>\n",
    "url": "https://unix.stackexchange.com/questions/359303/check-validity-of-gz-file"
  },
  {
    "question_title": "gunzip all .gz files in directory",
    "question_body": "<p>I have a directory with plenty of <code>.txt.gz</code> files (where the names do not follow a specific pattern.)</p>\n\n<p>What is the simplest way to <code>gunzip</code> them? I want to preserve their original names, so that they go from <code>whatevz.txt.gz</code> to <code>whatevz.txt</code></p>\n",
    "answer": "<p>How about just this?</p>\n\n<pre><code>$ gunzip *.txt.gz\n</code></pre>\n\n<p><code>gunzip</code> will create a gunzipped file without the <code>.gz</code> suffix and remove the original file by default (see below for details). <code>*.txt.gz</code> will be expanded by your shell to all the files matching.</p>\n\n<p>This last bit can get you into trouble if it expands to a very long list of files. In that case, try using <code>find</code> and <code>-exec</code> to do the job for you.</p>\n\n<hr>\n\n<p>From the man page <code>gzip(1)</code>:</p>\n\n<blockquote>\n<pre><code>gunzip takes a list of files on its command line and  replaces  each  file\nwhose  name  ends  with  .gz, -gz, .z, -z, or _z (ignoring case) and which\nbegins with the correct magic number with an uncompressed file without the\noriginal  extension.\n</code></pre>\n</blockquote>\n\n<hr>\n\n<h3>Note about 'original name'</h3>\n\n<p>gzip can store and restore the filename used at compression time. Even if you rename the compressed file, you can be surprised to find out it restores to the original name again.</p>\n\n<p>From the gzip manpage:</p>\n\n<blockquote>\n  <p>By default, gzip keeps the original file name and timestamp in  the  compressed\n  file.  These  are  used when decompressing the file with the <code>-N</code> option. This is\n  useful when the compressed file name was truncated or when the time  stamp  was\n  not preserved after a file transfer.</p>\n</blockquote>\n\n<p>And these file names stored in metadata can also be viewed with <code>file</code>:</p>\n\n<pre><code>$ echo \"foo\" &gt; myfile_orig\n$ gzip myfile_orig \n$ mv myfile_orig.gz myfile_new.gz \n$ file myfile_new.gz \nmyfile_new.gz: gzip compressed data, was \"myfile_orig\", last modified: Mon Aug  5 08:46:39 2019, from Unix\n$ gunzip myfile_new.gz        # gunzip without -N\n$ ls myfile_*\nmyfile_new\n\n$ rm myfile_*\n$ echo \"foo\" &gt; myfile_orig\n$ gzip myfile_orig\n$ mv myfile_orig.gz myfile_new.gz \n# gunzip with -N\n$ gunzip -N myfile_new.gz     # gunzip with -N\n$ ls myfile_*\nmyfile_orig\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/56421/gunzip-all-gz-files-in-directory"
  },
  {
    "question_title": "GZip doesn&#39;t produce the same compressed result on macOS vs Linux",
    "question_body": "<p>I have a few thousand files that are individually GZip compressed (passing of course the <code>-n</code> flag so the output is deterministic). They then go into a Git repository. I just discovered that for 3 of these files, Gzip doesn't produce the same output on macOS vs Linux. Here's an example:</p>\n\n<p>macOS</p>\n\n<pre><code>$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | shasum -a 256\n0ac378465b576991e1c7323008efcade253ce1ab08145899139f11733187e455  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip --fast -n | shasum -a 256\n6e145c6239e64b7e28f61cbab49caacbe0dae846ce33d539bf5c7f2761053712  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip -n | shasum -a 256\n3562fd9f1d18d52e500619b4a5d5dfa709f5da8601b9dd64088fb5da8de7b281  -\n\n$ gzip --version\nApple gzip 272.250.1\n</code></pre>\n\n<p>Linux</p>\n\n<pre><code>$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | shasum -a 256\n0ac378465b576991e1c7323008efcade253ce1ab08145899139f11733187e455  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip --fast -n | shasum -a 256\n10ac8b80af8d734ad3688aa6c7d9b582ab62cf7eda6bc1a0f08d6159cad96ddc  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip -n | shasum -a 256\ncbf249e3a35f62a4f3b13e2c91fe0161af5d96a58727d17cf7a62e0ac3806393  -\n\n$ gzip --version\ngzip 1.6\nCopyright (C) 2007, 2010, 2011 Free Software Foundation, Inc.\nCopyright (C) 1993 Jean-loup Gailly.\nThis is free software.  You may redistribute copies of it under the terms of\nthe GNU General Public License &lt;http://www.gnu.org/licenses/gpl.html&gt;.\nThere is NO WARRANTY, to the extent permitted by law.\n\nWritten by Jean-loup Gailly.\n</code></pre>\n\n<p>How is this possible? I thought the GZip implementation was completely standard?</p>\n\n<p><strong>UPDATE:</strong> Just to confirm that macOS and Linux versions do produce the same output most of the time, both OSes output the same hash for:</p>\n\n<pre><code>$ echo \"Vive la France\" | gzip --fast -n | shasum -a 256\naf842c0cb2dbf94ae19f31c55e05fa0e403b249c8faead413ac2fa5e9b854768  -\n</code></pre>\n",
    "answer": "<p>Note that the compression algorithm (Deflate) in GZip is not strictly bijective. To elaborate: For some data, there's more than one possible compressed output depending on the algorithmic implementation and used parameters. So there's no guarantee at all that Apple GZip and gzip 1.6 will return the same <em>compressed</em> output. These outputs are all valid GZip streams, the standard just guarantees that every of these possible outputs will be <em>decompressed</em> to the same original data.</p>\n",
    "url": "https://unix.stackexchange.com/questions/570477/gzip-doesnt-produce-the-same-compressed-result-on-macos-vs-linux"
  },
  {
    "question_title": "Why does the gzip version of files produce a different md5 checksum",
    "question_body": "<p>I have four files that I created using an <code>svndump</code></p>\n\n<pre><code>test.svn \ntest2.svn \ntest.svn.gz  \ntest2.svn.gz\n</code></pre>\n\n<p>now when I run this</p>\n\n<pre><code>md5sum test2.svn test.svn test.svn.gz test2.svn.gz\n</code></pre>\n\n<p>Here is the output</p>\n\n<pre><code>89fc1d097345b0255825286d9b4d64c3  test2.svn\n89fc1d097345b0255825286d9b4d64c3  test.svn\n8284ebb8b4f860fbb3e03e63168b9c9e  test.svn.gz\nab9411efcb74a466ea8e6faea5c0af9d  test2.svn.gz\n</code></pre>\n\n<p>So I can't understand why <code>gzip</code> is compressing files differently is it putting a timestamp somewhere before compressing? <em>I had a similar issue with <code>mysqldump</code> as it was using the date field on top</em></p>\n",
    "answer": "<p><code>gzip</code> stores some of the original file's metadata in record header, including the file modification time and filename, if available. See <a href=\"http://www.gzip.org/zlib/rfc-gzip.html#file-format\">GZIP file format specification</a>.</p>\n\n<p>So it's expected that your two <code>gzip</code> files aren't identical. You can work around this by passing <code>gzip</code> the <code>-n</code> flag, which stops it from including the original filename and timestamp in the header.</p>\n",
    "url": "https://unix.stackexchange.com/questions/31008/why-does-the-gzip-version-of-files-produce-a-different-md5-checksum"
  },
  {
    "question_title": "Fastest way of working out uncompressed size of large GZIPPED file",
    "question_body": "<p>Once a file is gzipped, is there a way of quickly querying it to say what the uncompressed file size is (without decompressing it), especially in cases where the uncompressed file is &gt; 4GB in size.</p>\n<p>According to the RFC <a href=\"https://www.rfc-editor.org/rfc/rfc1952#page-5\" rel=\"nofollow noreferrer\">https://www.rfc-editor.org/rfc/rfc1952#page-5</a> you can query the last 4 bytes of the file, but if the uncompressed file was &gt; 4GB then the value just represents the <code>uncompressed value modulo 2^32</code></p>\n<p>This value can also be retrieved by running <code>gunzip -l foo.gz</code>, however the &quot;uncompressed&quot; column just contains <code>uncompressed value modulo 2^32</code> again, presumably as it's reading the footer as described above.</p>\n<p>I was just wondering if there is a way of getting the uncompressed file size without having to decompress it first, this would be especially useful in the case where gzipped files contain 50GB+ of data and would take a while to decompress using methods like <code>gzcat foo.gz | wc -c</code></p>\n<hr />\n<p><strong>EDIT:</strong> The 4GB limitation is openly acknowledged in the <code>man</code> page of the <code>gzip</code> utility included with OSX (<code>Apple gzip 242</code>)</p>\n<pre><code>  BUGS\n    According to RFC 1952, the recorded file size is stored in a 32-bit\n    integer, therefore, it can not represent files larger than 4GB. This\n    limitation also applies to -l option of gzip utility.\n</code></pre>\n",
    "answer": "<p>I believe the fastest way is to modify <code>gzip</code> so that testing in verbose mode outputs the number of bytes decompressed; on my system, with a 7761108684-byte file, I get</p>\n<pre><code>% time gzip -tv test.gz\ntest.gz:     OK (7761108684 bytes)\ngzip -tv test.gz  44.19s user 0.79s system 100% cpu 44.919 total\n\n% time zcat test.gz| wc -c\n7761108684\nzcat test.gz  45.51s user 1.54s system 100% cpu 46.987 total\nwc -c  0.09s user 1.46s system 3% cpu 46.987 total\n</code></pre>\n<p>To modify gzip (1.6, as available in Debian), the patch is as follows:</p>\n<pre><code>--- a/gzip.c\n+++ b/gzip.c\n@@ -61,6 +61,7 @@\n #include &lt;stdbool.h&gt;\n #include &lt;sys/stat.h&gt;\n #include &lt;errno.h&gt;\n+#include &lt;inttypes.h&gt;\n \n #include &quot;closein.h&quot;\n #include &quot;tailor.h&quot;\n@@ -694,7 +695,7 @@\n \n     if (verbose) {\n         if (test) {\n-            fprintf(stderr, &quot; OK\\n&quot;);\n+            fprintf(stderr, &quot; OK (%jd bytes)\\n&quot;, (intmax_t) bytes_out);\n \n         } else if (!decompress) {\n             display_ratio(bytes_in-(bytes_out-header_bytes), bytes_in, stderr);\n@@ -901,7 +902,7 @@\n     /* Display statistics */\n     if(verbose) {\n         if (test) {\n-            fprintf(stderr, &quot; OK&quot;);\n+            fprintf(stderr, &quot; OK (%jd bytes)&quot;, (intmax_t) bytes_out);\n         } else if (decompress) {\n             display_ratio(bytes_out-(bytes_in-header_bytes), bytes_out,stderr);\n         } else {\n</code></pre>\n<p><a href=\"https://git.savannah.gnu.org/cgit/gzip.git/commit/?id=cf26200380585019e927fe3cf5c0ecb7c8b3ef14\" rel=\"nofollow noreferrer\">A similar approach</a> has been implemented in <code>gzip</code>, and will be included in the release following 1.11; <code>gzip -l</code> now decompresses the data to determine its size.</p>\n",
    "url": "https://unix.stackexchange.com/questions/183465/fastest-way-of-working-out-uncompressed-size-of-large-gzipped-file"
  },
  {
    "question_title": "read first line from .gz compressed file without decompressing entire file",
    "question_body": "<p>I have a huge log file compressed in .gz format and I want to just read the first line of it without uncompressing it to just check the date of the oldest log in the file.</p>\n\n<p>The logs are of the form:</p>\n\n<pre><code>YYYY-MM-DD Log content asnsenfvwen eaifnesinrng\nYYYY-MM-DD Log content asnsenfvwen eaifnesinrng\nYYYY-MM-DD Log content asnsenfvwen eaifnesinrng\n</code></pre>\n\n<p>I just want to read the date in the first line which I would do like this for an uncompressed file:</p>\n\n<pre><code>read logdate otherstuff &lt; logfile.gz\necho $logdate\n</code></pre>\n\n<p>Using zcat is taking too long.</p>\n",
    "answer": "<p>Piping <code>zcat</code>’s output to <code>head -n 1</code> will decompress a small amount of data, guaranteed to be enough to show the first line, but typically no more than a few buffer-fulls (96 KiB in my experiments):</p>\n\n<pre><code>zcat logfile.gz | head -n 1\n</code></pre>\n\n<p>Once <code>head</code> has finished reading one line, it closes its input, which closes the pipe, and <code>zcat</code> stops after receiving a <code>SIGPIPE</code> (which happens when it next tries to write into the closed pipe). You can see this by running</p>\n\n<pre><code>(zcat logfile.gz; echo $? &gt;&amp;2) | head -n 1\n</code></pre>\n\n<p>This will show that <code>zcat</code> exits with code 141, which indicates it stopped because of a <code>SIGPIPE</code> (13 + 128).</p>\n\n<p>You can add more post-processing, <em>e.g.</em> with AWK, to only extract the date:</p>\n\n<pre><code>zcat logfile.gz | awk '{ print $1; exit }'\n</code></pre>\n\n<p>(On macOS you might need to use <code>gzcat</code> rather than <code>zcat</code> to handle gzipped files.)</p>\n",
    "url": "https://unix.stackexchange.com/questions/452011/read-first-line-from-gz-compressed-file-without-decompressing-entire-file"
  },
  {
    "question_title": "How to turn a tar file to a tgz file?",
    "question_body": "<p>I usually create a tgz file for <code>my_files</code> with the command <code>tar -czvf my_files.tgz my_files</code>, and extract them with <code>tar -zxvf my_files.tgz</code>. Now I have a tar file created with the command <code>tar -cvf my_files.tar my_files</code>. I'm wondering how I can turn the <code>my_files.tar</code> into <code>my_files.tgz</code> so that later I can extract it with the command <code>tar -zxvf my_files.tgz</code>? Thanks.</p>\n",
    "answer": "<p>A plain <code>.tar</code> archive created with <code>cf</code> (with or without <code>v</code>) is uncompressed; to get a <code>.tar.gz</code> or <code>.tgz</code> archive, compress it:</p>\n\n<pre><code>gzip &lt; my_files.tar &gt; my_files.tgz\n</code></pre>\n\n<p>You might want to add <code>-9</code> for better compression:</p>\n\n<pre><code>gzip -9 &lt; my_files.tar &gt; my_files.tgz\n</code></pre>\n\n<p>Both variants will leave both archives around; you can use</p>\n\n<pre><code>gzip -9 my_files.tar\n</code></pre>\n\n<p>instead, which will produce <code>my_files.tar.gz</code> and delete <code>my_files.tar</code> (if everything goes well). You can then rename <code>my_files.tar.gz</code> to <code>my_files.tgz</code> if you wish.</p>\n\n<p>With many <code>tar</code> implementations you can extract archives without specifying the <code>z</code> option, and <code>tar</code> will figure out what to do — so you can use the same command with compressed and uncompressed archives.</p>\n",
    "url": "https://unix.stackexchange.com/questions/457949/how-to-turn-a-tar-file-to-a-tgz-file"
  },
  {
    "question_title": "On-the-fly stream compression that doesn&#39;t spill over into hardware resources?",
    "question_body": "<p>I have 200 GB free disk space, 16 GB of RAM (of which ~1 GB is occupied by the  desktop and kernel) and 6 GB of swap.</p>\n\n<p>I have a 240 GB external SSD, with 70 GB used<sup>1</sup> and the rest free, which I need to back up to my disk.</p>\n\n<p>Normally, I would <code>dd if=/dev/sdb of=Desktop/disk.img</code> the disk first, and then compress it, but making the image first is not an option since doing so would require far more disk space than I have, even though the compression step will result in the free space being squashed so the final archive can easily fit on my disk.</p>\n\n<p><code>dd</code> writes to STDOUT by default, and <code>gzip</code> can read from STDIN, so in theory I can write <code>dd if=/dev/sdb | gzip -9 -</code>, but <code>gzip</code> takes significantly longer to read bytes than <code>dd</code> can produce them.</p>\n\n<p>From <a href=\"https://linux.die.net/man/2/pipe\" rel=\"noreferrer\"><code>man pipe</code></a>: </p>\n\n<blockquote>\n  <p>Data written to the write end of the pipe is buffered by the kernel until it is read from the read end of the pipe.</p>\n</blockquote>\n\n<p>I visualise a <code>|</code> as being like a real pipe -- one application shoving data in and the other taking data out of the pipe's queue as quickly as possible. </p>\n\n<p>What when the program on the left side writes more data more quickly than the other side of the pipe can hope to process it? Will it cause extreme memory or swap usage, or will the kernel try to create a FIFO on disk, thereby filling up the disk? Or will it just fail with <code>SIGPIPE Broken pipe</code> if the buffer is too large?</p>\n\n<p><strong>Basically, this boils down to two questions:</strong> </p>\n\n<ol>\n<li>What are the implications and outcomes of shoving more data into a pipe than is read at a time?</li>\n<li>What's the reliable way to compress a datastream to disk without putting the entire uncompressed datastream on the disk?</li>\n</ol>\n\n<p><sub>Note 1: I cannot just copy exactly the first 70 used GB and expect to get a working system or filesystem, because of fragmentation and other things which will require the full contents to be intact.</sub></p>\n",
    "answer": "<p><code>dd</code> reads and writes data one block at a time, and it only ever has one block outstanding. So</p>\n\n<pre><code>valgrind dd if=/dev/zero status=progress of=/dev/null bs=1M\n</code></pre>\n\n<p>shows that <code>dd</code> uses approximately 1MB of memory. You can play around with the block size, and drop <code>valgrind</code>, to see the effect on <code>dd</code>’s speed.</p>\n\n<p>When you pipe into <code>gzip</code>, <code>dd</code> simply slows down to match <code>gzip</code>’s speed. Its memory usage doesn’t increase, nor does it cause the kernel to store the buffers on disk (the kernel doesn’t know how to do that, except <em>via</em> swap). A broken pipe only happens when one of the ends of the pipe dies; see <code>signal(7)</code> and <code>write(2)</code> for details.</p>\n\n<p>Thus</p>\n\n<pre><code>dd if=... iconv=fullblock bs=1M | gzip -9 &gt; ...\n</code></pre>\n\n<p>is a safe way to do what you’re after.</p>\n\n<p>When piping, the writing process ends up being blocked by the kernel if the reading process isn’t keeping up. You can see this by running</p>\n\n<pre><code>strace dd if=/dev/zero bs=1M | (sleep 60; cat &gt; /dev/null)\n</code></pre>\n\n<p>You’ll see that <code>dd</code> reads 1MB, then issues a <code>write()</code> which sits there waiting for one minute while <code>sleep</code> runs. That’s how both sides of a pipe balance out: the kernel blocks writes if the writing process is too fast, and it blocks reads if the reading process is too fast.</p>\n",
    "url": "https://unix.stackexchange.com/questions/371789/on-the-fly-stream-compression-that-doesnt-spill-over-into-hardware-resources"
  },
  {
    "question_title": "speed up gzip compression",
    "question_body": "<p>Is it possible to speed up the <code>gzip</code> process?</p>\n\n<p>I'm using</p>\n\n<pre><code>mysqldump \"$database_name\" | gzip &gt; $BACKUP_DIR/$database_name.sql.gz\n</code></pre>\n\n<p>to backup a database into a directory, <code>$BACKUP_DIR</code>.</p>\n\n<p>the manpage says:</p>\n\n<blockquote>\n  <p>-# --fast --best<br>\n            Regulate  the  speed  of compression using the\n            specified digit #, where -1  or  --fast  indi‐\n            cates  the  fastest  compression  method (less\n            compression) and -9 or  --best  indicates  the\n            slowest compression method (best compression).\n            The default compression level is -6 (that  is,\n            biased  towards high compression at expense of\n            speed).</p>\n</blockquote>\n\n<ul>\n<li>How effective would it be to use <code>--fast</code>?</li>\n<li>Is this effectively lowering the CPU usage on a modern computer?</li>\n</ul>\n\n<h3>My test results</h3>\n\n<p>I didn't notice any acceleration:</p>\n\n<ul>\n<li>7 min, 47 seconds (with default ratio <code>-6</code>)</li>\n<li>8 min, 36 seconds (with ratio <code>--fast</code> ( = 9 ))</li>\n</ul>\n\n<p>So it seems it takes even longer to use the fast compression?</p>\n\n<p>Only higher compression really slows it down:</p>\n\n<ul>\n<li>11 min, 57 seconds (with ratio <code>--best</code> ( = 1 ))</li>\n</ul>\n\n<p>After getting the Idea with <code>lzop</code> I tested that too and it really is faster:</p>\n\n<ul>\n<li>6 min, 14 seconds with <code>lzop -1 -f -o $BACKUP_DIR/$database_name.sql.lzo</code></li>\n</ul>\n",
    "answer": "<p>If you have a multi-core machine using <a href=\"http://www.zlib.net/pigz/\">pigz</a> is much faster than traditional gzip.</p>\n\n<blockquote>\n  <p>pigz, which stands for parallel implementation of gzip, is a fully functional replacement for gzip that exploits multiple processors and multiple cores to the hilt when compressing data. pigz was written by Mark Adler, and uses the zlib and pthread libraries.</p>\n</blockquote>\n\n<p>Pigz ca be used as a drop-in replacement for gzip. Note than only the compression can be parallelised, not the decompression.</p>\n\n<p>Using pigz the command line becomes</p>\n\n<pre><code>mysqldump \"$database_name\" | pigz &gt; $BACKUP_DIR/$database_name.sql.gz\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/88732/speed-up-gzip-compression"
  },
  {
    "question_title": "why should I use tar.xz instead of tar.gz? xz is a lossless data compression program and file format",
    "question_body": "<p>Today first time in my life I saw <code>tar.xz</code> download. I searched the internet and found Wikipedia articles (<a href=\"https://en.wikipedia.org/wiki/Xz\" rel=\"noreferrer\">xz</a> and <a href=\"https://en.wikipedia.org/wiki/XZ_Utils\" rel=\"noreferrer\">XZ Utils</a>)</p>\n\n<p>Interesting quote about the users of <code>xz</code></p>\n\n<blockquote>\n  <p>xz has gained notability for compressing packages in the GNU coreutils\n  project,[7] Debian family of systems deb (file format), openSUSE,[8]\n  Fedora,[9] Arch Linux,[10] Slackware,[11] FreeBSD,[12] Gentoo,[13]\n  GNOME,[14] and TeX Live,[15] as well as being an option to compress a\n  compiled Linux kernel.[16] In March 2013, kernel.org announced the use\n  of xz as the default compressed file format for distributing kernel\n  archive files.[17]</p>\n</blockquote>\n\n<p>I always use <code>tar.gz</code>. When and why should I use <code>tar.xz</code>? What's the use case?</p>\n\n<p>I found out after first comment that a <a href=\"https://unix.stackexchange.com/questions/108100/why-are-tar-archive-formats-switching-to-xz-compression-to-replace-bzip2-and-wha\">similar question</a> already posted. I often compress mongodump/mongoexport (BSON/JSON) and mysqldump (SQL text). Is there an advantage to use tar.xz for those backups?</p>\n",
    "answer": "<p><code>gzip</code> and <code>xz</code> uses two different algorithms, and therefore they perform differently, both in terms of what level of compression they achieve and in terms of the amount of resources they consume while compressing or decompressing.</p>\n\n<p>In <em>general</em>, <code>xz</code> achieves higher compression ratios, but needs a lot more memory and time.</p>\n\n<p>I personally use <code>xz</code> for archiving data; big files that I need to put away for a long time. I use <code>gzip</code> otherwise, since it's usually quicker.</p>\n\n<p>Do test them both and see how they perform on <em>your</em> average <code>tar</code> (or whatever) file.</p>\n",
    "url": "https://unix.stackexchange.com/questions/301587/why-should-i-use-tar-xz-instead-of-tar-gz-xz-is-a-lossless-data-compression-pro"
  },
  {
    "question_title": "Use gzip to compress the files in a directory except for already existing .gz files",
    "question_body": "<p>I have a directory of logs that I would like to set up a job to compress using <a href=\"http://en.wikipedia.org/wiki/Gzip\" rel=\"noreferrer\">gzip</a>. The issue is I don't want to recompress the logs I've already compressed.</p>\n\n<p>I tried using <code>ls | grep -v gz | gzip</code>, but that doesn't seem to work.</p>\n\n<p>Is there a way to do this? Basically I want to gzip every file in the directory that does not end in .gz. </p>\n",
    "answer": "<p>You can just do:</p>\n\n<pre><code>gzip *\n</code></pre>\n\n<p>gzip will tell you it skips the files that already have a <code>.gz</code>  ending.<br>\nIf that message gets in the way you can use:</p>\n\n<pre><code>gzip -q *\n</code></pre>\n\n<hr>\n\n<p>What you tried did not work, because <code>gzip</code> doesn't read the filenames of the files to compress from stdin, for that to work you would have to use:</p>\n\n<pre><code>ls | grep -v gz | xargs gzip\n</code></pre>\n\n<p>You will exclude files with the pattern <code>gz</code> anywhere in the file name, not just at the end.¹ You also have to take note that parsing the output of <code>ls</code> is dangerous when you have file names with spaces, newlines, etc., are involved.</p>\n\n<p>A more clean solution, not relying on <code>gzip</code> to skip files with a <code>.gz</code> ending is, that also handles non-compressed files in subdirectories:</p>\n\n<pre><code>find .  -type f ! -name \"*.gz\" -exec gzip {} \\;\n</code></pre>\n\n<p><br></p>\n\n<hr>\n\n<p>¹ <sub>As <code>izkata</code> commented: using <code>.gz</code> alone to improve this, would not work. You would need to use <code>grep -vF .gz</code> or <code>grep -v '\\.gz$'</code>. That still leaves the danger of processing <code>ls</code>' output</sub></p>\n",
    "url": "https://unix.stackexchange.com/questions/160405/use-gzip-to-compress-the-files-in-a-directory-except-for-already-existing-gz-fi"
  },
  {
    "question_title": "Fastest and most efficient way to get number of records (lines) in a gzip-compressed file",
    "question_body": "<p>I am trying to do a record count on a 7.6 GB gzip file. I found few approaches using the <code>zcat</code> command.</p>\n\n<pre><code>$ zcat T.csv.gz | wc -l\n423668947\n</code></pre>\n\n<p>This works but it takes too much time (more than 10 minutes to get the count). I tried a few more approaches like</p>\n\n<pre><code>$ sed -n '$=' T.csv.gz\n28173811\n$ perl -lne 'END { print $. }' &lt; T.csv.gz\n28173811\n$ awk 'END {print NR}' T.csv.gz\n28173811\n</code></pre>\n\n<p>All three of these commands are executing pretty fast but giving an incorrect count of 28173811.</p>\n\n<p>How can I perform a record count in a minimal amount of time?</p>\n",
    "answer": "<p>The <code>sed</code>, <code>perl</code> and <code>awk</code> commands that you mention may be correct, but they all read the <em>compressed</em> data and counts newline characters in that.  These newline characters have nothing to do with the newline characters in the uncompressed data.</p>\n\n<p>To count the number of lines in the uncompressed data, there is no way around uncompressing it.  Your approach with <code>zcat</code> is the correct approach and since the data is so large, it <em>will</em> take time to uncompress it.</p>\n\n<p>Most utilities that deals with <code>gzip</code> compression and decompression will most likely use the same shared library routines to do so.  The only way to speed it up would be to find an implementation of the <code>zlib</code> routines that are somehow faster than the default ones, and rebuild e.g. <code>zcat</code> to use those.</p>\n",
    "url": "https://unix.stackexchange.com/questions/363644/fastest-and-most-efficient-way-to-get-number-of-records-lines-in-a-gzip-compre"
  },
  {
    "question_title": "7zip, xz, gzip, tar, etc -- what are the differences?",
    "question_body": "<p>what factors should be considered when choosing among 7zip, xz, gzip, tar, etc. for compressing and archiving files?</p>\n",
    "answer": "<p>I first want to clarify that, of the list you provided, <code>tar</code> is the only one that is <strong>not</strong> a compression algorithm. <code>tar</code> is short for <strong>T</strong>ape <strong>Ar</strong>chive, and is used to create archive files. In short, a single file that consists of one or more files.  It is used to bundle files together so that they can be compressed by a compressor that is only able to compress a single file.</p>\n<p>In terms of availability, <code>zip</code> is widely available across UNIX (Linux/BSD/MacOS) and Windows systems. Therefore a <code>zip</code> file is highly portable. Tools to compress/decompress <code>xz</code> and <code>gzip</code> files are also available on Windows systems, but are more commonly seen and used on UNIX systems.</p>\n<p><code>xz</code> and <code>7zip</code> are known to have a better compression algorithm than <code>gzip</code>, but use more memory and time to compress/decompress. This topic is nicely discussed <a href=\"https://unix.stackexchange.com/a/108103/125535\">here</a>.</p>\n<p>I would recommend using <code>gzip</code> when less memory is available, and compression/decompression speed is a concern. <code>7zip</code> and <code>xz</code> can be used when space is a concern and compression/decompression speed is not.</p>\n<p>Some nice benchmarks on these algorithms can be found <a href=\"https://web.archive.org/web/20220702171752/http://catchchallenger.first-world.info/wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO\" rel=\"noreferrer\">here</a>. <strong>Note:</strong> <code>LZMA</code> is the compression algorithm used by <code>7zip</code> and <code>xz</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/322746/7zip-xz-gzip-tar-etc-what-are-the-differences"
  },
  {
    "question_title": "pv (progress bar) and gzip",
    "question_body": "<p>Why is this not possible?</p>\n<pre><code>pv ${dest_file} | gzip -1\n</code></pre>\n<p><code>pv</code> is a progress bar</p>\n<h1>error</h1>\n<pre><code>gzip: compressed data not written to a terminal. Use -f to force compression.\nFor help, type: gzip -h\n   0 B 0:00:00 [   0 B/s] [&gt;                                   ]  0%\n</code></pre>\n<p>This works</p>\n<pre><code>pv ${file_in} | tar -Jxf - -C /outdir\n</code></pre>\n",
    "answer": "<p>What are you trying to achieve is to see the progress bar of the compression process. But it is not possible using <code>pv</code>. It shows only transfer progress, which you can achieve by something like this (anyway, it is the <a href=\"http://www.commandlinefu.com/commands/view/9559/gzip-compression-with-progress-bar-and-remaining-time-displayed\" rel=\"noreferrer\">first link</a> in the google):</p>\n<pre><code>pv input_file | gzip &gt; compressed_file\n</code></pre>\n<p>The progress bar will run fast, and then it will wait for compression, which is not observable anymore using <code>pv</code>.</p>\n<p>But you can do that other way round and watch the output stream, bot here you will not be able to see the actual progress, because <code>pv</code> does not know the actual size of the compressed file:</p>\n<pre><code>gzip &lt;input_file | pv &gt; compressed_file\n</code></pre>\n<p>The best I found so far is the one from <a href=\"http://www.commandlinefu.com/commands/view/5247/tar-directory-and-compress-it-with-showing-progress-and-disk-io-limits\" rel=\"noreferrer\">commandlinefu</a> even with rate limiting and compression of directories:</p>\n<pre><code>$D=directory\ntar pcf - $D | pv -s $(du -sb $D | awk '{print $1}') --rate-limit 500k | gzip &gt; target.tar.gz\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/251691/pv-progress-bar-and-gzip"
  },
  {
    "question_title": "Is it possible to make zcat output text even if it&#39;s uncompressed?",
    "question_body": "<p>The problem is I have some database dumps which are either compressed or in plain text. There is no difference in file extension etc. Using <code>zcat</code> on uncompressed files produces an error instead of the output.</p>\n\n<p>Is there maybe another <code>cat</code> sort of tool that is smart enough to detect what type of input it gets?</p>\n",
    "answer": "<p>Just add the <code>-f</code> option.</p>\n\n<pre><code>$ echo foo | tee file | gzip &gt; file.gz\n$ zcat file file.gz\ngzip: file: not in gzip format\nfoo\n$ zcat -f file file.gz\nfoo\nfoo\n</code></pre>\n\n<p>(use <code>gzip -dcf</code> instead of <code>zcat -f</code> if your <code>zcat</code> is not the GNU (or GNU-emulated like in modern BSDs) one and only knows about <code>.Z</code> files).</p>\n",
    "url": "https://unix.stackexchange.com/questions/131918/is-it-possible-to-make-zcat-output-text-even-if-its-uncompressed"
  },
  {
    "question_title": "tar: Unexpected EOF in archive",
    "question_body": "<p>I was attempting to untar a <code>.tar.gz</code> file, but came across this error:</p>\n\n<pre><code>gzip: stdin: unexpected end of file\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n</code></pre>\n\n<p>The <code>tar.gz</code> file includes a <code>.tar</code> file, which when untarred results in:</p>\n\n<pre><code>tar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n</code></pre>\n\n<p>I tried both <code>–ignore-zeros</code> and <code>–ignore-failed-read</code>, although they both didn't work.</p>\n\n<p>Is there any way I could extract this file even if it is corrupted?</p>\n\n<p>The file type in question: <code>.tar.gz</code>: Gzip Compressed Data, from a UNIX system.</p>\n",
    "answer": "<p>Check two items:</p>\n\n<p>(1) Is the <strong>FILE INCOMPLETE</strong> due to a faulty download?  Re-download, and use the -c option if your are using wget. (happens all the time).</p>\n\n<p>(2) Does the .tar or .tar.gz filename have <strong>ILLEGAL CHARACTERS</strong>.  It's best to keep archive names simple, short, composed of letters and numbers.  (happens all the time).  So just rename the file.  This one nailed me recently as I thought it would be convenient to include a time/date stamp as part of the archive name.  BAD IDEA!</p>\n",
    "url": "https://unix.stackexchange.com/questions/53891/tar-unexpected-eof-in-archive"
  },
  {
    "question_title": "Is it possible to compress a very large file (~30 GB) using gzip?",
    "question_body": "<p>Is it possible to compress a very large file (~30 GB) using gzip?  If so, what commands, switches, and options should I use?</p>\n\n<p>Or is there another program (preferably one commonly available on Ubuntu distributions) that I can use to compress/zip very large files?  Do you have any experience with this?</p>\n",
    "answer": "<p>AFAIK there is no limit of size for <code>gzip</code> - at least not 30GB. Of course, you need the space for the zipped file on your disc, both versions will be there simultanously while compressing.</p>\n\n<p><code>bzip2</code> compresses files (not only big ones :-) better, but it is (sometimes a lot) slower.</p>\n",
    "url": "https://unix.stackexchange.com/questions/39934/is-it-possible-to-compress-a-very-large-file-30-gb-using-gzip"
  },
  {
    "question_title": "Efficiently remove file(s) from large .tgz",
    "question_body": "<p>Assume i have an gzip compressed tar-ball compressedArchive.tgz (+100 files, totaling +5gb). </p>\n\n<p>What would be the fastest way to remove all entries matching a given filename pattern for example prefix*.jpg and then store the remains in a gzip:ed tar-ball again?</p>\n\n<p>Replacing the old archive or creating a new one is not important, whichever is fastest. </p>\n",
    "answer": "<p>With GNU <code>tar</code>, you can do:</p>\n\n<pre><code>pigz -d &lt; file.tgz |\n  tar --delete --wildcards -f - '*/prefix*.jpg' |\n  pigz &gt; newfile.tgz\n</code></pre>\n\n<p>With <code>bsdtar</code>:</p>\n\n<pre><code>pigz -d &lt; file.tgz |\n  bsdtar -cf - --exclude='*/prefix*.jpg' @- |\n  pigz &gt; newfile.tgz\n</code></pre>\n\n<p>(<code>pigz</code> being the multi-threaded version of <code>gzip</code>).</p>\n\n<p>You could overwrite the file over itself like:</p>\n\n<pre><code>{ pigz -d &lt; file.tgz |\n    tar --delete --wildcards -f - '*/prefix*.jpg' |\n    pigz &amp;&amp;\n    perl -e 'truncate STDOUT, tell STDOUT'\n} 1&lt;&gt; file.tgz\n</code></pre>\n\n<p>But that's quite risky, especially if the result ends up being less compressed than the original file (in which case, the second <code>pigz</code> may end up overwriting areas of the file which the first one has not read yet).</p>\n",
    "url": "https://unix.stackexchange.com/questions/80239/efficiently-remove-files-from-large-tgz"
  },
  {
    "question_title": "How to recover a corrupted &quot;tar.gz&quot; file",
    "question_body": "<p>I suddenly needed to recover an old <code>tar.gz</code> file, but as soon as I execute so:</p>\n\n<pre><code>tar -zxvf filename.tar.gz\n</code></pre>\n\n<p>I get this:</p>\n\n<pre><code>gzip: stdin: invalid compressed data--format violated  \ntar: Child returned status 1  \ntar: Error is not recoverable: exiting now\n</code></pre>\n",
    "answer": "<p>What you should try is the following:</p>\n\n<ol>\n<li>Use <code>file</code> command on the archive to see if it's recognized as <code>gzip</code>-ped data.</li>\n<li>Run <code>strace gunzip</code> on the file.  This will print the last bytes read from the file which might help you identify the point in file where corruption occurs.</li>\n<li>Run a debug build of <code>gunzip</code> under <code>gdb</code>.  Try to correct the corrupted section (you have to be extra lucky to be able to do that) and see if it can continue to the end of the file.</li>\n</ol>\n\n<p>Depending on the nature of corruption, you might or might not be able to recover your data.</p>\n",
    "url": "https://unix.stackexchange.com/questions/17452/how-to-recover-a-corrupted-tar-gz-file"
  },
  {
    "question_title": "How to gzip 100 GB files faster with high compression?",
    "question_body": "<p>We have 100+ GB files on a Linux machine, and while trying to perform gzip using below command, gzip is taking minimum 1-2 hours to complete:</p>\n<pre><code>gzip file.txt\n</code></pre>\n<p>Is there a way we can make gzip to run fast with the same level of compression happening when we use gzip?</p>\n<hr />\n<p>CPU: Intel(R) Core(TM) i3-2350M CPU @2.30 GHz</p>\n",
    "answer": "<p>If you are using gzip, you use mostly one processor core (well, some parts of the task, like reading and writing data are kernel tasks and kernel will use another core). Have a look at some multicore-capable gzip replacements, e.g. MiGz (<a href=\"https://github.com/linkedin/migz\" rel=\"noreferrer\">https://github.com/linkedin/migz</a>) or Pigz (<a href=\"https://zlib.net/pigz/\" rel=\"noreferrer\">https://zlib.net/pigz/</a>, for some longer explanation see also e.g. <a href=\"https://medium.com/ngs-sh/pigz-a-faster-alternative-to-gzip-for-big-files-d5909e46d659\" rel=\"noreferrer\">https://medium.com/ngs-sh/pigz-a-faster-alternative-to-gzip-for-big-files-d5909e46d659</a>).</p>\n",
    "url": "https://unix.stackexchange.com/questions/623881/how-to-gzip-100-gb-files-faster-with-high-compression"
  },
  {
    "question_title": "Create an archive with command &quot;gzip&quot;",
    "question_body": "<p>I have to create an archive with the command gzip (not tar - it's necessary) and the archive should contain files from another directory - for example, /etc.\nI tried to use command</p>\n\n<pre><code>gzip myetc.gz /etc\n</code></pre>\n\n<p>But it didn't work.</p>\n",
    "answer": "<p><code>gzip</code> works with files, not directories.  Therefore to create an archive of a directory you need to use <code>tar</code> (which will create a single tarball out of multiple files):</p>\n\n<pre><code>tar cvf myetc.tar /etc\n</code></pre>\n\n<p>or, for a gzipped archive:</p>\n\n<pre><code>tar cvzf myetc.tar.gz /etc\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/284901/create-an-archive-with-command-gzip"
  },
  {
    "question_title": "How to create a gzip file without .gz file extension?",
    "question_body": "<p>I would like to create a gzipped file that retains the original file name.  For example gzipping \"example.txt\" should output a gzipped file named \"example.txt\" rather than \"example.txt.gz.\"  Is it possible to do this elegantly with one command (not doing a subsequent <code>mv</code>)?</p>\n",
    "answer": "<p>This does NOT work:</p>\n\n<pre><code># echo Hello World &gt; example.txt\n# gzip &lt; example.txt &gt; example.txt # WRONG!\n# file example.txt\nexample.txt: gzip compressed data, from Unix, last modified: Thu Mar 21 19:45:29 2013\n# gunzip &lt; example.txt\n&lt;empty file&gt;\n</code></pre>\n\n<p>This is a race condition:</p>\n\n<pre><code># echo Hello World &gt; example.txt\n# dd if=example.txt | gzip | dd of=example.txt # still WRONG!\n# gunzip &lt; example.txt \nHello World # may also be empty\n</code></pre>\n\n<p>The problem is that the <code>&gt; example.txt</code> (or <code>dd of=example.txt</code> for that matter) kills the file before the other process has the chance to read it. So there is no obvious solution, which is why you should stick to <code>mv</code>.</p>\n\n<p>There are a number of ways you could cheat. You can open the file, then unlink it - the file will continue to exist until you close it - and then create a new file with the same name and write the gzipped data to that. However I do not know an obvious way to coerce bash to use that, and even if I did, my answer would still be:</p>\n\n<p>Don't even do it.</p>\n\n<p>If <code>gzip</code> fails for any reason, or any problem occurs, like you running out of space while gzipping (because other processes are writing, or gzip result is larger than the input - which happens for random data - etc.), you just lost your file. Congratulations!</p>\n\n<p>Create a separate file and <code>mv</code> on success. That's the simplest, easy to understand, and most reliable method you will ever find.</p>\n",
    "url": "https://unix.stackexchange.com/questions/68722/how-to-create-a-gzip-file-without-gz-file-extension"
  },
  {
    "question_title": "How can I check if two gzipped files are equal?",
    "question_body": "<p>I am trying to save space while doing a \"dumb\" backup by simply dumping data into a text file. My backup script is executed daily and looks like this:</p>\n\n<ol>\n<li>Create a directory named after the backup date.</li>\n<li>Dump some data into a text file <code>\"$name\"</code>.</li>\n<li>If the file is valid, gzip it: <code>gzip \"$name\"</code>. Otherwise, <code>rm \"$name\"</code>.</li>\n</ol>\n\n<p>Now I want to add an additional step to remove a file if the same data was also available in the day before (and create symlink or hardlink).</p>\n\n<p>At first I thought of using <code>md5sum \"$name\"</code>, but this does not work because I also store the filename and creation date.</p>\n\n<p>Does <code>gzip</code> have an option to compare two gzipped files and tell me whether they are equal or not? If <code>gzip</code> does not have such an option, is there another way to achieve my goal?</p>\n",
    "answer": "<p><a href=\"https://unix.stackexchange.com/a/64207/8250\">@derobert</a>s answer is great, though I want to share some other information that I have found.</p>\n\n<h1>gzip -l -v</h1>\n\n<p>gzip-compressed files contain already a hash (not secure though, see <a href=\"https://stackoverflow.com/q/1515914/427545\">this SO post</a>):</p>\n\n<pre><code>$ echo something &gt; foo\n$ gzip foo\n$ gzip -v -l foo.gz \nmethod  crc     date  time           compressed        uncompressed  ratio uncompressed_name\ndefla 18b1f736 Feb  8 22:34                  34                  10 -20.0% foo\n</code></pre>\n\n<p>One can combine the CRC and uncompressed size to get a quick fingerprint:</p>\n\n<pre><code>gzip -v -l foo.gz | awk '{print $2, $7}'\n</code></pre>\n\n<h1>cmp</h1>\n\n<p>For checking whether two bytes are equal or not, use <code>cmp file1 file2</code>. Now, a gzipped file has some header with the data and footer (CRC plus original size) appended. The <a href=\"http://www.zlib.org/rfc-gzip.html#file-format\" rel=\"noreferrer\">description of the gzip format</a> shows that the header contains the time when the file was compressed and that the file name is a nul-terminated string that is appended after the 10-byte header.</p>\n\n<p>So, assuming that the file name is constant and the same command (<code>gzip \"$name\"</code>) is used, one can check whether two files are different by using <code>cmp</code> and skipping the first bytes including the time:</p>\n\n<pre><code>cmp -i 8 file1 file2\n</code></pre>\n\n<p><strong>Note</strong>: the assumption that the same compression options is important, otherwise the command will always report the file as different. This happens because the compression options are stored in the header and may affect the compressed data. <code>cmp</code> just looks at raw bytes and do not interpret it as gzip.</p>\n\n<p>If you have filenames of the same length, then you could try to calculate the bytes to be skipped after reading the filename. When the filenames are of different size, you could run <code>cmp</code> after skipping bytes, like <code>cmp &lt;(cut -b9- file1) &lt;(cut -b10- file2)</code>.</p>\n\n<h1>zcmp</h1>\n\n<p>This is definitely the best way to go, it first compresses data and starts comparing the bytes with <code>cmp</code> (really, this is what is done in the <code>zcmp</code> (<code>zdiff</code>) shellscript).</p>\n\n<p>One note, do not be afraid of the following note in the manual page:</p>\n\n<blockquote>\n  <p>When both files must be uncompressed before comparison, the second is uncompressed to /tmp.  In all other cases, zdiff and zcmp use only a pipe.</p>\n</blockquote>\n\n<p>When you have a sufficiently new Bash, compression will not use a temporary file, just a pipe. Or, as the <code>zdiff</code> source says:</p>\n\n<pre><code># Reject Solaris 8's buggy /bin/bash 2.03.\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/64200/how-can-i-check-if-two-gzipped-files-are-equal"
  },
  {
    "question_title": "Gzip decompress on file with other extension?",
    "question_body": "<p>Is it possible to use <code>gzip</code> to decompress a gzipped file, without the gz extension, and without moving the file?</p>\n",
    "answer": "<p>You can pass the <code>-S</code> option to use a suffix other than <code>.gz</code>.</p>\n\n<pre><code>gunzip -S .compressed file.compressed\n</code></pre>\n\n<p>If you want the uncompressed file to have some other name, run</p>\n\n<pre><code>gzip -dc &lt;compressed-file &gt;uncompressed-file\ngunzip &lt;compressed-file &gt;uncompressed-file\n</code></pre>\n\n<p>(these commands are equivalent).</p>\n\n<p>Normally unzipping restores the name and date of the original file (when it was compressed); this doesn't happen with <code>-c</code>.</p>\n\n<p>If you want the compressed file and the uncompressed file to have the same name, you can't do it directly, you need to either rename the compressed file or rename the uncompressed file. In particular, <code>gzip</code> removes and recreates its target file, so if you need to modify the file in place because you don't have write permission in the directory, you need to use <code>-c</code> or redirection.</p>\n\n<pre><code>cp somefile /tmp\ngunzip &lt;/tmp/somefile &gt;|somefile\n</code></pre>\n\n<p>Note that <code>gunzip &lt;somefile &gt;somefile</code> will not work, because the <code>gunzip</code> process would see a file truncated to 0 bytes when it starts reading. If you could invoke the truncation, then <code>gunzip</code> would feed back on its own output; either way, this one can't be done in place.</p>\n",
    "url": "https://unix.stackexchange.com/questions/32953/gzip-decompress-on-file-with-other-extension"
  },
  {
    "question_title": "Is gzip atomic?",
    "question_body": "<p>Is <code>gzip</code> atomic?</p>\n\n<p>What happens if I stop the <code>gzip</code> process while it's in the middle of gzipping a file? </p>\n\n<p>If it's not atomic, and if I already pressed Ctrl+C on a <code>gzip *.txt</code> process, how do I safely resume? </p>\n\n<p>(I am not just curious about how to resume, but also about whether <code>gzip</code> specifically is atomic.)</p>\n",
    "answer": "<blockquote>\n  <p>Is gzip atomic?</p>\n</blockquote>\n\n<p>No. It creates a compressed file and then removes the uncompressed original.</p>\n\n<p>Specifically, it does not compress a file <em>in situ</em> and there is a period of time while the file is being compressed where,</p>\n\n<ul>\n<li>the compressed target is incomplete</li>\n<li>the partially compressed file and its source both exist in the filesystem.</li>\n</ul>\n\n<blockquote>\n  <p>What happens if I stop the gzip process while it's in the middle of gzipping a file?</p>\n</blockquote>\n\n<p>If you stop the <code>gzip</code> process with a catchable signal (<code>SIGINT</code> from <kbd>Ctrl C</kbd>, for example) it will cleanup partially created files. Otherwise, depending on the point at which it's stopped, you may end up with a partially compressed file alongside the untouched original.</p>\n\n<blockquote>\n  <p>If it's not atomic, if I already pressed Ctrl+C on a gzip *.txt process, how do i safely resume?</p>\n</blockquote>\n\n<p>You delete the partially compressed version (if it still exists) and restart the <code>gzip</code>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/536984/is-gzip-atomic"
  },
  {
    "question_title": "Does gzip add integrity/crc check to a .tar?",
    "question_body": "<p>I run commands:</p>\n\n<pre><code>tar -cf myArchive.tar myDirectory/\ngzip myArchive.tar\n</code></pre>\n\n<p>then I copy the file over a lot of unreliable mediums, and later I unpack it using:</p>\n\n<pre><code>tar -xzf myArchive.tar.gz\n</code></pre>\n\n<p>The fact that I compressed the tar-ball, will that in any way guarantee the integrity, or at least a CRC of the unpacked content?</p>\n",
    "answer": "<p><code>tar</code> itself does not write down a checksum for later comparsion. If you <code>gzip</code> the <code>tar</code> archive you can have that functionality.</p>\n\n<p><code>tar</code> uses <code>compress</code>. If you use the <code>-Z</code> flag while creating the archive <code>tar</code> will use the <code>compress</code> program when reading or writing the archive. From the <code>gzip</code> manpage:</p>\n\n<blockquote>\n  <p>The standard <em>compress</em> format was not designed to allow consistency\n  checks.</p>\n</blockquote>\n\n<p>But, you can use the <code>-z</code> parameter. Then <code>tar</code> reads and writes the archive through <code>gzip</code>. And <code>gzip</code> writes a crc checksum. To display that checksum use that command:</p>\n\n<pre><code>$ gzip -lv archive.tar.gz\nmethod  crc     date  time           compressed        uncompressed  ratio uncompressed_name\ndefla 3f641c33 Sep 25 14:01               24270              122880  80.3% archive.tar\n</code></pre>\n\n<p>From the <code>gzip</code> manpage:</p>\n\n<blockquote>\n  <p>When using the first two formats (<em>gzip or zip is meant</em>), gunzip checks\n  a 32 bit CRC.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/160172/does-gzip-add-integrity-crc-check-to-a-tar"
  },
  {
    "question_title": "Print archive file list instantly (without decompressing entire archive)",
    "question_body": "<p>A problem with <code>.tar.gz</code> archives is that, when I try to just list an archive's content, the computer actually decompresses it, which would take a very long time if the file is large.</p>\n\n<p>Other file formats like <code>.7z</code>, <code>.rar</code>,<code>.zip</code> don't have this problem. Listing their contents takes just an instant.</p>\n\n<p>In my naive opinion, this is a huge drawback of the <code>.tar.gz</code> archive format. </p>\n\n<p>So I actually have 2 questions:  </p>\n\n<ol>\n<li>why do people use <code>.tar.gz</code> so much, despite this drawback?  </li>\n<li>what choices (I mean other software or tools) do I have if I want the \"instant content listing\" capability? </li>\n</ol>\n",
    "answer": "<p>It's important to understand there's a trade-off here.</p>\n<p><code>tar</code> means <em>tape archiver</em>. On a tape, you do mostly sequential reading and writing. Tapes are rarely used nowadays, but <code>tar</code> is still used for its ability to read and write its data as a stream.</p>\n<p>You can do:</p>\n<pre><code>tar cf - files | gzip | ssh host 'cd dest &amp;&amp; gunzip | tar xf -'\n</code></pre>\n<p>You can't do that with <code>zip</code> or the like.</p>\n<p>You can't even list the content of a <code>zip</code> archive without storing it locally in a seekable file first. Things like:</p>\n<pre><code>curl -s https://github.com/dwp-forge/columns/archive/v.2016-02-27.zip | unzip -l /dev/stdin\n</code></pre>\n<p>won't work.</p>\n<p>To achieve that quick reading of the content, <code>zip</code> or the like need to build an index. That index can be stored at the beginning of the file (in which case it can only be written to regular files, not streams), or at the end, which means the archiver needs to remember all the archive members before printing it in the end and means a truncated archive may not be recoverable.</p>\n<p>That also means archive members need to be compressed individually which means a much lower compression ratio especially if there's a lot of small files.</p>\n<p>Another drawback with formats like <code>zip</code> is that the archiving is linked to the compressing, you can't choose the compression algorithm. See how <code>tar</code> archives used to be compressed with <code>compress</code> (<code>tar.Z</code>), then with <code>gzip</code>, then <code>bzip2</code>, then <code>xz</code> as new more performant compression algorithms were devised. Same goes for encryption. Who would trust <code>zip</code>'s encryption nowadays?</p>\n<p>Now, the problem with <code>tar.gz</code> archives is not that much that you need to uncompress them. Uncompressing is often faster than reading off a disk (you'll probably find that listing the content of a large tgz archive is quicker that listing the same one uncompressed when not cached in memory), but that you need to read the whole archive.</p>\n<p>Not being able to read the index quickly is not really a problem. If you do foresee needing to read the table content of an archive often, you can just store that list in a separate file. For instance, at creation time, you can do:</p>\n<pre><code>tar cvvf - dir 2&gt; file.tar.xz.list | xz &gt; file.tar.xz\n</code></pre>\n<p>A bigger problem IMO is the fact that because of the sequential aspect of the archive, you can't extract individual files without reading the whole beginning section of the archive that leads to it. IOW, you can't do random reads within the archive.</p>\n<p>Now, for seekable files, it doesn't have to be that way.</p>\n<p>If you compress your <code>tar</code> archive with <code>gzip</code>, that compresses it as a whole, the compression algorithm uses data seen at the beginning to compress, so you have to start from the beginning to uncompress.</p>\n<p>But the <code>xz</code> format can be configured to compress data in separate individual chunks (large enough so as the compression to be efficient), that means that as long as you keep an index at the end of those compressed chunks, for seekable files, you access the uncompressed data randomly (in chunks at least).</p>\n<p><code>pixz</code> (parallel <code>xz</code>) uses that capability when compressing <code>tar</code> archives to also add an index of the start of each member of the archive at the end of the <code>xz</code> file.</p>\n<p>So, for seekable files, not only can you get a list of the content of the tar archive instantly (without metadata though) if they have been compressed with <code>pixz</code>:</p>\n<pre><code>pixz -l file.tar.xz\n</code></pre>\n<p>But you can also extract individual elements without having to read the whole archive:</p>\n<pre><code>pixz -x archive/member.txt &lt; file.tar.xz | tar xpf -\n</code></pre>\n<p>Now, as to why things like <code>7z</code> or <code>zip</code> are rarely used on Unix is mostly because they can't archive Unix files. They've been designed for other operating systems. You can't do a faithful backup of data using those. They can't store metadata like owner (id and name), permission, they can't store symlinks, devices, fifos..., they can't store information about hard links, and other metadata information like extended attributes or ACLs.</p>\n<p>Some of them can't even store members with arbitrary names (some will choke on backslash or newline or colon, or non-ascii filenames) (some <code>tar</code> formats also have limitations though).</p>\n<h2>Never uncompress a tgz/tar.xz file to disk!</h2>\n<p>In case it is not obvious, one doesn't use a <code>tgz</code> or <code>tar.bz2</code>, <code>tar.xz</code>... archive as:</p>\n<pre><strike>unxz file.tar.xz\ntar tvf file.tar\nxz file.tar</strike></pre>\n<p>If you've got an uncompressed <code>.tar</code> file lying about on your file system, it's that you've done something wrong.</p>\n<p>The whole point of those <code>xz</code>/<code>bzip2</code>/<code>gzip</code> being stream compressors is that they can be used on the fly, in pipelines as in</p>\n<pre><code>unxz &lt; file.tar.xz | tar tvf -\n</code></pre>\n<p>Though modern <code>tar</code> implementations know how to invoke <code>unxz</code>/<code>gunzip</code>/<code>bzip2</code> by themselves, so:</p>\n<pre><code>tar tvf file.tar.xz\n</code></pre>\n<p>would generally also work (and again uncompress the data on the fly and not store the uncompressed version of the archive on disk).</p>\n<h2>Example</h2>\n<p>Here's a Linux kernel source tree compressed with various formats.</p>\n<pre><code>$ ls --block-size=1 -sS1\n666210304 linux-4.6.tar\n173592576 linux-4.6.zip\n 97038336 linux-4.6.7z\n 89468928 linux-4.6.tar.xz\n</code></pre>\n<p>First, as noted above, the 7z and zip ones are slightly different because they can't store the few symlinks in there and are missing most of the metadata.</p>\n<p>Now a few timings to list the content after having flushed the system caches:</p>\n<pre><code>$ echo 3 | sudo tee /proc/sys/vm/drop_caches\n3\n$ time tar tvf linux-4.6.tar &gt; /dev/null\ntar tvf linux-4.6.tar &gt; /dev/null  0.56s user 0.47s system 13% cpu 7.428 total\n$ time tar tvf linux-4.6.tar.xz &gt; /dev/null\ntar tvf linux-4.6.tar.xz &gt; /dev/null  8.10s user 0.52s system 118% cpu 7.297 total\n$ time unzip -v linux-4.6.zip &gt; /dev/null\nunzip -v linux-4.6.zip &gt; /dev/null  0.16s user 0.08s system 86% cpu 0.282 total\n$ time 7z l linux-4.6.7z &gt; /dev/null\n7z l linux-4.6.7z &gt; /dev/null  0.51s user 0.15s system 89% cpu 0.739 total\n</code></pre>\n<p>You'll notice listing the <code>tar.xz</code> file is quicker than the <code>.tar</code> one even on this 7 years old PC as reading those extra megabytes from the disk takes longer than reading and decompressing the smaller file.</p>\n<p>Then OK, listing the archives with 7z or zip is quicker but that's a non-problem as as I said, it's easily worked around by storing the file list alongside the archive:</p>\n<pre><code>$ tar tvf linux-4.6.tar.xz | xz &gt; linux-4.6.tar.xz.list.xz\n$ ls --block-size=1 -sS1 linux-4.6.tar.xz.list.xz\n434176 linux-4.6.tar.xz.list.xz\n$ time xzcat linux-4.6.tar.xz.list.xz &gt; /dev/null\nxzcat linux-4.6.tar.xz.list.xz &gt; /dev/null  0.05s user 0.00s system 99% cpu 0.051 total\n</code></pre>\n<p>Even faster than 7z or zip even after dropping caches. You'll also notice that the cumulative size of the archive and its index is still smaller than the zip or 7z archives.</p>\n<p>Or use the <code>pixz</code> indexed format:</p>\n<pre><code>$ xzcat linux-4.6.tar.xz | pixz -9  &gt; linux-4.6.tar.pixz\n$ ls --block-size=1 -sS1 linux-4.6.tar.pixz\n89841664 linux-4.6.tar.pixz\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches\n3\n$ time pixz -l linux-4.6.tar.pixz &gt; /dev/null\npixz -l linux-4.6.tar.pixz &gt; /dev/null  0.04s user 0.01s system 57% cpu 0.087 total\n</code></pre>\n<p>Now, to extract individual elements of the archive, the worst case scenario for a tar archive is when accessing the last element:</p>\n<pre><code>$ xzcat linux-4.6.tar.xz.list.xz|tail -1\n-rw-rw-r-- root/root      5976 2016-05-15 23:43 linux-4.6/virt/lib/irqbypass.c\n$ time tar xOf linux-4.6.tar.xz linux-4.6/virt/lib/irqbypass.c | wc\n    257     638    5976\ntar xOf linux-4.6.tar.xz linux-4.6/virt/lib/irqbypass.c  7.27s user 1.13s system 115% cpu 7.279 total\nwc  0.00s user 0.00s system 0% cpu 7.279 total\n</code></pre>\n<p>That's pretty bad as it needs to read (and uncompress) the whole archive. Compare with:</p>\n<pre><code>$ time unzip -p linux-4.6.zip linux-4.6/virt/lib/irqbypass.c | wc\n    257     638    5976\nunzip -p linux-4.6.zip linux-4.6/virt/lib/irqbypass.c  0.02s user 0.01s system 19% cpu 0.119 total\nwc  0.00s user 0.00s system 1% cpu 0.119 total\n</code></pre>\n<p>My version of 7z seems not to be able to do random access, so it seems to be even worse than <code>tar.xz</code>:</p>\n<pre><code>$ time 7z e -so linux-4.6.7z linux-4.6/virt/lib/irqbypass.c 2&gt; /dev/null | wc\n    257     638    5976\n7z e -so linux-4.6.7z linux-4.6/virt/lib/irqbypass.c 2&gt; /dev/null  7.28s user 0.12s system 89% cpu 8.300 total\nwc  0.00s user 0.00s system 0% cpu 8.299 total\n</code></pre>\n<p>Now since we have our <code>pixz</code> generated one from earlier:</p>\n<pre><code>$ time pixz &lt; linux-4.6.tar.pixz -x linux-4.6/virt/lib/irqbypass.c  | tar xOf - | wc\n    257     638    5976\npixz -x linux-4.6/virt/lib/irqbypass.c &lt; linux-4.6.tar.pixz  1.37s user 0.06s system 84% cpu 1.687 total\ntar xOf -  0.00s user 0.01s system 0% cpu 1.693 total\nwc  0.00s user 0.00s system 0% cpu 1.688 total\n</code></pre>\n<p>It's faster but still relatively slow because the archive contains few large blocks:</p>\n<pre><code>$ pixz -tl linux-4.6.tar.pixz\n 17648865 / 134217728\n 15407945 / 134217728\n 18275381 / 134217728\n 19674475 / 134217728\n 18493914 / 129333248\n   336945 /   2958887\n</code></pre>\n<p>So <code>pixz</code> still needs to read and uncompress a (up to a) ~19MB large chunk of data.</p>\n<p>We can make random access faster by making archives will smaller blocks (and sacrifice a bit of disk space):</p>\n<pre><code>$ pixz -f0.25 -9 &lt; linux-4.6.tar &gt; linux-4.6.tar.pixz2\n$ ls --block-size=1 -sS1 linux-4.6.tar.pixz2\n93745152 linux-4.6.tar.pixz2\n$ time pixz &lt; linux-4.6.tar.pixz2 -x linux-4.6/virt/lib/irqbypass.c  | tar xOf - | wc\n    257     638    5976\npixz -x linux-4.6/virt/lib/irqbypass.c &lt; linux-4.6.tar.pixz2  0.17s user 0.02s system 98% cpu 0.189 total\ntar xOf -  0.00s user 0.00s system 1% cpu 0.188 total\nwc  0.00s user 0.00s system 0% cpu 0.187 total\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/287087/print-archive-file-list-instantly-without-decompressing-entire-archive"
  },
  {
    "question_title": "How do I use gunzip and tar to extract my tar.gz file to the specific directory I want?",
    "question_body": "<p>When I run <code>gunzip -dc /path_to_tar.gz_file/zip1.tar.gz | tar xf -</code> in the directory where the <code>tar.gz</code> file is located, it extracts just fine.</p>\n\n<p>How do I tell it to place the contents from the <code>tar.gz</code> file into a specific directory?</p>\n\n<p>I tried this <code>gunzip -dc /path_to_tar.gz_file/zip1.tar.gz | tar xf /target_directory</code> with a <code>tar</code> error.</p>\n\n<p>I should also note here that I am attempting to do this in a bash script and that I'm running Solaris 10.</p>\n",
    "answer": "<p>You can do a single <code>tar</code> command to extract the contents where you want:</p>\n\n<pre><code>tar -zxvf path_to_file -C output_directory\n</code></pre>\n\n<p>As explained in the <code>tar</code> <a href=\"http://man7.org/linux/man-pages/man1/tar.1.html\" rel=\"noreferrer\">manpages</a>:</p>\n\n<blockquote>\n  <p>-C directory, --cd directory, --directory directory</p>\n  \n  <p>In c and r mode, this changes the directory before adding the\n  following files.  In x mode, change directories after opening the\n  archive but before extracting entries from the archive.</p>\n</blockquote>\n\n<hr>\n\n<p>As you added that you are using <a href=\"http://docs.oracle.com/cd/E19455-01/806-0624/6j9vek5hv/index.html\" rel=\"noreferrer\">Solaris</a>, I think you could try:</p>\n\n<pre><code>gunzip -dc path_to_file | tar xf - -C path_to_extract\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/145342/how-do-i-use-gunzip-and-tar-to-extract-my-tar-gz-file-to-the-specific-directory"
  },
  {
    "question_title": "gzip: unexpected end of file with - how to read file anyway",
    "question_body": "<p>I have a job on a batch system that runs extremely long and produces tons of output. So much actually that I have to pipe the standard output through gzip to keep the batch node from filling its work area and subsequently crashing.</p>\n\n<pre><code>longscript | gzip -9 &gt; log.gz\n</code></pre>\n\n<p>Now, I would like to investigate the output of the job while it is still running.\nSo I do this:</p>\n\n<pre><code>gunzip log.gz\n</code></pre>\n\n<p>This runs very long, as it is huge file (several GB). I can see the output file being created while it is running and can look at it while it is being built.</p>\n\n<pre><code>tail log\n&gt; some-line-of-the-log-file\ntail log\n&gt; some-other-line-of-the-log-file\n</code></pre>\n\n<p>However, ultimately, gzip encounters the end of the gzipped file. Since the job is still running and gzip is still writing the file, there is no proper footer yet, so this happens:</p>\n\n<pre><code>gzip: log.gz: unexpected end of file\n</code></pre>\n\n<p>After this, the extracted log file is deleted, as gzip thinks that the corrupted extracted data is of no use to me. I, however, disagree - even if the last couple of lines are scrambled, the output is still highly interesting to me.</p>\n\n<p>How can I convince gzip to let me keep the \"corrupted\" file?</p>\n",
    "answer": "<p>Apart from the very end of the file, you will be able to see the uncompressed data with <code>zcat</code> (or <code>gzip -dc</code>, or <code>gunzip -c</code>):</p>\n\n<pre><code>zcat log.gz | tail\n</code></pre>\n\n<p>or</p>\n\n<pre><code>zcat log.gz | less\n</code></pre>\n\n<p>or</p>\n\n<pre><code>zless log.gz\n</code></pre>\n\n<p><code>gzip</code> will do buffering for obvious reasons (it needs to compress the data in chunks), so even though the program may have outputted some data, that data may not yet be in the <code>log.gz</code> file.</p>\n\n<p>You may also store the uncompressed log with</p>\n\n<pre><code>zcat log.gz &gt; log\n</code></pre>\n\n<p>... but that would be silly since there's obviously a reason why you compress the output in the first place.</p>\n",
    "url": "https://unix.stackexchange.com/questions/358587/gzip-unexpected-end-of-file-with-how-to-read-file-anyway"
  },
  {
    "question_title": "How portable is a gzip file over 4 GB in size?",
    "question_body": "<p>To backup a snapshot of my work, I run a command like <code>tar -czf work.tgz work</code> to create a gzipped tar file, which I can then drop in cloud storage. However, I have just noticed that gzip has a 4 GB size limit, and my <code>work.tgz</code> file is more than 4 GB.</p>\n<p>Despite that, if I create a gzip tar file on my current computer (running Mac OS X 10.15.4, gzip version is called Apple gzip 287.100.2) I can successfully retrieve it. So gunzip works on a &gt;4GB in my particular case. But I want to be able to create and read these large gzip files on either Mac OS X or Linux, and possibly other systems in the future.</p>\n<p>My question is: will I be able to untar/gunzip large files anywhere? In other words, how portable is a gzip file which is more than 4 GB in size? Does it matter if I create it on Mac OS, Linux, or something else?</p>\n<p>A bit of online reading suggests gzip will successfully gzip/gunzip a larger file, but will not correctly record the uncompressed size, because the size is stored as a 32 bit integer. Is that all the limit is?</p>\n",
    "answer": "<blockquote>\n<p>I have just noticed that gzip has a 4 GB size limit</p>\n</blockquote>\n<p>More accurately, the <code>gzip</code> format can’t correctly store uncompressed file <em>sizes</em> over 4GiB; it stores the lower 32 bits of the uncompressed size, and <code>gzip -l</code> misleadingly presents that as the size of the original data. The result is that, up to <code>gzip</code> 1.11 included, <code>gzip -l</code> won’t show the right size for any compressed file whose original size is over 4GiB.</p>\n<p>Apart from that, there is no limit due to <code>gzip</code> itself, and <code>gzip</code>ped files over 4GiB are portable. The format is specified by <a href=\"https://www.rfc-editor.org/rfc/rfc1952\" rel=\"nofollow noreferrer\">RFC 1952</a> and support for it is widely available.</p>\n<p>The confusion over the information presented by <code>gzip -l</code> <a href=\"https://savannah.gnu.org/forum/forum.php?forum_id=10157\" rel=\"nofollow noreferrer\">has been fixed in <code>gzip</code> 1.12</a>; <a href=\"https://git.savannah.gnu.org/cgit/gzip.git/commit/?id=32fef43b442c7abc70414863d64718cd06f6477a\" rel=\"nofollow noreferrer\"><code>gzip -l</code></a> now decompresses the data to determine the real size of the original data, instead of showing the stored size.</p>\n<blockquote>\n<p>Will I be able to untar/gunzip large files anywhere?</p>\n</blockquote>\n<p>Anywhere that can handle large files, and where spec-compliant implementations of <code>tar</code> and <code>gunzip</code> are available.</p>\n<blockquote>\n<p>In other words, how portable is a gzip file which is more than 4 GB in size?</p>\n</blockquote>\n<p>The <code>gzip</code> format itself is portable, and <code>gzip</code> files are also portable, regardless of the size of the data they contain.</p>\n<blockquote>\n<p>Does it matter if I create it on Mac OS, Linux, or something else?</p>\n</blockquote>\n<p>No, a <code>gzip</code> file created on any platform can be uncompressed on any other platform with the required capabilities (in particular, the ability to store large files, in the context of this question).</p>\n<p>See also <a href=\"https://unix.stackexchange.com/q/469298/86440\">Compression Utility Max Files Size Limit | Unix/Linux</a>.</p>\n",
    "url": "https://unix.stackexchange.com/questions/612905/how-portable-is-a-gzip-file-over-4-gb-in-size"
  },
  {
    "question_title": "Estimate the size of the extracted files before extraction a `tar.gz` archive?",
    "question_body": "<p>Before using <code>tar</code> to extract a <code>.tar.gz</code> archive, it is possible to get an estimate of how large the extracted files are in total?</p>\n",
    "answer": "<p><strong>For <code>gzip</code></strong>:</p>\n\n<pre><code>$ gzip -l binutils-2.24.tar.gz\n         compressed        uncompressed  ratio uncompressed_name\n           30809913           186997248  83.5% binutils-2.24.tar\n</code></pre>\n\n<p>Now you see a compressed and an uncompressed size of the content.</p>\n\n<p>Or alternatively use that command:</p>\n\n<pre><code>$ zcat binutils-2.24.tar.gz | wc --bytes\n186997248\n</code></pre>\n\n<p><strong>For <code>bzip2</code></strong>, there is <code>bzcat</code>:</p>\n\n<pre><code>$ bzcat binutils-2.24.tar.bz2 | wc -c\n186997248\n</code></pre>\n\n<p><strong>For <code>rar</code></strong>, use:</p>\n\n<pre><code>$ unrar l archive.rar\n...\n    1        465769002 102749558  22%\n</code></pre>\n\n<p>In the last line of the output there is the original size in bytes (the second digit).</p>\n\n<p><strong>For <code>zip</code></strong>, use</p>\n\n<pre><code>$ unzip -l archive.zip\n...\n700136                     4 files\n</code></pre>\n\n<p>Also the last line (the first digit)</p>\n",
    "url": "https://unix.stackexchange.com/questions/166709/estimate-the-size-of-the-extracted-files-before-extraction-a-tar-gz-archive"
  },
  {
    "question_title": "Why does gzipping a file on stdin yield a smaller output than the same file given as an argument?",
    "question_body": "<p>When I do:</p>\n\n<pre><code># gzip -c foo &gt; foo1.gz \n# gzip &lt; foo &gt; foo2.gz\n</code></pre>\n\n<p>Why does <code>foo2.gz</code> end up being smaller in size than <code>foo1.gz</code>?</p>\n",
    "answer": "<p>Because it's saving the filename and timestamp so that it can try to restore both after you decompress it later. Since <code>foo</code> is given to <code>gzip</code> via <code>&lt;stdin&gt;</code> in your second example, it can't store the filename and timestamp information.</p>\n\n<p>From the manpage:</p>\n\n<pre><code>   -n --no-name\n          When compressing, do not save the original file name and time stamp by default. (The original name is always saved if the name had\n          to  be truncated.) When decompressing, do not restore the original file name if present (remove only the gzip suffix from the com-\n          pressed file name) and do not restore the original time stamp if present (copy it from the compressed file). This  option  is  the\n          default when decompressing.\n\n   -N --name\n          When compressing, always save the original file name and time stamp; this is the default. When decompressing, restore the original\n          file name and time stamp if present. This option is useful on systems which have a limit on file name  length  or  when  the  time\n          stamp has been lost after a file transfer.\n</code></pre>\n\n<p>I've recreated the issue here:</p>\n\n<pre><code>[root@xxx601 ~]# cat /etc/fstab &gt; file.txt\n[root@xxx601 ~]# gzip &lt; file.txt &gt; file.txt.gz\n[root@xxx601 ~]# gzip -c file.txt &gt; file2.txt.gz\n[root@xxx601 ~]# ll -h file*\n-rw-r--r--. 1 root root  465 May 17 19:35 file2.txt.gz\n-rw-r--r--. 1 root root 1.2K May 17 19:34 file.txt\n-rw-r--r--. 1 root root  456 May 17 19:34 file.txt.gz\n</code></pre>\n\n<p>In my example, <code>file.txt.gz</code> is the equivalent of your <code>foo2.gz</code>. Using the <code>-n</code> option disables this behavior when it otherwise <em>would</em> have access to the information:</p>\n\n<pre><code>[root@xxx601 ~]# gzip -nc file.txt &gt; file3.txt.gz\n[root@xxx601 ~]# ll -h file*\n-rw-r--r--. 1 root root  465 May 17 19:35 file2.txt.gz\n-rw-r--r--. 1 root root  456 May 17 19:43 file3.txt.gz\n-rw-r--r--. 1 root root 1.2K May 17 19:34 file.txt\n-rw-r--r--. 1 root root  456 May 17 19:34 file.txt.gz\n</code></pre>\n\n<p>As you can see above, the file sizes for <code>file.txt</code> and <code>file3.txt</code> match since they're now both omitting name and date.</p>\n",
    "url": "https://unix.stackexchange.com/questions/203977/why-does-gzipping-a-file-on-stdin-yield-a-smaller-output-than-the-same-file-give"
  },
  {
    "question_title": "Wget returning binary instead of html?",
    "question_body": "<p>I am using wget to download a static html page.  The W3C Validator tells me the page is encoded in UTF-8.  Yet when I cat the file after download, I get a bunch of binary nonsense.  I'm on Ubuntu, and I thought the default encoding was UTF-8?  That's what my locale file seems to say.  Why is this happening and how can I correct it?</p>\n\n<p>Also, looks like <code>Content-Encoding: gzip</code>.  Perhaps this makes a diff?</p>\n\n<p>This is the simple request:</p>\n\n<pre><code>wget https://www.example.com/page.html\n</code></pre>\n\n<p>I also tried this: </p>\n\n<pre><code>wget https://www.example.com/page.html -q -O - | iconv -f utf-16 -t utf-8 &gt; output.html\n</code></pre>\n\n<p>Which returned: <code>iconv: illegal input sequence at position 40</code></p>\n\n<p>cat'ing the file returns binary that looks like this:</p>\n\n<pre><code>l�?חu�`�q\"�:)s��dġ__��~i��6n)T�$H�#���QJ\n</code></pre>\n\n<p>Result of <code>xxd output.html | head -20</code> :</p>\n\n<pre><code>00000000: 1f8b 0800 0000 0000 0003 bd56 518f db44  ...........VQ..D\n00000010: 107e a6bf 62d4 8a1e 48b9 d8be 4268 9303  .~..b...H...Bh..\n00000020: 8956 082a 155e 7a02 21dd cbd8 3bb6 97ae  .V.*.^z.!...;...\n00000030: 77cd ee38 39f7 a1bf 9d19 3bb9 0bbd 9c40  w..89.....;....@\n00000040: 2088 12c5 de9d 9df9 be99 6f67 f751 9699   .........og.Q..\n00000050: 500d 1d79 5eee a265 faec 7151 e4ab 6205  P..y^..e..qQ..b.\n00000060: 4dd3 0014 1790 e7d0 77c0 ef2f cbf8 cde3  M.......w../....\n00000070: cf1f 7d6c 7d69 ec16 d0d9 c67f 7d7d 56c9  ..}l}i......}}V.\n00000080: 04c5 eb33 35fc e49e 2563 e908 ca10 0d45  ...35...%c.....E\n00000090: 31ce afcf a022 e77a 34c6 fa46 46be d88f  1....\".z4..FF...\n000000a0: a41e ab79 446d 76d6 702b cf45 9e7f ba77  ...yDmv.p+.E...w\n000000b0: 7dc2 779c 274e cc18 483c 3a12 0f75 f07c  }.w.'N..H&lt;:..u.|\n000000c0: 5e63 67dd b886 ab48 e550 b5c4 f0e3 db0d  ^cg....H.P......\n000000d0: 54c1 85b8 8627 2ff3 2ff3 17f9 0626 d31d  T....'/./....&amp;..\n000000e0: d9a6 e5b5 4076 663f 94ec 7b5a 17cf 7ade  ....@vf?..{Z..z.\n000000f0: 00d3 0d9f 4fcc d733 ef8d a0bb 0a06 c7eb  ....O..3........\n00000100: b304 6fb1 b1cc 18ed 90e0 8710 43aa 424f  ..o.........C.BO\n00000110: 50c7 d0c1 2bac 09be 4d1c 2566 335e 666c  P...+...M.%f3^fl\n00000120: 1e20 951d 58fd 6774 f3e9 f317 749f 7fc4  . ..X.gt....t...\n00000130: d651 cdca f5a7 b0a5 aea4 08ab 055c e4c5  .Q...........\\..\n</code></pre>\n\n<p>Also, strangely, the output file seems to open properly in TextWrangler!</p>\n",
    "answer": "<p>This is a <a href=\"https://en.wikipedia.org/wiki/Gzip\" rel=\"noreferrer\">gzip</a> compressed file. You can find this out by running the <a href=\"https://en.wikipedia.org/wiki/File_(command)\" rel=\"noreferrer\"><code>file</code></a> command, which figures out the file format from <a href=\"https://en.wikipedia.org/wiki/Magic_number_(programming)\" rel=\"noreferrer\">magic numbers</a> in the data (this is how programs such as Text Wrangler figure out that the file is compressed as well):</p>\n\n<pre><code>file output.html\nwget -O - … | file -\n</code></pre>\n\n<p>The server (I guessed it from the content you showed) is sending gzipped data and correctly setting the header</p>\n\n<pre><code>Content-Encoding: gzip\n</code></pre>\n\n<p>but wget doesn't support that. In recent versions, wget sends <code>Accept-encoding: identity</code>, to tell the server not to compress or otherwise encode the data. In older versions, you can send the header manually:</p>\n\n<pre><code>wget --header 'Accept-encoding: identity' …\n</code></pre>\n\n<p>However this particular server appears to be broken: it sends compressed data even when told not to encode the data in any way. So you'll have to decompress the data manually.</p>\n\n<pre><code>wget -O output.html.gz … &amp;&amp; gunzip output.html.gz\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/321492/wget-returning-binary-instead-of-html"
  },
  {
    "question_title": "unpigz (and untar) to a specific directory",
    "question_body": "<p>I know how to <code>gunzip</code> a file to a selected location.</p>\n\n<p>But when it comes to utilizing all CPU power, many consider <code>pigz</code> instead of <code>gzip</code>. So, the question is how do I <code>unpigz</code> (and untar) a <strong>*.tar.gz</strong> file to a specific directory?</p>\n",
    "answer": "<p>I found three solutions:</p>\n<ol>\n<li><p>With GNU <code>tar</code>, using the awesome <code>-I</code> option:</p>\n<pre><code>tar -I pigz -xvf /path/to/archive.tar.gz -C /where/to/unpack/it/\n</code></pre>\n</li>\n<li><p>With a lot of Linux piping (a &quot;geek way&quot;):</p>\n<pre><code>unpigz &lt; /path/to/archive.tar.gz | tar -xvC /where/to/unpack/it/\n</code></pre>\n</li>\n<li><p>More portable (to other <code>tar</code> implementations):</p>\n<pre><code>unpigz &lt; /path/to/archive.tar.gz | (cd /where/to/unpack/it/ &amp;&amp; tar xvf -)\n</code></pre>\n</li>\n</ol>\n<p>(You can also replace <code>tar xvf -</code> with <code>pax -r</code> to make it <a href=\"https://en.wikipedia.org/wiki/POSIX\" rel=\"noreferrer\">POSIX</a>-compliant, though not necessarily more portable on Linux-based systems.)</p>\n<p>Credits go to <a href=\"https://unix.stackexchange.com/users/23692/pskocik\">@PSkocik</a> for a proper direction, <a href=\"https://unix.stackexchange.com/users/22565/st%C3%A9phane-chazelas\">@Stéphane Chazelas</a> for the 3rd variant and to the author of <a href=\"https://stackoverflow.com/a/29270282/2202101\">this</a> answer.</p>\n",
    "url": "https://unix.stackexchange.com/questions/198958/unpigz-and-untar-to-a-specific-directory"
  },
  {
    "question_title": "Splitting gzip-file into smaller gz-files without recompressing",
    "question_body": "<p>I have a big .gz file. I would like to split it into 100 smaller gzip files, that can each be decompressed by itself. In other words: I am not looking for a way of chopping up the .gz file into chunks that would have to be put back together to be able to decompress it. I want to be able to decompress each of the smaller files independently.</p>\n\n<p>Can it be done without recompressing the whole file?</p>\n\n<p>Can it be done if the original file is compressed with <code>--rsyncable</code>? (\"Cater better to the rsync program by periodically resetting the internal structure of the compressed data stream.\" sounds like these reset points might be good places to split at and probably prepend a header.)</p>\n\n<p>Can it be done for any of the other compressed formats? I would imagine <code>bzip2</code> would be doable - as it is compressed in blocks.</p>\n",
    "answer": "<p>Split and join of the big file works, but it is impossible to decompress pieces of the compressed file, because essential informations are distributed through the whole dataset. Another way; split the uncompressed file and compress the single parts. Now you can decompress each pieces. But why? You have to merge all decompressed parts before further processing.</p>\n",
    "url": "https://unix.stackexchange.com/questions/338195/splitting-gzip-file-into-smaller-gz-files-without-recompressing"
  },
  {
    "question_title": "How to convert existing gz (gzip) files to rsyncable",
    "question_body": "<p>I am using rsync to back up a repository that contains many gz files including many new ones each day. The rsync backup proceeds more slowly than it should because these gz files are not built with gzip's --rsyncable option (which makes gz files much more 'rsync-friendly' without significantly increasing their size or affecting their compatibility). And I can't fix the problem at creation time because the files are generated by a python script (rdiff-backup) which uses python's gzip module and this does not support an equivalent to gzip's --rsyncable.</p>\n\n<p>So before running rsync I can identify any new gz files in the source data (i.e. new since the last time rsync was run). Now I want to 're-gzip' these files so that they are gzipped in rsyncable-format. Then I can run rsync from the optimised source.</p>\n\n<p>I think this means running each file through gunzip and then gzip --rsyncable but I am not too sure how to do this in a way that won't risk losing data or metadata. Suggestions gratefully received.</p>\n",
    "answer": "<pre><code>#! /bin/bash\n\nset -euo pipefail\n\n##  TOKEN's creation time marks the time since last recompression\nTOKEN=.lastRecompression   \n\nif [ -f ${TOKEN} ]\nthen\n    find -name '*.gz' -cnewer \"${TOKEN}\"\nelse\n    # Process all compressed files if there is no token.\n    find -name '*.gz'\nfi | while read f\ndo\n    # Do it in two steps\n    gunzip &lt; \"$f\" | gzip --rsyncable &gt; \"$f.tmp\"\n\n    # Preserve attributes\n    cp \"$f\" \"$f.tmp\" --attributes-only\n\n    # and rename atomically.\n    # set -e ensures that a problem in the previous step \n    # will stop the full script. \n    mv -v \"$f.tmp\" \"$f\"\ndone\n\n# Update the token\ntouch ${TOKEN}\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/244449/how-to-convert-existing-gz-gzip-files-to-rsyncable"
  },
  {
    "question_title": "How to append a line in a zipped file without unzipping?",
    "question_body": "<pre><code>mknod /tmp/oracle.pipe p\n\nsqlplus / as sysdba &lt;&lt; _EOF\nset escape on\n\nhost nohup gzip -c &lt; /tmp/oracle.pipe &gt; /tmp/out1.gz \\&amp;\nspool /tmp/oracle.pipe\nselect * from employee;\nspool off\n\n_EOF\n\nrm /tmp/oracle.pip\n</code></pre>\n\n<p>I need to insert a trailer at the end of the zipped file out1.gz ,\nI can count the lines using</p>\n\n<pre><code>count=zcat out1.gz |wc -l\n</code></pre>\n\n<p>How do i insert the trailer</p>\n\n<pre><code>T5 (assuming count=5)\n</code></pre>\n\n<p>At the end of out1.gz without unzipping it.</p>\n",
    "answer": "<p>From <code>man gzip</code> you can read that <code>gzip</code>ped files can simply be concatenated:</p>\n<blockquote>\n<p>ADVANCED USAGE\nMultiple compressed files can be concatenated. In this case, gunzip will extract all members at once. For example:</p>\n<pre><code>        gzip -c file1  &gt; foo.gz\n        gzip -c file2 &gt;&gt; foo.gz\n\n  Then\n\n        gunzip -c foo\n\n  is equivalent to\n</code></pre>\n</blockquote>\n<pre><code>         cat file1 file2\n</code></pre>\n<p>This could also be done using <code>cat</code> for the <code>gzip</code>ped files, e.g.:</p>\n<pre><code>seq 1 4 &gt; A &amp;&amp; gzip A\necho 5 &gt; B &amp;&amp; gzip B\n#now 1 to 4 is in A.gz and 5 in B.gz, we want 1 to 5 in C.gz:\ncat A.gz B.gz &gt; C.gz &amp;&amp; zcat C.gz\n1\n2\n3\n4\n5\n#or for appending B.gz to A.gz:\ncat B.gz &gt;&gt; A.gz\n</code></pre>\n<p>For doing it without external file for you line to be appended, do as follows:</p>\n<pre><code>echo &quot;this is the new line&quot; | gzip - &gt;&gt; original_file.gz\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/264980/how-to-append-a-line-in-a-zipped-file-without-unzipping"
  },
  {
    "question_title": "Make grown extracted tar file small again",
    "question_body": "<p>I packed and compressed a folder to a .tar.gz archive.\nAfter unpacking it was nearly twice as big.</p>\n\n<pre><code>du -sh /path/to/old/folder       = 263M\ndu -sh /path/to/extracted/folder = 420M\n</code></pre>\n\n<p>I searched a lot and found out that tar is actually causing this issue by adding metadata or doing other weird stuff with it.</p>\n\n<p>I made a diff on 2 files inside the folder, as well as a md5sum. There is absolutely no diff and the checksum is the exact same value. Yet, one file is as twice as big as the original one.</p>\n\n<pre><code>root@server:~# du -sh /path/to/old/folder/subfolder/file.mcapm /path/to/extracted/folder/subfolder/file.mcapm\n1.1M    /path/to/old/folder/subfolder/file.mcapm\n2.4M    /path/to/extracted/folder/subfolder/file.mcapm\nroot@server:~# diff /path/to/old/folder/subfolder/file.mcapm /path/to/extracted/folder/subfolder/file.mcapm\nroot@server:~# \nroot@server:~# md5sum /path/to/old/folder/subfolder/file.mcapm\nroot@server:~# f11787a7dd9dcaa510bb63eeaad3f2ad\nroot@server:~# md5sum /path/to/extracted/folder/subfolder/file.mcapm\nroot@server:~# f11787a7dd9dcaa510bb63eeaad3f2ad\n</code></pre>\n\n<p>I am not searching for different methods, but for a way to reduce the size of those files again to their original size.</p>\n\n<p>How can I achieve that?</p>\n",
    "answer": "<p>[this answer is assuming GNU tar and GNU cp]</p>\n<blockquote>\n<p>There is absolutely no diff and the checksum is the exact same value. Yet, one file is as twice as big as the original one.</p>\n<pre><code>1.1M    /path/to/old/folder/subfolder/file.mcapm\n2.4M    /path/to/extracted/folder/subfolder/file.mcapm\n</code></pre>\n</blockquote>\n<p>That <code>.mcapm</code> file is probably <a href=\"https://en.wikipedia.org/wiki/Sparse_file\" rel=\"noreferrer\">sparse</a>. Use the <code>-S</code> (<code>--sparse</code>) <code>tar</code> option when creating the archive.</p>\n<p>Example:</p>\n<pre><code>$ dd if=/dev/null seek=100 of=dummy\n...\n$ mkdir extracted\n\n$ tar -zcf dummy.tgz dummy\n$ tar -C extracted -zxf dummy.tgz\n$ du -sh dummy extracted/dummy\n0       dummy\n52K     extracted/dummy\n\n$ tar -S -zcf dummy.tgz dummy\n$ tar -C extracted -zxf dummy.tgz\n$ du -sh dummy extracted/dummy\n0       dummy\n0       extracted/dummy\n</code></pre>\n<p>You can also &quot;re-sparse&quot; a file afterwards with <code>cp --sparse=always</code>:</p>\n<pre><code>$ dd if=/dev/zero of=junk count=100\n...\n$ du -sh junk\n52K     junk\n$ cp --sparse=always junk junk.sparse &amp;&amp; mv junk.sparse junk\n$ du -sh junk\n0       junk\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/562919/make-grown-extracted-tar-file-small-again"
  },
  {
    "question_title": "Uncompressed file estimation wrong?",
    "question_body": "<p>I had a large (~60G) compressed file (<code>tar.gz</code>).</p>\n\n<p>I used <code>split</code> to break it into 4 parts and then <code>cat</code> to join them back together.</p>\n\n<p>However, now, when I am trying to estimate the size of the uncompressed file, it turns out it is smaller than the original? How is this possible?</p>\n\n<pre><code>$ gzip -l myfile.tar.gz \n         compressed        uncompressed  ratio uncompressed_name\n        60680003101          3985780736 -1422.4% myfile.tar\n</code></pre>\n",
    "answer": "<p>This is caused by the size of the field used to store the uncompressed size in gzipped files: it’s only 32 bits, so <code>gzip</code> can only store sizes of files up to 4 GiB. Anything larger is compressed and uncompressed correctly, but <code>gzip -l</code> gives an incorrect uncompressed size in versions 1.11 and older.</p>\n<p>So splitting the tarball and reconstructing it hasn’t caused this, and shouldn’t have affected the file — if you want to make sure, you can check it with <code>gzip -tv</code>.</p>\n<p>See <a href=\"https://unix.stackexchange.com/q/183465/86440\">Fastest way of working out uncompressed size of large GZIPPED file</a> for more details, and <a href=\"https://www.gnu.org/software/gzip/manual/html_node/Invoking-gzip.html#Invoking-gzip\" rel=\"nofollow noreferrer\">the <code>gzip</code> manual</a>:</p>\n<blockquote>\n<p>The <code>gzip</code> format represents the input size modulo <em>2³²</em>, so the uncompressed size and compression ratio are listed incorrectly for uncompressed files 4 GiB and larger.</p>\n</blockquote>\n",
    "url": "https://unix.stackexchange.com/questions/472013/uncompressed-file-estimation-wrong"
  },
  {
    "question_title": "gzip - redirection or piping?",
    "question_body": "<p>I will be backing up a large (750GB) disk to an external USB disk using dd.<br>\nShould I be using redirection or piping? Which is more efficient? Or is there a difference?</p>\n\n<p>Also, what is the best block size? USB is likely to be the bottleneck here.</p>\n\n<pre><code>dd if=/dev/sda bs=1M | gzip -c &gt; /mnt/sdb1/backups/disk.img.gz\n\ngzip -dc /mnt/sdb1/backups/disk.img.gz | dd of=/dev/sda bs=1M\n</code></pre>\n\n<p>vs</p>\n\n<pre><code>dd if=/dev/sda bs=1M | gzip -c | dd of=/mnt/sdb1/backups/disk.img.gz\n\ndd if=/mnt/sdb1/backups/disk.img.gz | gzip -dc | dd of=/dev/sda bs=1M\n</code></pre>\n\n<p>Thanks.</p>\n",
    "answer": "<p>You don't need to use <code>dd</code> or piping at all.</p>\n\n<pre><code>&lt;/dev/sda gzip &gt;/mnt/sdb1/backups/disk.img.gz\n&lt;/mnt/sdb1/backups/disk.img.gz gunzip &gt;/dev/sda\n</code></pre>\n\n<p>I once <a href=\"https://unix.stackexchange.com/questions/9432/is-there-a-way-to-determine-the-optimal-value-for-the-bs-parameter-to-dd/9492#9492\">made a benchmark</a> and found using <code>dd</code> slower than <code>cat</code> for a straight copy between different disks. I would expect the pipe to make any solution involving <code>dd</code> even slower in this case.</p>\n",
    "url": "https://unix.stackexchange.com/questions/34003/gzip-redirection-or-piping"
  },
  {
    "question_title": "How can I check if a gzipped file is empty?",
    "question_body": "<p>Is there a quick way to check if a gzipped file is empty, or do I have to unzip it first?</p>\n\n<p>example:</p>\n\n<pre><code>$ touch foo\n$ if [ -s foo ]; then echo not empty; fi\n$ gzip foo\n$ if [ -s foo.gz ]; then echo not empty; fi\nnot empty\n$ wc -l foo.gz\n      1 foo.gz\n</code></pre>\n",
    "answer": "<p><code>gzip -l foo.gz | awk 'NR==2 {print $2}'</code> prints the size of the uncompressed data.</p>\n\n<pre><code>if LC_ALL=C gzip -l foo.gz | awk 'NR==2 {exit($2!=0)}'; then\n  echo foo is empty\nelse\n  echo foo is not empty\nfi\n</code></pre>\n\n<p>Alternatively you can start uncompressing the data.</p>\n\n<pre><code>if [ -n \"$(gunzip &lt;foo.gz | head -c 1 | tr '\\0\\n' __)\" ]; then\n    echo \"foo is not empty\"\nelse\n    echo \"foo is empty\"\nfi\n</code></pre>\n\n<p>(If your system doesn't have <code>head -c</code> to extract the first byte, use <code>head -n 1</code> to extract the first line instead.)</p>\n",
    "url": "https://unix.stackexchange.com/questions/6758/how-can-i-check-if-a-gzipped-file-is-empty"
  },
  {
    "question_title": "tar.gpg vs tar.gz.gpg",
    "question_body": "<p>Is it redundant to compress a tar archive before encrypting it using GPG? From my understanding, GPG will compress the file that it encrypts. Is it preferable to do <code>tar -&gt; gpg</code> instead of <code>tar -&gt; gzip -&gt; gpg</code>?</p>\n",
    "answer": "<p>GnuPG uses ZIP compression by default, which is slightly less performant in many cases than <code>gzip</code>. This would suggest that a separate <code>gzip</code> phase could be useful, but GnuPG also supports zlib compression, using the same compression algorithms as <code>gzip</code>; to select this, specify <code>--compress-algo zlib</code>.</p>\n<p>You can also specify <code>bzip2</code> instead to get better compression, with <code>--compress-algo bzip2</code>.</p>\n<p>In both cases, the compression “level” can be specified with <code>-z</code>, <em>e.g.</em> <code>-z 9</code>.</p>\n<p>PGP only supports ZIP compression, but that’s probably not much of a concern nowadays.</p>\n",
    "url": "https://unix.stackexchange.com/questions/697224/tar-gpg-vs-tar-gz-gpg"
  },
  {
    "question_title": "How to convert all files from gzip to xz on the fly (and recursively)?",
    "question_body": "<p>I have a directory tree with gzipped files like this:</p>\n\n<pre><code>basedir/a/file.dat.gz\nbasedir/b/file.dat.gz\nbasedir/c/file.dat.gz\netc.\n</code></pre>\n\n<p>How can I convert all of these from gzip to xz with a single command and without decompressing each file to disk?</p>\n\n<p>The trivial two-liner with decompressing to disk looks like this:</p>\n\n<pre><code>find basedir/ -type f -name '*.dat.gz' -exec gzip -d {} \\;\nfind basedir/ -type f -name '*.dat' -exec xz {} \\;\n</code></pre>\n\n<p>First command could even be shorter: <code>gunzip -r *</code></p>\n\n<p>For a single file on-the-fly conversion is simple (although this doesn't replace the .gz file):</p>\n\n<pre><code>gzip -cd basedir/a/file.dat.gz | xz &gt; basedir/a/file.dat.xz\n</code></pre>\n\n<p>Since gzip and xz are handling the extensions themselves I'd like to say:</p>\n\n<pre><code>gunzip -rc * &gt; xz\n</code></pre>\n\n<p>I looked at <code>find | xargs basename -s .gz { }</code> a bit but didn't get a working solution.</p>\n\n<p>I could write a shell script, but I feel there should be a simple solution.</p>\n\n<hr>\n\n<p><strong>Edit</strong></p>\n\n<p>Thanks for all who answered already. I know we all love 'commands that will never fail™'. So, to keep this simple:</p>\n\n<ul>\n<li>All subdirectories contain only numbers, letters (äöü, though), underscore and minus.</li>\n<li>All files are named file.dat[.n].gz, n being a positive integer</li>\n<li>No directory or file will have a '.gz' anywhere (other than as the final file suffix).</li>\n<li>This is the only content these directories contain.</li>\n<li>I control the naming and can restrict it if needed.</li>\n</ul>\n\n<p>Using a simple <code>find -exec ...</code> or <code>ls | xargs</code>, is there a command to replace '.gz' in the found filename by '.xz' on the fly? Then I could write something like (pseudo):</p>\n\n<pre><code>find basedir/ -type f -name '*.gz' -exec [ gzip -cd {} | xz &gt; {replace .gz by .xz} \\; ]\n</code></pre>\n",
    "answer": "<pre><code>find . -name '*.gz' -type f -exec bash -o pipefail -Cc '\n  for file do\n    gunzip &lt; \"$file\" | xz &gt; \"${file%.gz}.xz\" &amp;&amp; rm -f \"$file\"\n  done' bash {} +\n</code></pre>\n\n<p>The <code>-C</code> prevents overwriting an existing file and won't follow symlinks <em>except</em> if the exiting file is a non-regular file or a link to a non-regular file, so you would not lose data unless you have for instance a <code>file.gz</code> and a <code>file.xz</code> that is a symlink to <code>/dev/null</code>. To guard against that, you could use <code>zsh</code> instead and also use the <code>-execdir</code> feature of some <code>find</code> implementations for good measure and avoid some race conditions:</p>\n\n<pre><code>find . -name '*.gz' -type f -execdir zsh -o pipefail -c '\n  zmodload zsh/system || exit\n  for file do\n    gunzip &lt; \"$file\" | (\n      sysopen -u 1 -w -o excl -- \"${file%.gz}.xz\" &amp;&amp; xz) &amp;&amp;\n      rm -f -- \"$file\"\n  done' zsh {} +\n</code></pre>\n\n<p>Or to clean-up <code>xz</code> files upon failed recompressions:</p>\n\n<pre><code>find . -name '*.gz' -type f -execdir zsh -o pipefail -c '\n  zmodload zsh/system || exit\n  for file do\n    sysopen -u 1 -w -o excl -- \"${file%.gz}.xz\" &amp;&amp;\n      if gunzip &lt; \"$file\" | xz; then\n        rm -f -- \"$file\"\n      else\n        rm -f -- \"${file%.gz}.xz\"\n      fi\n  done' zsh {} +\n</code></pre>\n\n<p>If you'd rather it being short, and are ready to ignore some of those potential issues,  in <code>zsh</code>, you could do</p>\n\n<pre><code>for f (./**/*.gz(D.)) {gunzip &lt; $f | xz &gt; $f:r.xz &amp;&amp; rm -f $f}\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/308683/how-to-convert-all-files-from-gzip-to-xz-on-the-fly-and-recursively"
  },
  {
    "question_title": "Under what circumstances does gunzip &amp; tar xf work but tar xzf fail?",
    "question_body": "<p>To illustrate the point: I have downloaded the LEDA library from the <a href=\"http://www.algorithmic-solutions.info/free/d5.php\">company's website</a>. Using tar -xzf on it fails:</p>\n\n<pre><code>$ tar -xzf LEDA-6.3-free-fedora-core-8-64-g++-4.1.2-mt.tar.gz \ntar: This does not look like a tar archive\ntar: Skipping to next header\ntar: Exiting with failure status due to previous errors\n</code></pre>\n\n<p>However, gunzip followed by tar -xf works just fine:</p>\n\n<pre><code>$ gunzip LEDA-6.3-free-fedora-core-8-64-g++-4.1.2-mt.tar.gz\n$ tar -xf LEDA-6.3-free-fedora-core-8-64-g++-4.1.2-mt.tar\n# no error\n</code></pre>\n\n<p>Can anyone tell me why this could be?- I'd want the standard <code>tar</code> command to work all the time.</p>\n",
    "answer": "<p>What appears to have happened is that they've <em>double</em> compressed the archive.</p>\n\n<p>If you run <code>file</code> on your gunzip'd file, you'll find its still a gzip archive. And if you rename it to have .gz again, you can gunzip it again.</p>\n\n<p>It seems recently gnu tar will automatically add the <code>-z</code> option, provided the input is a file. So, that's why it works without the <code>-z</code> option after you'd already run <code>gunzip</code> once, tar automatically added it.</p>\n\n<p>This behavior is documented, from the info page:</p>\n\n<blockquote>\n  <p>\"Reading compressed archive is even simpler: you don't need to specify\n  any additional options as GNU `tar' recognizes its format automatically. [...]\n  The format recognition algorithm is based on \"signatures\", a special\n  byte sequences in the beginning of file, that are specific for certain\n  compression formats.\"</p>\n</blockquote>\n\n<p>That's from §8.1.1 \"Creating and Reading Compressed Archives.\"</p>\n",
    "url": "https://unix.stackexchange.com/questions/72528/under-what-circumstances-does-gunzip-tar-xf-work-but-tar-xzf-fail"
  },
  {
    "question_title": "How to get trailing data of gzip archive?",
    "question_body": "<p>I have a gzip archive with trailing data. If I unpack it using <code>gzip -d</code> it tells me: \"<em>decompression OK, trailing garbage ignored</em>\" (same goes for <code>gzip -t</code> which can be used as a method of detecting that there is such data).</p>\n\n<p>Now I would like to get to know this garbage, but strangely enough I couldn't find any way to extract it. <code>gzip -l --verbose</code> tells me that the \"compressed\" size of the archive is the size of the file (i.e. with the trailing data), that's wrong and not helpful. <code>file</code> is also of no help, so what can I do?</p>\n",
    "answer": "<p>Figured out now how to get the trailing data.</p>\n\n<p>I created Perl script which creates a file with the trailing data, it's heavily based on <a href=\"https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=604617#10\" rel=\"noreferrer\">https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=604617#10</a>:</p>\n\n<pre class=\"lang-perl prettyprint-override\"><code>#!/usr/bin/perl\nuse strict;\nuse warnings; \n\nuse IO::Uncompress::Gunzip qw(:all);\nuse IO::File;\n\nunshift(@ARGV, '-') unless -t STDIN;\n\nmy $input_file_name = shift;\nmy $output_file_name = shift;\n\nif (! defined $input_file_name) {\n  die &lt;&lt;END;\nUsage:\n\n  $0 ( GZIP_FILE | - ) [OUTPUT_FILE]\n\n  ... | $0 [OUTPUT_FILE]\n\nExtracts the trailing data of a gzip archive.\nOutputs to stdout if no OUTPUT_FILE is given.\n- as input file file causes it to read from stdin.\n\nExamples:\n\n  $0 archive.tgz trailing.bin\n\n  cat archive.tgz | $0\n\nEND\n}\n\nmy $in = new IO::File \"&lt;$input_file_name\" or die \"Couldn't open gzip file.\\n\";\ngunzip $in =&gt; \"/dev/null\",\n  TrailingData =&gt; my $trailing;\nundef $in;\n\nif (! defined $output_file_name) {\n  print $trailing;\n} else {\n  open(my $fh, \"&gt;\", $output_file_name) or die \"Couldn't open output file.\\n\";\n  print $fh $trailing;\n  close $fh;\n  print \"Output file written.\\n\";\n}\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/295702/how-to-get-trailing-data-of-gzip-archive"
  },
  {
    "question_title": "How to gunzip files recursively (or how to UNDO &#39;gzip -r&#39;)",
    "question_body": "<p>I am learning Linux and I was trying the gzip command. I tried it on a folder which has a hierarchy like</p>\n\n<pre><code>Personal/Folder1/file1.amr\nPersonal/Folder2/file2.amr\nPersonal/Folder3/file3.amr\nPersonal/Folder4/file4.amr\n</code></pre>\n\n<p>I ran\n    \"gzip -r Personal\"\nand now its like</p>\n\n<pre><code>Personal/Folder1/file1.amr.gz\nPersonal/Folder2/file2.amr.gz\nPersonal/Folder3/file3.amr.gz\nPersonal/Folder4/file4.amr.gz\n</code></pre>\n\n<p>How do I go back?</p>\n",
    "answer": "<p>You can use </p>\n\n<pre><code>gunzip -r Personal\n</code></pre>\n\n<p>which works the same as </p>\n\n<pre><code>gzip -d -r Personal\n</code></pre>\n\n<p>If <code>gzip</code> on your system does not have the <code>-r</code> option (e.g. <code>busybox</code>'s gzip) , you can use</p>\n\n<pre><code>find Personal -name \"*.gz\" -type f -print0 | xargs -0 gunzip\n</code></pre>\n",
    "url": "https://unix.stackexchange.com/questions/180247/how-to-gunzip-files-recursively-or-how-to-undo-gzip-r"
  },
  {
    "question_title": "Extract timestamp from a gzip file",
    "question_body": "<p>How can I know the raw original timestamp of a file <code>foo</code> compressed with <code>gzip</code> without having to decompress <code>foo.gz</code>?</p>\n\n<p><code>gzip --verbose --list foo.gz</code> and <code>file foo.gz</code> will print formatted date and time.</p>\n",
    "answer": "<p>Extract the timestamp manually. Assuming that the compressed file has a single member (this is normally the case with gzip):</p>\n\n<pre><code>&lt;foo.gz dd bs=4 skip=1 count=1 | od -t d4\n</code></pre>\n\n<p>This prints the raw timestamp, i.e. the number of seconds since 1970-01-01 00:00 UTC, in decimal.</p>\n",
    "url": "https://unix.stackexchange.com/questions/79543/extract-timestamp-from-a-gzip-file"
  },
  {
    "question_title": "gunzip a folder with many files",
    "question_body": "<p>I have a folder with 36,348 files gz files. I want to unzip all of them.</p>\n\n<p>Running: </p>\n\n<pre><code>gunzip ./*\n</code></pre>\n\n<p>results in </p>\n\n<pre><code>-bash: /usr/bin/gunzip: Argument list too long\n</code></pre>\n\n<p>What's the easiest way to get around this?  </p>\n",
    "answer": "<p>Try:</p>\n\n<pre><code>find . -type f -exec gunzip {} +\n</code></pre>\n\n<p>This assumes that current directory only contains files that you want to unzip.</p>\n",
    "url": "https://unix.stackexchange.com/questions/151815/gunzip-a-folder-with-many-files"
  }
]