[
  {
    "question_title": "Is there a parallel file archiver (like tar)?",
    "question_body": "Is there something out there for parallel archiving of files?\n\n\nTar is great, but I don't use tape archives, and it's more important to me that the archiving happens quickly (with compression like bzip2) since I have smp.",
    "answer": "I think you are looking for pbzip2:\n\n\n\n> PBZIP2 is a parallel implementation of\n> the bzip2 block-sorting file\n> compressor that uses pthreads and\n> achieves near-linear speedup on SMP\n> machines.\n\n\n\nHave a look at the \nproject homepage\n or check your favorite package repository.",
    "url": "https://unix.stackexchange.com/questions/2983/is-there-a-parallel-file-archiver-like-tar"
  },
  {
    "question_title": "There are stopped jobs (on bash exit)",
    "question_body": "I get the message \nThere are stopped jobs.\n when I try to exit a bash shell sometimes. Here is a reproducible scenario in python 2.x:\n\n\n\n\nctrl\n+\nc\n is handled by the interpreter as an exception.\n\n\nctrl\n+\nz\n 'stops' the process.\n\n\nctrl\n+\nd\n exits python for reals.\n\n\n\n\nHere is some real-world terminal output:\n\n\n\n```bash\nexample_user@example_server:~$ python\nPython 2.7.3 (default, Sep 26 2013, 20:03:06) \n[GCC 4.6.3] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\n```\n\n\n\nctrl+z\n\n\n\n```bash\n[1]+  Stopped                 python\nexample_user@example_server:~$ exit\nlogout\nThere are stopped jobs.\n```\n\n\n\nBash did not exit, I must \nexit\n again to exit the bash shell.\n\n\n\n\nQ:\n What is a 'stopped job', or what does this mean?\n\n\nQ:\n Can a stopped process be resumed?\n\n\nQ:\n Does the first \nexit\n kill the stopped jobs?\n\n\nQ:\n Is there a way to exit the shell the first time? (without entering \nexit\n twice)",
    "answer": "A stopped job is one that has been temporarily put into the background and is no longer running, but is still using resources (i.e. system memory). Because that job is not attached to the current terminal, it cannot produce output and is not receiving input from the user.\n\n\nYou can see jobs you have running using the \njobs\n builtin command in bash, probably other shells as well. Example:\n\n\n\n```bash\nuser@mysystem:~$ jobs\n[1] + Stopped                python\nuser@mysystem:~$\n```\n\n\n\nYou can resume a stopped job by using the \nfg\n (foreground) bash built-in command. If you have multiple commands that have been stopped you must specify which one to resume by passing jobspec number on the command line with \nfg\n. If only one program is stopped, you may use \nfg\n alone:\n\n\n\n```bash\nuser@mysystem:~$ fg 1\npython\n```\n\n\n\nAt this point you are back in the python interpreter and may exit by using control-D.\n\n\nConversely, you may \nkill\n the command with either it's jobspec or PID. For instance:\n\n\n\n```bash\nuser@mysystem:~$ ps\n  PID TTY          TIME CMD\n16174 pts/3    00:00:00 bash\n17781 pts/3    00:00:00 python\n18276 pts/3    00:00:00 ps\nuser@mysystem:~$ kill 17781\n[1]+  Killed                  python\nuser@mysystem:~$\n```\n\n\n\nTo use the jobspec, precede the number with the percent (%) key:\n\n\n\n```bash\nuser@mysystem:~$ kill %1\n[1]+  Terminated              python\n```\n\n\n\nIf you issue an exit command with stopped jobs, the warning you saw will be given. The jobs will be left running for safety. That's to make sure you are aware you are attempting to kill jobs you might have forgotten you stopped. The second time you use the exit command the jobs are terminated and the shell exits. This may cause problems for some programs that aren't intended to be killed in this fashion.\n\n\nIn bash it seems you can use the \nlogout\n command which will kill stopped processes and exit. This may cause unwanted results.\n\n\nAlso note that some programs may not exit when terminated in this way, and your system could end up with a lot of orphaned processes using up resources if you make a habit of doing that.\n\n\nNote that you can create background process that will stop if they require user input:\n\n\n\n```bash\nuser@mysystem:~$ python &\n[1] 19028\nuser@mysystem:~$ jobs\n[1]+  Stopped                 python\n```\n\n\n\nYou can resume and kill these jobs in the same way you did jobs that you stopped with the \nCtrl-z\n interrupt.",
    "url": "https://unix.stackexchange.com/questions/116959/there-are-stopped-jobs-on-bash-exit"
  },
  {
    "question_title": "What is the &quot;eval&quot; command in bash?",
    "question_body": "What can you do with the \neval\n command? Why is it useful? Is it some kind of a built-in function in bash? There is no \nman\n page for it..",
    "answer": "eval\n is part of POSIX. It's an interface which can be a shell built-in.\n\n\nIt's described in the \"POSIX Programmer's Manual\": \nhttp://www.unix.com/man-page/posix/1posix/eval/\n\n\n\n```bash\neval - construct command by concatenating arguments\n```\n\n\n\nIt will take an argument and construct a command of it, which will then be executed by the shell. This is the example from the manpage:\n\n\n\n```bash\nfoo=10 x=foo    # 1\ny='$'$x         # 2\necho $y         # 3\n$foo\neval y='$'$x    # 5\necho $y         # 6\n10\n```\n\n\n\n\n\nIn the first line you define \n$foo\n with the value \n'10'\n and \n$x\n with the value \n'foo'\n.\n\n\nNow define \n$y\n, which consists of the string \n'$foo'\n. The dollar sign must be escaped\nwith \n'$'\n.\n\n\nTo check the result, \necho $y\n.\n\n\nThe result of 1)-3) will be the string \n'$foo'\n\n\nNow we repeat the assignment with \neval\n. It will first evaluate \n$x\n to the string \n'foo'\n. Now we have the statement \ny=$foo\n which will get evaluated to \ny=10\n.\n\n\nThe result of \necho $y\n is now the value \n'10'\n.\n\n\n\n\nThis is a common function in many languages:\n\n\n\n\nPerl\n\n\nJavascript",
    "url": "https://unix.stackexchange.com/questions/23111/what-is-the-eval-command-in-bash"
  },
  {
    "question_title": "How do I use gunzip and tar to extract my tar.gz file to the specific directory I want?",
    "question_body": "When I run \ngunzip -dc /path_to_tar.gz_file/zip1.tar.gz | tar xf -\n in the directory where the \ntar.gz\n file is located, it extracts just fine.\n\n\nHow do I tell it to place the contents from the \ntar.gz\n file into a specific directory?\n\n\nI tried this \ngunzip -dc /path_to_tar.gz_file/zip1.tar.gz | tar xf /target_directory\n with a \ntar\n error.\n\n\nI should also note here that I am attempting to do this in a bash script and that I'm running Solaris 10.",
    "answer": "You can do a single \ntar\n command to extract the contents where you want:\n\n\n\n```bash\ntar -zxvf path_to_file -C output_directory\n```\n\n\n\nAs explained in the \ntar\n \nmanpages\n:\n\n\n\n> -C directory, --cd directory, --directory directory\n> In c and r mode, this changes the directory before adding the\n> following files.  In x mode, change directories after opening the\n> archive but before extracting entries from the archive.\n\n\n\n\n\nAs you added that you are using \nSolaris\n, I think you could try:\n\n\n\n```bash\ngunzip -dc path_to_file | tar xf - -C path_to_extract\n```",
    "url": "https://unix.stackexchange.com/questions/145342/how-do-i-use-gunzip-and-tar-to-extract-my-tar-gz-file-to-the-specific-directory"
  },
  {
    "question_title": "What does &lt;&lt;&lt; mean?",
    "question_body": "What does <<< mean? Here is an example:\n\n\n\n```bash\n$ sed 's/a/b/g' <<< \"aaa\"\nbbb\n```\n\n\n\nIs it something general that works with more Linux commands?\n\n\nIt looks like it's feeding the \nsed\n program with the string \naaa\n, but isn't << or < usually used for that?",
    "answer": "Others have answered the basic question: What is it? (Answer: It's a \nhere string\n.)\n\n\nLet's look at why it's useful.\n\n\nYou can also feed a string to a command's stdin like this:\n\n\n\n```bash\necho \"$string\" | command\n```\n\n\n\nHowever in Bash, introducing a pipe means the individual commands are run in subshells. Consider this:\n\n\n\n```bash\necho \"hello world\" | read first second\necho $second $first\n```\n\n\n\nThe output of the 2nd echo command prints just a single space. Whaaaa? What happened to my variables? Because the read command is in a pipeline, it is run in a subshell. It correctly reads 2 words from its stdin and assigns to the variables. But then the command completes, \nthe subshell exits\n and the variables are lost.\n\n\nSometimes you can work around this with braces:\n\n\n\n```bash\necho \"hello world\" | {\n    read first second\n    echo $second $first\n}\n```\n\n\n\nThat's OK if your need for the values is contained, but you still don't have those variables in the current shell of your script.\n\n\nTo remedy this confusing situation, use a here string:\n\n\n\n```bash\nread first second <<< \"hello world\"\necho $second $first\n```\n\n\n\nAh, much better!",
    "url": "https://unix.stackexchange.com/questions/80362/what-does-mean"
  },
  {
    "question_title": "Unmet dependencies while installing Git on Debian",
    "question_body": "I am attempting to install git on \nDebian 8.6 Jessie\n and have run into some dependency issues. What's odd is that I didn't have any issues the few times I recently installed \nGit\n in a VM while I was getting used to Linux.\n\n\napt-get install git\n \n\n\nResults in\n:\n\n\n\n```bash\nThe following packages have unmet dependencies:\n  git : Depends: liberror-perl but is not installable\n        Recommends: rsync but it is not installable\nE: Unable to correct problems, you have held broken packages.\n```\n\n\n\nUPDATE\n\n\nmy \nsources.list\n\n\n\n\nSeems to be an issue with my system. I can no longer properly install anything. I'm getting dependency issues installing things like \nPulseaudio\n which I've previously installed successfully a few days ago.",
    "answer": "You should edit your sources.list , by adding the following line:\n\n\n\n```bash\ndeb http://ftp.ca.debian.org/debian/ jessie main contrib\n```\n\n\n\nThen upgrade your package and install \ngit\n:\n\n\n\n```bash\napt-get update && apt-get upgrade && apt-get dist-upgrade\napt-get -f install\napt-get install git\n```\n\n\n\nEdit\n\n\nthe following package \ngit\n , \nliberror-perl\n and \n[rsync\n]\n3\n can be downloaded from the \nmain\n repo , because you don't have the \nmain\n repo on your \nsources.list\n you cannot install \ngit\n and its dependencies .\n\n\nYour \nsources.list\n should be (with \nnon-free\n packages):\n\n\n\n```bash\ndeb http://ftp.ca.debian.org/debian/ jessie main contrib non-free\ndeb-src http://ftp.ca.debian.org/debian/ jessie main contrib non-free\n\ndeb http://security.debian.org/ jessie/updates main contrib non-free\ndeb-src http://security.debian.org/ jessie/updates main contrib non-free\n\ndeb http://ftp.ca.debian.org/debian/ jessie-updates main contrib non-free\ndeb-src http://ftp.ca.debian.org/debian/ jessie-updates main contrib non-free\n\ndeb http://ftp.ca.debian.org/debian/ jessie-backports main contrib non-free\n```\n\n\n\nOn debian Stretch your \n/etc/apt/sources.list\n should be (at least):\n\n\n\n```bash\ndeb http://deb.debian.org/debian stretch main\ndeb http://security.debian.org/ stretch/updates main \ndeb http://deb.debian.org/debian/ stretch-updates main\n```",
    "url": "https://unix.stackexchange.com/questions/332862/unmet-dependencies-while-installing-git-on-debian"
  },
  {
    "question_title": "How do I get a linux kernel patch set from the mailing list?",
    "question_body": "I don't subscribe to the linux-kernel mailing list, but I want to get a set of patches that were posted a few weeks ago and apply them to my kernel for testing.  I'm very familiar with patching, building, etc.  My question is, what's the best way to get a copy of this patch set?  It's not applied to any Git repo that I'm aware of, it's just been posted to the mailing list for discussion.\n\n\nI find a number of sites that archive the linux-kernel mailing list and I can see the set of patches there, but none of these sites have any method (that I can find) of downloading the raw email so I can use \"git apply\" or \"patch\" or whatever.  Just copy/pasting the content from my web browser seems like it will not be very successful due to whitespace differences etc.\n\n\nHow do people manage this?",
    "answer": "http://marc.info/\n has a link for each message to get the raw body, and \nhttps://lkml.org/\n has (in the sidebar) links to download any contained diffs.\n\n\nThere are also archives with NNTP access that may provide raw messages, though I haven't tried this.",
    "url": "https://unix.stackexchange.com/questions/80519/how-do-i-get-a-linux-kernel-patch-set-from-the-mailing-list"
  },
  {
    "question_title": "In Bash, when to alias, when to script and when to write a function?",
    "question_body": "Noone should need 10 years for asking this question, like I did. If I were just starting out with Linux, I'd want to know: \nWhen to alias, when to script and when to write a function?\n\n\nWhere aliases are concerned, I use aliases for very simple operations that don't take arguments.\n\n\n\n```bash\nalias houston='cd /home/username/.scripts/'\n```\n\n\n\nThat seems obvious. But some people do this:\n\n\n\n```bash\nalias command=\"bash bashscriptname\"\n```\n\n\n\n(and add it to the \n.bashrc\n file).\n\n\nIs there a good reason to do that? I didn't come across a circumstance for this. If there is an edge case where that would make a difference, please answer below.\n\n\nThat's where I would just put something in my PATH and \nchmod +x\n it, which is another thing that came after years of Linux trial-and-error.\n\n\nWhich brings me to the next topic. For instance, I added a hidden folder (\n.scripts/\n) in the home directory to my PATH by just adding a line to my \n.bashrc\n (\nPATH=$PATH:/home/username/.scripts/\n), so anything executable in there automagically autocompletes.\n\n\nI don't really need that, do I? I would only use that for languages which are not the shell, like Python. If it's the shell, I can just write a function inside the very same \n.bashrc\n:\n\n\n\n```bash\nfuncname () {\n  somecommand -someARGS \"$@\"\n}\n```\n\n\n\nDid I miss anything?\n\nWhat would you tell a beginning Linux user about when to alias, when to script and when to write a function?\n\n\nIf it's not obvious, I'm assuming the people who answer this will make use of all three options. If you only use one or two of these three (aliases, scripts, functions), this question isn't really aimed at you.",
    "answer": "The other answers provide some soft general guidelines based on personal taste, but ignore many pertinent \nfacts\n that one should consider when deciding between scripts, functions, or aliases.\n\n\nAliases and Functions ¹\n\n\n\n\nThe entire contents of aliases and functions are stored in the shell's memory.  \n\n\nA natural consequence of this is aliases and functions can \nonly\n be used by the current shell, and not by any other programs you may invoke from the shell like text editors, scripts, or even child instances of the same shell. \n\n\nAliases and functions are executed by the current shell, i.e. they run within and affect the shell's current environment.²  No separate process is necessary to run an alias or function.\n\n\n\n\nScripts\n\n\n\n\nShells do not keep scripts in memory.  Instead, scripts are read from the files where they are stored every time they are needed.  If the script is found via a \n$PATH\n search, many shells store a hash of its path name in memory to save time on future \n$PATH\n look-ups, but that is the extent of a script's memory footprint when not in use.\n\n\nScripts can be invoked in more ways than functions and aliases can.  They can be passed as an argument to an interpreter, like \nsh script\n, or invoked directly as an executable, in which case the interpreter in the shebang line (e.g. \n#!/bin/sh\n) is invoked to run it.  In both cases, the script is run by a separate interpreter process with its own environment separate from that of your shell, whose environment the script cannot affect in any way.  Indeed, the interpreter shell does not even have to match the invoking shell.  Because scripts invoked this way appear to behave like any ordinary executable, they can be used by any program.  \n\n\nFinally, a script can be read and run by the current shell with \n.\n, or in some shells, \nsource\n.  In this case, the script behaves much like a function that is read on-demand instead of being constantly kept in memory.\n\n\n\n\nApplication\n\n\nGiven the above, we can come up with some general guidelines for whether to make something a script or function / alias.\n\n\n\n\nDo other programs besides your shell need to be able to use it?\n  If so, it has to be a script.\n\n\nDo you only want it to be available from an interactive shell?\n  It's common to want to change the default behavior of many commands when run interactively without affecting external commands / scripts.  For this case, use an alias / function set in the shell's \"interactive-mode-only\" rc file (for \nbash\n this is \n.bashrc\n).\n\n\nDoes it need to change the shell's environment?\n  Both a function / alias or a sourced script are possible choices.\n\n\nIs it something you use frequently?\n  It's probably more efficient to keep it in memory, so make it a function / alias if possible.\n\n\nConversely, is it something you use only rarely?\n  In that case, there's no sense having it hog memory when you don't need it, so make it a script.\n\n\n\n\n\n\n¹ While functions and aliases have some important differences, they are grouped together because functions can do everything aliases can.  Aliases can not have local variables nor can they process arguments, and they are inconvenient for anything longer than one line.\n\n\n² Every running process in a Unix system has an \nenvironment\n consisting of a bunch of \nvariable=value\n pairs which often contain global configuration settings, like \nLANG\n for the default locale and \nPATH\n for specifying executable search path.",
    "url": "https://unix.stackexchange.com/questions/30925/in-bash-when-to-alias-when-to-script-and-when-to-write-a-function"
  },
  {
    "question_title": "how can I add (subtract, etc.) two numbers with bash?",
    "question_body": "I can read the numbers and operation in with:\n\n\n\n```bash\necho \"First number please\"\nread num1\necho \"Second number please\"\nread num2\necho \"Operation?\"\nread op\n```\n\n\n\nbut then all my attempts to add the numbers fail:\n\n\n\n```bash\ncase \"$op\" in\n  \"+\")\n    echo num1+num2;;\n  \"-\")\n    echo `num1-num2`;;\nesac\n```\n\n\n\nRun:\n\n\n\n```bash\nFirst number please\n1\nSecond mumber please\n2\nOperation?\n+\n```\n\n\n\nOutput:\n\n\n\n```bash\nnum1+num2\n```\n\n\n\n...or...\n\n\n\n```bash\necho $num1+$num2;;\n\n# results in: 1+2\n```\n\n\n\n...or...\n\n\n\n```bash\necho `$num1`+`$num2`;;\n\n# results in: ...line 9: 1: command not found\n```\n\n\n\nSeems like I'm getting strings still perhaps when I try add add (\"2+2\" instead of \"4\").",
    "answer": "Arithmetic in POSIX shells\n is done with \n$\n and double parentheses \n(( ))\n:\n\n\n\n```bash\necho \"$(($num1+$num2))\"\n```\n\n\n\nYou can assign from that; also note the \n$\n operators on the variable names inside \n(())\n are optional):\n\n\n\n```bash\nnum1=\"$((num1+num2))\"\n```\n\n\n\nThere is also \nexpr\n:\n\n\n\n```bash\nexpr $num1 + $num2\n```\n\n\n\nIn scripting \n$(())\n is preferable since it avoids a fork/execute for the \nexpr\n command. In addition, \nexpr\n’s exit status can be confusing: it’s 0 if the result is \nnot\n empty or 0, 1 if the result is empty or 0 (this is less confusing if one considers that 0 is conventionally a \"normal\" exit status and != 0 an error or unusual state).",
    "url": "https://unix.stackexchange.com/questions/93029/how-can-i-add-subtract-etc-two-numbers-with-bash"
  },
  {
    "question_title": "How do I list every file in a directory except those with specified extensions?",
    "question_body": "Suppose that I have a folder containing \n.txt\n, \n.pdf\n, and other files.  I would like to list the \"other\" files (i.e., files not having the extensions \n.txt\n or \n.pdf\n). Do you have any advice on how to do this?\n\n\nI know how to list files not having a given extension.  For example, if I want to list all files except the \n.txt\n files, then either \n\n\n\n```bash\nfind -not -iname \"*.txt\"\n```\n\n\n\nor \n\n\n\n```bash\nls | grep -v '\\.txt$' | column\n```\n\n\n\nseem to work.  But, how can I list everything except \n.txt\n files \nor\n \n.pdf\n files?  It seems that I need to use some sort of logical \"or\" in \nfind\n or \ngrep\n.",
    "answer": "Assuming one has GNU \nls\n, this is possibly the simplest way:\n\n\n\n```bash\nls -I \"*.txt\" -I \"*.pdf\"\n```\n\n\n\nIf you want to iterate across all the subdirectories:\n\n\n\n```bash\nls -I \"*.txt\" -I \"*.pdf\" -R\n```",
    "url": "https://unix.stackexchange.com/questions/47151/how-do-i-list-every-file-in-a-directory-except-those-with-specified-extensions"
  },
  {
    "question_title": "grep: memory exhausted",
    "question_body": "I was doing a very simple search:\n\n\n\n```bash\ngrep -R Milledgeville ~/Documents\n```\n\n\n\nAnd after some time this error appeared:\n\n\n\n```bash\ngrep: memory exhausted\n```\n\n\n\nHow can I avoid this?\n\n\nI have 10GB of RAM on my system and few applications running, so I am really surprised a simple grep runs out of memory. \n~/Documents\n is about 100GB and contains all kinds of files.\n\n\ngrep -RI\n might not have this problem, but I want to search in binary files too.",
    "answer": "Two potential problems:\n\n\n\n\ngrep -R\n (except for the modified GNU \ngrep\n found on OS/X 10.8 and above) follows symlinks, so even if there's only 100GB of files in \n~/Documents\n, there might still be a symlink to \n/\n for instance and you'll end up scanning the whole file system including files like \n/dev/zero\n. Use \ngrep -r\n with newer GNU \ngrep\n, or use the standard syntax:\n\n\n\n```bash\nfind ~/Documents -type f -exec grep Milledgeville /dev/null {} +\n```\n\n\n\n(however note that the exit status won't reflect the fact that the pattern is matched or not).\n\n\ngrep\n finds the lines that match the pattern. For that, it has to load one line at a time in memory. GNU \ngrep\n as opposed to many other \ngrep\n implementations  doesn't have a limit on the size of the lines it reads and supports search in binary files. So, if you've got a file with a very big line (that is, with two newline characters very far appart), bigger than the available memory, it will fail.\n\n\nThat would typically happen with a sparse file. You can reproduce it with:\n\n\n\n```bash\ntruncate -s200G some-file\ngrep foo some-file\n```\n\n\n\nThat one is difficult to work around. You could do it as (still with GNU \ngrep\n):\n\n\n\n```bash\nfind ~/Documents -type f -exec sh -c 'for i do\n  tr -s \"\\0\" \"\\n\" < \"$i\" | grep --label=\"$i\" -He \"$0\"\n  done' Milledgeville {} +\n```\n\n\n\nThat converts sequences of NUL characters into one newline character prior to feeding the input to \ngrep\n. That would cover for cases where the problem is due to sparse files.\n\n\nYou could optimise it by doing it only for large files:\n\n\n\n```bash\nfind ~/Documents -type f \\( -size -100M -exec \\\n  grep -He Milledgeville {} + -o -exec sh -c 'for i do\n  tr -s \"\\0\" \"\\n\" < \"$i\" | grep --label=\"$i\" -He \"$0\"\n  done' Milledgeville {} + \\)\n```\n\n\n\nIf the files are \nnot\n sparse and you have a version of GNU \ngrep\n prior to \n2.6\n, you can use the \n--mmap\n option. The lines will be mmapped in memory as opposed to copied there, which means the system can always reclaim the memory by paging out the pages to the file. That option was removed in GNU \ngrep\n 2.6",
    "url": "https://unix.stackexchange.com/questions/90036/grep-memory-exhausted"
  },
  {
    "question_title": "tar + rsync + untar. Any speed benefit over just rsync?",
    "question_body": "I often find myself sending folders with 10K - 100K of files to a remote machine (within the same network on-campus). \n\n\nI was just wondering if there are reasons to believe that, \n\n\n\n```bash\ntar + rsync + untar\n```\n\n\n\nOr simply\n\n\n\n```bash\ntar (from src to dest) + untar\n```\n\n\n\ncould be faster in practice than\n\n\n\n```bash\nrsync\n```\n\n\n\nwhen transferring the files \nfor the first time\n. \n\n\nI am interested in an answer that addresses the above in two scenarios: using compression and not using it.\n\n\nUpdate\n\n\nI have just run some experiments moving 10,000 small files (total size = 50 MB), and \ntar+rsync+untar\n was consistently faster than running \nrsync\n directly (both without compression).",
    "answer": "When you send the same set of files, \nrsync\n is better suited because it will only send differences. \ntar\n will always send everything and this is a waste of resources when a lot of the data are already there. The \ntar + rsync + untar\n loses this advantage in this case, as well as the advantage of keeping the folders in-sync with \nrsync --delete\n.\n\n\nIf you copy the files for the first time, first packeting, then sending, then unpacking (AFAIK \nrsync\n doesn't take piped input) is cumbersome and always worse than just rsyncing, because \nrsync\n won't have to do any task more than \ntar\n anyway.\n\n\nTip: rsync version 3 or later does incremental recursion, meaning it starts copying almost immediately before it counts all files.\n\n\nTip2: If you use \nrsync\n over \nssh\n, you may also use either \ntar+ssh\n\n\n\n```bash\ntar -C /src/dir -jcf - ./ | ssh user@server 'tar -C /dest/dir -jxf -'\n```\n\n\n\nor just \nscp\n\n\n\n```bash\nscp -Cr srcdir user@server:destdir\n```\n\n\n\nGeneral rule, keep it simple.\n\n\nUPDATE:\n\n\nI've created 59M demo data\n\n\n\n```bash\nmkdir tmp; cd tmp\nfor i in {1..5000}; do dd if=/dev/urandom of=file$i count=1 bs=10k; done\n```\n\n\n\nand tested several times the file transfer to a remote server (not in the same lan), using both methods\n\n\n\n```bash\ntime rsync -r  tmp server:tmp2\n\nreal    0m11.520s\nuser    0m0.940s\nsys     0m0.472s\n\ntime (tar cf demo.tar tmp; rsync demo.tar server: ; ssh server 'tar xf demo.tar; rm demo.tar'; rm demo.tar)\n\nreal    0m15.026s\nuser    0m0.944s\nsys     0m0.700s\n```\n\n\n\nwhile keeping separate logs from the ssh traffic packets sent\n\n\n\n```bash\nwc -l rsync.log rsync+tar.log \n   36730 rsync.log\n   37962 rsync+tar.log\n   74692 total\n```\n\n\n\nIn this case, I can't see any advantage in less network traffic by using rsync+tar, which is expected when the default mtu is 1500 and while the files are 10k size. rsync+tar had more traffic generated, was slower for 2-3 seconds and left two garbage files that had to be cleaned up.\n\n\nI did the same tests on two machines on the same lan, and there the rsync+tar did much better times and much much less network traffic. I assume cause of jumbo frames.\n\n\nMaybe rsync+tar would be better than just rsync on a much larger data set. But frankly I don't think it's worth the trouble, you need double space in each side for packing and unpacking, and there are a couple of other options as I've already mentioned above.",
    "url": "https://unix.stackexchange.com/questions/30953/tar-rsync-untar-any-speed-benefit-over-just-rsync"
  },
  {
    "question_title": "How did the Linux Kernel project track bugs in the Early Days?",
    "question_body": "We all know that Linus Torvalds created Git because of issues with Bitkeeper. What is not known (at least to me) is, how were issues/tickets/bugs tracked up until then? I tried but didn't get anything interesting. The only discussion I was able to get on the subject was this one where \nLinus shared concerns with about using Bugzilla\n. \n\n\nSpeculation:\n - The easiest way for people to track bugs in the initial phase would have been to put tickets in a branch of its own but am sure that pretty quickly that wouldn't have scaled with the noise over-taking the good bugs. \n\n\nI've seen and used Bugzilla and unless you know the right 'keywords' at times you would be stumped. \nNOTE:\n I'm specifically interested in the early years (1991-1995) as to how they \nused to\n track issues. \n\n\nI did look at two threads, \"\nKernel SCM saga\n\", and \"\nTrivia: When did git self-host?\n\" but none of these made mention about bug-tracking of the kernel in the early days. \n\n\nI searched around and wasn't able to get of any FOSS bug-tracking software which was there in 1991-1992. Bugzilla, Request-tracker, and others came much later, so they appear to be out. \n\n\nKey questions\n\n\n\n\nHow did then Linus, the subsystem-maintainers, and users report and track bugs in those days? \n\n\nDid they use some bug-tracking software, made a branch of bugs and manually committed questions and discussions on the bug therein (would be expensive and painful to do that) or just use e-mail. \n\n\nMuch later, Bugzilla came along (first release 1998) and that seems to be the \nprimary way to report bugs afterwards\n.\n\n\n\n\nLooking forward to have a clearer picture of how things were done in the older days.",
    "answer": "In the beginning, if you had something to contribute (a patch or a bug report), you mailed it to Linus. This evolved into mailing it to the list (which was \nlinux-kernel@vger.rutgers.edu\n before \nkernel.org\n was created).\n\n\nThere was no version control. From time to time, Linus put a tarball on the FTP server. This was the equivalent of a \"tag\". The available tools at the beginning were RCS and CVS, and Linus hates those, so everybody just mailed patches. (There is an \nexplanation from Linus\n about why he didn't want to use CVS.)\n\n\nThere were other pre-Bitkeeper proprietary version control systems, but the decentralized, volunteer-based development of Linux made it impossible to use them. A random person who just found a bug will never send a patch if it has to go through a proprietary version control system with licenses starting in the thousands of dollars.\n\n\nBitkeeper got around both of those problems: it wasn't centralized like CVS, and while it was not Free Software, kernel contributors were allowed to use it without paying. That made it good enough for a while.\n\n\nEven with today's git-based development, the mailing lists are still where the action is. When you want to contribute something, you'll prepare it with git of course, but you'll have to discuss it on the relevant mailing list before it gets merged. Bugzilla is there to look \"professional\" and soak up half-baked bug reports from people who don't \nreally\n want to get involved.\n\n\nTo see some of the old bug-reporting instructions, get the \nhistorical Linux repository\n. (This is a git repository containing all the versions from before git existed; mostly it contains one commit per release since it was reconstructed from the tarballs). Files of interest include \nREADME\n, \nMAINTAINERS\n, and \nREPORTING-BUGS\n.\n\n\nOne of the interesting things you can find there is this from the Linux-0.99.12 README:\n\n\n\n```bash\n- if you have problems that seem to be due to kernel bugs, please mail\n   them to me (Linus.Torvalds@Helsinki.FI), and possibly to any other\n   relevant mailing-list or to the newsgroup.  The mailing-lists are\n   useful especially for SCSI and NETworking problems, as I can't test\n   either of those personally anyway.\n```",
    "url": "https://unix.stackexchange.com/questions/163639/how-did-the-linux-kernel-project-track-bugs-in-the-early-days"
  },
  {
    "question_title": "ansible extract without first directory",
    "question_body": "When extracting a tar.gz file in ansible I end up with a first directory\n\n\n\n```bash\n- name: Extract archive\n  unarchive: src=file.tar.gz\n             dest=/foo/bar\n```\n\n\n\nwhich results in \n/foo/bar/bar-version-someFirstLevelFolder/contentOfArchive\n\nHow can I prevent creating this extra level of hierarchy?",
    "answer": "In order to strip the \nbar-version-someFirstLevelFolder\n you need to use the \n--strip-components=1\n option in \ntar\n. So your playbook should look like\n\n\n\n```bash\n- name: Extract archive\n  unarchive:\n    src: file.tar.gz\n    dest: /foo/bar\n    extra_opts: ['--strip-components=1', '--show-stored-names']\n```\n\n\n\nThe \nshow-stored-names\n option, available since Ansible 2.1, will \nfix the idempotency\n problems.",
    "url": "https://unix.stackexchange.com/questions/346346/ansible-extract-without-first-directory"
  },
  {
    "question_title": "How to search text throughout entire file system?",
    "question_body": "Assuming that the grep tool should be used, I'd like to search for the text string \"800x600\" throughout the entire file system.\n\n\nI tried:\n\n\n\n```bash\ngrep -r 800x600 /\n```\n\n\n\nbut it doesn't work.\n\n\nWhat I believe my command should do is grep recursively through all files/folders under root for the text \"800x600\" and list the search results.\n\n\nWhat am I doing wrong?",
    "answer": "I normally use this style of command to run \ngrep\n over a number of files:\n\n\n\n```bash\nfind / -xdev -type f -print0 | xargs -0 grep -H \"800x600\"\n```\n\n\n\nWhat this actually does is make a list of every file on the system, and then for each file, execute \ngrep\n with the given arguments and the name of each file.\n\n\nThe \n-xdev\n argument tells find that it must ignore other filesystems - this is good for avoiding special filesystems such as \n/proc\n. However it will also ignore normal filesystems too - so if, for example, your /home folder is on a different partition, it won't be searched - you would need to say \nfind / /home -xdev ...\n.\n\n\n-type f\n means search for files only, so directories, devices and other special files are ignored (it will still recurse into directories and execute \ngrep\n on the files within - it just won't execute \ngrep\n on the directory itself, which wouldn't work anyway). And the \n-H\n option to \ngrep\n tells it to always print the filename in its output.\n\n\nfind\n accepts all sorts of options to filter the list of files. For example, \n-name '*.txt'\n processes only files ending in .txt. \n-size -2M\n means files that are smaller than 2 megabytes. \n-mtime -5\n means files modified in the last five days. Join these together with -a for \nand\n and -o for \nor\n, and use \n'('\n parentheses \n')'\n to group expressions (in quotes to prevent the shell from interpreting them). So for example:\n\n\n\n```bash\nfind / -xdev '(' -type f -a -name '*.txt' -a -size -2M -a -mtime -5 ')' -print0 | xargs -0 grep -H \"800x600\"\n```\n\n\n\nTake a look at \nman find\n to see the full list of possible filters.",
    "url": "https://unix.stackexchange.com/questions/16138/how-to-search-text-throughout-entire-file-system"
  },
  {
    "question_title": "Ignore whitespaces changes in all git commands",
    "question_body": "I've found tons of sites that explain how to have git warn you when you're changing line endings, or miscellaneous other techniques to prevent you from messing up an entire file. Assume it's too late for that -- the tree already has commits that toggle the line endings of files, so \ngit diff\n shows the subtraction of the old file followed by the addition of a new file with the same content\n\n\nI'm looking for a git configuration option or command-line flag that tells \ndiff\n to just ignore those -- if two lines differ only by whitespace, pretend they're the same. I need this config option/flag to work for anything that relies on file differences -- \ndiff\n, \nblame\n, even \nmerge\n/\nrebase\n ideally -- I want \ngit\n to completely ignore trailing whitespace, particularly line endings. How can I do that?",
    "answer": "For diff, there's \ngit diff --ignore-space-at-eol\n, which should be good enough. For diff and blame, you can ignore all whitespace changes with \n-w\n: \ngit diff -w\n, \ngit blame -w\n.\n\n\nFor \ngit apply\n and \ngit rebase\n, the documentation mentions \n--ignore-whitespace\n.\n\n\nFor merge, it looks like you need to use an external merge tool. You can use this wrapper script (untested), where \nfavorite-mergetool\n is your favorite \nmerge tool\n; run \ngit -c mergetool.nocr.cmd=/path/to/wrapper/script merge\n. The result of the merge will be in unix format; if you prefer another format, convert everything to that different format, or convert \n$MERGED\n after the merge.\n\n\n\n```bash\n#!/bin/sh\nset -e\nTEMP=$(mktemp)\ntr -d '\\013' <\"$BASE\" >\"$TEMP\"\nmv -f \"$TEMP\" \"$BASE\"\nTEMP=$(mktemp)\ntr -d '\\013' <\"$LOCAL\" >\"$TEMP\"\nmv -f \"$TEMP\" \"$LOCAL\"\nTEMP=$(mktemp)\ntr -d '\\013' <\"$REMOTE\" >\"$TEMP\"\nmv -f \"$TEMP\" \"$REMOTE\"\nfavorite-mergetool \"$@\"\n```\n\n\n\nTo minimize trouble with mixed line endings, make sure \ntext files are declared as such\n.\n\n\nSee also \nIs it possible for git-merge to ignore line-ending differences?\n on Stack Overflow.",
    "url": "https://unix.stackexchange.com/questions/10277/ignore-whitespaces-changes-in-all-git-commands"
  },
  {
    "question_title": "Grep &#39;OR&#39; regex problem",
    "question_body": "I am trying to use grep with a regex to find lines in a file that match 1 of 2 possible strings. Here is my grep:\n\n\n\n```bash\n$ grep \"^ID.*(ETS|FBS)\" my_file.txt\n```\n\n\n\nThe above grep returns no results. However if I execute either:\n\n\n\n```bash\n$ grep \"^ID.*ETS\" my_file.txt\n```\n\n\n\nor   \n\n\n\n```bash\n$ grep \"^ID.*FBS\" my_file.txt\n```\n\n\n\nI do match specific lines. Why is my OR regex not matching? Thanks in advance for the help!",
    "answer": "With normal regex, the characters \n(\n, \n|\n and \n)\n need to be escaped. So you should use\n\n\n\n```bash\n$ grep \"^ID.*\\(ETS\\|FBS\\)\" my_file.txt\n```\n\n\n\nYou don't need the escapes when you use the \nextended regex\n (\n-E\n)option. See \nman grep\n, section \"\nBasic vs Extended Regular Expressions\n\".",
    "url": "https://unix.stackexchange.com/questions/21764/grep-or-regex-problem"
  },
  {
    "question_title": "Fetch a specific branch using git",
    "question_body": "I want to download source code from the \nmaster-next branch\n using git as described in the \nXilinx wiki\n.\n\n\nI tried this:\n\n\n\n```bash\n#git clone git://github.com/Xilinx/u-boot-xlnx/tree/master-next.git\n\nInitialized empty Git repository in /home/Hannan/master-next/.git/\nfatal: remote error:\nXilinx/u-boot-xlnx/tree/master-next is not a valid repository name\nEmail support@github.com for help\n```\n\n\n\nEven this failed:\n\n\n\n```bash\n# git clone git://github.com/Xilinx/tree/master-next/u-boot-xlnx.git\nInitialized empty Git repository in /home/Hannan/u-boot-xlnx/.git/\nfatal: remote error:\nXilinx/tree/master-next/u-boot-xlnx is not a valid repository name\nEmail support@github.com for help\n```\n\n\n\nThe command that works is:\n\n\n\n```bash\ngit clone git://github.com/Xilinx/u-boot-xlnx.git\n```\n\n\n\nBut how do I know that this will indeed fetch the \nmaster-next branch\n and not the \nmaster branch\n? How do I correctly fetch a specific branch using git?\n\n\nI am using RHEL 6, accessed via PuTTY.",
    "answer": "Like the error message tells you, \ngit clone\n expects a git repository. You can't \"add\" the name of a branch in the path like that. See \nbranches in git\n.\n\n\nYou can clone a single branch (and it's history) :\n\ngit clone <url> --branch <branch> --single-branch\n \n\n\nSee \ngit help clone\n.\n\n\nBut the clone command you ran gave you a copy of the whole repository, you can see the existing branches with \ngit branch\n and more details about them with \ngit show-branch\n.\n\n\nSwitch to the branch you want with \ngit checkout branch-name\n.\n\n\nThe first chapters of the \nPro Git\n book (available online) give more details on the basic commands.",
    "url": "https://unix.stackexchange.com/questions/127066/fetch-a-specific-branch-using-git"
  },
  {
    "question_title": "Match exact string using grep",
    "question_body": "I have a text file:\n\n\n\n```bash\ndeiauk 1611516 afsdf 765\nminkra 18415151 asdsf 4152\nlinkra sfsfdsfs sdfss 4555\ndeiauk1 sdfsfdsfs 1561 51\ndeiauk2 115151 5454 4\ndeiauk 1611516 afsdf ddfgfgd\nluktol1 4545 4 9\nluktol 1\n```\n\n\n\nand I want to match exactly \ndeiauk\n.  When I do this:\n\n\n\n```bash\ngrep \"deiauk\" file.txt\n```\n\n\n\nI get this result:\n\n\n\n```bash\ndeiauk 1611516 afsdf 765\ndeiauk1 sdfsfdsfs 1561 51\ndeiauk2 115151 5454 4\n```\n\n\n\nbut I only need this:\n\n\n\n```bash\ndeiauk 1611516 afsdf 765\ndeiauk 1611516 afsdf ddfgfgd\n```\n\n\n\nI know there's a \n-w\n option, but then my string has to mach whole line.",
    "answer": "Try one of:\n\n\n\n```bash\ngrep -w \"deiauk\" textfile\n\ngrep \"\\<deiauk\\>\" textfile\n```",
    "url": "https://unix.stackexchange.com/questions/206903/match-exact-string-using-grep"
  },
  {
    "question_title": "Answer yes in a bash script",
    "question_body": "I'm trying to do a \ngit clone\n trough a bash script, but the first time that I run the script and the server is not known yet the script fails. I have something like this:\n\n\n\n```bash\nyes | git clone git@github.com:repo/repoo.git\n```\n\n\n\n\n```bash\nThe authenticity of host 'github.com (207.97.227.239)' can't be established.\nRSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.\nAre you sure you want to continue connecting (yes/no)?\n```\n\n\n\nBut it's ignoring the \nyes\n. Do you know how to force \ngit clone\n to add the key to the known hosts?",
    "answer": "Add the following to your \n~/.ssh/config\n file:\n\n\n\n```bash\nHost github.com\n    StrictHostKeyChecking no\n```\n\n\n\nAnything using the open-ssh client to establish a remote shell (with the git client does) should skip the key checks to github.com.\n\n\nThis is actually a bad idea since any form of skipping the checks (whether you automatically hit yes or skip the check in the first place) creates room for a man in the middle security compromise. A better way would be to retrieve and validate the fingerprint and store it in the \nknown_hosts\n file before needing to run some script that automatically connects.",
    "url": "https://unix.stackexchange.com/questions/18853/answer-yes-in-a-bash-script"
  },
  {
    "question_title": "what does &quot; gbp:error: upstream/1.5.13 is not a valid treeish&quot; mean?",
    "question_body": "I want to build a debian package with git build package.(gbp)\nI passed all steps, and at least, when I entered \ngbp buildpackage\n, This error appeared.\n\n\nwhat does it mean?\nand what should I do?\n\n\n\n```bash\ngbp:error: upstream/1.5.13 is not a valid treeish\n```",
    "answer": "This can be caused by the tarball not being present in the parent directory. I get this (highly crypric) message even with a debian/ folder present. \n\n\nMy solution was to use uscan to get the watch file to download a fresh copy of the tarball\n\n\n\n```bash\nuscan --force-download\n```",
    "url": "https://unix.stackexchange.com/questions/167533/what-does-gbperror-upstream-1-5-13-is-not-a-valid-treeish-mean"
  },
  {
    "question_title": "How to convert all files from gzip to xz on the fly (and recursively)?",
    "question_body": "I have a directory tree with gzipped files like this:\n\n\n\n```bash\nbasedir/a/file.dat.gz\nbasedir/b/file.dat.gz\nbasedir/c/file.dat.gz\netc.\n```\n\n\n\nHow can I convert all of these from gzip to xz with a single command and without decompressing each file to disk?\n\n\nThe trivial two-liner with decompressing to disk looks like this:\n\n\n\n```bash\nfind basedir/ -type f -name '*.dat.gz' -exec gzip -d {} \\;\nfind basedir/ -type f -name '*.dat' -exec xz {} \\;\n```\n\n\n\nFirst command could even be shorter: \ngunzip -r *\n\n\nFor a single file on-the-fly conversion is simple (although this doesn't replace the .gz file):\n\n\n\n```bash\ngzip -cd basedir/a/file.dat.gz | xz > basedir/a/file.dat.xz\n```\n\n\n\nSince gzip and xz are handling the extensions themselves I'd like to say:\n\n\n\n```bash\ngunzip -rc * > xz\n```\n\n\n\nI looked at \nfind | xargs basename -s .gz { }\n a bit but didn't get a working solution.\n\n\nI could write a shell script, but I feel there should be a simple solution.\n\n\n\n\nEdit\n\n\nThanks for all who answered already. I know we all love 'commands that will never fail™'. So, to keep this simple:\n\n\n\n\nAll subdirectories contain only numbers, letters (äöü, though), underscore and minus.\n\n\nAll files are named file.dat[.n].gz, n being a positive integer\n\n\nNo directory or file will have a '.gz' anywhere (other than as the final file suffix).\n\n\nThis is the only content these directories contain.\n\n\nI control the naming and can restrict it if needed.\n\n\n\n\nUsing a simple \nfind -exec ...\n or \nls | xargs\n, is there a command to replace '.gz' in the found filename by '.xz' on the fly? Then I could write something like (pseudo):\n\n\n\n```bash\nfind basedir/ -type f -name '*.gz' -exec [ gzip -cd {} | xz > {replace .gz by .xz} \\; ]\n```",
    "answer": "```bash\nfind . -name '*.gz' -type f -exec bash -o pipefail -Cc '\n  for file do\n    gunzip < \"$file\" | xz > \"${file%.gz}.xz\" && rm -f \"$file\"\n  done' bash {} +\n```\n\n\n\nThe \n-C\n prevents overwriting an existing file and won't follow symlinks \nexcept\n if the exiting file is a non-regular file or a link to a non-regular file, so you would not lose data unless you have for instance a \nfile.gz\n and a \nfile.xz\n that is a symlink to \n/dev/null\n. To guard against that, you could use \nzsh\n instead and also use the \n-execdir\n feature of some \nfind\n implementations for good measure and avoid some race conditions:\n\n\n\n```bash\nfind . -name '*.gz' -type f -execdir zsh -o pipefail -c '\n  zmodload zsh/system || exit\n  for file do\n    gunzip < \"$file\" | (\n      sysopen -u 1 -w -o excl -- \"${file%.gz}.xz\" && xz) &&\n      rm -f -- \"$file\"\n  done' zsh {} +\n```\n\n\n\nOr to clean-up \nxz\n files upon failed recompressions:\n\n\n\n```bash\nfind . -name '*.gz' -type f -execdir zsh -o pipefail -c '\n  zmodload zsh/system || exit\n  for file do\n    sysopen -u 1 -w -o excl -- \"${file%.gz}.xz\" &&\n      if gunzip < \"$file\" | xz; then\n        rm -f -- \"$file\"\n      else\n        rm -f -- \"${file%.gz}.xz\"\n      fi\n  done' zsh {} +\n```\n\n\n\nIf you'd rather it being short, and are ready to ignore some of those potential issues,  in \nzsh\n, you could do\n\n\n\n```bash\nfor f (./**/*.gz(D.)) {gunzip < $f | xz > $f:r.xz && rm -f $f}\n```",
    "url": "https://unix.stackexchange.com/questions/308683/how-to-convert-all-files-from-gzip-to-xz-on-the-fly-and-recursively"
  },
  {
    "question_title": "How do I extract with tar to a different directory?",
    "question_body": "This doesn't work:\n\n\n\n```bash\ntar xf /tmp/foo.tar.gz foo/bar\ntar: foo/bar: Not found in archive\n```\n\n\n\nIt's not obvious to me what would do this beyond extracting it in place and moving the files over.",
    "answer": "From \nman tar\n:\n\n\n\n```bash\n-C directory\n         In c and r mode, this changes the directory before adding the\n         following files.  In x mode, change directories after opening the\n         archive but before extracting entries from the archive.\n```\n\n\n\ni.e, \ntar xC /foo/bar -f /tmp/foo.tar.gz\n should do the job.\n(on FreeBSD, \nbut GNU tar is basically the same in this respect, see \"Changing the Working Directory\" in its manual\n)",
    "url": "https://unix.stackexchange.com/questions/23744/how-do-i-extract-with-tar-to-a-different-directory"
  },
  {
    "question_title": "How to set multiple `core.excludesfile` in `.gitconfig`?",
    "question_body": "I'm syncing \n~/.gitconfig\n and \n~/.gitignore\n files in ubuntu and Mac by using dropbox and created symlink for it.\n\n\nAnd \nexcludesfile\n is declared like this.\n\n\n\n```bash\n[core]\n        editor = /usr/bin/vim\n        excludesfile = /Users/username/.gitignore\n```\n\n\n\nThe problem is home directory differs by os, therefore I need multiple setting for excludesfile.\n\n\nIs it possible to define multiple \ncore.excludesfile\n?",
    "answer": "You can only have a single \ncore.excludesfile\n; the last setting is the one that's used. However, you don't need multiple files: git supports \n~\n as an abbreviation for your home directory.\n\n\n\n```bash\n[core]\n    excludesfile = ~/.gitignore\n```\n\n\n\nIn general, if you really needed to have multiple excludes files, the simplest solution would be to generate a single file that's the concatenation of the others, and update it whenever one of the files changes.",
    "url": "https://unix.stackexchange.com/questions/116408/how-to-set-multiple-core-excludesfile-in-gitconfig"
  },
  {
    "question_title": "Pipe filelist into &#39;git add&#39;",
    "question_body": "I performed a fresh clone and copied/pasted a working directory into the cloned directory. Now have a list of changed files:\n\n\n\n```bash\n$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 \nGNUmakefile\nReadme.txt\nbase32.h\nbase64.h\n...\n```\n\n\n\nWhen I try to get Git to add them, it results in an error (I don't care about adding 1 at a time):\n\n\n\n```bash\n$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add\nNothing specified, nothing added.\nMaybe you wanted to say 'git add .'?\n```\n\n\n\nAdding the \n-\n:\n\n\n\n```bash\n$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add -\nfatal: pathspec '-' did not match any files\n```\n\n\n\nAnd \n--\n:\n\n\n\n```bash\n$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add --\nNothing specified, nothing added.\nMaybe you wanted to say 'git add .'?\n```\n\n\n\nTrying to use \ninteractive\n from the man page appears to have made a greater mess of things:\n\n\n\n```bash\n$ git status --short | grep -v \"??\" | cut -d \" \" -f 3 | git add -i\n           staged     unstaged path\n  1:    unchanged        +1/-1 GNUmakefile\n  2:    unchanged      +11/-11 Readme.txt\n  ...\n\n*** Commands ***\n  1: status   2: update   3: revert   4: add untracked\n  5: patch    6: diff     7: quit     8: help\nHuh (GNUmakefile)?\nWhat now> *** Commands ***\n  1: status   2: update   3: revert   4: add untracked\n  5: patch    6: diff     7: quit     8: help\nHuh (Readme.txt)?\n```\n\n\n\n(I've already deleted the directory Git made a mess of, so I'm not trying to solve that issue).\n\n\nHow do I tell Git to add the files piped into it?",
    "answer": "git add\n is expecting the files to be listed as arguments, not piped into \nstdin\n.  Try either\n\n\n\n```bash\ngit status --short | grep -v \"??\" | cut -d \" \" -f 3 | xargs git add\n```\n\n\n\nor\n\n\n\n```bash\nfor file in $(git status --short | grep -v \"??\" | cut -d \" \" -f 3); do\n    git add $file;\ndone\n```",
    "url": "https://unix.stackexchange.com/questions/244169/pipe-filelist-into-git-add"
  },
  {
    "question_title": "Why is egrep [wW][oO][rR][dD] faster than grep -i word?",
    "question_body": "I've been using \ngrep -i\n more often and I found out that it is slower than its \negrep\n equivalent, where I match against the upper or lower case of each letter:\n\n\n\n```bash\n$ time grep -iq \"thats\" testfile\n\nreal    0m0.041s\nuser    0m0.038s\nsys     0m0.003s\n$ time egrep -q \"[tT][hH][aA][tT][sS]\" testfile\n\nreal    0m0.010s\nuser    0m0.003s\nsys     0m0.006s\n```\n\n\n\nDoes \ngrep -i\n do additional tests that \negrep\n doesn't?",
    "answer": "grep -i 'a'\n is equivalent to \ngrep '[Aa]'\n in an ASCII-only locale. In a Unicode locale, character equivalences and conversions can be complex, so \ngrep\n may have to do extra work to determine which characters are equivalent. The relevant locale setting is \nLC_CTYPE\n, which determines how bytes are interpreted as characters.\n\n\nIn my experience, GNU \ngrep\n can be slow when invoked in a UTF-8 locale. If you know that you're searching for ASCII characters only, invoking it in an ASCII-only locale may be faster. I expect that\n\n\n\n```bash\ntime LC_ALL=C grep -iq \"thats\" testfile\ntime LC_ALL=C egrep -q \"[tT][hH][aA][tT][sS]\" testfile\n```\n\n\n\nwould produce indistinguishable timings.\n\n\nThat being said, I can't reproduce your finding with GNU \ngrep\n on Debian jessie (but you didn't specify your test file). If I set an ASCII locale (\nLC_ALL=C\n), \ngrep -i\n is faster. The effects depend on the exact nature of the string, for example a string with repeated characters reduces the performance (\nwhich is to be expected\n).",
    "url": "https://unix.stackexchange.com/questions/269587/why-is-egrep-wwoorrdd-faster-than-grep-i-word"
  },
  {
    "question_title": "How can I get the current working directory?",
    "question_body": "I want to have a script that takes the current working directory to a variable. The section that needs the directory is like this \ndir = pwd\n. It just prints \npwd\n how do I get the current working directory into a variable?",
    "answer": "There's no need to do that, it's already \nin\n a variable:\n\n\n\n```bash\n$ echo \"$PWD\"\n/home/terdon\n```\n\n\n\nThe \nPWD\n variable is \ndefined by POSIX\n and will work on all POSIX-compliant shells:\n\n\n\n> PWD\n\n\n\n\n> Set by the shell and by the cd utility. In the shell the value\n> shall be initialized from the environment as follows. If a value for\n> PWD is passed to the shell in the environment when it is executed, the\n> value is an absolute pathname of the current working directory that is\n> no longer than {PATH_MAX} bytes including the terminating null byte,\n> and the value does not contain any components that are dot or dot-dot,\n> then the shell shall set PWD to the value from the environment.\n> Otherwise, if a value for PWD is passed to the shell in the\n> environment when it is executed, the value is an absolute pathname of\n> the current working directory, and the value does not contain any\n> components that are dot or dot-dot, then it is unspecified whether the\n> shell sets PWD to the value from the environment or sets PWD to the\n> pathname that would be output by pwd -P. Otherwise, the sh utility\n> sets PWD to the pathname that would be output by pwd -P. In cases\n> where PWD is set to the value from the environment, the value can\n> contain components that refer to files of type symbolic link. In cases\n> where PWD is set to the pathname that would be output by pwd -P, if\n> there is insufficient permission on the current working directory, or\n> on any parent of that directory, to determine what that pathname would\n> be, the value of PWD is unspecified. Assignments to this variable may\n> be ignored. If an application sets or unsets the value of PWD, the\n> behaviors of the cd and pwd utilities are unspecified.\n\n\n\n\n\nFor the more general answer, the way to save the output of a command in a variable is to enclose the command in \n$()\n or \n` `\n (backticks):\n\n\n\n```bash\nvar=$(command)\n```\n\n\n\nor\n\n\n\n```bash\nvar=`command`\n```\n\n\n\nOf the two, the \n$()\n is preferred since it is easier to build complex commands like:\n\n\n\n```bash\ncommand0 \"$(command1 \"$(command2 \"$(command3)\")\")\"\n```\n\n\n\nWhose backtick equivalent would look like:\n\n\n\n```bash\ncommand0 \"`command1 \\\"\\`command2 \\\\\\\"\\\\\\`command3\\\\\\`\\\\\\\"\\`\\\"`\"\n```",
    "url": "https://unix.stackexchange.com/questions/188182/how-can-i-get-the-current-working-directory"
  },
  {
    "question_title": "Make vim stop splitting my Git commit messages",
    "question_body": "Vim has this awful annoying habit of splitting my commit messages and automatically line-breaking them, rendering the second line with a red background for some reason.\n\n\n\n\nHow can I make vim \nstop\n doing this?",
    "answer": "I found the setting I needed in \n~/.vim/after/ftplugin/gitcommit.vim\n was:\n\n\nsetlocal textwidth=0",
    "url": "https://unix.stackexchange.com/questions/138148/make-vim-stop-splitting-my-git-commit-messages"
  },
  {
    "question_title": "What is the purpose of .bashrc and how does it work?",
    "question_body": "I found the \n.bashrc\n file and I want to know the purpose/function of it. Also how and when is it used?",
    "answer": ".bashrc\n is a Bash \nshell script\n that Bash runs whenever it is started interactively. It initializes an interactive shell session. You can put any command in that file that you could type at the command prompt.\n\n\nYou put commands here to set up the shell for use in your particular environment, or to customize things to your preferences. A common thing to put in \n.bashrc\n are \naliases\n that you want to always be available.\n\n\n.bashrc\n runs on \nevery\n interactive shell launch. If you say:\n\n\n\n```bash\n$ bash ; bash ; bash\n```\n\n\n\nand then hit \nCtrl-D\n three times, \n.bashrc\n will run three times.  But if you say this instead:\n\n\n\n```bash\n$ bash -c exit ; bash -c exit ; bash -c exit\n```\n\n\n\nthen \n.bashrc\n won't run at all, since \n-c\n makes the Bash call non-interactive. The same is true when you run a shell script from a file.\n\n\nContrast \n.bash_profile\n and \n.profile\n which are only run at the start of a new login shell. (\nbash -l\n) You choose whether a command goes in \n.bashrc\n vs \n.bash_profile\n depending on whether you want it to run once or for every interactive shell start.\n\n\nAs a counterexample to aliases, which I prefer to put in \n.bashrc\n, you want to do \nPATH\n adjustments in \n.bash_profile\n instead, since these changes are typically not \nidempotent\n:\n\n\n\n```bash\nexport PATH=\"$PATH:/some/addition\"\n```\n\n\n\nIf you put that in \n.bashrc\n instead, every time you launched an interactive sub-shell, \n:/some/addition\n would get tacked onto the end of the \nPATH\n again, creating extra work for the shell when you mistype a command.\n\n\nYou get a new interactive Bash shell whenever you \nshell out of \nvi\n with \n:sh\n, for example.",
    "url": "https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work"
  },
  {
    "question_title": "diff to show only the additions in a changed file",
    "question_body": "Normally \ndiff\n and \ngit diff\n show both the original and the modified line with \n-\n and \n+\n respectively. Is there any way, I can filter only to see the modified line? This would reduce the number of lines to read by a factor of 2 instantly.\n\n\nI was assuming \n\n\n\n```bash\ngit diff test.yml | grep '^+' | less -R\n```\n\n\n\nand \n\n\n\n```bash\ngit diff test.yml | egrep '^+' | less -R\n```\n\n\n\nto have the same result. ie they would show any new additions in a file. However \negrep\n shows me the entire file. Why is that so?\n\n\nWith the above method anyways, I lose the color. Is there any way to retain the color?",
    "answer": "You can use \n--word-diff\n to condense the \n+\n and \n-\n lines together with the changes highlighted using red/green text and stop using grep all together.\n\n\n\n\nYou can combine this with \n-U0\n to remove all context around the diffs if you really want to condense it down further.\n\n\n\n\nThis approch is better than using grep as you don't lose output, you can tell when a line was added or simply changed and you don't completely lose removals while still condensing the output down into something that is easy to read. \n\n\nThe answer to the question regarding egrep is already answered by @Stephen Kitt \nhere",
    "url": "https://unix.stackexchange.com/questions/392191/diff-to-show-only-the-additions-in-a-changed-file"
  },
  {
    "question_title": "Temporarily suspend bash_history on a given shell?",
    "question_body": "Is there a way to temporarily suspend history tracking in bash, so as to enter a sort of \"incognito\" mode? I'm entering stuff into my terminal that I don't want recorded, sensitive financial info.",
    "answer": "This will prevent bash from saving any new history when exiting the shell:\n\n\n\n```bash\nunset HISTFILE\n```\n\n\n\nSpecifically, according to \nman bash\n:\n\n\n\n> If HISTFILE is unset, or if the history file is unwritable, the history is not saved.\n\n\n\nNote that if you re-set \nHISTFILE\n, history will be saved normally. This only affects the moment the shell session ends.\n\n\nAlternatively, if you want to toggle it off and then back on again during a session, it may be easier to use \nset\n:\n\n\n\n```bash\nset +o history # temporarily turn off history\n\n# commands here won't be saved\n\nset -o history # turn it back on\n```",
    "url": "https://unix.stackexchange.com/questions/10922/temporarily-suspend-bash-history-on-a-given-shell"
  },
  {
    "question_title": "Return only the portion of a line after a matching pattern",
    "question_body": "So pulling open a file with \ncat\n and then using \ngrep\n to get matching lines only gets me so far when I am working with the particular log set that I am dealing with. It need a way to match lines to a pattern, but only to return the portion of the line after the match. The portion before and after the match will consistently vary. I have played with using \nsed\n or \nawk\n, but have not been able to figure out how to filter the line to either delete the part before the match, or just return the part after the match, either will work.\nThis is an example of a line that I need to filter:\n\n\n\n```bash\n2011-11-07T05:37:43-08:00 <0.4> isi-udb5-ash4-1(id1) /boot/kernel.amd64/kernel: [gmp_info.c:1758](pid 40370=\"kt: gmp-drive-updat\")(tid=100872) new group: <15,1773>: { 1:0-25,27-34,37-38, 2:0-33,35-36, 3:0-35, 4:0-9,11-14,16-32,34-38, 5:0-35, 6:0-15,17-36, 7:0-16,18-36, 8:0-14,16-32,34-36, 9:0-10,12-36, 10-11:0-35, 12:0-5,7-30,32-35, 13-19:0-35, 20:0,2-35, down: 8:15, soft_failed: 1:27, 8:15, stalled: 12:6,31, 20:1 }\n```\n\n\n\nThe portion I need is everything after \"stalled\".\n\n\nThe background behind this is that I can find out how often something stalls:\n\n\n\n```bash\ncat messages | grep stalled | wc -l\n```\n\n\n\nWhat I need to do is find out how many times a certain node has stalled (indicated by the portion before each colon after \"stalled\". If I just grep for that (ie 20:) it may return lines that have soft fails, but no stalls, which doesn't help me. I need to filter only the stalled portion so I can then grep for a specific node out of those that have stalled.\n\n\nFor all intents and purposes, this is a freebsd system with standard GNU core utils, but I cannot install anything extra to assist.",
    "answer": "The canonical tool for that would be \nsed\n.\n\n\n\n```bash\nsed -n -e 's/^.*stalled: //p'\n```\n\n\n\nDetailed explanation:\n\n\n\n\n-n\n means not to print anything by default.\n\n\n-e\n is followed by a sed command.\n\n\ns\n is the pattern replacement command.\n\n\nThe regular expression \n^.*stalled: \n matches the pattern you're looking for, plus any preceding text (\n.*\n meaning any text, with an initial \n^\n to say that the match begins at the beginning of the line). Note that if \nstalled: \n occurs several times on the line, this will match the last occurrence.\n\n\nThe match, i.e. everything on the line up to \nstalled: \n, is replaced by the empty string (i.e. deleted).\n\n\nThe final \np\n means to print the transformed line.\n\n\n\n\nIf you want to retain the matching portion, use a backreference: \n\\1\n in the replacement part designates what is inside a group \n\\(…\\)\n in the pattern. Here, you could write \nstalled: \n again in the replacement part; this feature is useful when the pattern you're looking for is more general than a simple string.\n\n\n\n```bash\nsed -n -e 's/^.*\\(stalled: \\)/\\1/p'\n```\n\n\n\nSometimes you'll want to remove the portion of the line after the match. You can include it in the match by including \n.*$\n at the end of the pattern (any text \n.*\n followed by the end of the line \n$\n). Unless you put that part in a group that you reference in the replacement text, the end of the line will not be in the output.\n\n\nAs a further illustration of groups and backreferences, this command swaps the part before the match and the part after the match.\n\n\n\n```bash\nsed -n -e 's/^\\(.*\\)\\(stalled: \\)\\(.*\\)$/\\3\\2\\1/p'\n```\n\n\n\nTo get the part after the \nfirst\n occurrence of the string instead of last (for those lines where the string can occur several times), a common trick is to replace that string once with a newline character (which is the one character that won't occur inside a line), and then remove everything up to that newline:\n\n\n\n```bash\nsed -n '\n  /stalled: / {\n    s//\\\n/\n    s/.*\\n//p\n  }'\n```\n\n\n\nWith some \nsed\n implementations, the first \ns\n command can be written \ns//\\n/\n though that's not standard/portable.",
    "url": "https://unix.stackexchange.com/questions/24140/return-only-the-portion-of-a-line-after-a-matching-pattern"
  },
  {
    "question_title": "How can I update to a newer version of Git using apt-get?",
    "question_body": "I've just set up a new machine with Ubuntu Oneiric 11.10 and then run\n\n\n\n```bash\napt-get update\napt-get upgrade\napt-get install git\n```\n\n\n\nNow if I run \ngit --version\n it tells me I have \ngit version 1.7.5.4\n but on my local machine I have the much newer \ngit version 1.7.9.2\n\n\nI know I can install from source to get the newest version, but I thought that it was a good idea to use the package manager as much as possible to keep everything standardized.\n\n\nSo is it possible to use \napt-get\n to get a newer version of \ngit\n, and what is the right way to do it?",
    "answer": "Here are the commands you need to run, if you just want to get it done:\n\n\n\n```bash\nsudo add-apt-repository ppa:git-core/ppa -y\nsudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com A1715D88E1DF1F24 40976EAF437D05B5 3B4FE6ACC0B21F32 A6616109451BBBF2\nsudo apt-get update\nsudo apt-get install git -y\ngit --version\n```\n\n\n\nAs of Dec 2018, I got git 2.20.1 that way, while the version in the Ubuntu Xenial repositories was 2.7.4.\n\n\nIf your system doesn't have \nadd-apt-repository\n, you can install it via:\n\n\n\n```bash\nsudo apt-get install python-software-properties software-properties-common\n```",
    "url": "https://unix.stackexchange.com/questions/33617/how-can-i-update-to-a-newer-version-of-git-using-apt-get"
  },
  {
    "question_title": "Extract timestamp from a gzip file",
    "question_body": "How can I know the raw original timestamp of a file \nfoo\n compressed with \ngzip\n without having to decompress \nfoo.gz\n?\n\n\ngzip --verbose --list foo.gz\n and \nfile foo.gz\n will print formatted date and time.",
    "answer": "Extract the timestamp manually. Assuming that the compressed file has a single member (this is normally the case with gzip):\n\n\n\n```bash\n<foo.gz dd bs=4 skip=1 count=1 | od -t d4\n```\n\n\n\nThis prints the raw timestamp, i.e. the number of seconds since 1970-01-01 00:00 UTC, in decimal.",
    "url": "https://unix.stackexchange.com/questions/79543/extract-timestamp-from-a-gzip-file"
  },
  {
    "question_title": "Precedence of the shell logical operators &amp;&amp;, ||",
    "question_body": "I am trying to understand how the logical operator precedence works in bash. For example, I would have expected, that the following command does not echo anything.\n\n\n\n```bash\ntrue || echo aaa && echo bbb\n```\n\n\n\nHowever, contrary to my expectation, \nbbb\n gets printed. \n\n\nCan somebody please explain, how can I make sense of compounded \n&&\n and \n||\n operators in bash?",
    "answer": "In many computer languages, operators with the same precedence are \nleft-associative\n. That is, in the absence of grouping structures, leftmost operations are executed first. Bash is \nno exception\n to this rule.\n\n\nThis is important because in Bash and other shells \n&&\n and \n||\n have the same precedence. This is different from most other programming languages which usually give \n&&\n a higher precedence than \n||\n.\n\n\nSo what happens in your example is that the leftmost operation (\n||\n) is carried out first:\n\n\n\n```bash\ntrue || echo aaa\n```\n\n\n\nSince \ntrue\n is obviously true, the \n||\n operator short-circuits and the whole statement is considered true without the need to evaluate \necho aaa\n as you would expect. Now it remains to do the rightmost operation:\n\n\n\n```bash\n(...) && echo bbb\n```\n\n\n\nSince the first operation evaluated to true (i.e. had a 0 exit status), it's as if you're executing\n\n\n\n```bash\ntrue && echo bbb\n```\n\n\n\nso the \n&&\n will not short-circuit, which is why you see \nbbb\n echoed.\n\n\nYou would get the same behavior with\n\n\n\n```bash\nfalse && echo aaa || echo bbb\n```\n\n\n\nNotes based on the comments\n\n\n\n\nYou should note that the left-associativity rule is \nonly\n followed when both operators have \nthe same\n precedence. This is not the case when you use these operators in conjunction with keywords such as \n[[...]]\n or \n((...))\n or use the \n-o\n and \n-a\n operators as arguments to the \ntest\n or \n[\n commands. In such cases, AND (\n&&\n or \n-a\n) takes precedence over OR (\n||\n or \n-o\n). Thanks to Stephane Chazelas' comment for clarifying this point.\n\n\n\n\nIt seems that \nin C\n and C-like languages \n&&\n has higher precedence than \n||\n which is probably why you expected your original construct to behave like\n\n\n\n```bash\ntrue || (echo aaa && echo bbb).\n```\n\n\n\n\n\n\n\nThis is not the case with Bash, however, in which both operators have the same precedence, which is why Bash parses your expression using the left-associativity rule. Thanks to Kevin's comment for bringing this up.\n\n\n\n\nThere might also be cases where \nall 3\n expressions are evaluated. If the first command returns a non-zero exit status, the \n||\n won't short circuit and goes on to execute the second command. If the second command returns with a zero exit status, then the \n&&\n won't short-circuit as well and the third command will be executed. Thanks to Ignacio Vazquez-Abrams' comment for bringing this up.",
    "url": "https://unix.stackexchange.com/questions/88850/precedence-of-the-shell-logical-operators"
  },
  {
    "question_title": "How to gunzip files recursively (or how to UNDO &#39;gzip -r&#39;)",
    "question_body": "I am learning Linux and I was trying the gzip command. I tried it on a folder which has a hierarchy like\n\n\n\n```bash\nPersonal/Folder1/file1.amr\nPersonal/Folder2/file2.amr\nPersonal/Folder3/file3.amr\nPersonal/Folder4/file4.amr\n```\n\n\n\nI ran\n    \"gzip -r Personal\"\nand now its like\n\n\n\n```bash\nPersonal/Folder1/file1.amr.gz\nPersonal/Folder2/file2.amr.gz\nPersonal/Folder3/file3.amr.gz\nPersonal/Folder4/file4.amr.gz\n```\n\n\n\nHow do I go back?",
    "answer": "You can use \n\n\n\n```bash\ngunzip -r Personal\n```\n\n\n\nwhich works the same as \n\n\n\n```bash\ngzip -d -r Personal\n```\n\n\n\nIf \ngzip\n on your system does not have the \n-r\n option (e.g. \nbusybox\n's gzip) , you can use\n\n\n\n```bash\nfind Personal -name \"*.gz\" -type f -print0 | xargs -0 gunzip\n```",
    "url": "https://unix.stackexchange.com/questions/180247/how-to-gunzip-files-recursively-or-how-to-undo-gzip-r"
  },
  {
    "question_title": "Grep: how to add an &quot;OR&quot; condition?",
    "question_body": "How can I introduce a conditional OR into grep?  Something like, grepping a file's type for (JPEG OR JPG), and then sending only those files into the photos folder.  For example.  I know how to send the file where I want it, and get the file type, I just need some help with the grep part.\n\n\nI'm on OS X, which IMO seems to have modified/customized *nix utilities than what I'm used to in a *nix environment.  So hopefully the answers can be as generic/portable as possible.",
    "answer": "You can pass multiple regexes to \ngrep\n by using the \n-e\n option more than once:\n\n\n\n```bash\ngrep -e regex1 -e regex2  input_file\n```\n\n\n\nwill match lines matching \nregex1\n or \nregex2\n.",
    "url": "https://unix.stackexchange.com/questions/25821/grep-how-to-add-an-or-condition"
  },
  {
    "question_title": "Why are tar archive formats switching to xz compression to replace bzip2 and what about gzip?",
    "question_body": "More and more \ntar\n archives use the \nxz\n format based on LZMA2 for compression instead of the traditional \nbzip2(bz2)\n compression. In fact \nkernel.org\n made a late \"\nGood-bye bzip2\n\" \nannouncement, 27th Dec. 2013\n, indicating kernel sources would from this point on be released in both tar.gz and tar.xz format - and on the main page of the \nwebsite\n what's directly offered is in \ntar.xz\n.\n\n\nAre there any specific reasons explaining why this is happening and what is the relevance of \ngzip\n in this context?",
    "answer": "For distributing archives over the Internet, the following things are generally a priority:\n\n\n\n\nCompression ratio (i.e., how small the compressor makes the data);\n\n\nDecompression time (CPU requirements);\n\n\nDecompression memory requirements; and\n\n\nCompatibility (how wide-spread the decompression program is)\n\n\n\n\nCompression memory & CPU requirements aren't very important, because you can use a large fast machine for that, and you only have to do it once.\n\n\nCompared to bzip2, xz has a better compression ratio and lower (better) decompression time. It, however—at the compression settings typically used—requires more memory to decompress\n[1]\n and is somewhat less widespread. Gzip uses less memory than either.\n\n\nSo, both gzip and xz format archives are posted, allowing you to pick:\n\n\n\n\nNeed to decompress on a machine with \nvery\n limited memory (<32 MB): gzip. Given, not very likely when talking about kernel sources.\n\n\nNeed to decompress minimal tools available: gzip\n\n\nWant to save download time and/or bandwidth: xz\n\n\n\n\nThere isn't really a realistic combination of factors that'd get you to pick bzip2. So its being phased out.\n\n\nI looked at compression comparisons in \na blog post\n. I didn't attempt to replicate the results, and I suspect some of it has changed (mostly, I expect \nxz\n has improved, as its the newest.)\n\n\n(There are some specific scenarios where a good bzip2 implementation may be preferable to xz: bzip2 can compresses a file with lots of zeros and genome DNA sequences better than xz. Newer versions of xz now have an (optional) block mode which allows data recovery after the point of corruption and parallel compression and [in theory] decompression. Previously, only bzip2 offered these.\n[2]\n However none of these are relevant for kernel distribution)\n\n\n\n\n1: In archive size, \nxz -3\n is around \nbzip -9\n. Then xz uses less memory to decompress. But \nxz -9\n (as, e.g., used for Linux kernel tarballs) uses much more than \nbzip -9\n. (And even \nxz -0\n needs more than \ngzip -9\n).\n\n\n2: \nF21 System Wide Change: lbzip2 as default bzip2 implementation",
    "url": "https://unix.stackexchange.com/questions/108100/why-are-tar-archive-formats-switching-to-xz-compression-to-replace-bzip2-and-wha"
  },
  {
    "question_title": "How can I set my default shell to start up tmux",
    "question_body": "I would like my default bash shell to go straight into tmux instead of my always having to type tmux every time.",
    "answer": "@StarNamer's answer\n is generally accurate, though I typically include the following tests to make sure that\n\n\n\n\ntmux\n exists on the system\n\n\nwe're in an interactive shell, and\n\n\ntmux\n doesn't try to run within itself\n\n\n\n\nSo, I would add this to the \n.bashrc\n:\n\n\n\n```bash\nif command -v tmux &> /dev/null && [ -n \"$PS1\" ] && [[ ! \"$TERM\" =~ screen ]] && [[ ! \"$TERM\" =~ tmux ]] && [ -z \"$TMUX\" ]; then\n  exec tmux\nfi\n```\n\n\n\n\n\nReferences\n\n\n\n\nUsing bash's \ncommand\n to check for existence of a command - \nhttp://man7.org/linux/man-pages/man1/bash.1.html#SHELL_BUILTIN_COMMANDS\n\n\nWhy to use \ncommand\n instead of \nwhich\n to check for the existence of commands - \nhttps://unix.stackexchange.com/a/85250\n\n\nUsing \n$PS1\n to check for interactive shell - \nhttps://www.gnu.org/software/bash/manual/html_node/Is-this-Shell-Interactive_003f.html\n\n\nExpected state of \n$TERM\n environment variable \"for all programs running inside tmux\" - \nhttp://man7.org/linux/man-pages/man1/tmux.1.html#WINDOWS_AND_PANES",
    "url": "https://unix.stackexchange.com/questions/43601/how-can-i-set-my-default-shell-to-start-up-tmux"
  },
  {
    "question_title": "Is gzip atomic?",
    "question_body": "Is \ngzip\n atomic?\n\n\nWhat happens if I stop the \ngzip\n process while it's in the middle of gzipping a file? \n\n\nIf it's not atomic, and if I already pressed Ctrl+C on a \ngzip *.txt\n process, how do I safely resume? \n\n\n(I am not just curious about how to resume, but also about whether \ngzip\n specifically is atomic.)",
    "answer": "> Is gzip atomic?\n\n\n\nNo. It creates a compressed file and then removes the uncompressed original.\n\n\nSpecifically, it does not compress a file \nin situ\n and there is a period of time while the file is being compressed where,\n\n\n\n\nthe compressed target is incomplete\n\n\nthe partially compressed file and its source both exist in the filesystem.\n\n\n\n\n\n> What happens if I stop the gzip process while it's in the middle of gzipping a file?\n\n\n\nIf you stop the \ngzip\n process with a catchable signal (\nSIGINT\n from \nCtrl C\n, for example) it will cleanup partially created files. Otherwise, depending on the point at which it's stopped, you may end up with a partially compressed file alongside the untouched original.\n\n\n\n> If it's not atomic, if I already pressed Ctrl+C on a gzip *.txt process, how do i safely resume?\n\n\n\nYou delete the partially compressed version (if it still exists) and restart the \ngzip\n.",
    "url": "https://unix.stackexchange.com/questions/536984/is-gzip-atomic"
  },
  {
    "question_title": "Fastest way of working out uncompressed size of large GZIPPED file",
    "question_body": "Once a file is gzipped, is there a way of quickly querying it to say what the uncompressed file size is (without decompressing it), especially in cases where the uncompressed file is > 4GB in size.\n\n\nAccording to the RFC \nhttps://www.rfc-editor.org/rfc/rfc1952#page-5\n you can query the last 4 bytes of the file, but if the uncompressed file was > 4GB then the value just represents the \nuncompressed value modulo 2^32\n\n\nThis value can also be retrieved by running \ngunzip -l foo.gz\n, however the \"uncompressed\" column just contains \nuncompressed value modulo 2^32\n again, presumably as it's reading the footer as described above.\n\n\nI was just wondering if there is a way of getting the uncompressed file size without having to decompress it first, this would be especially useful in the case where gzipped files contain 50GB+ of data and would take a while to decompress using methods like \ngzcat foo.gz | wc -c\n\n\n\n\nEDIT:\n The 4GB limitation is openly acknowledged in the \nman\n page of the \ngzip\n utility included with OSX (\nApple gzip 242\n)\n\n\n\n```bash\nBUGS\n    According to RFC 1952, the recorded file size is stored in a 32-bit\n    integer, therefore, it can not represent files larger than 4GB. This\n    limitation also applies to -l option of gzip utility.\n```",
    "answer": "I believe the fastest way is to modify \ngzip\n so that testing in verbose mode outputs the number of bytes decompressed; on my system, with a 7761108684-byte file, I get\n\n\n\n```bash\n% time gzip -tv test.gz\ntest.gz:     OK (7761108684 bytes)\ngzip -tv test.gz  44.19s user 0.79s system 100% cpu 44.919 total\n\n% time zcat test.gz| wc -c\n7761108684\nzcat test.gz  45.51s user 1.54s system 100% cpu 46.987 total\nwc -c  0.09s user 1.46s system 3% cpu 46.987 total\n```\n\n\n\nTo modify gzip (1.6, as available in Debian), the patch is as follows:\n\n\n\n```bash\n--- a/gzip.c\n+++ b/gzip.c\n@@ -61,6 +61,7 @@\n #include <stdbool.h>\n #include <sys/stat.h>\n #include <errno.h>\n+#include <inttypes.h>\n \n #include \"closein.h\"\n #include \"tailor.h\"\n@@ -694,7 +695,7 @@\n \n     if (verbose) {\n         if (test) {\n-            fprintf(stderr, \" OK\\n\");\n+            fprintf(stderr, \" OK (%jd bytes)\\n\", (intmax_t) bytes_out);\n \n         } else if (!decompress) {\n             display_ratio(bytes_in-(bytes_out-header_bytes), bytes_in, stderr);\n@@ -901,7 +902,7 @@\n     /* Display statistics */\n     if(verbose) {\n         if (test) {\n-            fprintf(stderr, \" OK\");\n+            fprintf(stderr, \" OK (%jd bytes)\", (intmax_t) bytes_out);\n         } else if (decompress) {\n             display_ratio(bytes_out-(bytes_in-header_bytes), bytes_out,stderr);\n         } else {\n```\n\n\n\nA similar approach\n has been implemented in \ngzip\n, and will be included in the release following 1.11; \ngzip -l\n now decompresses the data to determine its size.",
    "url": "https://unix.stackexchange.com/questions/183465/fastest-way-of-working-out-uncompressed-size-of-large-gzipped-file"
  },
  {
    "question_title": "Colorizing your terminal and shell environment?",
    "question_body": "I spend most of my time working in Unix environments and using terminal emulators. I try to use color on the command line, because color makes the output more useful and intuitive.\n\n\nWhat options exist to add color to my terminal environment? What tricks do you use? What pitfalls have you encountered?\n\n\nUnfortunately, support for color varies depending on terminal type, OS, TERM setting, utility, buggy implementations, etc.\n\n\nHere are some tips from my setup, after a lot of experimentation:\n\n\n\n\nI tend to set \nTERM=xterm-color\n, which is supported on most hosts (but not all).\n\n\nI work on a number of different hosts, different OS versions, etc. I use everything from macOS X, Ubuntu Linux, RHEL/CentOS/Scientific Linux and FreeBSD. I'm trying to keep things simple and generic, if possible.\n\n\nI do a bunch of work using GNU \nscreen\n, which adds another layer of fun.\n\n\nMany OSs set things like \ndircolors\n and by default, and I don't want to modify this on a hundred different hosts. So I try to stick with the defaults. Instead, I tweak my terminal's color configuration.\n\n\nUse color for some \nUnix commands\n (\nls\n, \ngrep\n, \nless\n, \nvim\n) and the \nBash prompt\n. These commands seem to use the standard \"\nANSI escape sequences\n\". For example:\n\n\n\n```bash\nalias less='less --RAW-CONTROL-CHARS'\nexport LS_OPTS='--color=auto'\nalias ls='ls ${LS_OPTS}'\n```\n\n\n\n\n\nI'll post my \n.bashrc\n and answer my own question Jeopardy Style.",
    "answer": "Here are a couple of things you can do:\n\n\nEditors + Code\n\nA lot of editors have syntax highlighting support. \nvim\n and \nemacs\n have it on by default. You can also \nenable it under \nnano\n.\n\n\nYou can also syntax highlight code on the terminal by using \nPygments\n as a command-line tool.\n\n\ngrep\n\n\ngrep --color=auto\n highlights all matches. You can also use \nexport GREP_OPTIONS='--color=auto'\n to make it persistent without an alias. If you use \n--color=always\n, it'll \nuse colour even when piping\n, which confuses things.\n\n\nls\n\n\nls --color=always\n\n\nColors specified by:\n\n\n\n```bash\nexport LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33'\n```\n\n\n\n(hint: \ndircolors\n can be helpful)\n\n\nPS1\n\nYou can set your PS1 (shell prompt) to use colours. For example:\n\n\n\n```bash\nPS1='\\e[33;1m\\u@\\h: \\e[31m\\W\\e[0m\\$ '\n```\n\n\n\nWill produce a PS1 like:\n\n\n[yellow]lucas@ubuntu: [red]~[normal]$ \n\n\nYou can get really creative with this. As an idea:\n\n\n\n```bash\nPS1='\\e[s\\e[0;0H\\e[1;33m\\h    \\t\\n\\e[1;32mThis is my computer\\e[u[\\u@\\h:  \\w]\\$ '\n```\n\n\n\nPuts a bar at the top of your terminal with some random info. (For best results, also use \nalias clear=\"echo -e '\\e[2J\\n\\n'\"\n.)\n\n\nGetting Rid of Escape Sequences\n\n\nIf something is stuck outputting colour when you don't want it to, I use this \nsed\n line to strip the escape sequences:\n\n\n\n```bash\nsed \"s/\\[^[[0-9;]*[a-zA-Z]//gi\"\n```\n\n\n\nIf you want a more authentic experience, you can also get rid of lines starting with \n\\e[8m\n, which instructs the terminal to hide the text. (Not widely supported.)\n\n\n\n```bash\nsed \"s/^\\[^[8m.*$//gi\"\n```\n\n\n\nAlso note that those ^[s should be actual, literal ^[s. You can type them by pressing ^V^[ in bash, that is \nCtrl\n + \nV\n, \nCtrl\n + \n[\n.",
    "url": "https://unix.stackexchange.com/questions/148/colorizing-your-terminal-and-shell-environment"
  },
  {
    "question_title": "git status with submodule shows &quot;new commits&quot;",
    "question_body": "If I have a project with submodules, make changes in the submodule, and then commit those changes, the main project will show \"new commits\" on that submodule.  \n\n\nHowever, if I \"git pull\" and update my local project, and in the pull comes a change in the submodule commit, \"git status\" will also show \"new commits\".\n\n\nI'm confused that \"new commits\" can either mean \"you have local changes you need to commit\", or \"you have been updated with a new reference that you need to update\".\n\n\nIt seems like \"new commits\" can tell you two totally opposite things.\n\n\nIs there a better way to know from the top level if you have changes you need to push, vs you have a new reference you need to update to?",
    "answer": "Comment from mfurseman is the best solution, as far as I know:\n\n\n\n```bash\ngit submodule update --init --recursive\n```",
    "url": "https://unix.stackexchange.com/questions/537893/git-status-with-submodule-shows-new-commits"
  },
  {
    "question_title": "Find files that are not in .gitignore",
    "question_body": "I have find command that display files in my project:\n\n\n\n```bash\nfind . -type f -not -path './node_modules*' -a -not -path '*.git*' \\\n       -a -not -path './coverage*' -a -not -path './bower_components*' \\\n       -a -not -name '*~'\n```\n\n\n\nHow can I filter the files so it don't show the ones that are in .gitignore?\n\n\nI thought that I use:\n\n\n\n```bash\nwhile read file; do\n    grep $file .gitignore > /dev/null && echo $file;\ndone\n```\n\n\n\nbut .gitignore file can have glob patterns (also it will not work with paths if file is in .gitignore), How can I filter files based on patterns that may have globs?",
    "answer": "git\n provides \ngit-check-ignore\n to check whether a file is excluded by \n.gitignore\n.\n\n\nSo you could use:\n\n\n\n```bash\nfind . -type f -not -path './node_modules*' \\\n       -a -not -path '*.git*'               \\\n       -a -not -path './coverage*'          \\\n       -a -not -path './bower_components*'  \\\n       -a -not -name '*~'                   \\\n       -exec sh -c '\n         for f do\n           git check-ignore -q \"$f\" ||\n           printf '%s\\n' \"$f\"\n         done\n       ' find-sh {} +\n```\n\n\n\nNote that you would pay big cost for this because the check was performed for each file.",
    "url": "https://unix.stackexchange.com/questions/358270/find-files-that-are-not-in-gitignore"
  },
  {
    "question_title": "How do I grep recursively through .gz files?",
    "question_body": "I am using a script to regularly download my gmail messages that compresses the raw .eml into .gz files. The script creates a folder for each day, and then compresses every message into its own file.\n\n\nI would like a way to search through this archive for a \"string.\"\n\n\nGrep alone doesn't appear to do it. I also tried SearchMonkey.",
    "answer": "If you want to grep recursively in all .eml.gz files in the current directory, you can use:\n\n\n\n```bash\nfind . -name \\*.eml.gz -print0 | xargs -0 zgrep \"STRING\"\n```\n\n\n\nYou have to escape the first \n*\n so that the shell does not interpret it. \n-print0\n tells find to print a null character after each file it finds; \nxargs -0\n reads from standard input and runs the command after it for each file; \nzgrep\n works like \ngrep\n, but uncompresses the file first.",
    "url": "https://unix.stackexchange.com/questions/187742/how-do-i-grep-recursively-through-gz-files"
  },
  {
    "question_title": "How to grep lines which does not begin with &quot;#&quot; or &quot;;&quot;?",
    "question_body": "I want to grep \nsmb.conf\n and see only lines which are not commented.",
    "answer": "```bash\ngrep \"^[^#;]\" smb.conf\n```\n\n\n\nThe first \n^\n refers to the beginning of the line, so lines with comments starting after the first character will not be excluded.  \n[^#;]\n means any character which is not \n#\n or \n;\n.\n\n\nIn other words, it reports lines that start with any character other than \n#\n and \n;\n. It's not the same as reporting the lines that don't start with \n#\n and \n;\n (for which you'd use \ngrep -v '^[#;]'\n) in that it also excludes \nempty\n lines, but that's probably preferable in this case as I doubt you care about empty lines.\n\n\nIf you wanted to ignore leading blank characters, you could change it to:\n\n\n\n```bash\ngrep '^[[:blank:]]*[^[:blank:]#;]' smb.conf\n```\n\n\n\nor\n\n\n\n```bash\ngrep -vxE '[[:blank:]]*([#;].*)?' smb.conf\n```\n\n\n\nOr\n\n\n\n```bash\nawk '$1 ~ /^[^;#]/' smb.conf\n```",
    "url": "https://unix.stackexchange.com/questions/60994/how-to-grep-lines-which-does-not-begin-with-or"
  },
  {
    "question_title": "How to pipe the result of a grep search into a new vi file",
    "question_body": "I'm using \ngrep -e Peugeot -e PeuGeot carlist.txt\n to search through carlist.txt and pull out some items and I presumed that \ngrep -e Peugeot -e PeuGeot carlist.txt | vi\n would pipe it through for me but this is what I get:\n\n\n\n```bash\nVim: Warning: Input is not from a terminal\nVim: Error reading input, exiting...\nVim: preserving files...\nVim: Finished.\n```",
    "answer": "Running vi or vim with '-' as an argument makes it read the file to edit from standard input. Hence:\n\n\n\n```bash\ngrep -e Peugeot -e PeuGeot carlist.txt | vi -\n```\n\n\n\nwill do what you need.",
    "url": "https://unix.stackexchange.com/questions/102089/how-to-pipe-the-result-of-a-grep-search-into-a-new-vi-file"
  },
  {
    "question_title": "How to start multi-threaded grep in terminal?",
    "question_body": "I have a folder which has 250+ files of 2 GB each. I need to search for a string/pattern in those files and output the result in an \noutput\n file. I know I can run the following command, but it is too slow!!\n\n\n\n```bash\ngrep mypattern * > output\n```\n\n\n\nI want to speed it up. Being a programmer in Java, I know multi-threading can be used for speeding up the process. I'm stuck on how to start \ngrep\n in \"multi-threaded mode\" and write the output into a single \noutput\n file.",
    "answer": "There are two easy solutions for this. Basically, using \nxargs\n or \nparallel\n.\n\n\nxargs Approach:\n\n\nYou can use \nxargs\n with \nfind\n as follows:\n\n\n\n```bash\nfind . -type f -print0  | xargs -0 -P number_of_processes grep mypattern > output\n```\n\n\n\nWhere you will replace \nnumber_of_processes\n by the maximum number of processes you want to be launched.\nHowever, this is not guaranteed to give you a significant performance in case your performance is I/O limited. In which case you might try to start more processes to compensate for the time lost waiting for I/Os.\n\n\nAlso, with the inclusion of find, you can specify more advanced options instead of just file patterns, like modification time, etc ...\n\n\nOne possible issue with this approach as explained by Stéphane's comments, if there are few files, \nxargs\n may not start sufficiently many processes for them. One solution will be to use the \n-n\n option for \nxargs\n to specify how many arguments should it take from the pipe at a time. Setting \n-n1\n will force \nxargs\n to start a new process for each single file. This might be a desired behavior if the files are very large (like in the case of this question) and there is a relatively small number of files. However, if the files themselves are small, the overhead of starting a new process may undermine the advantage of parallelism, in which case a greater \n-n\n value will be better. Thus, the \n-n\n option might be fine tuned according to the file sizes and number.\n\n\nParallel Approach:\n\n\nAnother way to do it is to use Ole Tange GNU Parallel tool \nparallel\n, (available \nhere\n). This offers greater fine grain control over parallelism and can even be distributed over multiple hosts (would be beneficial if your directory is shared for example). \n Simplest syntax using parallel will be:\n\n\nfind . -type f | parallel -j+1 grep mypattern\n\n\nwhere the option \n-j+1\n instructs parallel to start one process in excess of the number of cores on your machine (This can be helpful for I/O limited tasks, you may even try to go higher in number).\n\n\nParallel also has the advantage over \nxargs\n of actually retaining the order of the output from each process and generating a contiguous output. For example, with \nxargs\n, if process 1 generates a line say \np1L1\n, process 2 generates a line \np2L1\n, process 1 generates another line \np1L2\n, the output will be:\n\n\n\n```bash\np1L1\np2L1\np1L2\n```\n\n\n\nwhereas with \nparallel\n the output should be:\n\n\n\n```bash\np1L1\np1L2\np2L1\n```\n\n\n\nThis is usually more useful than \nxargs\n output.",
    "url": "https://unix.stackexchange.com/questions/197352/how-to-start-multi-threaded-grep-in-terminal"
  },
  {
    "question_title": "How do I remove a directory and all its contents?",
    "question_body": "In bash all I know is that \n\n\n\n```bash\nrmdir directoryname\n```\n\n\n\nwill remove the directory but only if it's empty. Is there a way to force remove subdirectories?",
    "answer": "The following command will do it for you. Use caution though if this isn't your intention as this also removes files in the directory and subdirectories.\n\n\n\n```bash\nrm -rf directoryname\n```",
    "url": "https://unix.stackexchange.com/questions/45676/how-do-i-remove-a-directory-and-all-its-contents"
  },
  {
    "question_title": "Bash - how to run a command after the previous finished?",
    "question_body": "Currently I have a \nscript.sh\n file with the following content:\n\n\n\n```bash\n#!/bin/bash\nwget -q http://exemple.com/page1.php;\nwget -q http://exemple.com/page2.php;\nwget -q http://exemple.com/page3.php;\n```\n\n\n\nI want to execute the commands one by one, when the previous finishes. Am I doing it in the right way? I've never worked with Linux before and tried to search for it but found no solutions.",
    "answer": "Yes, you're doing it the right way. Shell scripts will run each command sequentially, waiting for the first to finish before the next one starts. You can either join commands with \n;\n or have them on separate lines:\n\n\n\n```bash\ncommand1; command2\n```\n\n\n\nor \n\n\n\n```bash\ncommand1\ncommand2\n```\n\n\n\nThere is no need for \n;\n if the commands are on separate lines. You can also choose to run the second command only if the first exited successfully. To do so, join them with \n&&\n:\n\n\n\n```bash\ncommand1 && command2\n```\n\n\n\nor \n\n\n\n```bash\ncommand1 &&\ncommand2\n```\n\n\n\nFor more information on the various control operators available to you, see \nhere\n.",
    "url": "https://unix.stackexchange.com/questions/184502/bash-how-to-run-a-command-after-the-previous-finished"
  },
  {
    "question_title": "How to colorize output of git?",
    "question_body": "Is there a way to color output for git (or any command)?\n\n\nConsider:\n\n\n\n```bash\nballer@Laptop:~/rails/spunky-monkey$ git status\n# On branch new-message-types\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#       modified:   app/models/message_type.rb\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nballer@Laptop:~/rails/spunky-monkey$ git add app/models\n```\n\n\n\nAnd\n\n\n\n```bash\nballer@Laptop:~/rails/spunky-monkey$ git status\n# On branch new-message-types\n# Changes to be committed:\n#   (use \"git reset HEAD <file>...\" to unstage)\n#\n#       modified:   app/models/message_type.rb\n#\n```\n\n\n\nThe output looks the same, but the information is totally different: the file has gone from unstaged to staged for commit.\n\n\nIs there a way to colorize the output? For example, files that are unstaged are red, staged are green? \n\n\nOr even \nChanges not staged for commit:\n to red and \n# Changes to be committed:\n to green?\n\n\nWorking in Ubuntu.\n\n\nEDIT: Googling found this answer which works great: \ngit config --global --add color.ui true\n.\n\n\nHowever, is there any more general solution for adding color to a command output?",
    "answer": "You can create a section \n[color]\n in your \n~/.gitconfig\n with e.g. the following content\n\n\n\n```bash\n[color]\n  diff = auto\n  status = auto\n  branch = auto\n  interactive = auto\n  ui = true\n  pager = true\n```\n\n\n\nYou can also fine control what you want to have coloured in what way, e.g.\n\n\n\n```bash\n[color \"status\"]\n  added = green\n  changed = red bold\n  untracked = magenta bold\n\n[color \"branch\"]\n  remote = yellow\n```\n\n\n\nI hope this gets you started. And of course, you need a terminal which supports colour.\n\n\nAlso see \nthis answer\n for a way to add colorization directly from the command line.",
    "url": "https://unix.stackexchange.com/questions/44266/how-to-colorize-output-of-git"
  },
  {
    "question_title": "Looping through files with spaces in the names?",
    "question_body": "I wrote the following script to diff the outputs of two directores with all the same files in them as such:\n\n\n\n```bash\n#!/bin/bash\n\nfor file in `find . -name \"*.csv\"`  \ndo\n     echo \"file = $file\";\n     diff $file /some/other/path/$file;\n     read char;\ndone\n```\n\n\n\nI know there are other ways to achieve this.  Curiously though, this script fails when the files have spaces in them.  How can I deal with this?\n\n\nExample output of find:\n\n\n\n```bash\n./zQuery - abc - Do Not Prompt for Date.csv\n```",
    "answer": "Short answer (closest to your answer, but handles spaces)\n\n\n\n```bash\nOIFS=\"$IFS\"\nIFS=$'\\n'\nfor file in `find . -type f -name \"*.csv\"`  \ndo\n     echo \"file = $file\"\n     diff \"$file\" \"/some/other/path/$file\"\n     read line\ndone\nIFS=\"$OIFS\"\n```\n\n\n\nBetter answer (also handles wildcards and newlines in file names)\n\n\n\n```bash\nfind . -type f -name \"*.csv\" -print0 | while IFS= read -r -d '' file; do\n    echo \"file = $file\"\n    diff \"$file\" \"/some/other/path/$file\"\n    read line </dev/tty\ndone\n```\n\n\n\nBest answer (based on \nGilles' answer\n)\n\n\n\n```bash\nfind . -type f -name '*.csv' -exec sh -c '\n  file=\"$0\"\n  echo \"$file\"\n  diff \"$file\" \"/some/other/path/$file\"\n  read line </dev/tty\n' exec-sh {} ';'\n```\n\n\n\nOr even better, to avoid running one \nsh\n per file:\n\n\n\n```bash\nfind . -type f -name '*.csv' -exec sh -c '\n  for file do\n    echo \"$file\"\n    diff \"$file\" \"/some/other/path/$file\"\n    read line </dev/tty\n  done\n' exec-sh {} +\n```\n\n\n\n\n\nLong answer\n\n\nYou have three problems:\n\n\n\n\nBy default, the shell splits the output of a command on spaces, tabs, and newlines\n\n\nFilenames could contain wildcard characters which would get expanded\n\n\nWhat if there is a directory whose name ends in \n*.csv\n?\n\n\n\n\n1. Splitting only on newlines\n\n\nTo figure out what to set \nfile\n to, the shell has to take the output of \nfind\n and interpret it somehow, otherwise \nfile\n would just be the entire output of \nfind\n.\n\n\nThe shell reads the \nIFS\n variable, which is set to \n<space><tab><newline>\n by default.\n\n\nThen it looks at each character in the output of \nfind\n.  As soon as it sees any character that's in \nIFS\n, it thinks that marks the end of the file name, so it sets \nfile\n to whatever characters it saw until now and runs the loop.  Then it starts where it left off to get the next file name, and runs the next loop, etc., until it reaches the end of output.\n\n\nSo it's effectively doing this:\n\n\n\n```bash\nfor file in \"zquery\" \"-\" \"abc\" ...\n```\n\n\n\nTo tell it to only split the input on newlines, you need to do\n\n\n\n```bash\nIFS=$'\\n'\n```\n\n\n\nbefore your \nfor ... find\n command.\n\n\nThat sets \nIFS\n to a single newline, so it only splits on newlines, and not spaces and tabs as well.\n\n\nIf you are using \nsh\n or \ndash\n instead of \nksh93\n, \nbash\n or \nzsh\n, you need to write \nIFS=$'\\n'\n like this instead:\n\n\n\n```bash\nIFS='\n'\n```\n\n\n\nThat is probably enough to get your script working, but if you're interested to handle some other corner cases properly, read on...\n\n\n2. Expanding \n$file\n without wildcards\n\n\nInside the loop where you do\n\n\n\n```bash\ndiff $file /some/other/path/$file\n```\n\n\n\nthe shell tries to expand \n$file\n (again!).\n\n\nIt could contain spaces, but since we already set \nIFS\n above, that won't be a problem here.\n\n\nBut it could also contain wildcard characters such as \n*\n or \n?\n, which would lead to unpredictable behavior.  (Thanks to Gilles for pointing this out.)\n\n\nTo tell the shell not to expand wildcard characters, put the variable inside double quotes, e.g.\n\n\n\n```bash\ndiff \"$file\" \"/some/other/path/$file\"\n```\n\n\n\nThe same problem could also bite us in\n\n\n\n```bash\nfor file in `find . -name \"*.csv\"`\n```\n\n\n\nFor example, if you had these three files\n\n\n\n```bash\nfile1.csv\nfile2.csv\n*.csv\n```\n\n\n\n(very unlikely, but still possible)\n\n\nIt would be as if you had run\n\n\n\n```bash\nfor file in file1.csv file2.csv *.csv\n```\n\n\n\nwhich will get expanded to\n\n\n\n```bash\nfor file in file1.csv file2.csv *.csv file1.csv file2.csv\n```\n\n\n\ncausing \nfile1.csv\n and \nfile2.csv\n to be processed twice.\n\n\nInstead, we have to do\n\n\n\n```bash\nfind . -name \"*.csv\" -print | while IFS= read -r file; do\n    echo \"file = $file\"\n    diff \"$file\" \"/some/other/path/$file\"\n    read line </dev/tty\ndone\n```\n\n\n\nread\n reads lines from standard input, splits the line into words according to \nIFS\n and stores them in the variable names that you specify.\n\n\nHere, we're telling it not to split the line into words, and to store the line in \n$file\n.\n\n\nAlso note that \nread line\n has changed to \nread line </dev/tty\n.\n\n\nThis is because inside the loop, standard input is coming from \nfind\n via the pipeline.\n\n\nIf we just did \nread\n, it would be consuming part or all of a file name, and some files would be skipped.\n\n\n/dev/tty\n is the terminal where the user is running the script from.  Note that this will cause an error if the script is run via cron, but I assume this is not important in this case.\n\n\nThen, what if a file name contains newlines?\n\n\nWe can handle that by changing \n-print\n to \n-print0\n and using \nread -d ''\n on the end of a pipeline:\n\n\n\n```bash\nfind . -name \"*.csv\" -print0 | while IFS= read -r -d '' file; do\n    echo \"file = $file\"\n    diff \"$file\" \"/some/other/path/$file\"\n    read char </dev/tty\ndone\n```\n\n\n\nThis makes \nfind\n put a null byte at the end of each file name.  Null bytes are the only characters not allowed in file names, so this should handle all possible file names, no matter how weird.\n\n\nTo get the file name on the other side, we use \nIFS= read -r -d ''\n.\n\n\nWhere we used \nread\n above, we used the default line delimiter of newline, but now, \nfind\n is using null as the line delimiter. In \nbash\n, you can't pass a NUL character in an argument to a command (even builtin ones), but \nbash\n understands \n-d ''\n as meaning \nNUL delimited\n. So we use \n-d ''\n to make \nread\n use the same line delimiter as \nfind\n. Note that \n-d $'\\0'\n, incidentally, works as well, because \nbash\n not supporting NUL bytes treats it as the empty string.\n\n\nTo be correct, we also add \n-r\n, which says don't handle backslashes in file names specially.  For example, without \n-r\n, \n\\<newline>\n are removed, and \n\\n\n is converted into \nn\n.\n\n\nA more portable way of writing this that doesn't require \nbash\n or \nzsh\n or remembering all the above rules about null bytes (again, thanks to Gilles):\n\n\n\n```bash\nfind . -name '*.csv' -exec sh -c '\n  file=\"$0\"\n  echo \"$file\"\n  diff \"$file\" \"/some/other/path/$file\"\n  read char </dev/tty\n' exec-sh {} ';'\n```\n\n\n\n*\n3. Skipping directories whose names end in \n.csv\n\n\n\n```bash\nfind . -name \"*.csv\"\n```\n\n\n\nwill also match directories that are called \nsomething.csv\n.\n\n\nTo avoid this, add \n-type f\n to the \nfind\n command.\n\n\n\n```bash\nfind . -type f -name '*.csv' -exec sh -c '\n  file=\"$0\"\n  echo \"$file\"\n  diff \"$file\" \"/some/other/path/$file\"\n  read line </dev/tty\n' exec-sh {} ';'\n```\n\n\n\nAs \nglenn jackman\n points out, in both of these examples, the commands to execute for each file are being run in a subshell, so if you change any variables inside the loop, they will be forgotten.\n\n\nIf you need to set variables and have them still set at the end of the loop, you can rewrite it to use process substitution like this:\n\n\n\n```bash\ni=0\nwhile IFS= read -r -d '' file; do\n    echo \"file = $file\"\n    diff \"$file\" \"/some/other/path/$file\"\n    read line </dev/tty\n    i=$((i+1))\ndone < <(find . -type f -name '*.csv' -print0)\necho \"$i files processed\"\n```\n\n\n\nNote that if you try copying and pasting this at the command line, \nread line\n will consume the \necho \"$i files processed\"\n, so that command won't get run.\n\n\nTo avoid this, you could remove \nread line </dev/tty\n and send the result to a pager like \nless\n.\n\n\n\n\nNOTES\n\n\nI removed the semi-colons (\n;\n) inside the loop.  You can put them back if you want, but they are not needed.\n\n\nThese days, \n$(command)\n is more common than \n`command`\n.  This is mainly because it's easier to write \n$(command1 $(command2))\n than \n`command1 \\`command2\\``\n.\n\n\nread char\n doesn't really read a character.  It reads a whole line so I changed it to \nread line\n.",
    "url": "https://unix.stackexchange.com/questions/9496/looping-through-files-with-spaces-in-the-names"
  },
  {
    "question_title": "gunzip all .gz files in directory",
    "question_body": "I have a directory with plenty of \n.txt.gz\n files (where the names do not follow a specific pattern.)\n\n\nWhat is the simplest way to \ngunzip\n them? I want to preserve their original names, so that they go from \nwhatevz.txt.gz\n to \nwhatevz.txt",
    "answer": "How about just this?\n\n\n\n```bash\n$ gunzip *.txt.gz\n```\n\n\n\ngunzip\n will create a gunzipped file without the \n.gz\n suffix and remove the original file by default (see below for details). \n*.txt.gz\n will be expanded by your shell to all the files matching.\n\n\nThis last bit can get you into trouble if it expands to a very long list of files. In that case, try using \nfind\n and \n-exec\n to do the job for you.\n\n\n\n\nFrom the man page \ngzip(1)\n:\n\n\n\n> ```bash\n> gunzip takes a list of files on its command line and  replaces  each  file\n> whose  name  ends  with  .gz, -gz, .z, -z, or _z (ignoring case) and which\n> begins with the correct magic number with an uncompressed file without the\n> original  extension.\n> ```\n\n\n\n\n\nNote about 'original name'\n\n\ngzip can store and restore the filename used at compression time. Even if you rename the compressed file, you can be surprised to find out it restores to the original name again.\n\n\nFrom the gzip manpage:\n\n\n\n> By default, gzip keeps the original file name and timestamp in  the  compressed\n> file.  These  are  used when decompressing the file with the -N option. This is\n> useful when the compressed file name was truncated or when the time  stamp  was\n> not preserved after a file transfer.\n\n\n\nAnd these file names stored in metadata can also be viewed with \nfile\n:\n\n\n\n```bash\n$ echo \"foo\" > myfile_orig\n$ gzip myfile_orig \n$ mv myfile_orig.gz myfile_new.gz \n$ file myfile_new.gz \nmyfile_new.gz: gzip compressed data, was \"myfile_orig\", last modified: Mon Aug  5 08:46:39 2019, from Unix\n$ gunzip myfile_new.gz        # gunzip without -N\n$ ls myfile_*\nmyfile_new\n\n$ rm myfile_*\n$ echo \"foo\" > myfile_orig\n$ gzip myfile_orig\n$ mv myfile_orig.gz myfile_new.gz \n# gunzip with -N\n$ gunzip -N myfile_new.gz     # gunzip with -N\n$ ls myfile_*\nmyfile_orig\n```",
    "url": "https://unix.stackexchange.com/questions/56421/gunzip-all-gz-files-in-directory"
  },
  {
    "question_title": "Preserve bash history in multiple terminal windows",
    "question_body": "I consistently have more than one terminal open. Anywhere from two to ten, doing various bits and bobs. Now let's say I restart and open up another set of terminals. Some remember certain things, some forget.\n\n\nI want a history that:\n\n\n\n\nRemembers everything from every terminal\n\n\nIs instantly accessible from every terminal (eg if I \nls\n in one, switch to another already-running terminal and then press up, \nls\n shows up)\n\n\nDoesn't forget command if there are spaces at the front of the command.\n\n\n\n\nAnything I can do to make bash work more like that?",
    "answer": "Add the following to your \n~/.bashrc\n:\n\n\n\n```bash\n# Avoid duplicates\nHISTCONTROL=ignoredups:erasedups\n# When the shell exits, append to the history file instead of overwriting it\nshopt -s histappend\n\n# After each command, append to the history file and reread it\nPROMPT_COMMAND=\"${PROMPT_COMMAND:+$PROMPT_COMMAND$'\\n'}history -a; history -c; history -r\"\n```",
    "url": "https://unix.stackexchange.com/questions/1288/preserve-bash-history-in-multiple-terminal-windows"
  },
  {
    "question_title": "Count total number of occurrences using grep",
    "question_body": "grep -c\n is useful for finding how many times a string occurs in a file, but it only counts each occurence once per line. How to count multiple occurences per line?\n\n\nI'm looking for something more elegant than:\n\n\n\n```bash\nperl -e '$_ = <>; print scalar ( () = m/needle/g ), \"\\n\"'\n```",
    "answer": "grep's \n-o\n will only output the matches, ignoring lines; \nwc\n can count them:\n\n\n\n```bash\ngrep -o 'needle' file | wc -l\n```\n\n\n\nThis will also match 'needles' or 'multineedle'.\n\n\nTo match only single words use one of the following commands:\n\n\n\n```bash\ngrep -ow 'needle' file | wc -l\ngrep -o '\\bneedle\\b' file | wc -l\ngrep -o '\\<needle\\>' file | wc -l\n```",
    "url": "https://unix.stackexchange.com/questions/6979/count-total-number-of-occurrences-using-grep"
  },
  {
    "question_title": "In a bash script, using the conditional &quot;or&quot; in an &quot;if&quot; statement",
    "question_body": "This question is a sequel of sorts to my \nearlier question\n.  The users on this site kindly helped me determine how to write a bash \nfor\n loop that iterates over string values.  For example, suppose that a loop control variable \nfname\n iterates over the strings \n\"a.txt\" \"b.txt\" \"c.txt\"\n.  I would like to \necho\n \"yes!\" when \nfname\n has the value \n\"a.txt\"\n or \n\"c.txt\"\n, and \necho\n \"no!\" otherwise.  I have tried the following bash shell script:\n\n\n\n```bash\n#!/bin/bash\n\nfor fname in \"a.txt\" \"b.txt\" \"c.txt\"\ndo\n  echo $fname\n  if [ \"$fname\" = \"a.txt\" ] | [ \"$fname\" = \"c.txt\" ]; then\n    echo \"yes!\"\n  else\n    echo \"no!\"\n  fi\ndone\n```\n\n\n\nI obtain the output:\n\n\n\n> a.txt\n> no!\n> b.txt\n> no!\n> c.txt\n> yes!\n\n\n\nWhy does the \nif\n statement apparently yield true when \nfname\n has the value \n\"a.txt\"\n?  Have I used \n|\n incorrectly?",
    "answer": "If you want to say \nOR\n use double pipe (\n||\n).\n\n\n\n```bash\nif [ \"$fname\" = \"a.txt\" ] || [ \"$fname\" = \"c.txt\" ]\n```\n\n\n\n(The original OP code using \n|\n was simply piping the output of the left side to the right side, in the same way any ordinary pipe works.)\n\n\n\n\nAfter many years of comments and misunderstanding, allow me to clarify.\n\n\nTo do \nOR\n you use \n||\n.\n\n\nWhether you use \n[\n or \n[[\n or \ntest\n or \n((\n all depends on what you need on a case by case basis. It's wrong to say that one of those is preferred in all cases. Sometimes \n[\n is right and \n[[\n is wrong. But that's not what the question was. OP asked why \n|\n didn't work. The answer is because it should be \n||\n instead.",
    "url": "https://unix.stackexchange.com/questions/47584/in-a-bash-script-using-the-conditional-or-in-an-if-statement"
  },
  {
    "question_title": "How to suspend and resume processes",
    "question_body": "In the bash terminal I can hit \nControl\n+\nZ\n to suspend any running process... then I can type \nfg\n to resume the process.\n\n\nIs it possible to suspend a process if I only have it's PID? And if so, what command should I use?\n\n\nI'm looking for something like:\n\n\n\n```bash\nsuspend-process $PID_OF_PROCESS\n```\n\n\n\nand then to resume it with\n\n\n\n```bash\nresume-process $PID_OF_PROCESS\n```",
    "answer": "You can use \nkill\n to stop the process.\n\n\nFor a 'polite' stop to the process (prefer this for normal use), send \nSIGTSTP\n:\n\n\n\n```bash\nkill -TSTP [pid]\n```\n\n\n\nFor a 'hard' stop, send \nSIGSTOP\n:\n\n\n\n```bash\nkill -STOP [pid]\n```\n\n\n\nNote that if the process you are trying to stop by PID is in your shell's job table, it may remain visible there, but terminated, until the process is \nfg\n'd again.\n\n\nTo resume execution of the process, sent \nSIGCONT\n:\n\n\n\n```bash\nkill -CONT [pid]\n```",
    "url": "https://unix.stackexchange.com/questions/2107/how-to-suspend-and-resume-processes"
  },
  {
    "question_title": "Test tar file integrity in bash",
    "question_body": "I have a bash script that creates a '.tar' file. Once the file is created, I would like to test its integrity and send an email to the root user if the integrity is bad.\n\n\nI know I would need to use the command \ntar -tf /root/archive.tar\n to check the integrity of the file, but how would I implement this in a bash if statement and check for errors?",
    "answer": "If \ntar\n finds errors in its input it will \nexit(3)\n¹ with a non-zero exit value.  This — with most \ntar\n implementations — is also done when listing archive contents with \nt\n.  So you could simply check for the exit value of \ntar\n to determine if something has gone wrong:\n\n\n\n```bash\nif ! tar tf /root/archive.tar &> /dev/null; then\n    write_an_email_to_root\nfi\n```\n\n\n\nIf your \ntar\n does not find all errors with \nt\n, you could still extract the archive to \nstdout\n and redirect \nstdout\n to \n/dev/null\n, which would be the slower but more reliable approach:\n\n\n\n```bash\nif ! tar xOf /root/archive.tar &> /dev/null; then\n    write_an_email_to_root\nfi\n```\n\n\n\n¹ This notation denotes the manpage, not the actual call.  See \nman 3 exit\n.",
    "url": "https://unix.stackexchange.com/questions/129599/test-tar-file-integrity-in-bash"
  },
  {
    "question_title": "Git: edit previous commits&#39; messages only",
    "question_body": "For lazy reasons I pushed a bunch of commits with default messages and now it has become cumbersome, as I don't really know what I've changed in each commit.\n\n\nHow do I edit just the messages of previous commits and (if possible) keep the commit tree?",
    "answer": "To edit the commit messages of a series of commits, I run\n\n\n\n```bash\ngit rebase -i firstsha\n```\n\n\n\nwhere \nfirstsha\n is an identifier for the parent commit of the first commit I want to edit. (You can use any valid reference here, so \ngit rebase -i HEAD~4\n will show the last four commits.)\n\n\nIn the editor that opens, change all the “pick” entries to “reword” on commits you wish to modify, then close the editor; you will then be asked to enter commit messages for all the commits you chose.\n\n\nNote that this \nwill\n change the commit tree, because the hashes of the commits will change. You will have to force-push your new tree, or push it to a new branch. Take care when rebasing merges; you’ll need to use \n-r\n (\n--rebase-merges\n), and read \nthe “Rebasing merges” section of the \ngit rebase\n manpage\n.\n\n\nTo quickly edit only the last commit, run\n\n\n\n```bash\ngit commit --amend\n```\n\n\n\n(but beware of anything already staged for commit).",
    "url": "https://unix.stackexchange.com/questions/485918/git-edit-previous-commits-messages-only"
  },
  {
    "question_title": "Argument string to integer in bash",
    "question_body": "Trying to figure out how to convert an argument to an integer to perform arithmetic on, and then print it out, say for \naddOne.sh\n:\n\n\n\n```bash\necho $1 + 1\n>>sh addOne.sh 1\nprints 1 + 1\n```",
    "answer": "In bash, one does not \"convert an argument to an integer to perform arithmetic\".  In bash, variables are treated as integer or string depending on context.\n\n\n(If you are using a variable in an integer context, then, obviously, the variable better contain a string that looks like a valid integer.  Otherwise, you'll get an error.)\n\n\nTo perform arithmetic, you should invoke the arithmetic expansion operator \n$((...))\n.  For example:\n\n\n\n```bash\n$ a=2\n$ echo \"$a + 1\"\n2 + 1\n$ echo \"$(($a + 1))\"\n3\n```\n\n\n\nor generally preferred:\n\n\n\n```bash\n$ echo \"$((a + 1))\"\n3\n```\n\n\n\nYou should be aware that bash (as opposed to ksh93, zsh or yash) only performs \ninteger\n arithmetic.  If you have floating point numbers (numbers with decimals), then there are other tools to assist.  For example, use \nbc\n:\n\n\n\n```bash\n$ b=3.14\n$ echo \"$(($b + 1))\"\nbash: 3.14 + 1: syntax error: invalid arithmetic operator (error token is \".14 + 1\")\n$ echo \"$b + 1\" | bc -l\n4.14\n```\n\n\n\nOr you can use a shell with floating point arithmetic support instead of bash:\n\n\n\n```bash\nzsh> echo $((3.14 + 1))\n4.14\n```",
    "url": "https://unix.stackexchange.com/questions/232384/argument-string-to-integer-in-bash"
  },
  {
    "question_title": "What is the difference between &quot;sort -u&quot; and &quot;sort | uniq&quot;?",
    "question_body": "Everywhere I see someone needing to get a sorted, unique list, they always pipe to \nsort | uniq\n. I've never seen any examples where someone uses \nsort -u\n instead. Why not? What's the difference, and why is it better to use uniq than the unique flag to sort?",
    "answer": "sort | uniq\n existed before \nsort -u\n, and is compatible with a wider range of systems, although almost all modern systems do support \n-u\n -- it's POSIX. It's mostly a throwback to the days when \nsort -u\n didn't exist (and people don't tend to change their methods if the way that they know continues to work, just look at \nifconfig\n vs. \nip\n adoption).\n\n\nThe two were likely merged because removing duplicates within a file requires sorting (at least, in the standard case), and is an extremely common use case of sort. It is also faster internally as a result of being able to do both operations at the same time (and due to the fact that it doesn't require IPC (\nInter-process communication\n) between \nuniq\n and \nsort\n). Especially if the file is big, \nsort -u\n will likely use fewer intermediate files to sort the data.\n\n\nOn my system I consistently get results like this:\n\n\n\n```bash\n$ dd if=/dev/urandom of=/dev/shm/file bs=1M count=100\n100+0 records in\n100+0 records out\n104857600 bytes (105 MB) copied, 8.95208 s, 11.7 MB/s\n$ time sort -u /dev/shm/file >/dev/null\n\nreal        0m0.500s\nuser        0m0.767s\nsys         0m0.167s\n$ time sort /dev/shm/file | uniq >/dev/null\n\nreal        0m0.772s\nuser        0m1.137s\nsys         0m0.273s\n```\n\n\n\nIt also doesn't mask the return code of \nsort\n, which may be important (in modern shells there are ways to get this, for example, \nbash\n's \n$PIPESTATUS\n array, but this wasn't always true).",
    "url": "https://unix.stackexchange.com/questions/76049/what-is-the-difference-between-sort-u-and-sort-uniq"
  },
  {
    "question_title": "How can I prevent &#39;grep&#39; from showing up in ps results?",
    "question_body": "When I  search for some process that doesn't exist, e.g.\n\n\n\n```bash\n$ ps aux | grep fnord                          \nwayne    15745  0.0  0.0  13580   928 pts/6    S+   03:58   0:00 grep fnord\n```\n\n\n\nObviously I don't care about grep - that makes as much sense as searching for the \nps\n process!\n\n\nHow can I prevent grep from showing up in the results?",
    "answer": "Turns out there's a solution found in \nkeychain\n.\n\n\n\n```bash\n$ ps aux | grep \"[f]nord\"\n```\n\n\n\nBy putting the brackets around the letter and quotes around the string you search for the regex, which says, \"Find the character 'f' followed by 'nord'.\"\n\n\nBut since you put the brackets in the pattern 'f' is now followed by ']', so \ngrep\n won't show up in the results list. Neato!",
    "url": "https://unix.stackexchange.com/questions/74185/how-can-i-prevent-grep-from-showing-up-in-ps-results"
  },
  {
    "question_title": "How to use watch command with a piped chain of commands/programs",
    "question_body": "I usually use \nwatch\n Linux utility to watch the output of a command repeatedly every \nn\n seconds, like in \nwatch df -h /some_volume/\n.\n\n\nBut I seem not to be able to use \nwatch\n with a piped series of command like:\n\n\n\n```bash\n$ watch ls -ltr|tail -n 1\n```\n\n\n\nIf I do that, \nwatch\n is really watching \nls -ltr\n and the output is being passed to \ntail -n 1\n which doesn't output anything.\n\n\nIf I try this:\n\n\n\n```bash\n$ watch (ls -ltr|tail -n 1)\n```\n\n\n\nI get\n\n\n\n```bash\n$ watch: syntax error near unexpected token `ls'\n```\n\n\n\nAnd any of the following fails some reason or another:\n\n\n\n```bash\n$ watch <(ls -ltr|tail -n 1)\n\n$ watch < <(ls -ltr|tail -n 1)\n\n$ watch $(ls -ltr|tail -n 1)\n\n$ watch `ls -ltr|tail -n 1)`\n```\n\n\n\nAnd finally if do this:\n\n\n\n```bash\n$ watch echo $(ls -ltr|tail -n 1)\n```\n\n\n\nI see no change in the output at the given interval because the command inside \n$()\n is run just once and the resulting output string is always printed (\"watched\") as a literal.\n\n\nSo, how do I make the \nwatch\n command work with a piped chain of commands [other that putting them inside a script]?",
    "answer": "```bash\nwatch 'command | othertool | yet-another-tool'\n```",
    "url": "https://unix.stackexchange.com/questions/318859/how-to-use-watch-command-with-a-piped-chain-of-commands-programs"
  },
  {
    "question_title": "grep returns &quot;Binary file (standard input) matches&quot; when trying to find a string pattern in file",
    "question_body": "I'm on Ubuntu and I typed \ncat .bash_history | grep git\n and it returned\n\n\n\n> Binary file (standard input) matches\n\n\n\nMy \nbash_history\n does exist and there are many lines in it that starts with \ngit\n.\n\n\nWhat caused it to display this error and how can I fix it?",
    "answer": "You can use \ngrep -a 'pattern'\n.\n\n\nfrom \nman grep\n page:\n\n\n\n> -a, --text\n> Process a binary file as if it were text; this is equivalent to the --binary-files=text option.",
    "url": "https://unix.stackexchange.com/questions/335716/grep-returns-binary-file-standard-input-matches-when-trying-to-find-a-string"
  },
  {
    "question_title": "git-upload-pack hangs indefinitely",
    "question_body": "I have the following call structure:\n\n\n\n\nJenkins runs \nfab -Huser@host set_repository_commit_hash:123abc\n.\n\n\nset_repository_commit_hash\n runs \ngit fetch\n with \npty = False\n.\n\n\nThe child process \nssh git@github.com git-upload-pack 'user/repository.git'\n never finishes.\n\n\n\n\nI've tried running \ngit fetch\n in a local clone and that succeeds, but running \nssh git@github.com git-upload-pack 'user/repository.git'\n just returns the following and hangs:\n\n\n\n```bash\n00ab84249d3bb20930c185c08848c60b71f7b28990d6 HEADmulti_ack thin-pack side-band side-band-64k ofs-delta shallow no-progress include-tag multi_ack_detailed agent=git/1.8.4\n0041cb34b1c8ca75d478df38c794fc15c5f01cc6377e refs/heads/branch_name\n004012577068adf47015001bfa0cff9386d6cdf497ce refs/heads/[...]\n003f84249d3bb20930c185c08848c60b71f7b28990d6 refs/heads/master\n[a couple more lines like the ones above, then:]\n0000\n```\n\n\n\nIs this a known SSH/Git/Fabric/Jenkins problem?\n\n\nI did \nstrace\n it, but I haven't recorded the session. I believe it was stuck on a \nread\n.\n\n\nPossibly relevant links:\n\n\n\n\nJenkins issue 14752: SCM Polling / Max # of concurrent polling = 1\nhangs github polling\n\n\nWhy would git-upload-pack (during git clone) hang?\n\n\ntortoisegit issue 1880: tortoisegit fetch hangs due to running/never-exiting tortoisegitplink\n (especially \ncomment #7\n)\n\n\nWhat is this random never-ending 'git-upload-pack' process?",
    "answer": "This problem appears to have gone away on its own, as can be expected by a rapidly evolving piece of software. Since I have not observed this issue for probably a couple years now I'd like to extend my thanks to whoever fixed it and consider this question obsolete.\n\n\nIf you are experiencing this issue with recent Git versions please consider \nasking a separate question\n, since it is likely not the exact same issue.",
    "url": "https://unix.stackexchange.com/questions/98959/git-upload-pack-hangs-indefinitely"
  },
  {
    "question_title": "How to create a gzip file without .gz file extension?",
    "question_body": "I would like to create a gzipped file that retains the original file name.  For example gzipping \"example.txt\" should output a gzipped file named \"example.txt\" rather than \"example.txt.gz.\"  Is it possible to do this elegantly with one command (not doing a subsequent \nmv\n)?",
    "answer": "This does NOT work:\n\n\n\n```bash\n# echo Hello World > example.txt\n# gzip < example.txt > example.txt # WRONG!\n# file example.txt\nexample.txt: gzip compressed data, from Unix, last modified: Thu Mar 21 19:45:29 2013\n# gunzip < example.txt\n<empty file>\n```\n\n\n\nThis is a race condition:\n\n\n\n```bash\n# echo Hello World > example.txt\n# dd if=example.txt | gzip | dd of=example.txt # still WRONG!\n# gunzip < example.txt \nHello World # may also be empty\n```\n\n\n\nThe problem is that the \n> example.txt\n (or \ndd of=example.txt\n for that matter) kills the file before the other process has the chance to read it. So there is no obvious solution, which is why you should stick to \nmv\n.\n\n\nThere are a number of ways you could cheat. You can open the file, then unlink it - the file will continue to exist until you close it - and then create a new file with the same name and write the gzipped data to that. However I do not know an obvious way to coerce bash to use that, and even if I did, my answer would still be:\n\n\nDon't even do it.\n\n\nIf \ngzip\n fails for any reason, or any problem occurs, like you running out of space while gzipping (because other processes are writing, or gzip result is larger than the input - which happens for random data - etc.), you just lost your file. Congratulations!\n\n\nCreate a separate file and \nmv\n on success. That's the simplest, easy to understand, and most reliable method you will ever find.",
    "url": "https://unix.stackexchange.com/questions/68722/how-to-create-a-gzip-file-without-gz-file-extension"
  },
  {
    "question_title": "How to view the output of a running process in another bash session?",
    "question_body": "I have left a script running on a remote machine from when I was locally working at it. I can connect over SSH to the machine as the same user and see the script running in \nps\n. \n\n\n\n```bash\n$ ps aux | grep ipcheck\nmyuser  18386  0.0  0.0  18460  3476 pts/0    S+   Dec14   1:11 /bin/bash ./ipchecker.sh\n```\n\n\n\nIt is simply outputting to stdout on a local session (I ran \n./ipchecker.sh\n form a local terminal window, no redirection, no use of \nscreen\n etc).\n\n\nIs there anyway from an SSH session I can view the output of this running command (without stopping it)?\n\n\nSo far the best I have found is to use \nstrace -p 18386\n but I get hordes of text flying up the screen, its far too detailed. I can stop \nstrace\n and then sift through the output and find the text bring printed to stdout but its very long and confusing, and obviously whilst it's stopped I might miss something. I would like to find a way to see the script output live as if I was working locally.\n\n\nCan anyone improve on this? The obvious answer is to restart the script with redirection or in a \nscreen\n session etc, this isn't a mission critical script so I could do that. Rather though, I see this as a fun learning exercise.",
    "answer": "You can access the output via the \nproc\n filesystem.\n\n\n\n```bash\ntail -f /proc/<pid>/fd/1\n```\n\n\n\n1\n = stdout, \n2\n = stderr\n\n\n(or like @jmhostalet says: \ncat /proc/<pid>/fd/1\n if tail doesn't work)",
    "url": "https://unix.stackexchange.com/questions/58550/how-to-view-the-output-of-a-running-process-in-another-bash-session"
  },
  {
    "question_title": "Using grep with the --exclude-dir flag to exclude multiple directories",
    "question_body": "I am searching through a Ruby on Rails application for a word using \ngrep\n on OSX, and I would like to exclude directories that match a certain pattern. \n\n\nI am using the following command:\n\n\n\n```bash\ngrep -inRw -E 'direct' . --exclude-dir -E 'git|log|asset'\n```\n\n\n\nThis command is not doing what I thought it would do. Here is how I \nthought\n it would work:\n\n\n\n\ni\n - case insensitive search\n\n\nn\n - print line number in which pattern is found\n\n\nR\n - search recursively\n\n\nw\n - I only want whole words - i.e., match \"direct\" but not \"directory\"\n\n\n-E\n - use extended regular expression\n\n\n'direct'\n - the regular expression I want to match\n\n\n.\n - search in the current directory\n\n\n--exclude-dir -E 'git|log|asset'\n - exclude directories that match git or log or asset. \n\n\n\n\nIn terms of the exclude directories, the command still ends up searching in the \n'./git'\n and \n'./log'\n directories, as well as in \n'./app/assets'\n\n\nI'm obviously lacking a fundamental piece of knowledge, but I do not know what it is.",
    "answer": "It's pattern as in \nglobs\n not pattern as in \nregex\n. Per the \ninfo\n page\n:\n\n\n\n> --exclude-dir=GLOB\n> Skip any command-line directory with a name suffix that matches the\n> pattern GLOB. When searching recursively, skip any subdirectory whose\n> base name matches GLOB. Ignore any redundant trailing slashes in GLOB.\n\n\n\nSo, \nyou either use the switch multiple times\n or, if your shell supports brace expansion, you could golf it shorter and have the shell expand the list of patterns e.g.:\n\n\n\n```bash\ngrep -inRw -E 'direct' . --exclude-dir={git,log,assets}\n```\n\n\n\nto exclude directories named \ngit\n, \nlog\n and \nassets\n or e.g.\n\n\n\n```bash\ngrep -inRw -E 'direct' . --exclude-dir={\\*git,asset\\*}\n```\n\n\n\nto exclude directory names ending in \ngit\n or starting with \nasset\n.\n\n\nNote that the shell expands the list only if there are \nat least two dirnames/globs inside braces\n.",
    "url": "https://unix.stackexchange.com/questions/282648/using-grep-with-the-exclude-dir-flag-to-exclude-multiple-directories"
  },
  {
    "question_title": "Fastest way combine many files into one (tar czf is too slow)",
    "question_body": "Currently I'm running \ntar czf\n to combine backup files. The files are in a specific directory.\n\n\nBut the number of files is growing. Using \ntzr czf\n takes too much time (more than 20 minutes and counting).\n\n\nI need to combine the files more quickly and in a scalable fashion.\n\n\nI've found \ngenisoimage\n, \nreadom\n and \nmkisofs\n. But I don't know which is fastest and what the limitations are for each of them.",
    "answer": "You should check if most of your time are being spent on CPU or in I/O. Either way, there are ways to improve it:\n\n\nA: don't compress\n\n\nYou didn't mention \"compression\" in your list of requirements so try dropping the \"z\" from your arguments list: \ntar cf\n. This might be speed up things a bit.\n\n\nThere are other techniques to speed-up the process, like using \"-N \" to skip files you already backed up before.\n\n\nB: backup the whole partition with dd\n\n\nAlternatively, if you're backing up an entire partition, take a copy of the whole disk image instead. This would save processing and a \nlot\n of disk head seek time. \ntar\n and any other program working at a higher level have a  overhead of having to read and process directory entries and inodes to find where the file content is and to do more head \ndisk seeks\n, reading each file from a different place from the disk.\n\n\nTo backup the underlying data much faster, use:\n\n\ndd bs=16M if=/dev/sda1 of=/another/filesystem\n\n\n(This assumes you're not using RAID, which may change things a bit)",
    "url": "https://unix.stackexchange.com/questions/19129/fastest-way-combine-many-files-into-one-tar-czf-is-too-slow"
  },
  {
    "question_title": "What is the meaning of IFS=$&#39;\\n&#39; in bash scripting?",
    "question_body": "At the beginning of a bash shell script is the following line: \n\n\n\n```bash\nIFS=$'\\n'\n```\n\n\n\nWhat is the meaning behind this collection of symbols?",
    "answer": "IFS\n stands for \"internal field separator\". It is used by the shell to determine how to do word splitting, i. e. how to recognize word boundaries.\n\n\nTry this in a shell like bash (other shells may handle this differently, for example zsh):\n\n\n\n```bash\nmystring=\"foo:bar baz rab\"\nfor word in $mystring; do\n  echo \"Word: $word\"\ndone\n```\n\n\n\nThe default value for \nIFS\n consists of whitespace characters (to be precise: space, tab and newline). Each character can be a word boundary. So, with the default value of \nIFS\n, the loop above will print:\n\n\n\n```bash\nWord: foo:bar\nWord: baz\nWord: rab\n```\n\n\n\nIn other words, the shell thinks that whitespace is a word boundary.\n\n\nNow, try setting \nIFS=:\n before executing the loop. This time, the result is:\n\n\n\n```bash\nWord: foo\nWord: bar baz rab\n```\n\n\n\nNow, the shell splits \nmystring\n into words as well -- but now, it only treats a colon as the word boundary.\n\n\nThe first character of \nIFS\n is special: It is used to delimit words in the output when using the special \n$*\n variable (example taken from the \nAdvanced Bash Scripting Guide\n, where you can also find more information on special variables like that one):\n\n\n\n```bash\n$ bash -c 'set w x y z; IFS=\":-;\"; echo \"$*\"'\nw:x:y:z\n```\n\n\n\nCompare to:\n\n\n\n```bash\n$ bash -c 'set w x y z; IFS=\"-:;\"; echo \"$*\"'\nw-x-y-z\n```\n\n\n\nNote that in both examples, the shell will still treat all of the characters \n:\n, \n-\n and \n;\n as word boundaries. The only thing that changes is the behaviour of \n$*\n.\n\n\nAnother important thing to know is how so-called \"IFS whitespace\" \nis treated\n. Basically, as soon as \nIFS\n includes whitespace characters, leading and trailing whitespace is stripped from the string to be split before processing it and a \nsequence\n of consecutive whitespace characters delimits fields as well. However, this only applies to those whitespace characters which are actually present in \nIFS\n.\n\n\nFor example, let's look at the string \n\"a:b:: c  d \"\n (trailing space and two space characters between \nc\n and \nd\n).\n\n\n\n\nWith \nIFS=:\n it would be split into four fields: \n\"a\"\n, \n\"b\"\n, \n\"\"\n (empty string) and \n\" c  d \"\n (again, two spaces between \nc\n and \nd\n). Note the leading and trailing whitespace in the last field.\n\n\nWith \nIFS=' :'\n, it would be split into five fields: \n\"a\"\n, \n\"b\"\n, \n\"\"\n (empty string), \n\"c\"\n and \n\"d\"\n. No leading and trailing whitespace anywhere.\n\n\n\n\nNote how multiple, consecutive whitespace characters delimit two fields in the second example, while multiple, consecutive colons don't (since they are not whitespace characters).\n\n\nAs for \nIFS=$'\\n'\n, that is a \nksh93\n syntax also supported by \nbash\n, \nzsh\n, \nmksh\n and FreeBSD \nsh\n (with variations between all shells). Quoting the bash manpage:\n\n\n\n> Words of the form $'string' are treated specially.  The word expands to \"string\", with backslash-escaped characters replaced as specified by the ANSI C standard.\n\n\n\n\\n\n is the escape sequence for a newline, so \nIFS\n ends up being set to a single newline character.",
    "url": "https://unix.stackexchange.com/questions/184863/what-is-the-meaning-of-ifs-n-in-bash-scripting"
  },
  {
    "question_title": "How to solve “tar: invalid magic” error on Linux Alpine",
    "question_body": "I'm installing sqlite on Alpine Linux. I download \nsqlite-autoconf-3130000.tar.gz\n but \ntar\n could not open it. I tried this \nanswer\n but it's not working. \ntar\n gives this message:\n\n\n\n```bash\ntar: invalid magic\ntar: short read\n```\n\n\n\nI wrote these commands.\n\n\n\n```bash\nwget https://www.sqlite.org/2015/sqlite-autoconf-3090100.tar.gz\ntar -zxvf sqlite-autoconf-3090100.tar.gz\n```",
    "answer": "Try to install the tar package (apk add tar). Busybox tar (default) doesn't support all features.",
    "url": "https://unix.stackexchange.com/questions/302192/how-to-solve-tar-invalid-magic-error-on-linux-alpine"
  },
  {
    "question_title": "How did `git pull` eat my homework?",
    "question_body": "I feel like a kid in the principal's office explaining that the dog ate my homework the night before it was due, but I'm staring some crazy data loss bug in the face and I can't figure out how it happened. I would like to know how git could eat my repository whole! I've put git through the wringer many times and it's never blinked. I've used it to split a 20 Gig Subversion repo into 27 git repos and filter-branched the foo out of them to untangle the mess and it's never lost a byte on me. The reflog is always there to fall back on. This time the carpet is gone!\n\n\nFrom my perspective, all I did is run \ngit pull\n and it nuked my entire local repository. I don't mean it \"messed up the checked out version\" or \"the branch I was on\" or anything like that. I mean \nthe entire thing is gone\n.\n\n\nHere is a screen-shot of my terminal at the incident:\n\n\n\n\nLet me walk you through that. My command prompt includes data about the current git repo (using prezto's vcs_info implementation) so you can see when the git repo disappeared. The first command is normal enough:\n\n\n\n```bash\n» caleb » jaguar » ~/p/w/incil.info » ◼  zend ★ »\n❯❯❯ git co master\nSwitched to branch 'master'\nYour branch is up-to-date with 'origin/master'.\n```\n\n\n\nThere you can see I was on the 'zend' branch, and checked out master. So far so good. You'll see in the prompt before my next command that it successfully switched branches:\n\n\n\n```bash\n» caleb » jaguar » ~/p/w/incil.info » ◼  master ★ »\n❯❯❯ git pull\nremote: Counting objects: 37, done.\nremote: Compressing objects: 100% (37/37), done.\nremote: Total 37 (delta 25), reused 0 (delta 0)\nUnpacking objects: 100% (37/37), done.\nFrom gitlab.alerque.com:ipk/incil.info\n + 7412a21...eca4d26 master     -> origin/master  (forced update)\n   f03fa5d..c8ea00b  devel      -> origin/devel\n + 2af282c...009b8ec verse-spinner -> origin/verse-spinner  (forced update)\nFirst, rewinding head to replay your work on top of it...\n>>> elapsed time 11s\n```\n\n\n\nAnd just like that it's gone. The elapsed time marker outputs before the next prompt if more than 10 seconds have elapsed. Git did not give any output beyond the notice that it was rewinding to replay. No indication that it finished.\n\n\nThe next prompt includes no data about what branch we are on or the state of git.\n\n\nNot noticing it had failed I obliviously tried to run another git command only to be told I wasn't in a git repo. Note the PWD has not changed:\n\n\n\n```bash\n» caleb » jaguar » ~/p/w/incil.info »\n❯❯❯ git fetch --all\nfatal: Not a git repository (or any parent up to mount point /home)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n```\n\n\n\nAfter this a look around showed that I was in a completely empty directory. Nothing. No '.git' directory, nothing. Empty.\n\n\nMy local git is at version 2.0.2. Here are a couple tidbits from my git config that might be relevant to making out what happened:\n\n\n\n```bash\n[branch]\n        autosetuprebase = always\n        rebase = preserve\n[pull]\n        rebase = true\n[rebase]\n        autosquash = true\n        autostash = true\n[alias]\n        co = checkout\n```\n\n\n\nFor example I have \ngit pull\n set to always do a rebase instead of a merge, so that part of the output above is normal.\n\n\nI can recover the data. I don't think there were any git objects other than some unimportant stashes that hadn't been pushed to other repos, but \nI'd like to know what happened\n.\n\n\nI have checked for:\n\n\n\n\nMessages in dmesg or the systemd journal. Nothing even remotely relevant.\n\n\nThere is no indication of drive or file system failure (LVM + LUKS + EXT4 all look normal). There is nothing in lost+found.\n\n\nI didn't run anything else. There is nothing in the history I'm not showing above, and no other terminals were used during this time. There are no \nrm\n commands floating around that might have executed in the wrong CWD, etc.\n\n\nPoking at another git repo in another directory shows no apparent abnormality executing \ngit pull\ns.\n\n\n\n\nWhat else should I be looking for here?",
    "answer": "Yes, \ngit\n ate my homework. All of it.\n\n\nI made a \ndd\n image of this disk after the incident and messed around with it later. Reconstructing the series of events from system logs, I deduce what happened was something like this:\n\n\n\n\nA system update command (\npacman -Syu\n) had been issued days before this incident.\n\n\nAn extended network outage meant that it was left re-trying to download packages. Frustrated at the lack of internet, I'd put the system to sleep and gone to bed.\n\n\nDays later the system was woken up and it started finding and downloading packages again.\n\n\nPackage download finished sometime just before I happened to be messing around with this repository.\n\n\nThe system \nglibc\n installation got updated after the \ngit checkout\n and before the \ngit pull\n.\n\n\nThe \ngit\n binary got replaced after the \ngit pull\n started and before it finished.\n\n\nAnd on the seventh day, \ngit\n rested from all its labors. And deleted the world so everybody else had to rest too.\n\n\n\n\nI don't know \nexactly\n what race condition occurred that made this happen, but swapping out binaries in the middle of an operation is certainly not nice nor a testable / repeatable condition. Usually a copy of a running binary is stored in memory, but \ngit\n is weird and something about the way it re-spawns versions of itself I'm sure led to this mess. Obviously it should have died rather than destroying everything, but that's what happened.",
    "url": "https://unix.stackexchange.com/questions/146299/how-did-git-pull-eat-my-homework"
  },
  {
    "question_title": "Does gzip add integrity/crc check to a .tar?",
    "question_body": "I run commands:\n\n\n\n```bash\ntar -cf myArchive.tar myDirectory/\ngzip myArchive.tar\n```\n\n\n\nthen I copy the file over a lot of unreliable mediums, and later I unpack it using:\n\n\n\n```bash\ntar -xzf myArchive.tar.gz\n```\n\n\n\nThe fact that I compressed the tar-ball, will that in any way guarantee the integrity, or at least a CRC of the unpacked content?",
    "answer": "tar\n itself does not write down a checksum for later comparsion. If you \ngzip\n the \ntar\n archive you can have that functionality.\n\n\ntar\n uses \ncompress\n. If you use the \n-Z\n flag while creating the archive \ntar\n will use the \ncompress\n program when reading or writing the archive. From the \ngzip\n manpage:\n\n\n\n> The standard compress format was not designed to allow consistency\n> checks.\n\n\n\nBut, you can use the \n-z\n parameter. Then \ntar\n reads and writes the archive through \ngzip\n. And \ngzip\n writes a crc checksum. To display that checksum use that command:\n\n\n\n```bash\n$ gzip -lv archive.tar.gz\nmethod  crc     date  time           compressed        uncompressed  ratio uncompressed_name\ndefla 3f641c33 Sep 25 14:01               24270              122880  80.3% archive.tar\n```\n\n\n\nFrom the \ngzip\n manpage:\n\n\n\n> When using the first two formats (gzip or zip is meant), gunzip checks\n> a 32 bit CRC.",
    "url": "https://unix.stackexchange.com/questions/160172/does-gzip-add-integrity-crc-check-to-a-tar"
  },
  {
    "question_title": "Reading grep patterns from a file",
    "question_body": "I have a couple of big text files and in the file \nUNIQS.txt\n I have a list of strings to \ngrep\n from another file. The code I use is\n\n\n\n```bash\ngrep -f UNIQS.txt EEP_VSL.uniqs.sam > UNIQ_templates.sam\n```\n\n\n\nwhich does nothing - the file generated is empty. But when I do\n\n\n\n```bash\ngrep -F -f UNIQS.txt EEP_VSL.uniqs.sam > UNIQ_templates.sam\n```\n\n\n\nit works correctly. This confuses me because I didn't think \ngrep\n would interpret the entries in \nUNIQS.txt\n as regexp patterns without quotes and slashes and so on being in the file (which there aren't). Is it the case in general that if you are getting the patterns from a file then it will automatically think that they are regexp patterns?\n\n\nEdit:\n In the \nUNIQS.txt\n file, there are newline separated strings of the form\n\n\n\n```bash\nHWI-ST365:215:D0GH0ACXX:2:1101:10034:186783\n```\n\n\n\n(called template names) and the file \nEEP_VSL...\n tab separated columns, with about 14 columns and the first column is the template name, so basically I want to extract the line corresponding to each template in the file.",
    "answer": "The \n-f\n option specifies a file where grep reads patterns. That's just like passing patterns on the command line (with the \n-e\n option if there's more than one), except that when you're calling from a shell you may need to quote the pattern to protect special characters in it from being expanded by the shell.\n\n\nThe argument \n-E\n or \n-F\n or \n-P\n, if any, tells grep which syntax the patterns are written in. With no argument, grep expects \nbasic regular expressions\n; with \n-E\n, grep expects \nextended regular expressions\n; with \n-P\n (if supported), grep expects \nPerl regular expressions\n; and with \n-F\n, grep expects literal strings. Whether the patterns come from the command line or from a file doesn't matter.\n\n\nNote that the strings are substrings: if you pass \na+b\n as a pattern then a line containing \na+b+c\n is matched. If you want to search for lines containing exactly one of the supplied strings and no more, then pass the \n-x\n option.",
    "url": "https://unix.stackexchange.com/questions/83260/reading-grep-patterns-from-a-file"
  },
  {
    "question_title": "How portable is a gzip file over 4 GB in size?",
    "question_body": "To backup a snapshot of my work, I run a command like \ntar -czf work.tgz work\n to create a gzipped tar file, which I can then drop in cloud storage. However, I have just noticed that gzip has a 4 GB size limit, and my \nwork.tgz\n file is more than 4 GB.\n\n\nDespite that, if I create a gzip tar file on my current computer (running Mac OS X 10.15.4, gzip version is called Apple gzip 287.100.2) I can successfully retrieve it. So gunzip works on a >4GB in my particular case. But I want to be able to create and read these large gzip files on either Mac OS X or Linux, and possibly other systems in the future.\n\n\nMy question is: will I be able to untar/gunzip large files anywhere? In other words, how portable is a gzip file which is more than 4 GB in size? Does it matter if I create it on Mac OS, Linux, or something else?\n\n\nA bit of online reading suggests gzip will successfully gzip/gunzip a larger file, but will not correctly record the uncompressed size, because the size is stored as a 32 bit integer. Is that all the limit is?",
    "answer": "> I have just noticed that gzip has a 4 GB size limit\n\n\n\nMore accurately, the \ngzip\n format can’t correctly store uncompressed file \nsizes\n over 4GiB; it stores the lower 32 bits of the uncompressed size, and \ngzip -l\n misleadingly presents that as the size of the original data. The result is that, up to \ngzip\n 1.11 included, \ngzip -l\n won’t show the right size for any compressed file whose original size is over 4GiB.\n\n\nApart from that, there is no limit due to \ngzip\n itself, and \ngzip\nped files over 4GiB are portable. The format is specified by \nRFC 1952\n and support for it is widely available.\n\n\nThe confusion over the information presented by \ngzip -l\n \nhas been fixed in \ngzip\n 1.12\n; \ngzip -l\n now decompresses the data to determine the real size of the original data, instead of showing the stored size.\n\n\n\n> Will I be able to untar/gunzip large files anywhere?\n\n\n\nAnywhere that can handle large files, and where spec-compliant implementations of \ntar\n and \ngunzip\n are available.\n\n\n\n> In other words, how portable is a gzip file which is more than 4 GB in size?\n\n\n\nThe \ngzip\n format itself is portable, and \ngzip\n files are also portable, regardless of the size of the data they contain.\n\n\n\n> Does it matter if I create it on Mac OS, Linux, or something else?\n\n\n\nNo, a \ngzip\n file created on any platform can be uncompressed on any other platform with the required capabilities (in particular, the ability to store large files, in the context of this question).\n\n\nSee also \nCompression Utility Max Files Size Limit | Unix/Linux\n.",
    "url": "https://unix.stackexchange.com/questions/612905/how-portable-is-a-gzip-file-over-4-gb-in-size"
  },
  {
    "question_title": "Git pull in a Linux environment changes the owner of the files",
    "question_body": "Why will every time I execute a \ngit pull\n on my live server, the owner of the files be changed? I keep on manually doing a \nchown john:john index.php\n.\n\n\nWhat do I need to change so that, when I do a \ngit pull\n, the owner of the files will be automatically be or stay \njohn\n?",
    "answer": "When you use \ngit pull\n (which is equivalent to \ngit fetch; git merge\n), git create new files and does not care about previous ownership (git does not store that information).\n\n\nThere is different solution to solve this. The easier is probably to add a \npost-update\n hooks (in your \n.git/hooks\n directory) to automatically call \nchown john:john . -R\n after merge/pull  (\nsee that SO solution for an example\n).\n\n\nAlternate solutions are:\n\n\n\n\nRun \ngit pull\n as the john user instead of root (that will require write permissions to john for .git directory).\n\n\nCreate a hook to make the deployment, which will either use john as user, either do the chown after update (so you will have to do \ngit pull\n inside the \npost-receive\n hook of the bare repo).\n\n\n\n\nIn linux, it might be another solution with \nsetuid\n / \nsetgid",
    "url": "https://unix.stackexchange.com/questions/154859/git-pull-in-a-linux-environment-changes-the-owner-of-the-files"
  },
  {
    "question_title": "Grep from the end of a file to the beginning",
    "question_body": "I have a file with about 30.000.000 lines (Radius Accounting) and I need to find  the last match of a given pattern.\n\n\nThe command:\n\n\n\n```bash\ntac accounting.log | grep $pattern\n```\n\n\n\ngives what I need, but it's too slow because the OS has to first read the whole file and then send to the pipe.\n\n\nSo, I need something fast that can read the file from the last line to the first.",
    "answer": "tac\n only helps if you also use \ngrep -m 1\n (assuming GNU \ngrep\n) to have \ngrep\n stop after the first match:\n\n\n\n```bash\ntac accounting.log | grep -m 1 foo\n```\n\n\n\nFrom \nman grep\n:\n\n\n\n```bash\n-m NUM, --max-count=NUM\n          Stop reading a file after NUM matching lines.\n```\n\n\n\nIn the example in your question, both \ntac\n and \ngrep\n need to process the entire file so using \ntac\n is kind of pointless.\n\n\nIf you can't use \ngrep -m\n, just parse the output of \ngrep\n to get the last match:\n\n\n\n```bash\ngrep foo accounting.log | tail -n 1\n```\n\n\n\n\n\nAnother approach would be to use Perl or any other scripting language. For example (where \n$pattern=foo\n):\n\n\n\n```bash\nperl -ne '$l=$_ if /foo/; END{print $l}' file\n```\n\n\n\nor\n\n\n\n```bash\nawk '/foo/{k=$0}END{print k}' file\n```",
    "url": "https://unix.stackexchange.com/questions/112159/grep-from-the-end-of-a-file-to-the-beginning"
  },
  {
    "question_title": "Is there a way to tar multiple files in a directory (Linux/Unix)?",
    "question_body": "Is there a fairly simple way to \ntar\n several (specific) files in a directory? For example let's say the following is a directory:\n\n\n\n```bash\n-rw-r--r-- 1 allend bin     98 Jul 20 15:50 scriptlog.log\n-rw-r--r-- 1 allend bin  19533 Jul 29 21:47 serveralert.log\n-rwxr--r-- 1 allend bin   1625 Jul 29 21:47 orion\n-rw-r--r-- 1 allend bin  24064 Jul 29 21:49 orion_files.tar\n-rwxr--r-- 1 allend bin    156 Aug  4 21:22 htmltest\n-rw-r--r-- 1 allend bin    131 Aug  4 21:23 page.html\n```\n\n\n\nWhat if I only want to \ntar\n the files \nserveralert.log\n and \npage.html\n?",
    "answer": "Is there something wrong with listing the files you'd like to add to the .tar file?\n\n\n\n```bash\n$ tar cvf some.tar file1 file2 file3\n```\n\n\n\nExample\n\n\n\n```bash\n$ tar cvf some.tar serveralert.log page.html\nserveralert.log\npage.html\n```",
    "url": "https://unix.stackexchange.com/questions/85517/is-there-a-way-to-tar-multiple-files-in-a-directory-linux-unix"
  },
  {
    "question_title": "Create target directory when extracting tarball",
    "question_body": "Is it possible to create a target directory, similar to \nmkdir -p\n, where I can define a non-existent target directory within my tar command, and tar will create the directory for me?\n\n\nI know I can redirect the output to a directory using \ntar -C /target/dir\n, but this doesn't work if the target directory is non-existent.",
    "answer": "```bash\nmkdir -p /target/dir && tar -C /target/dir\n```",
    "url": "https://unix.stackexchange.com/questions/25311/create-target-directory-when-extracting-tarball"
  },
  {
    "question_title": "What is the .gitignore pattern equivalent of the regex (Big|Small)(State|City)-[0-9]*\\.csv",
    "question_body": "I have a regex I stuck in my \n.gitignore\n similar to:\n\n\n\n```bash\n(Big|Small)(State|City)-[0-9]*\\.csv\n```\n\n\n\nIt didn't work so I tested it against \nRegexLab.NET\n. \n\n\nI then found the \ngitignore\n man page which led me to learn that gitignore doesn't use regexes, but rather \nfnmatch(3)\n. \n\n\nHowever, fnmatch it doesn't seem to have an equivalent of the capture groups. Is this feasible or do I need to break this into three lines?",
    "answer": "There's no way to express this regular expression with the patterns that gitignore supports. The problem is not the lack of capture groups (in fact, you are not using capture groups as such), the problem is the lack of a \n|\n operator. You need to break this into four lines.\n\n\n\n```bash\nBigState-[0-9]*.csv\nSmallState-[0-9]*.csv\nBigCity-[0-9]*.csv\nSmallCity-[0-9]*.csv\n```\n\n\n\nNote that the patterns match e.g. \nBigState-4foo.csv\n, since \n*\n matches any sequence of characters. You can't do better than this with glob patterns, unless you're willing to match only a fixed number of digits.",
    "url": "https://unix.stackexchange.com/questions/31790/what-is-the-gitignore-pattern-equivalent-of-the-regex-bigsmallstatecity"
  },
  {
    "question_title": "tar: Removing leading `/&#39; from member names",
    "question_body": "```bash\nroot@server # tar fcz bkup.tar.gz /home/foo/\ntar: Removing leading `/' from member names\n```\n\n\n\nHow can I solve this problem and keep the \n/\n on file names ?",
    "answer": "If you want to get rid of \"Removing leading `/' from member names\" being printed to STDERR, but still want to leave off those leading slashes as tar wisely does by default, I saw an excellent solution \nhere\n by commenter timsoft.\n\n\nThe solution involves using -C option to change directory to the root (/), then specifying the file tree to archive \nwithout\n a leading slash, because now you only need a relative path.  This does the same thing as a normal tar create command, but no stripping is needed:\n\n\n\n```bash\ntar fcz bkup.tar.gz -C / home/foo/\n```\n\n\n\nor\n\n\n\n```bash\ncd / && tar fcz $HOME/bkup.tar.gz home/foo/*\n```",
    "url": "https://unix.stackexchange.com/questions/59243/tar-removing-leading-from-member-names"
  },
  {
    "question_title": "Git submodule shows new commits, submodule status says nothing to commit",
    "question_body": "In a git repository, I have set up my .gitmodules file to reference a github repository:\n\n\n\n```bash\n[submodule \"src/repo\"]\n    path = src/repo\n    url = repourl\n```\n\n\n\nwhen I 'git status' on this repo, it shows:\n\n\n\n```bash\nOn branch master\nYour branch is up-to-date with 'origin/master'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\nmodified:   src/repo (new commits)\n```\n\n\n\nIf I cd into src/repo and git status on repo, it says that there is nothing to commit.\n\n\nWhy is my top-level git repo complaining?",
    "answer": "I just ran into this same class of problem, and I was able to use the solution offered by \n@AugustinAmenabar\n in the comments section of the accepted answer. My setup was a bit more complex, so I added the \n--recursive\n flag to bring all dependencies up to date.\n\n\ngit submodule update --recursive src/repo",
    "url": "https://unix.stackexchange.com/questions/214879/git-submodule-shows-new-commits-submodule-status-says-nothing-to-commit"
  },
  {
    "question_title": "How to find the first tag that contains a git commit?",
    "question_body": "I have the SHA ID of a commit that I am interested in and would like to know how to find the first tag that contains it.",
    "answer": "As previously stated, this can be done with \ngit describe\n.  In your particular case, however, you may find it more convenient to run  \ngit name-rev --tags --name-only <SHA>\n, which outputs exactly what you want.  See \ngit-name-rev(1)\n.",
    "url": "https://unix.stackexchange.com/questions/47659/how-to-find-the-first-tag-that-contains-a-git-commit"
  },
  {
    "question_title": "How to tell gzip to keep original file?",
    "question_body": "I would like to compress a text file using gzip command line tool while \nkeeping\n the original file. By default running the following command\n\n\n\n```bash\ngzip file.txt\n```\n\n\n\nresults in modifying this file and renaming it \nfile.txt.gz\n. instead of this behavior I would like to have this new compressed file in \naddition\n to the existing one \nfile.txt\n. For now I am using the following command to do that\n\n\n\n```bash\ngzip -c file.txt > file.txt.gz\n```\n\n\n\nIt works but I am wondering why there is no easier solution to do such a common task ? Maybe I missed the option doing that ?",
    "answer": "For GNU \ngzip\n 1.6 or above, FreeBSD and derivatives or recent versions of NetBSD, see \ndon_cristi's answer\n.\n\n\nWith any version, you can use shell redirections as in:\n\n\n\n```bash\ngzip < file.txt > file.txt.gz\n```\n\n\n\nWhen not given any argument, \ngzip\n reads its standard input, compresses it and writes the compressed version to its standard output. As a bonus, when using shell redirections, you don't have to worry about files called \n\"--help\"\n or \n\"-\"\n (that latter one still being a problem for \ngzip -c --\n).\n\n\nAnother benefit over \ngzip -c file.txt > file.txt.gz\n is that if \nfile.txt\n can't be opened, the command will fail without creating an empty \nfile.txt.gz\n (or overwriting an existing \nfile.txt.gz\n) and without running \ngzip\n at all.\n\n\nA significant difference compared to \ngzip -k\n though is that there will be no attempt at copying the \nfile.txt\n's metadata (ownership, permissions, modification time, name of uncompressed file) to \nfile.txt.gz\n.\n\n\nAlso if \nfile.txt.gz\n already existed, it will silently override it unless you have turned the \nnoclobber\n option on in your shell (with \nset -o noclobber\n for instance in POSIX shells).",
    "url": "https://unix.stackexchange.com/questions/46786/how-to-tell-gzip-to-keep-original-file"
  },
  {
    "question_title": "How to determine where an environment variable came from?",
    "question_body": "I have a Linux instance that I set up some time ago.  When I fire it up and log in as \nroot\n there are some environment variables that I set up but I can't remember or find where they came from.  \n\n\n\n\nI've checked \n~/.bash_profile\n, \n/etc/.bash_rc\n, and all the startup\nscripts. \n\n\nI've run \nfind\n and \ngrep\n to no avail.\n\n\n\n\nI feel like I must be forgetting to look in some place obvious. Is there a trick for figuring this out?",
    "answer": "If \nzsh\n is your login shell:\n\n\n\n```bash\nzsh -xl\n```\n\n\n\nWith \nbash\n:\n\n\n\n```bash\nPS4='+$BASH_SOURCE> ' BASH_XTRACEFD=7 bash -xl 7>&2\n```\n\n\n\nThat will simulate a login shell and show everything that is done (except in areas where stderr is redirected with \nzsh\n) along with the name of the file currently being interpreted.\n\n\nSo all you need to do is look for the name of your environment variable in that output. (you can use the \nscript\n command to help you store the whole shell session output, or for the \nbash\n approach, use \n7> file.log\n instead of \n7>&2\n to store the \nxtrace\n output to \nfile.log\n instead of on the terminal).\n\n\nIf your variable is not in there, then probably the shell inherited it on startup, so it was set before like in PAM configuration, in \n~/.ssh/environment\n, or things read upon your X11 session startup (\n~/.xinitrc\n, \n~/.xsession\n) or set upon the service definition that started your login manager or even earlier in some boot script. Then a \nfind /etc -type f -exec grep -Fw THE_VAR {} +\n may help.",
    "url": "https://unix.stackexchange.com/questions/813/how-to-determine-where-an-environment-variable-came-from"
  },
  {
    "question_title": "Have tree hide gitignored files",
    "question_body": "Is there a way to make \ntree\n not show files that are ignored in \n.gitignore\n?",
    "answer": "Another way is possible if you're using \ntree 1.8.0\n\nsince it supports the \n--fromfile\n flag:\n\n\n\n> --fromfile Reads a directory listing from a file\n> rather than the file-system.\n> Paths provided on the command\n> line are files to read from rather than directories to search.\n> The dot (.)  directory  indicates  that  tree\n> should read paths from standard input.\n\n\n\nWe could use \ngit ls-tree\n to get all non-git-ignored files in a project, and pipe the output to \ntree\n.\n\n\nAssuming we have a git repository, where \nignored\n files are ignored in \n.gitignore\n:\n\n\n\n```bash\ngit_repo\n├── .gitignore\n├── bar\n│   ├── b.txt\n│   └── ignored\n├── foo\n│   ├── a.txt\n│   └── ignored\n└── ignored\n```\n\n\n\nThe following command:\n\n\n\n```bash\ngit ls-tree -r --name-only HEAD | tree --fromfile\n```\n\n\n\nGives:\n\n\n\n```bash\n.\n├── .gitignore\n├── bar\n│   └── b.txt\n└── foo\n    └── a.txt\n\n2 directories, 3 files\n```\n\n\n\nOr if you need a specific path:\n\n\n\n```bash\ngit ls-tree -r --name-only HEAD foo | tree --fromfile\n```\n\n\n\nGives:\n\n\n\n```bash\n.\n└── a.txt\n\n0 directories, 1 file\n```\n\n\n\nCaveats\n\n\n\n\nBeware\n that changes such as deleted or renamed files that haven't already been committed can cause \ngit ls-tree\n to appear out of sync.",
    "url": "https://unix.stackexchange.com/questions/291282/have-tree-hide-gitignored-files"
  },
  {
    "question_title": "Is there a way to modify a file in-place?",
    "question_body": "I have a fairly large file (35Gb), and I would like to filter this file in situ (i.e. I don't have enough disk space for another file), specifically I want to grep and ignore some patterns — is there a way to do this without using another file?\n\n\nLet's say I want to filter out all the lines containing \nfoo:\n for example...",
    "answer": "At the system call level this should be possible. A program can open your target file for writing without truncating it and start writing what it reads from stdin. When reading EOF, the output file can be truncated.\n\n\nSince you are filtering lines from the input, the output file write position should always be less than the read position. This means you should not corrupt your input with the new output.\n\n\nHowever, finding a program that does this is the problem. \ndd(1)\n has the option \nconv=notrunc\n that does not truncate the output file on open, but it also does not truncate at the end, leaving the original file contents after the grep contents (with a command like \ngrep pattern bigfile | dd of=bigfile conv=notrunc\n)\n\n\nSince it is very simple from a system call perspective, I wrote a small program and tested it on a small (1MiB) full loopback filesystem. It did what you wanted, but you really want to test this with some other files first. It's always going to be risky overwriting a file.\n\n\noverwrite.c\n\n\n\n```bash\n/* This code is placed in the public domain by camh */\n\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <errno.h>\n\nint main(int argc, char **argv)\n{\n        int outfd;\n        char buf[1024];\n        int nread;\n        off_t file_length;\n\n        if (argc != 2) {\n                fprintf(stderr, \"usage: %s <output_file>\\n\", argv[0]);\n                exit(1);\n        }\n        if ((outfd = open(argv[1], O_WRONLY)) == -1) {\n                perror(\"Could not open output file\");\n                exit(2);\n        }\n        while ((nread = read(0, buf, sizeof(buf))) > 0) {\n                if (write(outfd, buf, nread) == -1) {\n                        perror(\"Could not write to output file\");\n                        exit(4);\n                }\n        }\n        if (nread == -1) {\n                perror(\"Could not read from stdin\");\n                exit(3);\n        }\n        if ((file_length = lseek(outfd, 0, SEEK_CUR)) == (off_t)-1) {\n                perror(\"Could not get file position\");\n                exit(5);\n        }\n        if (ftruncate(outfd, file_length) == -1) {\n                perror(\"Could not truncate file\");\n                exit(6);\n        }\n        close(outfd);\n        exit(0);\n}\n```\n\n\n\nYou would use it as:\n\n\n\n```bash\ngrep pattern bigfile | overwrite bigfile\n```\n\n\n\nI'm mostly posting this for others to comment on before you try it. Perhaps someone else knows of a program that does something similar that is more tested.",
    "url": "https://unix.stackexchange.com/questions/11067/is-there-a-way-to-modify-a-file-in-place"
  },
  {
    "question_title": "Remember password for ssh key for some time",
    "question_body": "I created ssh key. I use it to connect with git repositories. When creating the key, I noticed the prompt that said the password should be hard to guess. Therefore, I came up with 40+ characters-long password. Now, every time I do \ngit clone\n, \npush\n or anything similar, I need to input the password(which takes some time, especially when I don't get it right).\n\n\nI certainly am glad that I'm enjoying security features; however, I'd prefer for ssh password to cache for 5-15 minutes(or any other arbitrary amount); sometimes I do many operations on repository in small time frame, and typing password is taking too much time. How can I do this?",
    "answer": "You can do this using an SSH agent. Most desktop environments start one for you; you can add your key to it by running\n\n\n\n```bash\nssh-add\n```\n\n\n\nIf you need to start the agent, run\n\n\n\n```bash\neval $(ssh-agent)\n```\n\n\n\n(this sets up a number of environment variables).\n\n\nThe \n-t\n option to \nssh-agent\n will allow you to specify the timeout. See \nConfiguring the default timeout for the SSH agent\n for more details.",
    "url": "https://unix.stackexchange.com/questions/271430/remember-password-for-ssh-key-for-some-time"
  },
  {
    "question_title": "Passing named arguments to shell scripts",
    "question_body": "Is there any easy way to pass (receive) named parameters to a shell script?\n\n\nFor example, \n\n\n\n```bash\nmy_script -p_out '/some/path' -arg_1 '5'\n```\n\n\n\nAnd inside \nmy_script.sh\n receive them as:\n\n\n\n```bash\n# I believe this notation does not work, but is there anything close to it?\np_out=$ARGUMENTS['p_out']\narg1=$ARGUMENTS['arg_1']\n\nprintf \"The Argument p_out is %s\" \"$p_out\"\nprintf \"The Argument arg_1 is %s\" \"$arg1\"\n```\n\n\n\nIs this possible in Bash or Zsh?",
    "answer": "If you don't mind being limited to single-letter argument names i.e. \nmy_script -p '/some/path' -a5\n, then in bash you could use the built-in \ngetopts\n, e.g.\n\n\n\n```bash\n#!/bin/bash\n\nwhile getopts \":a:p:\" opt; do\n  case $opt in\n    a) arg_1=\"$OPTARG\"\n    ;;\n    p) p_out=\"$OPTARG\"\n    ;;\n    \\?) echo \"Invalid option -$OPTARG\" >&2\n    exit 1\n    ;;\n  esac\n\n  case $OPTARG in\n    -*) echo \"Option $opt needs a valid argument\"\n    exit 1\n    ;;\n  esac\ndone\n\nprintf \"Argument p_out is %s\\n\" \"$p_out\"\nprintf \"Argument arg_1 is %s\\n\" \"$arg_1\"\n```\n\n\n\nThen you can do\n\n\n\n```bash\n$ ./my_script -p '/some/path' -a5\nArgument p_out is /some/path\nArgument arg_1 is 5\n```\n\n\n\nThere is a helpful \nSmall getopts tutorial\n or you can type \nhelp getopts\n at the shell prompt.\n\n\nEdit: The second \ncase\n statement in \nwhile\n loop triggers if the \n-p\n option has no arguments and is followed by another option, e.g. \nmy_script -p -a5\n, and \nexit\ns the program.",
    "url": "https://unix.stackexchange.com/questions/129391/passing-named-arguments-to-shell-scripts"
  },
  {
    "question_title": "How to exit a git merge asking for commit message?",
    "question_body": "I'm using git. I did a normal merge, but it keeps asking this:\n\n\n\n```bash\n# Please enter a commit message to explain why this merge is necessary,\n# especially if it merges an updated upstream into a topic branch.\n```\n\n\n\nAnd even if I write something, I can't exit from here. I can't find docs explaining this. How should I do?",
    "answer": "This is depend on the editor you're using.\n\n\nIf \nvim\n you can use \nESC\n and \n:wq\n or \nESC\n and \nShift\n+\nzz\n. Both command save file and exit.\n\n\nYou also can check \n~/.gitconfig\n for editor, in my case (\ncat ~/.gitconfig\n):\n\n\n\n```bash\n[user]\n    name = somename\n    email = somemail@gmail.com\n[core]\n    editor = vim\n    excludesfile = /home/mypath/.gitignore_global\n[color]\n  ui = auto\n  # other settings here\n```",
    "url": "https://unix.stackexchange.com/questions/181280/how-to-exit-a-git-merge-asking-for-commit-message"
  },
  {
    "question_title": "Read lines into array, one element per line using bash",
    "question_body": "I am trying to get a bash array of all the unstaged modifications of files in a directory (using Git). The following code works to print out all the modified files in a directory:\n\n\n\n```bash\ngit -C $dir/.. status --porcelain | grep \"^.\\w\" | cut -c 4-\n```\n\n\n\nThis prints\n\n\n\n```bash\n\"Directory Name/File B.txt\"\n\"File A.txt\"\n```\n\n\n\n\n\nI tried using\n\n\n\n```bash\narr1=($(git status --porcelain | grep \"^.\\w\" | cut -c 4-))\n```\n\n\n\nbut then\n\n\n\n```bash\nfor a in \"${arr1[@]}\"; do echo \"$a\"; done\n```\n\n\n\n(both with and without the quotes around \n${arr1[@]}\n prints\n\n\n\n```bash\n\"Directory\nName/File\nB.txt\"\n\"File\nA.txt\"\n```\n\n\n\n\n\nI also tried \n\n\n\n```bash\ngit -C $dir/.. status --porcelain | grep \"^.\\w\" | cut -c 4- | readarray arr2\n```\n\n\n\nbut then\n\n\n\n```bash\nfor a in \"${arr2[@]}\"; do echo \"$a\"; done\n```\n\n\n\n(both with and without the quotes around \n${arr2[@]}\n) prints nothing. Using \ndeclare -a arr2\n beforehand does absolutely nothing either.\n\n\n\n\nMy question is this: How can I read in these values into an array? (This is being used for my argos plugin \ngitbar\n, in case it matters, so you can see all my code).",
    "answer": "TL;DR\n\n\nIn bash:\n\n\n\n```bash\nreadarray -t arr2 < <(git … )\nprintf '%s\\n' \"${arr2[@]}\"\n```\n\n\n\n\n\nThere are two distinct problems on your question\n\n\n\n\nShell splitting.\n\nWhen you did:\n\n\n\n```bash\narr1=($(git … ))\n```\n\n\n\nthe \"command expansion\" is unquoted, and so: it is subject to shell split and glob.\n\n\nThe exactly see what that shell splitting do, use printf:\n\n\n\n```bash\n$ printf '<%s>  '  $(echo word '\"one simple sentence\"')\n<word>  <\"one>  <simple>  <sentence\">\n```\n\n\n\nThat would be avoided by \nquoting\n:\n\n\n\n```bash\n$ printf '<%s>  '  \"$(echo word '\"one simple sentence\"')\"\n<word \"one simple sentence\">\n```\n\n\n\nBut that, also, would avoid the splitting on newlines that you want.\n\n\nPipe\n\nWhen you executed:\n\n\n\n```bash\ngit … | … | … | readarray arr2\n```\n\n\n\nThe array variable \narr2\n got set but it went away when the pipe (\n|\n) was closed.\n\n\nYou could use the value if you stay inside the last subshell:\n\n\n\n```bash\n$ printf '%s\\n' \"First value.\" \"Second value.\" | \n        { readarray -t arr2; printf '%s\\n' \"${arr2[@]}\"; }\nFirst value.\nSecond value.\n```\n\n\n\nBut the value of \narr2\n will not survive out of the pipe.\n\n\n\n\nSolution(s)\n\n\nYou need to use read to split on newlines but not with a pipe.\n\n From older to newer:\n\n\n\n\nLoop.\n\nFor old shells without arrays (using positional arguments, the only quasi-array):\n\n\n\n```bash\nset --\nwhile IFS='' read -r value; do\n    set -- \"$@\" \"$value\"\ndone <<-EOT\n$(printf '%s\\n' \"First value.\" \"Second value.\")\nEOT\n\nprintf '%s\\n' \"$@\"\n```\n\n\n\nTo set an array (ksh, zsh, bash)\n\n\n\n```bash\ni=0; arr1=()\nwhile IFS='' read -r value; do\n    arr1+=(\"$value\")\ndone <<-EOT\n$(printf '%s\\n' \"First value.\" \"Second value.\")\nEOT\n\nprintf '%s\\n' \"${arr1[@]}\"\n```\n\n\n\nHere-string\n\nInstead of the here document (\n<<\n) we can use a here-string (\n<<<\n):\n\n\n\n```bash\ni=0; arr1=()\nwhile IFS='' read -r value; do\n    arr1+=(\"$value\")\ndone <<<\"$(printf '%s\\n' \"First value.\" \"Second value.\")\"\n\nprintf '%s\\n' \"${arr1[@]}\"\n```\n\n\n\nProcess substitution\n\nIn shells that support it (ksh, zsh, bash) you can use \n<( … )\n to replace the here-string:\n\n\n\n```bash\ni=0; arr1=()\nwhile IFS='' read -r value; do\n    arr1+=(\"$value\")\ndone < <(printf '%s\\n' \"First value.\" \"Second value.\")\n\nprintf '%s\\n' \"${arr1[@]}\"\n```\n\n\n\nWith differences: \n<( )\n is able to emit NUL bytes while a here-string might remove (or emit a warning) the NULs. A here-string adds a trailing newline by default. There may be others AFAIK.\n\n\nreadarray\n\nUse \nreadarray\n in \nbash\n[a]\n (a.k.a \nmapfile\n) to avoid the loop:\n\n\n\n```bash\nreadarray -t arr2 < <(printf '%s\\n' \"First value.\" \"Second value.\")\nprintf '%s\\n' \"${arr2[@]}\"\n```\n\n\n\n\n\n\n\n[a]\nIn ksh you will need to use \nread -A\n, which clears the variable before use, but needs some \"magic\" to split on newlines and read the whole input at once.\n\n\n\n```bash\nIFS=$'\\n' read -d '' -A arr2 < <(printf '%s\\n' \"First value.\" \"Second value.\")\n```\n\n\n\nYou will need to \nload a \nmapfile\n module in zsh\n to do something similar.",
    "url": "https://unix.stackexchange.com/questions/485221/read-lines-into-array-one-element-per-line-using-bash"
  },
  {
    "question_title": "Does tar actually compress files, or just group them together?",
    "question_body": "I usually assumed that \ntar\n was a compression utility, but I am unsure, does it actually compress files, or is it just like an ISO file, a file to hold files?",
    "answer": "Tar is an archiving tool (Tape ARchive), it only collects files and their metadata together and produces one file. If you want to compress that file later you can use gzip/bzip2/xz. For convenience, many modern implementations of \ntar\n provide arguments to compress the archive automatically for you. Checkout the \ntar man page\n for more details.",
    "url": "https://unix.stackexchange.com/questions/127169/does-tar-actually-compress-files-or-just-group-them-together"
  },
  {
    "question_title": "List the files containing a particular word in their text",
    "question_body": "I would like to list the files recursively and uniquely that contain the given word. \n\n\nExample\n: Checking for word 'check', I normal do is a grep\n\n\n\n```bash\n$ grep check * -R\n```\n\n\n\nBut as there are many occurrence of this word, I get a lot of output. So I just need to list the filenames that contain the given search word. I guess some trick with \nfind\n and \nxargs\n would suffice here, but not sure.\n\n\nAny ideas?",
    "answer": "Use the \n-l\n or \n--files-with-matches\n option which is documented as follows:\n\n\n\n> Suppress  normal  output;  instead  print the name of each input file\n> from which output would normally have  been  printed.   The scanning\n> will  stop  on  the  first match.  (-l is specified by POSIX.)\n\n\n\nSo, for you example you can use the following:\n\n\n\n```bash\n$ grep check * -lR\n```",
    "url": "https://unix.stackexchange.com/questions/65181/list-the-files-containing-a-particular-word-in-their-text"
  },
  {
    "question_title": "How can we run a command stored in a variable?",
    "question_body": "```bash\n$ ls -l /tmp/test/my\\ dir/\ntotal 0\n```\n\n\n\nI was wondering why the following ways to run the above command fail or succeed?\n\n\n\n```bash\n$ abc='ls -l \"/tmp/test/my dir\"'\n\n$ $abc\nls: cannot access '\"/tmp/test/my': No such file or directory\nls: cannot access 'dir\"': No such file or directory\n\n$ \"$abc\"\nbash: ls -l \"/tmp/test/my dir\": No such file or directory\n\n$ bash -c $abc\n'my dir'\n\n$ bash -c \"$abc\"\ntotal 0\n\n$ eval $abc\ntotal 0\n\n$ eval \"$abc\"\ntotal 0\n```",
    "answer": "This has been discussed in a number of questions on unix.SE, I'll try to collect all issues I can come up with here. Below is\n\n\n\n\na description of why and how the various attempts fail,\n\n\na way to do it properly with a function (for a fixed command), or\n\n\nwith shell arrays (Bash/ksh/zsh) or the \n$@\n pseudo-array (POSIX sh), both of which also allow building the command line pieces, if you e.g. only need to vary some optoins\n\n\nand notes about using \neval\n to do this.\n\n\n\n\nSome references at the end.\n\n\nFor the purposes here, it doesn't matter much if it's only the command arguments or also the command name that is to be stored in a variable. They're processed similarly up to the point where the command is launched, at which point the shell just takes the first word as the name of the command to run.\n\n\n\n\nWhy it fails\n\n\nThe reason you face those problems is the fact that \nword splitting\n is quite simple and doesn't lend itself to complex cases, and the fact that quotes expanded from variables don't act as quotes, but are just ordinary characters.\n\n\n(Note that the part about quotes is similar to every other programming language: e.g. \nchar *s = \"foo()\"; printf(\"%s\\n\", s)\n does not call the function \nfoo()\n in C, but just prints the string \nfoo()\n. That's different in macro processors, like m4, the C preprocessor, or Make (to some extent). The shell is a programming language, not a macro processor.)\n\n\nOn Unix-like systems, it's the shell that processes quotes and variable expansions on the command line, turning it from a single string into the list of strings that the underlying system call passes to the launched command. The program itself doesn't see the quotes the shell processed. E.g. if given the command \nls -l \"foo bar\"\n, the shell turns that into the three strings \nls\n, \n-l\n and \nfoo bar\n (removing the quotes), and passes those to \nls\n. \n(Even the command name is passed, though not all programs use it.)\n\n\nThe cases presented in the question:\n\n\nThe assignment here assigns the single string \nls -l \"/tmp/test/my dir\"\n to \nabc\n:\n\n\n\n```bash\n$ abc='ls -l \"/tmp/test/my dir\"'\n```\n\n\n\nBelow, \n$abc\n is split on whitespace, and \nls\n gets the three arguments \n-l\n, \n\"/tmp/test/my\n and \ndir\"\n. The quotes here are just data, so there's one at the front of the second argument and another at the back of the third. The option works, but the path gets incorrectly processed as \nls\n sees the quotes as part of the filenames:\n\n\n\n```bash\n$ $abc\nls: cannot access '\"/tmp/test/my': No such file or directory\nls: cannot access 'dir\"': No such file or directory\n```\n\n\n\nHere, the expansion is quoted, so it's kept as a single word. The shell tries to find a program literally called \nls -l \"/tmp/test/my dir\"\n, spaces and quotes included.\n\n\n\n```bash\n$ \"$abc\"\nbash: ls -l \"/tmp/test/my dir\": No such file or directory\n```\n\n\n\nAnd here, \n$abc\n is split, and only the first resulting word is taken as the argument to \n-c\n, so Bash just runs \nls\n in the current directory. The other words are arguments to bash, and are used to fill \n$0\n, \n$1\n, etc.\n\n\n\n```bash\n$ bash -c $abc\n'my dir'\n```\n\n\n\nWith \nbash -c \"$abc\"\n, and \neval \"$abc\"\n, there's an additional shell processing step, which does make the quotes work, but \nalso causes all shell expansions to be processed again\n, so there's a risk of accidentally running e.g. a command substitution from user-provided data, unless you're very careful about quoting.\n\n\n\n\nBetter ways to do it\n\n\nThe two better ways to store a command are a) use a function instead, b) use an array variable (or the positional parameters).\n\n\nUsing functions:\n\n\nSimply declare a function with the command inside, and run the function as if it were a command. Expansions in commands within the function are only processed when the command runs, not when it's defined, and you don't need to quote the individual commands. Though this really only helps if you have a fixed command  you need to store (or more than one fixed command).\n\n\n\n```bash\n# define it\nmyls() {\n    ls -l \"/tmp/test/my dir\"\n}\n\n# run it\nmyls\n```\n\n\n\nIt's also possible to define multiple functions and use a variable to store the name of the function you want to run in the end.\n\n\nUsing an array:\n\n\nArrays allow creating multi-word variables where the individual words contain white space. Here, the individual words are stored as distinct array elements, and the \n\"${array[@]}\"\n expansion expands each element as separate shell words:\n\n\n\n```bash\n# define the array\nmycmd=(ls -l \"/tmp/test/my dir\")\n\n# expand the array, run the command\n\"${mycmd[@]}\"\n```\n\n\n\nThe command is written inside the parentheses exactly as it would be written when running the command. The processing the shell does is the same in both cases, just in one it only saves the resulting list of strings, instead of using it to run a program.\n\n\nThe syntax for expanding the array later is slightly horrible, though, and the quotes around it are important.\n\n\nArrays also allow you to build the command line piece-by-piece. For example:\n\n\n\n```bash\nmycmd=(ls)               # initial command\nif [ \"$want_detail\" = 1 ]; then\n    mycmd+=(-l)          # optional flag, append to array\nfi\nmycmd+=(\"$targetdir\")    # the filename\n\n\"${mycmd[@]}\"\n```\n\n\n\nor keep parts of the command line constant and use the array fill just a part of it, like options or filenames:\n\n\n\n```bash\noptions=(-x -v)\nfiles=(file1 \"file name with whitespace\")\ntarget=/somedir\n\nsomecommand \"${options[@]}\" \"${files[@]}\" \"$target\"\n```\n\n\n\n(\nsomecommand\n being a generic placeholder name here, not any real command.)\n\n\nThe downside of arrays is that they're not a standard feature, so plain POSIX shells (like \ndash\n, the default \n/bin/sh\n in Debian/Ubuntu) don't support them (but see below). Bash, ksh and zsh do, however, so it's likely your system has some shell that supports arrays.\n\n\nUsing \n\"$@\"\n\n\nIn shells with no support for named arrays, one can still use the positional parameters (the pseudo-array \n\"$@\"\n) to hold the arguments of a command.\n\n\nThe following should be portable script bits that do the equivalent of the code bits in the previous section. The array is replaced with \n\"$@\"\n, the list of positional parameters.  Setting \n\"$@\"\n is done with \nset\n, and the double quotes around \n\"$@\"\n are important (these cause the elements of the list to be individually quoted).\n\n\nFirst, simply storing a command with arguments in \n\"$@\"\n and running it:\n\n\n\n```bash\nset -- ls -l \"/tmp/test/my dir\"\n\"$@\"\n```\n\n\n\nConditionally setting parts of the command line options for a command:\n\n\n\n```bash\nset -- ls\nif [ \"$want_detail\" = 1 ]; then\n    set -- \"$@\" -l\nfi\nset -- \"$@\" \"$targetdir\"\n\n\"$@\"\n```\n\n\n\nOnly using \n\"$@\"\n for options and operands:\n\n\n\n```bash\nset -- -x -v\nset -- \"$@\" file1 \"file name with whitespace\"\nset -- \"$@\" /somedir\n\nsomecommand \"$@\"\n```\n\n\n\nOf course, \n\"$@\"\n is usually filled with the arguments to the script itself, so you'll have to save them somewhere before re-purposing \n\"$@\"\n.\n\n\nTo conditionally pass a single argument, you can also use the alternate value expansion \n${var:+word}\n with some careful quoting. Here, we include \n-f\n  and the filename only if the filename is nonempty:\n\n\n\n```bash\nfile=\"foo bar\"\nsomecommand ${file:+-f \"$file\"}\n```\n\n\n\n\n\nUsing \neval\n (be careful here!)\n\n\neval\n takes a string and runs it as a command, just like if it was entered on the shell command line. This includes all quote and expansion processing, which is both useful and dangerous.\n\n\nIn the simple case, it allows doing just what we want:\n\n\n\n```bash\ncmd='ls -l \"/tmp/test/my dir\"'\neval \"$cmd\"\n```\n\n\n\nWith \neval\n, the quotes are processed, so \nls\n eventually sees just the two arguments \n-l\n and \n/tmp/test/my dir\n, like we want. \neval\n is also smart enough to concatenate any arguments it gets, so \neval $cmd\n could also work in some cases, but e.g. all runs of whitespace would be changed to single spaces. It's still better to quote the variable there as that will ensure it gets unmodified to \neval\n.\n\n\nHowever, \nit's dangerous to include user input in the command string to \neval\n. For example, this seems to work:\n\n\n\n```bash\nread -r filename\ncmd=\"ls -ld '$filename'\"\neval \"$cmd\";\n```\n\n\n\nBut if the user gives input that contains single quotes, they can break out of the quoting and run arbitrary commands!\n E.g. with the input \n'$(whatever)'.txt\n, your script happily runs the command substitution. That it could have been \nrm -rf\n (or worse) instead.\n\n\nThe issue there is that the value of \n$filename\n was embedded in the command line that \neval\n runs. It was expanded before \neval\n, which saw e.g. the command \nls -l ''$(whatever)'.txt'\n. You would need to pre-process the input to be safe.\n\n\nIf we do it the other way, keeping the filename in the variable, and letting the \neval\n command expand it, it's safer again:\n\n\n\n```bash\nread -r filename\ncmd='ls -ld \"$filename\"'\neval \"$cmd\";\n```\n\n\n\nNote the outer quotes are now single quotes, so expansions within do not happen. Hence, \neval\n sees the command \nls -l \"$filename\"\n and expands the filename safely itself.\n\n\nBut that's not much different from just storing the command in a function or an array. With functions or arrays, there is no such problem since the words are kept separate for the whole time, and there's no quote or other processing for the contents of \nfilename\n.\n\n\n\n```bash\nread -r filename\ncmd=(ls -ld -- \"$filename\")\n\"${cmd[@]}\"\n```\n\n\n\nPretty much the only reason to use \neval\n is one where the varying part involves shell syntax elements that can't be brought in via variables (pipelines, redirections, etc.). However, you'll then need to quote/escape everything \nelse\n on the command line that needs protection from the additional parsing step (see link below). In any case, it's best to \navoid\n embedding input from the user in the \neval\n command!\n\n\n\n\nReferences\n\n\n\n\nWord Splitting\n in \nBashGuide\n\n\nBashFAQ/050 or \"I'm trying to put a command in a variable, but the complex cases always fail!\"\n\n\nThe question \nWhy does my shell script choke on whitespace or other special characters?\n, which discusses a number of issues related to quoting and whitespace, including storing commands.\n\n\nEscape a variable for use as content of another script\n\n\nHow can I conditionally pass an argument from a POSIX shell script?",
    "url": "https://unix.stackexchange.com/questions/444946/how-can-we-run-a-command-stored-in-a-variable"
  },
  {
    "question_title": "Create a tar archive split into blocks of a maximum size",
    "question_body": "I need to backup a fairly large directory, but I am limited by the size of individual files. I'd like to essentially create a \ntar.(gz|bz2)\n archive which is split into 200MB maximum archives. Clonezilla does something similar to this by splitting image backups named like so:\n\n\n\n```bash\nsda1.backup.tar.gz.aa\nsda1.backup.tar.gz.ab\nsda1.backup.tar.gz.ac\n```\n\n\n\nIs there a way I can do this in one command? I understand how to use the \nsplit\n command, but I'd like to not have to create one giant archive, then split it into smaller archives, as this would double the disk space I'd need in order to initially create the archive.",
    "answer": "You can pipe tar to the split command:\n\n\n\n```bash\ntar cvzf - dir/ | split --bytes=200MB - sda1.backup.tar.gz.\n```\n\n\n\nOn some *nix systems (like OS X) you may get the following error:\n\n\n\n```bash\nsplit: illegal option -- -\n```\n\n\n\nIn that case try this (note the \n-b 200m\n):\n\n\n\n```bash\ntar cvzf - dir/ | split -b 200m - sda1.backup.tar.gz.\n```\n\n\n\nIf you happen to be trying to split the file to fit on a FAT32 formatted drive,\nuse a byte limit of 4294967295. For example:\n\n\n\n```bash\ntar cvzf - /Applications/Install\\ macOS\\ Sierra.app/ | \\\nsplit -b 4294967295 - /Volumes/UNTITLED/install_macos_sierra.tgz.\n```\n\n\n\nWhen you want to extract the files use the following command (as of @Naftuli Kay commented):\n\n\n\n```bash\ncat sda1.backup.tar.gz.* | tar xzvf -\n```",
    "url": "https://unix.stackexchange.com/questions/61774/create-a-tar-archive-split-into-blocks-of-a-maximum-size"
  },
  {
    "question_title": "Why does the gzip version of files produce a different md5 checksum",
    "question_body": "I have four files that I created using an \nsvndump\n\n\n\n```bash\ntest.svn \ntest2.svn \ntest.svn.gz  \ntest2.svn.gz\n```\n\n\n\nnow when I run this\n\n\n\n```bash\nmd5sum test2.svn test.svn test.svn.gz test2.svn.gz\n```\n\n\n\nHere is the output\n\n\n\n```bash\n89fc1d097345b0255825286d9b4d64c3  test2.svn\n89fc1d097345b0255825286d9b4d64c3  test.svn\n8284ebb8b4f860fbb3e03e63168b9c9e  test.svn.gz\nab9411efcb74a466ea8e6faea5c0af9d  test2.svn.gz\n```\n\n\n\nSo I can't understand why \ngzip\n is compressing files differently is it putting a timestamp somewhere before compressing? \nI had a similar issue with \nmysqldump\n as it was using the date field on top",
    "answer": "gzip\n stores some of the original file's metadata in record header, including the file modification time and filename, if available. See \nGZIP file format specification\n.\n\n\nSo it's expected that your two \ngzip\n files aren't identical. You can work around this by passing \ngzip\n the \n-n\n flag, which stops it from including the original filename and timestamp in the header.",
    "url": "https://unix.stackexchange.com/questions/31008/why-does-the-gzip-version-of-files-produce-a-different-md5-checksum"
  },
  {
    "question_title": "How to extract specific file(s) from tar.gz",
    "question_body": "How can we extract specific files from a large tar.gz file? I found \nthe process of extracting files from a tar in this question\n but, when I tried the mentioned command there, I got the error:\n\n\n\n```bash\n$ tar --extract --file={test.tar.gz} {extract11}\ntar: {test.tar.gz}: Cannot open: No such file or directory\ntar: Error is not recoverable: exiting now\n```\n\n\n\nHow do I then extract a file from \ntar.gz\n?",
    "answer": "List the paths in tar:\n\n\n\n```bash\ntar -ztf test.tar.gz\n```\n\n\n\nNote the file name / path exactly as it is listed. e.g. If it says e.g. \n./extract11\n, or \nsome/bunch/of/dirs/extract11\n that's what you have to use.\n\n\nYou can extract that file in this format:\n\n\n\n```bash\ntar -zxvf <tar filename> <file you want to extract>\n```\n\n\n\nThe file the file will show up under exactly that name, needed directories are created automatically.\n\n\n\n\nOptions used:\n\n\n\n\n-z\n: filter archive through gzip, use to decompress .gz files.\n\n\n-t\n: List the contents of an archive\n\n\n-x\n: instructs tar to extract files.\n\n\n-v\n: Verbose (show progress while extracting files).\n\n\n-f\n: specifies filename / tarball name.",
    "url": "https://unix.stackexchange.com/questions/61461/how-to-extract-specific-files-from-tar-gz"
  },
  {
    "question_title": "Encrypting and compressing",
    "question_body": "Is there a better way to compress and then encrypt files other than \ntar\n followed by \nopenssl\n or \ngpg\n?",
    "answer": "tar\n is the usual tool to bundle files. Plain \ntar\n itself doesn't compress. There are separate tools such as \ngzip\n, \nbzip2\n and \nxz\n (in increasing order of compression ratio on typical files) that compress one file. Many \ntar\n implementation, including GNU tar (the normal implementation on Linux), can automatically compress with an option (\n-z\n for gzip, \n-j\n for bzip2, \n-J\n for xz):\n\n\n\n```bash\ntar -cJf myarchive.tar.xz file1 file2 file3\n```\n\n\n\nTo encrypt a file, use \ngpg\n. Create a key and associate it with your email address (GPG/PGP key identifiers usually contain an email address, though it is not necessary ). Encrypt your files, specifying your email as the recipient. To decrypt a file, you'll need to enter the passphrase to unlock your private key.\n\n\nGPG also lets you encrypt a file with a password. This is less secure and less flexible. It's less flexible because you need to specify the password when encrypting (so for example you can't make unattended backups). It's less secure because the only security is the password, whereas key-based encryption splits the security between the password and the key.\n\n\nDon't use the \nopenssl\n command line tool. It's a showcase for the OpenSSL library, not designed for production use. Although you can do some things with it (in particular, it does have all the primitives needed for a basic certification authority), it's hard to use correctly and it doesn't have all you need to do things right. Where GPG gives you a bicycle, OpenSSL gives you some metal rods of various sizes and a couple of rubber chambers (screws and pump not included). Use GPG.",
    "url": "https://unix.stackexchange.com/questions/144391/encrypting-and-compressing"
  },
  {
    "question_title": "tar with relative paths",
    "question_body": "I try to create an archive with tar using relative paths. I use the following command:\n\n\n\n```bash\ntar czf ~/files/wp/my-page-order.tar.gz -C ~/webapps/zers/wp-content/plugins/ ~/webapps/zers/wp-content/plugins/my-page-order\n```\n\n\n\nBut the archived files still have absolute paths. How can I use tar with relative paths?",
    "answer": "~\n is expanded by the shell. Don't use \n~\n with -C:\n\n\n\n```bash\ntar czf ~/files/wp/my-page-order.tar.gz \\\n      -C ~ \\\n       webapps/zers/wp-content/plugins/my-page-order\n```\n\n\n\n(tar will include \nwebapps/zers/wp-content/plugins/my-page-order\n path)\nor\n\n\n\n```bash\ntar czf ~/files/wp/my-page-order.tar.gz \\\n      -C ~/webapps/zers/wp-content/plugins \\\n       my-page-order\n```\n\n\n\n(tar will include \nmy-page-order\n path)\n\n\nOr just \ncd\n first....\n\n\n\n```bash\ncd ~/webapps/zers/wp-content/plugins\ntar czf ~/files/wp/my-page-order.tar.gz my-page-order\n```",
    "url": "https://unix.stackexchange.com/questions/22842/tar-with-relative-paths"
  },
  {
    "question_title": "Can I &quot;export&quot; functions in bash?",
    "question_body": "```bash\nsource some_file\n```\n\n\n\nsome_file:\n\n\n\n```bash\ndoit ()\n{\n  echo doit $1\n}\nexport TEST=true\n```\n\n\n\nIf I source \nsome_file\n the function \"doit\" and the variable TEST are available on the command line. But running this script:\n\n\nscript.sh:\n\n\n\n```bash\n#/bin/sh\necho $TEST\ndoit test2\n```\n\n\n\nWill return the value of TEST, but will generate an error about the unknown function \"doit\".\n\n\nCan I \"export\" the function, too, or do I have to source some_file in script.sh to use the function there?",
    "answer": "In Bash you can export function definitions to other shell scripts that your script calls with\n\n\n\n```bash\nexport -f function_name\n```\n\n\n\nFor example you can try this simple example:\n\n\n./script1\n:\n\n\n\n```bash\n#!/bin/bash\n\nmyfun() {\n    echo \"Hello!\"\n}\n\nexport -f myfun\n./script2\n```\n\n\n\n./script2\n:\n\n\n\n```bash\n#!/bin/bash\n\nmyfun\n```\n\n\n\nThen if you call \n./script1\n you will see the output \nHello!\n.",
    "url": "https://unix.stackexchange.com/questions/22796/can-i-export-functions-in-bash"
  },
  {
    "question_title": "What does env x=&#39;() { :;}; command&#39; bash do and why is it insecure?",
    "question_body": "There is apparently a vulnerability (CVE-2014-6271) in bash: \nBash specially crafted environment variables code injection attack\n\n\nI am trying to figure out what is happening, but I'm not entirely sure I understand it. How can the \necho\n be executed as it is in single quotes?\n\n\n\n```bash\n$ env x='() { :;}; echo vulnerable' bash -c \"echo this is a test\"\nvulnerable\nthis is a test\n```\n\n\n\n\n\nEDIT 1\n: A patched system looks like this:\n\n\n\n```bash\n$ env x='() { :;}; echo vulnerable' bash -c \"echo this is a test\"\nbash: warning: x: ignoring function definition attempt\nbash: error importing function definition for `x'\nthis is a test\n```\n\n\n\nEDIT 2\n: There is a related vulnerability / patch: \nCVE-2014-7169\n which uses a slightly different test:\n\n\n\n```bash\n$ env 'x=() { :;}; echo vulnerable' 'BASH_FUNC_x()=() { :;}; echo vulnerable' bash -c \"echo test\"\n```\n\n\n\nunpatched output\n:\n\n\n\n```bash\nvulnerable\nbash: BASH_FUNC_x(): line 0: syntax error near unexpected token `)'\nbash: BASH_FUNC_x(): line 0: `BASH_FUNC_x() () { :;}; echo vulnerable'\nbash: error importing function definition for `BASH_FUNC_x'\ntest\n```\n\n\n\npartially (early version) patched output\n:\n\n\n\n```bash\nbash: warning: x: ignoring function definition attempt\nbash: error importing function definition for `x'\nbash: error importing function definition for `BASH_FUNC_x()'\ntest\n```\n\n\n\npatched output\n up to and including CVE-2014-7169:\n\n\n\n```bash\nbash: warning: x: ignoring function definition attempt\nbash: error importing function definition for `BASH_FUNC_x'\ntest\n```\n\n\n\nEDIT 3\n: story continues with:\n\n\n\n\nCVE-2014-7186\n\n\nCVE-2014-7187\n\n\nCVE-2014-6277",
    "answer": "bash stores exported function definitions as environment variables. Exported functions look like this:\n\n\n\n```bash\n$ foo() { bar; }\n$ export -f foo\n$ env | grep -A1 foo\nfoo=() {  bar\n}\n```\n\n\n\nThat is, the environment variable \nfoo\n has the literal contents:\n\n\n\n```bash\n() {  bar\n}\n```\n\n\n\nWhen a new instance of bash launches, it looks for these specially crafted environment variables, and interprets them as function definitions. You can even write one yourself, and see that it still works:\n\n\n\n```bash\n$ export foo='() { echo \"Inside function\"; }'\n$ bash -c 'foo'\nInside function\n```\n\n\n\nUnfortunately, the parsing of function definitions from strings (the environment variables) can have wider effects than intended. In unpatched versions, it also interprets arbitrary commands that occur after the termination of the function definition. This is due to insufficient constraints in the determination of acceptable function-like strings in the environment. For example:\n\n\n\n```bash\n$ export foo='() { echo \"Inside function\" ; }; echo \"Executed echo\"'\n$ bash -c 'foo'\nExecuted echo\nInside function\n```\n\n\n\nNote that the echo outside the function definition has been unexpectedly executed during bash startup. The function definition is just a step to get the evaluation and exploit to happen, the function definition itself and the environment variable used are arbitrary. The shell looks at the environment variables, sees \nfoo\n, which looks like it meets the constraints it knows about what a function definition looks like, and it evaluates the line, unintentionally also executing the echo (which could be any command, malicious or not).\n\n\nThis is considered insecure because variables are not typically allowed or expected, by themselves, to directly cause the invocation of arbitrary code contained in them. Perhaps your program sets environment variables from untrusted user input. It would be highly unexpected that those environment variables could be manipulated in such a way that the user could run arbitrary commands without your explicit intent to do so using that environment variable for such a reason declared in the code.\n\n\nHere is an example of a viable attack. You run a web server that runs a vulnerable shell, somewhere, as part of its lifetime. This web server passes environment variables to a bash script, for example, if you are using CGI, information about the HTTP request is often included as environment variables from the web server. For example, \nHTTP_USER_AGENT\n might be set to the contents of your user agent. This means that if you spoof your user agent to be something like '() { :; }; echo foo', when that shell script runs, \necho foo\n will be executed. Again, \necho foo\n could be anything, malicious or not.",
    "url": "https://unix.stackexchange.com/questions/157329/what-does-env-x-command-bash-do-and-why-is-it-insecure"
  },
  {
    "question_title": "How to create backup from symbolic links?",
    "question_body": "I have a directory with symbolic links to other directories. I want to archive the symbolic links, not as links but as archives containing the files of directories they refer to, using tar command. How can I do this?",
    "answer": "Use the \n-h\n tar option. From the man page:\n\n\n\n```bash\n-h, --dereference\n    don't archive symlinks; archive the files they point to\n```",
    "url": "https://unix.stackexchange.com/questions/32633/how-to-create-backup-from-symbolic-links"
  },
  {
    "question_title": "Is it true that a tarball can change where files are extracted to regardless of the commands used to extract it?",
    "question_body": "I have a coworker who says you need to be careful extracting tarballs because they can make changes you don't know about. I always thought a tarball was just a hierarchy of compressed files, so if you extract it to /tmp/example/ it can't possibly sneak a file into /etc/ or anything like that.",
    "answer": "Different tar utilities behave differently in this regard, so it's good to be careful. For a tar file that you didn't create, always list the table of contents before extracting it.\n\n\nSolaris tar\n:\n\n\n\n> The named files are extracted from the tarfile and written to the directory specified in the tarfile, relative to the current directory. Use the relative path names of files and directories to be extracted.\n> Absolute path names contained in the tar archive are unpacked using the absolute path names, that is, the leading forward slash (/) is not stripped off.\n\n\n\nIn the case of a tar file with full (absolute) path names, such as:\n\n\n\n```bash\n/tmp/real-file\n/etc/sneaky-file-here\n```\n\n\n\n... if you extract such a file, you'll end up with both files.\n\n\nGNU tar\n:\n\n\n\n> By default, GNU tar drops a leading / on input or output, and complains about file names containing a .. component. There is an option that turns off this behavior:\n> --absolute-names\n> -P\n> Do not strip leading slashes from file names, and permit file names containing a .. file name component.\n\n\n\n... if you extract a fully-pathed tar file using GNU tar \nwithout\n using the \n-P\n option, it will tell you:\n\n\n\n> tar: Removing leading / from member names\n\n\n\nand will extract the file into subdirectories of your current directory.\n\n\nAIX tar\n:\n\n\nsays nothing about it, and behaves as the Solaris tar -- it will create and extract tar files with full/absolute path names.\n\n\nHP-UX tar\n:\n\n\n(better online reference welcomed)\n\n\n\n> WARNINGS\n> There is no way to restore an absolute path name to a relative position.\n\n\n\nOpenBSD tar\n:\n\n\n\n> -P\n> Do not strip leading slashes (/) from pathnames.  The default is to strip leading slashes.\n\n\n\nThere are \n-P\n options implemented for \ntar\n on macOS, FreeBSD and NetBSD as well, with the same semantics, with the addition that \ntar\n on \nFreeBSD\n and macOS will \"refuse to extract archive entries whose pathnames contain \n..\n or\n         whose target directory would be altered by a symlink\" without \n-P\n.\n\n\nschilytools star\n:\n\n\n\n> -/\n> Don't  strip  leading  slashes  from  file  names  when extracting  an  archive.  Tar archives containing absolute pathnames are usually a bad idea. With  other  tar implementations,  they  may possibly never be extracted without clobbering existing files. Star for  that  reason,  by  default strips leading slashes from filenames when in extract mode.",
    "url": "https://unix.stackexchange.com/questions/465758/is-it-true-that-a-tarball-can-change-where-files-are-extracted-to-regardless-of"
  },
  {
    "question_title": "GZip doesn&#39;t produce the same compressed result on macOS vs Linux",
    "question_body": "I have a few thousand files that are individually GZip compressed (passing of course the \n-n\n flag so the output is deterministic). They then go into a Git repository. I just discovered that for 3 of these files, Gzip doesn't produce the same output on macOS vs Linux. Here's an example:\n\n\nmacOS\n\n\n\n```bash\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | shasum -a 256\n0ac378465b576991e1c7323008efcade253ce1ab08145899139f11733187e455  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip --fast -n | shasum -a 256\n6e145c6239e64b7e28f61cbab49caacbe0dae846ce33d539bf5c7f2761053712  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip -n | shasum -a 256\n3562fd9f1d18d52e500619b4a5d5dfa709f5da8601b9dd64088fb5da8de7b281  -\n\n$ gzip --version\nApple gzip 272.250.1\n```\n\n\n\nLinux\n\n\n\n```bash\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | shasum -a 256\n0ac378465b576991e1c7323008efcade253ce1ab08145899139f11733187e455  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip --fast -n | shasum -a 256\n10ac8b80af8d734ad3688aa6c7d9b582ab62cf7eda6bc1a0f08d6159cad96ddc  -\n\n$ cat Engine/Extras/ThirdPartyNotUE/NoRedist/EnsureIT/9.7.0/bin/finalizer | gzip -n | shasum -a 256\ncbf249e3a35f62a4f3b13e2c91fe0161af5d96a58727d17cf7a62e0ac3806393  -\n\n$ gzip --version\ngzip 1.6\nCopyright (C) 2007, 2010, 2011 Free Software Foundation, Inc.\nCopyright (C) 1993 Jean-loup Gailly.\nThis is free software.  You may redistribute copies of it under the terms of\nthe GNU General Public License <http://www.gnu.org/licenses/gpl.html>.\nThere is NO WARRANTY, to the extent permitted by law.\n\nWritten by Jean-loup Gailly.\n```\n\n\n\nHow is this possible? I thought the GZip implementation was completely standard?\n\n\nUPDATE:\n Just to confirm that macOS and Linux versions do produce the same output most of the time, both OSes output the same hash for:\n\n\n\n```bash\n$ echo \"Vive la France\" | gzip --fast -n | shasum -a 256\naf842c0cb2dbf94ae19f31c55e05fa0e403b249c8faead413ac2fa5e9b854768  -\n```",
    "answer": "Note that the compression algorithm (Deflate) in GZip is not strictly bijective. To elaborate: For some data, there's more than one possible compressed output depending on the algorithmic implementation and used parameters. So there's no guarantee at all that Apple GZip and gzip 1.6 will return the same \ncompressed\n output. These outputs are all valid GZip streams, the standard just guarantees that every of these possible outputs will be \ndecompressed\n to the same original data.",
    "url": "https://unix.stackexchange.com/questions/570477/gzip-doesnt-produce-the-same-compressed-result-on-macos-vs-linux"
  },
  {
    "question_title": "Find files containing one string but not the other",
    "question_body": "I am in a folder with lots of \n.txt\n files, I would like to find all the files which contain \nstringA\n but don't contain \nstringB\n (they are not necessarily in the same line). Does anyone know how to do this?",
    "answer": "As long as your filenames do not contain spaces, tabs, newline (assuming an unmodified \n$IFS\n) or wildcard characters and don't start with \n-\n, and if your \ngrep\n supports the \n-L\n option, you can do it as follows:\n\n\n\n```bash\n$ cat file1\nstringA\nstringC\n$ cat file2\nstringA\nstringB\n$ grep -L stringB $(grep -l stringA file?)\nfile1\n```\n\n\n\nThe \ngrep\n executed in the subshell \n$()\n, will print all filenames which contain \nstringA\n. This filelist is input for the main \ngrep\n command, which lists all files that do not contain \nstringB\n.\n\n\nFrom \nman grep\n\n\n\n> ```bash\n> -v, --invert-match\n> ```\n\n\n\n\n```bash\nInvert the sense of matching, to select non-matching lines.  (-v is specified by POSIX.)\n```\n\n\n\n\n> ```bash\n> -L, --files-without-match\n> ```\n\n\n\n\n```bash\nSuppress normal output; instead print the name of each input file from which no output would normally have been printed.  The scanning will stop on the first match.\n```\n\n\n\n\n> ```bash\n> -l, --files-with-matches\n> ```\n\n\n\n\n```bash\nSuppress normal output; instead print the name of each input file from which output would normally have been printed.  The scanning will stop on the first match.  (-l is specified by POSIX.)\n```",
    "url": "https://unix.stackexchange.com/questions/128434/find-files-containing-one-string-but-not-the-other"
  },
  {
    "question_title": "How can I execute local script on remote machine and include arguments?",
    "question_body": "I have written a script that runs fine when executed locally:\n\n\n\n```bash\n./sysMole -time Aug 18 18\n```\n\n\n\nThe arguments \n\"-time\"\n, \n\"Aug\"\n, \n\"18\"\n, and \n\"18\"\n are successfully passed on to the script.\n\n\nNow, this script is designed to be executed on a remote machine but, from a local directory on the local machine. Example:\n\n\n\n```bash\nssh root@remoteServer \"bash -s\" < /var/www/html/ops1/sysMole\n```\n\n\n\nThat also works fine. But the problem arises when I try to include those aforementioned arguments \n(-time Aug 18 18)\n, for example:\n\n\n\n```bash\nssh root@remoteServer \"bash -s\" < /var/www/html/ops1/sysMole -time Aug 18 18\n```\n\n\n\nAfter running that script I get the following error:\n\n\n\n```bash\nbash: cannot set terminal process group (-1): Invalid argument\nbash: no job control in this shell\n```\n\n\n\nPlease tell me what I'm doing wrong, this greatly frustrating.",
    "answer": "You were pretty close with your example. It works just fine when you use it with arguments such as these.\n\n\nSample script:\n\n\n\n```bash\n$ more ex.bash \n#!/bin/bash\n\necho $1 $2\n```\n\n\n\nExample that works:\n\n\n\n```bash\n$ ssh serverA \"bash -s\" < ./ex.bash \"hi\" \"bye\"\nhi bye\n```\n\n\n\nBut it fails for these types of arguments:\n\n\n\n```bash\n$ ssh serverA \"bash -s\" < ./ex.bash \"--time\" \"bye\"\nbash: --: invalid option\n...\n```\n\n\n\nWhat's going on?\n\n\nThe problem you're encountering is that the argument, \n-time\n, or \n--time\n in my example, is being interpreted as a switch to \nbash -s\n. You can pacify \nbash\n by terminating it from taking any of the remaining command line arguments for itself using the \n--\n argument.\n\n\nLike this:\n\n\n\n```bash\n$ ssh root@remoteServer \"bash -s\" -- < /var/www/html/ops1/sysMole -time Aug 18 18\n```\n\n\n\nExamples\n\n\n#1:\n\n\n\n```bash\n$ ssh serverA \"bash -s\" -- < ./ex.bash \"-time\" \"bye\"\n-time bye\n```\n\n\n\n#2:\n\n\n\n```bash\n$ ssh serverA \"bash -s\" -- < ./ex.bash \"--time\" \"bye\"\n--time bye\n```\n\n\n\n#3:\n\n\n\n```bash\n$ ssh serverA \"bash -s\" -- < ./ex.bash --time \"bye\"\n--time bye\n```\n\n\n\n#4:\n\n\n\n```bash\n$ ssh  < ./ex.bash serverA \"bash -s -- --time bye\"\n--time bye\n```\n\n\n\nNOTE:\n Just to make it clear that wherever the redirection appears on the command line makes no difference, because \nssh\n calls a remote shell with the concatenation of its arguments anyway, quoting doesn't make much difference, except when you need quoting on the remote shell like in example \n#4:\n\n\n\n```bash\n$ ssh  < ./ex.bash serverA \"bash -s -- '<--time bye>' '<end>'\"\n<--time bye> <end>\n```",
    "url": "https://unix.stackexchange.com/questions/87405/how-can-i-execute-local-script-on-remote-machine-and-include-arguments"
  },
  {
    "question_title": "Can grep return true/false or are there alternative methods",
    "question_body": "As a part of this script, I need to be able to check if the first argument given matches the first word of file. If it does, exit with an error message; if it doesn't, append the arguments to the file. I understand how to write the \nif\n statement, but not how to use \ngrep\n within a script. I understand that \ngrep\n will look something like this\n\n\n\n```bash\ngrep ^$1 schemas.txt\n```\n\n\n\nI feel like this should be much easier than I am making it.\n\n\nI'm getting an error \"too many arguments\" on the \nif\n statement. I got rid of the space between \ngrep -q\n and then got an error binary operator expected. \n\n\n\n```bash\nif [ grep -q ^$1 schemas.txt ]\nthen\n        echo \"Schema already exists. Please try again\"\n        exit 1\nelse\n        echo \"$@\" >> schemas.txt\nfi\n```",
    "answer": "grep\n returns a different exit code if it found something (zero) vs. if it hasn't found anything (non-zero). In an \nif\n statement, a zero exit code is mapped to \"true\" and a non-zero exit code is mapped to false. In addition, grep has a \n-q\n argument to not output the matched text (but only return the exit status code)\n\n\nSo, you can use grep like this:\n\n\n\n```bash\nif grep -q PATTERN file.txt; then\n    echo found\nelse\n    echo not found\nfi\n```\n\n\n\nAs a quick note, when you do something like \nif [ -z \"$var\" ]…\n, it turns out that \n[\n is actually a command you're running, just like grep. On my system, it's \n/usr/bin/[\n. (Well, technically, your shell probably has it built-in, but that's an optimization. It behaves as if it were a command). It works the same way, \n[\n returns a zero exit code for true, a non-zero exit code for false. (\ntest\n is the same thing as \n[\n, except for the closing \n]\n)",
    "url": "https://unix.stackexchange.com/questions/48535/can-grep-return-true-false-or-are-there-alternative-methods"
  },
  {
    "question_title": "How to get the pid of the last executed command in shell script?",
    "question_body": "I want to have a shell script like this:\n\n\n\n```bash\nmy-app &\necho $my-app-pid\n```\n\n\n\nBut I do not know how the get the pid of the just executed command.\n\n\nI know I can just use the \njobs -p my-app\n command to grep the pid. But if I want to execute the shell multiple times, this method will not work. Because the \njobspec\n is ambiguous.",
    "answer": "The PID of the most recently executed background (asynchronous) command is in the \n$!\n shell variable:\n\n\n\n```bash\nmy-app &\necho $!\n```",
    "url": "https://unix.stackexchange.com/questions/30370/how-to-get-the-pid-of-the-last-executed-command-in-shell-script"
  },
  {
    "question_title": "Detecting pattern at the end of a line with grep",
    "question_body": "If I do:\n\n\n\n```bash\n$ ls -R\n.:\n4Shared/  Cloud/\n\n./4Shared:\nUFAIZLV2R7.part3.rar\n\n./Cloud:\nUFAIZLV2R7.part2.rar.part\nUFAIZLV2R7.part1.rar.part\nUFAIZLV2R7.part4.rar.part\n```\n\n\n\nIf I want to list the \n.rar\n files only, and I use \ngrep\n, it will show me too the \n.rar.part\n files, what is not my wish.\n\nI am solving this using \nfind\n or \nls **/*.rar\n as told in \nthis thread\n and they work fine, but I would like to learn if it is possible to do it via \ngrep\n.  \n\n\nI have tried (thinking about \nEOL\n):\n\n\n\n```bash\nls -R | grep \".rar\\n\"\n```\n\n\n\nwith no results.\n\nI think that the problem lies in discover if the greping is found \nat the end of the line\n, but I am not sure.  \n\n\nAny help out here, please?",
    "answer": "The \n$\n anchor matches the end of a line.\n\n\n\n```bash\nls -R | grep '\\.rar$'\n```\n\n\n\nYou can also use \nfind\n for this:\n\n\n\n```bash\nfind . -name '*.rar'\n```",
    "url": "https://unix.stackexchange.com/questions/124462/detecting-pattern-at-the-end-of-a-line-with-grep"
  },
  {
    "question_title": "Estimate the size of the extracted files before extraction a `tar.gz` archive?",
    "question_body": "Before using \ntar\n to extract a \n.tar.gz\n archive, it is possible to get an estimate of how large the extracted files are in total?",
    "answer": "For \ngzip\n:\n\n\n\n```bash\n$ gzip -l binutils-2.24.tar.gz\n         compressed        uncompressed  ratio uncompressed_name\n           30809913           186997248  83.5% binutils-2.24.tar\n```\n\n\n\nNow you see a compressed and an uncompressed size of the content.\n\n\nOr alternatively use that command:\n\n\n\n```bash\n$ zcat binutils-2.24.tar.gz | wc --bytes\n186997248\n```\n\n\n\nFor \nbzip2\n, there is \nbzcat\n:\n\n\n\n```bash\n$ bzcat binutils-2.24.tar.bz2 | wc -c\n186997248\n```\n\n\n\nFor \nrar\n, use:\n\n\n\n```bash\n$ unrar l archive.rar\n...\n    1        465769002 102749558  22%\n```\n\n\n\nIn the last line of the output there is the original size in bytes (the second digit).\n\n\nFor \nzip\n, use\n\n\n\n```bash\n$ unzip -l archive.zip\n...\n700136                     4 files\n```\n\n\n\nAlso the last line (the first digit)",
    "url": "https://unix.stackexchange.com/questions/166709/estimate-the-size-of-the-extracted-files-before-extraction-a-tar-gz-archive"
  },
  {
    "question_title": "How to do max-depth search in ack and grep?",
    "question_body": "Is there any way to tell \nack\n to only search for text on the current folder? (or specify a \nmax-depth\n level?) And with \ngrep\n?",
    "answer": "You can couple find with the \n-exec\n argument. Example:\n\n\n\n```bash\nfind . -maxdepth 1 -exec grep foo {} \\;\n```\n\n\n\nThis can be scaled, i.e. \n-maxdepth 2\n.\n\n\nEdit\n\n\nAs mentioned in the [answer by @Stéphane Chazelas], it is advisable to restrict \nfind\n to regular files so that \ngrep\n doesn't produce an error when the argument \n{}\n actually is a directory path:\n\n\n\n```bash\nfind . -maxdepth 1 -type f -exec grep -H foo {} \\;\n```\n\n\n\n\n\n-type f\n is a filter for \nfind\n that limits the search results to files\n\n\n-H\n is a \ngrep\n option used to print a filename for every match (desired behavior when more than one file match)",
    "url": "https://unix.stackexchange.com/questions/111722/how-to-do-max-depth-search-in-ack-and-grep"
  },
  {
    "question_title": "How do I pipe ls to grep, and delete the files filtered by grep?",
    "question_body": "I wanted to delete some package in my home file, but the filename was too long (\ngoogle-chrome-stable_current_i386.deb\n). So, I decided to use the command \nls|grep chrome|rm\n to pipe the files to grep to filter out the chrome file, and then remove it. It didn't work, so I would like to see how I can do this.",
    "answer": "This almost made me wince.\n\n\n\n\nYou might want to stop pointing that \nshotgun\n at your foot. Basically any kind of parsing of \nls\n is going to be more \ncomplicated and error-prone\n than established methods like \nfind [...] -exec\n or \nglobs\n.\n\n\nUnless someone installed a troll distro for you, your shell has \nTab\n completion. Just type \nrm google\n and press \nTab\n. If it doesn't complete immediately, press \nTab\n again to see a list of matching files. Type more characters of the filename to narrow it down until it does complete, then run the command.\n\n\nPipes\n != \nparameters\n. Standard input is a \nbinary data stream\n which can be fed to a command asynchronously. Parameters are space separated strings which are passed once and only once to a command when running it. These are very rarely interchangeable.",
    "url": "https://unix.stackexchange.com/questions/247924/how-do-i-pipe-ls-to-grep-and-delete-the-files-filtered-by-grep"
  },
  {
    "question_title": "How to print the first line using grep command?",
    "question_body": "I have a file called \nfile.txt\n. How can I print the first line only using the \ngrep\n command?",
    "answer": "Although it's an unconventional application of grep, you can do it in GNU grep using\n\n\n\n```bash\ngrep -m1 \"\" file.txt\n```\n\n\n\nIt works because the empty expression matches anything, while \n-m1\n causes grep to exit after the first match\n\n\n\n```bash\n-m NUM, --max-count=NUM\n       Stop reading a file after NUM matching lines.\n```",
    "url": "https://unix.stackexchange.com/questions/294486/how-to-print-the-first-line-using-grep-command"
  },
  {
    "question_title": "How to use &quot;less -F&quot; without &quot;-X&quot;, but still display output if only one page?",
    "question_body": "I'm tweaking the pager of Git, but I've got some issues with it.\n\n\nWhat I want is:\n\n\n\n\nAlways colored output\n\n\nScrolling by touchpad or mouse\n\n\nQuit-if-one-screen\n\n\n\n\nAnd my current configuration is:\n\n\n\n```bash\n$ git config --global core.pager\nless -+F -+X -+S\n```\n\n\n\nThis does everything except the last one.\n\n\nBut, if I remove \n-+F\n, there will be no output in case of one-screen. If I remove \n-+X\n as well, the output is back but I cannot scroll by touchpad in \nless\n.\n\n\nIs there a workaround which can meet all the requirements above?",
    "answer": "UPDATE\n\n\ntl;dr Solution: upgrade to less 530\n\n\nFrom \nhttp://www.greenwoodsoftware.com/less/news.530.html\n:\n\n\n\n> Don't output terminal init sequence if using -F and file fits on one screen.\n\n\n\nSo with this fix we don't even need to bother determining whether to use \n-X\n on our own, \nless -F\n just takes care of it.\n\n\nPS. Some other less configs that I use:\n\n\n\n```bash\nexport PAGER='less -F -S -R -M -i'\nexport MANPAGER='less -R -M -i +Gg'\ngit config --global core.pager 'less -F -S -R -i'\n#alias less='less -F -S -R -M -i'\n```\n\n\n\n\n\nI eventually ended up with writing a wrapper on my own.\n\n\n\n```bash\n#!/usr/local/bin/bash\n\n# BSD/OSX compatibility\n[[ $(type -p gsed) ]] && SED=$(type -p gsed) || SED=$(type -p sed)\nCONTEXT=$(expand <&0)\n[[ ${#CONTEXT} -eq 0 ]] && exit 0\nCONTEXT_NONCOLOR=$( $SED -r \"s/\\x1B\\[([0-9]{1,2}(;[0-9]{1,2})?)?[mGK]//g\" <<< \"$CONTEXT\")\nLINE_COUNT=$( (fold -w $(tput cols) | wc -l) <<< \"$CONTEXT_NONCOLOR\" )\n\n[[ $LINE_COUNT -ge $(tput lines) ]] && less -+X -+S -R <<< \"$CONTEXT\" || echo \"$CONTEXT\"\n```\n\n\n\nBSD/OSX users should manually install \ngnu-sed\n. The amazing regexp, which helps remove color codes, is from \nhttps://stackoverflow.com/a/18000433/2487227\n\n\nI've saved this script to \n/usr/local/bin/pager\n and then \ngit config --global core.pager /usr/local/bin/pager\n\n\nThe treatment for OCD patients, hooray!",
    "url": "https://unix.stackexchange.com/questions/329093/how-to-use-less-f-without-x-but-still-display-output-if-only-one-page"
  },
  {
    "question_title": "What makes grep consider a file to be binary?",
    "question_body": "I have some database dumps from a Windows system on my box. They are text files. I'm using cygwin to grep through them. These appear to be plain text files; I open them with text editors such as notepad and wordpad and they look legible. However, when I run grep on them, it will say \nbinary file foo.txt matches\n.\n\n\nI have noticed that the files contain some ascii \nNUL\n characters, which I believe are artifacts from the database dump.\n\n\nSo what makes grep consider these files to be binary? The \nNUL\n character? Is there a flag on the filesystem? What do I need to change to get grep to show me the line matches?",
    "answer": "If there is a \nNUL\n character anywhere in the file, grep will consider it as a binary file.\n\n\nThere might a workaround like this \ncat file | tr -d '\\000' | yourgrep\n to eliminate all null first, and then to search through file.",
    "url": "https://unix.stackexchange.com/questions/19907/what-makes-grep-consider-a-file-to-be-binary"
  },
  {
    "question_title": "How do I delete the first n lines and last line of a file using shell commands?",
    "question_body": "I have a file named \nElement_query\n containing the result of a query :\n\n\n\n```bash\nSQL> select count (*) from element;\n\n[Output of the query which I want to keep in my file]\n\nSQL> spool off;\n```\n\n\n\nI want to delete 1st line and last line using shell command.",
    "answer": "Using GNU \nsed\n:\n\n\n\n```bash\nsed -i '1d;$d' Element_query\n```\n\n\n\nHow it works :\n\n\n\n\n-i\n option edit the file itself. You could also remove that option and redirect the output to a new file or another command if you want.\n\n\n1d\n deletes the first line (\n1\n to only act on the first line, \nd\n to delete it)\n\n\n$d\n deletes the last line (\n$\n to only act on the last line, \nd\n to delete it)\n\n\n\n\nGoing further :\n\n\n\n\nYou can also delete a range. For example, \n1,5d\n would delete the first 5 lines.\n\n\nYou can also delete every line that begins with \nSQL>\n using the statement \n/^SQL> /d\n\n\nYou could delete every blank line with \n/^$/d\n\n\nFinally, you can combine any of the statement by separating them with a semi-colon (\nstatement1;statement2;satement3;...\n) or by specifying them separately on the command line (\n-e 'statement1' -e 'statement 2' ...\n)",
    "url": "https://unix.stackexchange.com/questions/209068/how-do-i-delete-the-first-n-lines-and-last-line-of-a-file-using-shell-commands"
  },
  {
    "question_title": "Is there a &#39;rc&#39; configuration file for grep/egrep? (~/.egreprc?)",
    "question_body": "I usually use \ngrep\n when developing, and there are some extensions that I always want to exclude (like \n*.pyc\n).\n\n\nIs it possible to create a \n~/.egreprc\n or something like that, and add filtering to exclude \npyc\n files from all results?\n\n\nIs this possible, or will I have to create an alias for using \ngrep\n in this manner, and call the alias instead of \ngrep\n?",
    "answer": "No, there's no rc file for grep. \n\n\nGNU grep 2.4 through 2.21 applied options from the environment variable \nGREP_OPTIONS\n, but more recent versions no longer honor it.\n\n\nFor interactive use, define an alias in your shell initialization file (\n.bashrc\n or \n.zshrc\n). I use a variant of the following:\n\n\n\n```bash\nalias regrep='grep -Er --exclude=*~ --exclude=*.pyc --exclude-dir=.bzr --exclude-dir=.git --exclude-dir=.svn'\n```\n\n\n\nIf you call the alias \ngrep\n, and you occasionally want to call \ngrep\n without the options, type \n\\grep\n. The backslash bypasses the alias.",
    "url": "https://unix.stackexchange.com/questions/8214/is-there-a-rc-configuration-file-for-grep-egrep-egreprc"
  },
  {
    "question_title": "How can I grep in PDF files?",
    "question_body": "Is there a way to search PDF files using grep, without converting to text first in Ubuntu?",
    "answer": "Install the package \npdfgrep\n, then use the command:\n\n\n\n```bash\nfind /path -iname '*.pdf' -exec pdfgrep pattern {} +\n```\n\n\n\n——————\n\n\nSimplest way to do that:\n\n\n\n```bash\npdfgrep 'pattern' *.pdf\npdfgrep 'pattern' file.pdf\n```",
    "url": "https://unix.stackexchange.com/questions/6704/how-can-i-grep-in-pdf-files"
  },
  {
    "question_title": "How can I make &quot;Press any key to continue&quot;",
    "question_body": "I'm making a script to install my theme, after it finished installing it will appear the changelog and there will be \"Press any key to continue\" so that after users read the changelog then press any key to continue",
    "answer": "You can use the \nread\n command. If you are using \nbash\n:\n\n\n\n```bash\nread -p \"Press enter to continue\"\n```\n\n\n\nIn other shells, you can do:\n\n\n\n```bash\nprintf \"%s \" \"Press enter to continue\"\nread ans\n```\n\n\n\nAs mentioned in the comments above, this command does actually require the user to press \nenter\n; a solution that works with any key in \nbash\n would be:\n\n\n\n```bash\nread -n 1 -s -r -p \"Press any key to continue\"\n```\n\n\n\nExplanation by \nRayne\n and \nwchargin\n\n\n-n\n defines the required character count to stop reading\n\n\n-s\n hides the user's input\n\n\n-r\n causes the string to be interpreted \"raw\" (without considering backslash escapes)",
    "url": "https://unix.stackexchange.com/questions/293940/how-can-i-make-press-any-key-to-continue"
  },
  {
    "question_title": "How to get trailing data of gzip archive?",
    "question_body": "I have a gzip archive with trailing data. If I unpack it using \ngzip -d\n it tells me: \"\ndecompression OK, trailing garbage ignored\n\" (same goes for \ngzip -t\n which can be used as a method of detecting that there is such data).\n\n\nNow I would like to get to know this garbage, but strangely enough I couldn't find any way to extract it. \ngzip -l --verbose\n tells me that the \"compressed\" size of the archive is the size of the file (i.e. with the trailing data), that's wrong and not helpful. \nfile\n is also of no help, so what can I do?",
    "answer": "Figured out now how to get the trailing data.\n\n\nI created Perl script which creates a file with the trailing data, it's heavily based on \nhttps://bugs.debian.org/cgi-bin/bugreport.cgi?bug=604617#10\n:\n\n\n\n```bash\n#!/usr/bin/perl\nuse strict;\nuse warnings; \n\nuse IO::Uncompress::Gunzip qw(:all);\nuse IO::File;\n\nunshift(@ARGV, '-') unless -t STDIN;\n\nmy $input_file_name = shift;\nmy $output_file_name = shift;\n\nif (! defined $input_file_name) {\n  die <<END;\nUsage:\n\n  $0 ( GZIP_FILE | - ) [OUTPUT_FILE]\n\n  ... | $0 [OUTPUT_FILE]\n\nExtracts the trailing data of a gzip archive.\nOutputs to stdout if no OUTPUT_FILE is given.\n- as input file file causes it to read from stdin.\n\nExamples:\n\n  $0 archive.tgz trailing.bin\n\n  cat archive.tgz | $0\n\nEND\n}\n\nmy $in = new IO::File \"<$input_file_name\" or die \"Couldn't open gzip file.\\n\";\ngunzip $in => \"/dev/null\",\n  TrailingData => my $trailing;\nundef $in;\n\nif (! defined $output_file_name) {\n  print $trailing;\n} else {\n  open(my $fh, \">\", $output_file_name) or die \"Couldn't open output file.\\n\";\n  print $fh $trailing;\n  close $fh;\n  print \"Output file written.\\n\";\n}\n```",
    "url": "https://unix.stackexchange.com/questions/295702/how-to-get-trailing-data-of-gzip-archive"
  },
  {
    "question_title": "Create symlink - overwrite if one exists",
    "question_body": "I want to take down data in \n/path/to/data/folder/month/date/hour/minute/file\n and symlink it to \n/path/to/recent/file\n and do this automatically every time a file is created.\n\n\nAssuming I will not know ahead of time if \n/path/to/recent/file\n exists, how can I go about creating it (if it doesn't exist) or replacing it (if it does exist)? I am sure I can just check if it exists and then do a delete, symlink, but I'm wondering if there is a simple command which will do what I want in one step.",
    "answer": "This is the purpose of \nln\n's \n-f\n option: it removes existing destination files, if any, before creating the link.\n\n\n\n```bash\nln -sf /path/to/data/folder/month/date/hour/minute/file /path/to/recent/file\n```\n\n\n\nwill create the symlink \n/path/to/recent/file\n pointing to \n/path/to/data/folder/month/date/hour/minute/file\n, replacing any existing file or symlink to a file if necessary (and working fine if nothing exists there already).\n\n\nIf a directory, or symlink to a directory, already exists with the target name, the symlink will be created inside it (so you'd end up with \n/path/to/recent/file/file\n in the example above). The \n-n\n option, available in some versions of \nln\n, will take care of symlinks to directories for you, replacing them as necessary:\n\n\n\n```bash\nln -sfn /path/to/data/folder/month/date/hour/minute/file /path/to/recent/file\n```\n\n\n\nPOSIX \nln\n doesn’t specify \n-n\n so you can’t rely on it generally. Much of \nln\n’s behaviour is implementation-defined so you really need to check the specifics of the system you’re using. If you’re using \nGNU \nln\n, you can use the \n-t\n and \n-T\n options too, to make its behaviour fully predictable in the presence of directories (\ni.e.\n fail instead of creating the link inside the existing directory with the same name).",
    "url": "https://unix.stackexchange.com/questions/207294/create-symlink-overwrite-if-one-exists"
  },
  {
    "question_title": "pv (progress bar) and gzip",
    "question_body": "Why is this not possible?\n\n\n\n```bash\npv ${dest_file} | gzip -1\n```\n\n\n\npv\n is a progress bar\n\n\nerror\n\n\n\n```bash\ngzip: compressed data not written to a terminal. Use -f to force compression.\nFor help, type: gzip -h\n   0 B 0:00:00 [   0 B/s] [>                                   ]  0%\n```\n\n\n\nThis works\n\n\n\n```bash\npv ${file_in} | tar -Jxf - -C /outdir\n```",
    "answer": "What are you trying to achieve is to see the progress bar of the compression process. But it is not possible using \npv\n. It shows only transfer progress, which you can achieve by something like this (anyway, it is the \nfirst link\n in the google):\n\n\n\n```bash\npv input_file | gzip > compressed_file\n```\n\n\n\nThe progress bar will run fast, and then it will wait for compression, which is not observable anymore using \npv\n.\n\n\nBut you can do that other way round and watch the output stream, bot here you will not be able to see the actual progress, because \npv\n does not know the actual size of the compressed file:\n\n\n\n```bash\ngzip <input_file | pv > compressed_file\n```\n\n\n\nThe best I found so far is the one from \ncommandlinefu\n even with rate limiting and compression of directories:\n\n\n\n```bash\n$D=directory\ntar pcf - $D | pv -s $(du -sb $D | awk '{print $1}') --rate-limit 500k | gzip > target.tar.gz\n```",
    "url": "https://unix.stackexchange.com/questions/251691/pv-progress-bar-and-gzip"
  },
  {
    "question_title": "git completion with zsh: filenames with spaces aren&#39;t being escaped properly",
    "question_body": "Git completion:\n\n\nI'm having difficulty with git's filename autocompletions on my system. I'm using \nzsh\n (5.0.5) with \ngit\n (1.9.3) on OS X (10.9.3). Both \nzsh\n and \ngit\n have been installed via homebrew. (Full version output are at the bottom of the post.)\n\n\ngit\n's filename completion isn't inserting spaces like I expect. When I type the name of a file with a space in the name, the shell inserts the filename without spaces escaped. \nzsh\n's built-in completion doesn't do this, but \ngit\n's does.\n\n\nHere's an example of what I'm seeing.\n\n\nI have a repository with a few files with spaces in their names.\n\n\n\n```bash\n% ls -la\ntest\ntest four - latest.txt\ntest three.txt\ntest two\n```\n\n\n\nThe shell backslash escapes the filenames as expected when I use tab completion to insert the file name.\n\n\n\n```bash\n% echo \"testing\" >> test<tab>\n```\n\n\n\nautocompletes to this after hitting tab three times.\n\n\n\n```bash\n% echo \"testing\" >> test\\ four\\ -\\ latest.txt\n––– file\ntest                       test\\ four\\ -\\ latest.txt  test\\ three.txt            test\\ two\n```\n\n\n\ngit status\n shows these filenames in quotes (it totally understands what's up):\n\n\n\n```bash\n% git status --short\n M test\n M \"test four - latest.txt\"\n M \"test three.txt\"\n M \"test two\"\n```\n\n\n\nbut when I try to \ngit add\n with tab autocompletion, it goes sideways.\n\n\n\n```bash\n% git add test<tab>\n```\n\n\n\nresults in this after hitting tab three times:\n\n\n\n```bash\n% git add test four - latest.txt\ntest                    test four - latest.txt  test three.txt          test two\n```\n\n\n\nI've tried regressing this a bit: my dotfiles are in version control, so I've tried \nzsh 4.3.15\n, \ngit 1.8.3\n, and my dotfiles from a year ago, when I'm nearly certain this worked. Weirdly, this setup was still broken.\n\n\nI \nhave\n narrowed it down to the \n_git\n completion file that is being sourced from \n/usr/local/share/zsh/site-functions\n:\n\n\n\n```bash\n% echo $FPATH\n/usr/local/share/zsh/site-functions:/usr/local/Cellar/zsh/5.0.5/share/zsh/functions\n% ls -l /usr/local/share/zsh/site-functions\n_git@ -> ../../../Cellar/git/1.9.3/share/zsh/site-functions/_git\n_hg@ -> ../../../Cellar/mercurial/3.0/share/zsh/site-functions/_hg\n_j@ -> ../../../Cellar/autojump/21.7.1/share/zsh/site-functions/_j\ngit-completion.bash@ -> ../../../Cellar/git/1.9.3/share/zsh/site-functions/git-completion.bash\ngo@ -> ../../../Cellar/go/HEAD/share/zsh/site-functions/go\n```\n\n\n\nIf I manually change \n$FPATH\n before my \n.zshrc\n runs \ncompinit\n (or simply remove the \n/usr/local/share/zsh/site-functions/_git\n symbolic link), then completions fall back to \nzsh\n and work as expected.\n\n\nThe \nzsh\n completion without \n_git\n:\n\n\n\n```bash\n% git add test<tab>\n```\n\n\n\nhitting tab three times produces correct results:\n\n\n\n```bash\n% git add test\\ four\\ -\\ latest.txt\n––– modified file\ntest                       test\\ four\\ -\\ latest.txt  test\\ three.txt            test\\ two\n```\n\n\n\nSide note: I've tried removing the \ngit-completion.bash\n link, and it just totally breaks things:\n\n\n\n```bash\n% git add test<tab>\n```\n\n\n\nproduces this busted-ness:\n\n\n\n```bash\n% git add test__git_zsh_bash_func:9: command not found: __git_aliased_command\n    git add test\n––– file\ntest                       test\\ four\\ -\\ latest.txt  test\\ three.txt            test\\ two\n```\n\n\n\n\n\nI \nreally\n want to get this working properly: the rest of the \n_git\n completions were great because they're more repo-aware than the \nzsh\n ones, but I need filenames with spaces or other special characters to be properly escaped.\n\n\n\n\nSoftware versions:\n\n\n\n```bash\n% zsh --version\nzsh 5.0.5 (x86_64-apple-darwin13.0.0)\n\n% git --version\ngit version 1.9.3\n\n% sw_vers\nProductName:    Mac OS X\nProductVersion: 10.9.3\nBuildVersion:   13D65\n```\n\n\n\n\n\nI've uploaded the \n_git\n and \ngit-completion.bash\n files: \ngit-completion.bash\n and \n_git\n (renamed to \n_git.sh\n so CloudApp will make it viewable in the browser.)",
    "answer": "This bug is mentioned on the \nmailing list\n.\n\n\nThe fix is to edit the file \ngit-completion.zsh\n and remove the \n-Q\n option from \ncompadd\n, in in \n__gitcomp_file\n.\n\n\n\n```bash\n--- i/contrib/completion/git-completion.zsh\n+++ w/contrib/completion/git-completion.zsh\n@@ -90,7 +90,7 @@ __gitcomp_file ()\n\n    local IFS=$'\\n'\n    compset -P '*[=:]'\n-   compadd -Q -p \"${2-}\" -f -- ${=1} && _ret=0\n+   compadd -p \"${2-}\" -f -- ${=1} && _ret=0\n }\n\n __git_zsh_bash_func ()\n```\n\n\n\nThis file is installed from the \ncontrib/completion\n directory, and its path may vary with your package manager. If you installed with homebrew on macOS, it's located in \n/usr/local/Cellar/git/2.10.2/share/zsh/site-functions\n.",
    "url": "https://unix.stackexchange.com/questions/132290/git-completion-with-zsh-filenames-with-spaces-arent-being-escaped-properly"
  },
  {
    "question_title": "What is a &quot;loop device&quot; when mounting?",
    "question_body": "I am mounting an ISO file, and looking at \nthis tutorial\n. They use the command:\n\n\n\n```bash\n$ mount -o loop disk1.iso /mnt/disk\n```\n\n\n\nI'm trying to understand the use of \n-o loop\n. I have two questions:\n\n\n\n\nWhen I look at the long man page for mount, it takes time to find that \n-o\n option. If I do \nman mount | grep \"-o\"\n I get an error, and when I look in the file I do not find any info that \"loop\" is a command text for option \n-o\n. Where is that documented?\n\n\nAlso, what is the \"loop device\" concept for mounting?",
    "answer": "A loop device is a pseudo (\"fake\") device (actually just a file) that acts as a \nblock-based device\n. You want to mount a file \ndisk1.iso\n that will act as an entire filesystem, so you use loop.\n\n\nThe \n-o\n is short for \n--options\n.\n\n\nAnd the last thing, if you want to search for \"-o\" you need to escape the '-'.\n\n\nTry:\n\n\n\n```bash\nman mount | grep \"\\-o\"\n```",
    "url": "https://unix.stackexchange.com/questions/4535/what-is-a-loop-device-when-mounting"
  },
  {
    "question_title": "Using git to manage /etc?",
    "question_body": "I am thinking on a system, where \n/etc\n were tracked on a remote git repository. I am thinking on a git workflow, where every host machine where a different branch.\n\n\nEvery previous versions on every machine could be easily tracked, compared, merged.\n\n\nIf a \n/etc\n modification had to be committed on many machines, it could be easily done by some merging script.\n\n\nIn case of an \"unwanted\" \n/etc\n change, this could be good visible (even alarm scripts could be tuned to watch that).\n\n\nAnybody used already a such configuration? Are there any security problems with it?",
    "answer": "The program \netckeeper\n does manage \n/etc\n in \ngit\n, you just need to change the default vcs backend from \nbzr\n to \ngit\n in \n/etc/etckeeper/etckeeper.conf\n.\n\n\nIt is installed by default in Ubuntu Linux, and handles the common cases of when to commit automatically.\n\nIt commits before installing packages in case there are uncomitted manual changes, and after installing.",
    "url": "https://unix.stackexchange.com/questions/138665/using-git-to-manage-etc"
  },
  {
    "question_title": "I would like to grep all files except some file types?",
    "question_body": "How do I recursively \ngrep\n files within a given folders except a couple file types?  \n\n\nFor example, I'm looking for a string within my workspace folder but it ends up searching inside sql files and generates serialized strings.\n\n\nSo in this case, I'd like to \ngrep\n the workspace folder except sql files.\n\n\nI'm preferably looking for a one-liner if possible.",
    "answer": "If you have GNU \ngrep\n you can use the \n--exclude=GLOB\n option, like\n\n\n\n```bash\ngrep -r --exclude='*.sql' pattern dir/\n```",
    "url": "https://unix.stackexchange.com/questions/31224/i-would-like-to-grep-all-files-except-some-file-types"
  },
  {
    "question_title": "How to add/update a file to an existing tar.gz archive?",
    "question_body": "Is there a way to add/update a file in a tar.gz archive? Basically, I have an archive which contains a file at \n/data/data/com.myapp.backup/./files/settings.txt\n and I'd like to pull that file from the archive (already done) and push it back into the archive once the edit has been done. How can I accomplish this? Is it problematic because of the \n.\n in the path?",
    "answer": "To pull your file from your archive, you can use \ntar xzf archive.tar.gz my/path/to/file.txt\n.  Note that the directories in the file's path will be created as well.  Use \ntar t\n (i.e. \ntar tzf archive.tar.gz\n) to list the files in the archive.\n\n\ntar\n does not support \"in-place\" updating of files.  However, you can add files to the end of an archive, even if they have the same path as a file already in the archive.  In that case, both copies of the file will be in the archive, and the file added later will override the earlier one.  The command to use for this is \ntar r\n (or \ntar u\n to only add files that are newer than the archive) is the command to use.  The \n.\n in the path should not be a problem.\n\n\nThere is a catch, though: you can't add to a compressed archive.  So you would have to do:\n\n\n\n```bash\ngunzip archive.tar.gz\ntar rf archive.tar data/data/com.myapp.backup/./files/settings.txt\ngzip archive.tar\n```\n\n\n\nWhich is probably not what you want to hear, since it means rewriting the entire archive twice over.  If it's not a very large archive, it might be better to untar the whole thing and then re-tar it after editing.  Alternately, you could use an uncompressed archive.",
    "url": "https://unix.stackexchange.com/questions/13093/how-to-add-update-a-file-to-an-existing-tar-gz-archive"
  },
  {
    "question_title": "Zsh git filename completion with &quot;--git-dir=... --work-tree=...&quot;: not a git repository",
    "question_body": "I track my dotfiles using this method:\n\n\n\n\nA bare repository resides in \n$HOME/repos/dotfiles\n.\n\n\nAll my dotfiles reside in their normal location, e.g. \n$HOME/.vim/vimrc\n, not \n$HOME/repos/dotfiles/vimrc\n.\n\n\nI run \ngit --git-dir=$HOME/repos/dotfiles --work-tree=$HOME ...\n to manage things.\n\n\n\n\n(Actually, I have a function \ng()\n that expands to the above command when I'm in \n$HOME\n, and to just \ngit\n otherwise.)\n\n\nEverything works great, except...\n\n\nThe problem:\n Zsh git filename completion doesn't work.\n\n\nExample:\n\n\n\n```bash\n% pwd\n/home/brian\n% g status                                                                                                                                         ~\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n        modified:   .vim/vimrc\n        modified:   .xmonad/xmonad.hs\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n% g add <Tab>\nCompleting not a git repository\n```\n\n\n\n(The \"Completing ...\" stuff is due to \nzstyle ':completion:*' format $'%{\\e[0;31m%}Completing %B%d%b%{\\e[0m%}'\n.)\n\n\nNotably, it would not suffice to somehow tell zsh's git completion to \"move to/follow\" the given value of \n--work-tree\n, as if \ngit\n were being called from that directory, because explicitly doing that doesn't work either:\n\n\n\n```bash\n% cd repos/dotfiles\n% g status\nfatal: This operation must be run in a work tree\n% g add <Tab>\nCompleting not a git repository\n```\n\n\n\nThe question:\n Is there an easy way to get extend zsh's git completion to this kind of a case?",
    "answer": "Sadly, its a bug in \nzsh\n completion for git. You can find discussion on 'zsh' mailing list \nhere\n.\n\n\nDaniel Shahaf did provided a patch for '_git':\n\n\n\n```bash\ndiff --git a/Completion/Unix/Command/_git b/Completion/Unix/Command/_git\nindex 518e6d198..45a0fa622 100644\n--- a/Completion/Unix/Command/_git\n+++ b/Completion/Unix/Command/_git\n@@ -6609,20 +6609,33 @@ __git_files_relative () {\n (( $+functions[__git_files] )) ||\n __git_files () {\n   local compadd_opts opts tag description gitcdup gitprefix files expl\n+  local pref\n\n   zparseopts -D -E -a compadd_opts V: J: 1 2 n f X: M: P: S: r: R: q F:\n   zparseopts -D -E -a opts -- -cached -deleted -modified -others -ignored -unmerged -killed x+: --exclude+:\n   tag=$1 description=$2; shift 2\n\n-  gitcdup=$(_call_program gitcdup git rev-parse --show-cdup 2>/dev/null)\n-  __git_command_successful $pipestatus || return 1\n+  case $(_call_program gitinworktree git rev-parse --is-inside-work-tree 2>/dev/null) in\n+    (true)\n+      gitcdup=$(_call_program gitcdup git rev-parse --show-cdup 2>/dev/null)\n+      __git_command_successful $pipestatus || return 1\n\n-  gitprefix=$(_call_program gitprefix git rev-parse --show-prefix 2>/dev/null)\n-  __git_command_successful $pipestatus || return 1\n+      gitprefix=$(_call_program gitprefix git rev-parse --show-prefix 2>/dev/null)\n+      __git_command_successful $pipestatus || return 1\n+\n+      local pref=$gitcdup$gitprefix$PREFIX\n+      ;;\n+    (false)\n+      local pref=\n+      ;;\n+    (*)\n+      # XXX what to do?\n+      return 1\n+      ;;\n+  esac\n\n   # TODO: --directory should probably be added to $opts when --others is given.\n\n-  local pref=$gitcdup$gitprefix$PREFIX\n\n   # First allow ls-files to pattern-match in case of remote repository\n   files=(${(0)\"$(_call_program files git ls-files -z --exclude-standard ${(q)opts} -- ${(q)${pref:+$pref\\\\\\*}} 2>/dev/null)\"})\n@@ -7585,7 +7598,8 @@ _git() {\n         ;;\n       (option-or-argument)\n         curcontext=${curcontext%:*:*}:git-$words[1]:\n-       (( $+opt_args[--git-dir] )) && local -x GIT_DIR=$opt_args[--git-dir]\n+        (( $+opt_args[--git-dir] )) && local -x GIT_DIR=${(e)opt_args[--git-dir]}\n+        (( $+opt_args[--work-tree] )) && local -x GIT_WORK_TREE=${(e)opt_args[--work-tree]}\n        if ! _call_function ret _git-$words[1]; then\n          if zstyle -T :completion:$curcontext: use-fallback; then\n            _default && ret=0\n```\n\n\n\nIt applies clearly to \nzsh 5.4.1\n but it didn't work form me,YMMV.\n\n\nI'll update this answer as the issue is being worked on.\n\n\nEDIT:\n\n\nWith the above patch it does work, but where you put \nadd\n is important - it has to be at the end:\n\n\n\n```bash\ngit --git-dir=$HOME/.dotfiles --work-tree=$HOME/ add\n```\n\n\n\nOne more thing worth noticing - its somewhat slow.",
    "url": "https://unix.stackexchange.com/questions/350797/zsh-git-filename-completion-with-git-dir-work-tree-not-a-git-repo"
  },
  {
    "question_title": "How do you extract a single folder from a large tar.gz archive?",
    "question_body": "I am using this command on a \n5GB\n archive\n\n\n\n```bash\ntar -zxvf archive.tar.gz /folder/in/archive\n```\n\n\n\nis this the correct way to do this?  It seems to be taking forever with no command line output...",
    "answer": "tar\n stores relative paths \nby default\n. GNU tar even says so if you try to store an absolute path:\n\n\n\n```bash\ntar -cf foo.tar /home/foo\ntar: Removing leading `/' from member names\n```\n\n\n\nIf you need to extract a particular folder, have a look at what's in the tar file:\n\n\n\n```bash\ntar -tvf foo.tar\n```\n\n\n\nAnd note the exact filename. In the case of my \nfoo.tar\n file, I could extract \n/home/foo/bar\n by saying:\n\n\n\n```bash\ntar -xvf foo.tar home/foo/bar # Note: no leading slash\n```\n\n\n\nSo no, the way you posted isn't (necessarily) the correct way to do it. You have to leave out the leading slash. If you want to simulate absolute paths, do \ncd /\n first and make sure you're the superuser. Also, this does the same:\n\n\n\n```bash\ntar -C / -xvf foo.tar home/foo/bar # -C is the ‘change directory’ option\n```\n\n\n\nThere are very obvious, good reasons why \ntar\n converts paths to relative ones. One is the ability to restore an archive in places other than its original source. The other is security. You could extract an archive, expect its files to appear in your current working directory, and instead overwrite system files (or your own work) elsewhere by mistake.\n\n\nNote: if you use the \n-P\n option, \ntar\n \nwill\n archive absolute paths. So it always pays to check the contents of big archives before extracting.",
    "url": "https://unix.stackexchange.com/questions/35311/how-do-you-extract-a-single-folder-from-a-large-tar-gz-archive"
  },
  {
    "question_title": "How to get execution time of a script effectively?",
    "question_body": "I would like to display the completion time of a script. \n\n\nWhat I currently do is -\n\n\n\n```bash\n#!/bin/bash\ndate  ## echo the date at start\n# the script contents\ndate  ## echo the date at end\n```\n\n\n\nThis just show's the time of start and end of the script. Would it be possible to display a fine grained output like processor time/ io time , etc?",
    "answer": "Just use \ntime\n when you call the script:\n\n\n\n```bash\ntime yourscript.sh\n```\n\n\n\nOutput (\"# comments not really in the output\"):\n\n\n\n```bash\nreal    2m5.034s # <-- Actual time taken from start to finish.\nuser    0m0.000s # <-- CPU time user-space.\nsys     0m0.003s # <-- CPU time kernel-space.\n```",
    "url": "https://unix.stackexchange.com/questions/52313/how-to-get-execution-time-of-a-script-effectively"
  },
  {
    "question_title": "How to print all lines after a match up to the end of the file?",
    "question_body": "Input file1 is:\n\n\n\n```bash\ndog 123 4335\ncat 13123 23424 \ndeer 2131 213132\nbear 2313 21313\n```\n\n\n\nI give the match the  pattern from in \nother file\n ( like \ndog 123 4335\n from   file2).\n\n\nI match the pattern of the line is \ndog 123 4335\n and after printing\nall lines without match line my output is: \n\n\n\n```bash\ncat 13123 23424\ndeer 2131 213132\nbear 2313 21313\n```\n\n\n\nIf only use without address of line only use the pattern, for example \n1s\n\nhow to  match and  print the lines?",
    "answer": "In practice, I'd probably use \nAet3miirah's answer\n most of the time, and \nalexey's answer\n is wonderful for navigating through the lines (also, it works with \nless\n). OTOH, I really like another approach (which is kind of the reversed \nGilles' answer\n):\n\n\n\n```bash\nsed -n '/dog 123 4335/,$p'\n```\n\n\n\nWhen called with the \n-n\n flag, \nsed\n does not print by default the lines it processes anymore. Then we use a 2-address form that says to apply a command from the line matching \n/dog 123 4335/\n until the end of the file (represented by \n$\n). The command in question is \np\n, which prints the current line. So, this means \"print all lines from the one matching \n/dog 123 4335/\n until the end.\"",
    "url": "https://unix.stackexchange.com/questions/56429/how-to-print-all-lines-after-a-match-up-to-the-end-of-the-file"
  },
  {
    "question_title": "Can&#39;t use exclamation mark (!) in bash?",
    "question_body": "I'm trying to use the curl command to access a http url with a exclamation mark (\n!\n) in its path. e.g:\n\n\n\n```bash\ncurl -v \"http://example.org/!287s87asdjh2/somepath/someresource\"\n```\n\n\n\nthe console replies with \nbash: ... event not found\n.\n\n\nWhat is going on here? and what would be the proper syntax to escape the exclamation mark?",
    "answer": "The exclamation mark is part of \nhistory expansion\n in bash.  To use it you need it \nenclosed in single quotes\n (eg: \n'http://example.org/!132'\n).\n\n\nYou might try to directly escape it with a backslash (\n\\\n) before the character (eg: \n\"http://example.org/\\!132\"\n). However, even though a backslash before the exclamation mark does prevent history expansion, the backslash is \nnot\n removed on some shells in such a case.  So it's better to use single quotes, so you're not passing a literal backslash to \ncurl\n as part of the URL.",
    "url": "https://unix.stackexchange.com/questions/33339/cant-use-exclamation-mark-in-bash"
  },
  {
    "question_title": "Delete any file mentioned in .gitignore",
    "question_body": "I have the following entries in a \n.gitignore\n file and I want to remove them. The reason why is because these files are temporary-junk files created during a TeX compilation and I want to be able to remove them.\n\n\nHow I can do that?",
    "answer": "```bash\ngit clean -Xdf\n```\n\n\n\nWith capital \n-X\n, \ngit clean\n \nonly\n removes files that match \n.gitignore\n.\n\n\nWith \n-d\n, \ngit clean\n also deletes entire directories that match \n.gitignore\n.\n\n\nWith \n-f\n, \ngit clean\n will actually delete things instead of warning you. Be careful though. There might be other things (like personal \ndirenv\n configuration) that are \n.gitignored\n for other reasons, so be sure it is always understood what this command will do before it is run.\n\n\nA special thanks to @Kusalananda for sharing \ngit clean\n.\n\n\n\n\nAs an alternative, if you have, say, a folder for dependencies that includes symlinked directories and you don't want \ngit clean\n to recursively delete everything \ninside\n those directories, you could use this longer form:\n\n\n\n```bash\ngit ls-files --others --ignored --exclude-standard --directory | head -c -1 | tr '\\n' '\\0' | xargs -0 -r rm -r\n```\n\n\n\ngit ls-files\n with these options lists the same directories as \ngit clean -Xdn\n, but more machine-readable.\n\n\nhead -c -1\n trims the trailing newline byte.\n\n\ntr '\\n' '\\0'\n translates all newlines to \nNUL\ns.\n\n\nxargs -0 -r rm -r\n deletes the files and directories. \n-0\n makes \nNUL\n the delimiter. \n-r\n causes it to skip empty input (warning: this is a non-portable GNU extension).",
    "url": "https://unix.stackexchange.com/questions/435960/delete-any-file-mentioned-in-gitignore"
  },
  {
    "question_title": "how to set up username and passwords for different git repos?",
    "question_body": "```bash\n─[$] cat ~/.gitconfig\n\n[user]\n    name = Shirish Agarwal\n    email = xxxx@xxxx.xxx\n[core]\n    editor = leafpad\n    excludesfiles = /home/shirish/.gitignore\n    gitproxy = \\\"ssh\\\" for gitorious.org\n[merge]\n    tool = meld\n[push]\n    default = simple\n[color]\n    ui = true\n    status = auto\n    branch = auto\n```\n\n\n\nNow I want to put my git credentials for github, gitlab and gitorious so each time I do not have to lookup the credentials on the browser. How can this be done so it's automated ?\n\n\nI am running zsh",
    "answer": "Using SSH\n\n\nThe common approach for handling git authentication is to delegate it to SSH. Typically you set your SSH public key in the remote repository (\ne.g.\n \non GitHub\n), and then you use that whenever you need to authenticate. You can use a key agent of course, either handled by your desktop environment or manually with \nssh-agent\n and \nssh-add\n.\n\n\nTo avoid having to specify the username, you can configure that in SSH too, in \n~/.ssh/config\n; for example I have\n\n\n\n```bash\nHost git.opendaylight.org\n  User skitt\n```\n\n\n\nand then I can clone using\n\n\n\n```bash\ngit clone ssh://git.opendaylight.org:29418/aaa\n```\n\n\n\n(note the absence of a username there).\n\n\nUsing \ngitcredentials\n\n\nIf the SSH approach doesn't apply (\ne.g.\n you're using a repository accessed over HTTPS), git does have its own way of handling credentials, using \ngitcredentials\n (and typically \ngit-credential-store\n). You specify your username using\n\n\n\n```bash\ngit config credential.${remote}.username yourusername\n```\n\n\n\nand the credential helper using\n\n\n\n```bash\ngit config credential.helper store\n```\n\n\n\n(specify \n--global\n if you want to use this setup everywhere).\n\n\nThen the first time you access a repository, git will ask for your password, and it will be stored (by default in \n~/.git-credentials\n). Subsequent accesses to the repository will use the stored password instead of asking you.\n\n\nWarning\n: This does store your credentials plaintext in your home directory. So it is inadvisable unless you understand what this means and are happy with the risk.",
    "url": "https://unix.stackexchange.com/questions/335704/how-to-set-up-username-and-passwords-for-different-git-repos"
  },
  {
    "question_title": "Are there naming conventions for variables in shell scripts?",
    "question_body": "Most languages have naming conventions for variables, the most common style I see in shell scripts is \nMY_VARIABLE=foo\n.  Is this the convention or is it only for global variables?  What about variables local to the script?",
    "answer": "Environment variables or shell variables introduced by the operating system, shell startup scripts, or the shell itself, etc., are usually all in \nCAPITALS\n1\n.\n\n\nTo prevent your variables from conflicting with these variables, it is a good practice to use \nlower_case\n variable names.\n\n\n\n\n1\nA notable exception that may be worth knowing about is the \npath\n array, used by the \nzsh\n shell.  This is the same as the common \nPATH\n variable but represented as an array.",
    "url": "https://unix.stackexchange.com/questions/42847/are-there-naming-conventions-for-variables-in-shell-scripts"
  },
  {
    "question_title": "How to define &#39;tab&#39; delimiter with &#39;cut&#39; in Bash?",
    "question_body": "Here is an example of using \ncut\n to break input into fields using a space delimiter, and obtaining the second field:\n\n\ncut -f2 -d' '\n \n\n\nHow can the delimiter be defined as a tab, instead of a space?",
    "answer": "Two ways:\n\n\nPress \nCtrl\n+\nV\n and then \nTab\n to use \"verbatim\" \nquoted insert\n.\n\n\n\n```bash\ncut -f2 -d'   ' infile\n```\n\n\n\nor write it like this to use \nANSI-C quoting\n:\n\n\n\n```bash\ncut -f2 -d$'\\t' infile\n```\n\n\n\nThe \n$'...'\n form of quotes isn't part of the POSIX shell language (\nnot yet\n), but works at least in ksh, mksh, zsh and Busybox in addition to Bash.",
    "url": "https://unix.stackexchange.com/questions/35369/how-to-define-tab-delimiter-with-cut-in-bash"
  },
  {
    "question_title": "Using grep and looking for unique occurrences",
    "question_body": "I have a text file of this type, and I would look for any lines containing the string \nValidating Classification\n and then obtain uniquely the reported errors. I do not know the types of possible errors.\n\n\nInput file: \n\n\n\n```bash\n201600415 10:40 Error Validating Classification: error1\n201600415 10:41 Error Validating Classification: error1\n201600415 10:42 Error Validating Classification: error2\n201600415 10:43 Error Validating Classification: error3\n201600415 10:44 Error Validating Classification: error3\n```\n\n\n\nOutput file\n\n\n\n```bash\n201600415 10:40 Error Validating Classification: error1\n201600415 10:42 Error Validating Classification: error2\n201600415 10:43 Error Validating Classification: error3\n```\n\n\n\nCan I achieve this using grep, pipes and other commands?",
    "answer": "You will need to discard the timestamps, but 'grep' and 'sort --unique' together can do it for you.\n\n\n\n```bash\ngrep --only-matching 'Validating Classification.*' | sort --unique\n```\n\n\n\nSo \ngrep -o\n will only show the parts of the line that match your regex (which is why you need to include the \n.*\n to include everything \nafter\n the \"Validating Classification\" match). Then once you have just the list of errors, you can use \nsort -u\n to get just the unique list of errors.",
    "url": "https://unix.stackexchange.com/questions/276741/using-grep-and-looking-for-unique-occurrences"
  },
  {
    "question_title": "Create an archive with command &quot;gzip&quot;",
    "question_body": "I have to create an archive with the command gzip (not tar - it's necessary) and the archive should contain files from another directory - for example, /etc.\nI tried to use command\n\n\n\n```bash\ngzip myetc.gz /etc\n```\n\n\n\nBut it didn't work.",
    "answer": "gzip\n works with files, not directories.  Therefore to create an archive of a directory you need to use \ntar\n (which will create a single tarball out of multiple files):\n\n\n\n```bash\ntar cvf myetc.tar /etc\n```\n\n\n\nor, for a gzipped archive:\n\n\n\n```bash\ntar cvzf myetc.tar.gz /etc\n```",
    "url": "https://unix.stackexchange.com/questions/284901/create-an-archive-with-command-gzip"
  },
  {
    "question_title": "tar: Unexpected EOF in archive",
    "question_body": "I was attempting to untar a \n.tar.gz\n file, but came across this error:\n\n\n\n```bash\ngzip: stdin: unexpected end of file\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n```\n\n\n\nThe \ntar.gz\n file includes a \n.tar\n file, which when untarred results in:\n\n\n\n```bash\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n```\n\n\n\nI tried both \n–ignore-zeros\n and \n–ignore-failed-read\n, although they both didn't work.\n\n\nIs there any way I could extract this file even if it is corrupted?\n\n\nThe file type in question: \n.tar.gz\n: Gzip Compressed Data, from a UNIX system.",
    "answer": "Check two items:\n\n\n(1) Is the \nFILE INCOMPLETE\n due to a faulty download?  Re-download, and use the -c option if your are using wget. (happens all the time).\n\n\n(2) Does the .tar or .tar.gz filename have \nILLEGAL CHARACTERS\n.  It's best to keep archive names simple, short, composed of letters and numbers.  (happens all the time).  So just rename the file.  This one nailed me recently as I thought it would be convenient to include a time/date stamp as part of the archive name.  BAD IDEA!",
    "url": "https://unix.stackexchange.com/questions/53891/tar-unexpected-eof-in-archive"
  },
  {
    "question_title": "Given a git commit hash, how to find out which kernel release contains it?",
    "question_body": "Assume I have some issue that was fixed by a recent patch to the official Linux git repository. I have a work around, but I’d like to undo it when a release happens that contains my the fix. I know the exact git commit hash, e.g. \nf3a1ef9cee4812e2d08c855eb373f0d83433e34c\n.\n\n\nWhat is the easiest way to answer the question: What kernel releases so far contain this patch? Bonus points if no local Linux git repository is needed.\n\n\n(\nLWM\n discusses some ideas, but these do require a local repository.)",
    "answer": "As \nmentioned on LWN\n, the easiest is:\n\n\n\n```bash\ngit describe --contains f3a1ef9cee4812e2d08c855eb373f0d83433e34c\n```\n\n\n\nIf you don't want a local clone, gitweb's \"plain\" formatted commit contains the same info in the \nX-Git-Tag\n header. \nUnfortunately kernel.org switched over to cgit which apparently does not disclose this information.\n Previously it was possible to find it out like this:\n\n\nhttp://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff_plain;h=f3a1ef9cee4812e2d08c855eb373f0d83433e34c\n\n\nHere, \nX-Git-Tag\n is actually missing at the moment because that commit isn't in a tagged release in that repository.  But you can look at an earlier commit, like:\n\n\nhttp://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff_plain;h=dc0827c128c0ee5a58b822b99d662b59f4b8e970\n\n\nHere, you see:\n\n\n\n```bash\nX-Git-Tag: v3.4-rc1~184^2~10\n```\n\n\n\nwhich tells me that the tag \"v3.4-rc1\" was the first tag to follow my patch, so I'd expect to see it in v3.4.",
    "url": "https://unix.stackexchange.com/questions/45120/given-a-git-commit-hash-how-to-find-out-which-kernel-release-contains-it"
  },
  {
    "question_title": "How to display lines 2-4 after each grep result?",
    "question_body": "I'm parsing a mailbox file that stores e-mail server reports for unsuccessfully delivered e-mail. I wish to extract bad e-mail addresses, so that I remove them from the system. The log file looks like this:\n\n\n\n```bash\n...some content...\n                   The mail system\n\n<slavicatomic118@hotmail.com>: host mx1.hotmail.com[65.54.188.94] said: 550\n    Requested action not taken: mailbox unavailable (in reply to RCPT TO\n    command)\n\n...some content...\n                   The mail system\n\n<oki88@optimumpro.net>: host viking.optimumpro.net[79.101.51.82] said: 550\n    Unknown user (in reply to RCPT TO command)\n\n...some content...\n                   The mail system\n\n<sigirna_luka@yahoo.com>: host mta5.am0.yahoodns.net[74.6.140.64] said: 554\n    delivery error: dd This user doesn't have a yahoo.com account\n    (sigirna_luka@yahoo.com) [0] - mta1172.mail.sk1.yahoo.com (in reply to end\n    of DATA command)\n\n...etc.\n```\n\n\n\nE-mail address comes 2 lines after a line with \"The mail system\". Using grep like this gives me the \"The mail system\" line and the next two lines:\n\n\n\n```bash\ngrep -A 2 \"The mail system\" mbox_file\n```\n\n\n\nHowever, I don't know how to remove the \"The mail system\" line and the second empty line from this output. I guess I could write PHP/Perl/Python script to do it, but I wonder if this is possible with grep or some other standard tool. I tried to give negative offset to -B parameter:\n\n\n\n```bash\ngrep -A 2 -B -2 \"The mail system\" mbox_file\n```\n\n\n\nBut grep complains:\n\n\n\n```bash\ngrep: -2: invalid context length argument\n```\n\n\n\nIs there a way to do this with grep?",
    "answer": "The simplest way to solve it using \ngrep\n only, is to pipe one more inverted \ngrep\n at the end.\nFor example:\n\n\n\n```bash\ngrep -A 4 \"The mail system\" temp.txt | grep -v \"The mail system\" | grep -v '^\\d*$'\n```",
    "url": "https://unix.stackexchange.com/questions/31531/how-to-display-lines-2-4-after-each-grep-result"
  },
  {
    "question_title": "What are the differences between bsdtar and GNU tar?",
    "question_body": "I've always used GNU \ntar\n. However, all GNU/Linux distributions that I've seen ship \nbsdtar\n in their repositories. I've even seen it installed by default in some, IIRC. I know for sure that Arch GNU/Linux requires it as a part of \nbasedevel\n (maybe \nbase\n, but I'm not sure), as I've seen it in PKGBUILDs.\n\n\nWhy would you want to use \nbsdtar\n instead of GNU \ntar\n? What are the advantages?\n\n\nNote that I am the person who asked \nWhat are the main differences between BSD and GNU/Linux userland?\n.",
    "answer": "The Ubuntu \nbsdtar\n is actually the tar implementation bundled with \nlibarchive\n; and that should be differentiated from classical \nbsdtar\n. Some BSD variants do use \nlibarchive\n for their tar implementation, eg FreeBSD.\n\n\nGNUtar\n does support the \nother tar variants\n and automatic compression detection.\n\n\nAs \nvisualication\n pasted the blurb from Ubuntu, there are a few things in there that are specific to \nlibarchive\n:\n\n\n\n\nlibarchive\n is by definition a library, and different from both classical \nbsdtar\n and \nGNUtar\n in that way.\n\n\nlibarchive\n cannot read some older obscure GNU tar variations, most notable was encoding of some headers in base64, so that the tar file would be 7-bit clean ASCII (this was the case for 1.13.6-1.13.11 and changed in 1.13.12, that code was only officially in tar for 2 weeks)\n\n\nlibarchive\n's \nbsdtar\n will read non-tar files (eg zip, iso9660, cpio), but classical bsdtar will not.\n\n\n\n\nNow that we've gotten \nlibarchive\n out of the way, it mostly comes down to what is supported in classical \nbsdtar\n.\n\n\nYou can see the manpages yourself here:\n\n\n\n\nGNU tar(1)\n\n\nFreeBSD tar(1)\n - libarchive-based\n\n\nNetBSD tar(1)\n\n\nOpenBSD tar(1)\n\n\nStandard/Schily tar(1)\n - the oldest free tar implementation, no heritage to any other\n\n\nbusybox (1)\n - Mini tar implementation for BusyBox, common in embedded systems\n\n\n\n\nIn your original question, you asked what are the advantages to the classical \nbsdtar\n, and I'm not sure there are really any. The only time it really matters is if you're trying to writing shell scripts that need to work on all systems; you need to make sure what you pass to \ntar\n is actually valid in all variants.\n\n\nGNUtar\n, \nlibarchive\n's \nbsdtar\n, classical \nbsdtar\n, \nstar\n and \nBusyBox\n's \ntar\n are certainly the tar implementations that you'll run into most of the time, but I'm certain there are others out there (early QNX for example). \nlibarchive\n/\nGNUtar\n/\nstar\n are the most feature-packed, but in many ways they have long deviated from the original standards (possibly for the better).",
    "url": "https://unix.stackexchange.com/questions/101561/what-are-the-differences-between-bsdtar-and-gnu-tar"
  },
  {
    "question_title": "Under what circumstances does gunzip &amp; tar xf work but tar xzf fail?",
    "question_body": "To illustrate the point: I have downloaded the LEDA library from the \ncompany's website\n. Using tar -xzf on it fails:\n\n\n\n```bash\n$ tar -xzf LEDA-6.3-free-fedora-core-8-64-g++-4.1.2-mt.tar.gz \ntar: This does not look like a tar archive\ntar: Skipping to next header\ntar: Exiting with failure status due to previous errors\n```\n\n\n\nHowever, gunzip followed by tar -xf works just fine:\n\n\n\n```bash\n$ gunzip LEDA-6.3-free-fedora-core-8-64-g++-4.1.2-mt.tar.gz\n$ tar -xf LEDA-6.3-free-fedora-core-8-64-g++-4.1.2-mt.tar\n# no error\n```\n\n\n\nCan anyone tell me why this could be?- I'd want the standard \ntar\n command to work all the time.",
    "answer": "What appears to have happened is that they've \ndouble\n compressed the archive.\n\n\nIf you run \nfile\n on your gunzip'd file, you'll find its still a gzip archive. And if you rename it to have .gz again, you can gunzip it again.\n\n\nIt seems recently gnu tar will automatically add the \n-z\n option, provided the input is a file. So, that's why it works without the \n-z\n option after you'd already run \ngunzip\n once, tar automatically added it.\n\n\nThis behavior is documented, from the info page:\n\n\n\n> \"Reading compressed archive is even simpler: you don't need to specify\n> any additional options as GNU `tar' recognizes its format automatically. [...]\n> The format recognition algorithm is based on \"signatures\", a special\n> byte sequences in the beginning of file, that are specific for certain\n> compression formats.\"\n\n\n\nThat's from §8.1.1 \"Creating and Reading Compressed Archives.\"",
    "url": "https://unix.stackexchange.com/questions/72528/under-what-circumstances-does-gunzip-tar-xf-work-but-tar-xzf-fail"
  },
  {
    "question_title": "giving grep output to rm",
    "question_body": "I'm trying to pipe \ngrep\n output to \nrm\n, but it outputs useless stuff. Is any switch required for \nrm\n? Or can \nrm\n can be provided a regexp directly?\n\n\n\n```bash\nls | grep '^\\[Daruchini'| rm\n```\n\n\n\nrm: missing operand\nTry `rm --help' for more information.",
    "answer": "You need to use \nxargs\n to turn standard input into arguments for \nrm\n.\n\n\n\n```bash\n$ ls | grep '^Dar' | xargs rm\n```\n\n\n\n(Beware of special characters in filenames; with GNU grep, you might prefer\n\n\n\n```bash\n$ ls | grep -Z '^Dar' | xargs -0 rm\n```\n\n\n\n)\n\n\nAlso, while the shell doesn't use regexps, that's a simple pattern:\n\n\n\n```bash\n$ rm Dar*\n```\n\n\n\n(meanwhile, I think I need more sleep.)",
    "url": "https://unix.stackexchange.com/questions/9597/giving-grep-output-to-rm"
  },
  {
    "question_title": "View a file in a tar archive without extracting it",
    "question_body": "I want to view the content of the tarred file without extracting it, \nScenario: I have a.tar and inside there is a file called \n./x/y.txt\n. I want to view the content of \ny.txt\n without actually extracting the \na.tar\n.",
    "answer": "It's probably a GNU specific option, but you could use the \n-O\n or \n--to-stdout\n to extract files to standard output\n\n\n\n```bash\n$ tar -axf file.tgz foo/bar -O\n```",
    "url": "https://unix.stackexchange.com/questions/208482/view-a-file-in-a-tar-archive-without-extracting-it"
  },
  {
    "question_title": "What is the actual purpose of GNU grep&#39;s -X option and why is it undocumented?",
    "question_body": "By reading \nthis question\n, I have discovered that GNU \ngrep\n has a \n-X\n option which expects an argument. Strangely, it is mentioned neither in the man page nor in the info page.\n\n\nLooking at the source code, there is that comment \nright in the middle of the \n--help\n output\n:\n\n\n\n```bash\n/* -X is deliberately undocumented.  */\n```\n\n\n\nLooking further, it appears that the \n-X matcher\n option \nsets the engine used for the regexp\n, \nmatcher\n being \none of\n \ngrep\n, \negrep\n, \nfgrep\n, \nawk\n, \ngawk\n, \nposixawk\n and \nperl\n (as of version 2.25).\n\n\nSome of those values are strictly identical to existing options (namely \ngrep -G\n, \ngrep -E\n, \ngrep -F\n and \ngrep -P\n). On the other hand, the three \nawk\n variants have no corresponding options.\n\n\nDoes someone know what is the actual purpose of this option, especially with one of the \nawk\n regexp engines? Can someone tell me why it is purposely not documented?",
    "answer": "Its purpose is to provide access to the various matchers implemented in GNU \ngrep\n in one form or another, in particular AWK matchers which aren’t available otherwise, probably for testing purposes (see \nbug 16481\n which discusses adding the \ngawk\n and \nposixawk\n matchers).\n\n\nHowever it is currently buggy, which \nis the reason why it’s documented as being undocumented\n:\n\n\n\n> On Thu, Jan 27, 2005 at 04:06:04PM -0500, Charles Levert wrote:\n> > The '-X' option, and in particular its use with the \"awk\" matcher\n> > (\"-X awk\") is undocumented.\n> please leave it undocumented.\n> It doesn't provide any new functionality besides -X awk.\n> And the implementation of awk regexps is not perfect, I think.\n> The new GNU regex conatins some means to set AWK style syntax, yes.\n> Yet gawk doesn't use it directly: it parses the regex first.\n> In particular, awk regexps allow escape sequences \\NNN, where NNN is\n> an octal value. So /\\040/ mathes space. grep -X awk doesn't seem to\n> support this.\n> I'm afraid that regex.c doesn't support these escape sequences.\n> We would have to make sure that the regexes are fully compatible with\n> awk regexes before we decided to document (and thus support) this\n> feature.\n> I think it's not worth the trouble.\n> Stepan\n\n\n\nA \nfollow-up\n asked for the comment to be added, and provided a bit more background on the \n-X\n option:\n\n\n\n> My own inclination is to suggest just removing -X entirely. I suspect it was added by the original author mainly for testing purposes. If it's going to stay in, at least add a comment like this.\n> ```bash\n> /* -X is undocumented on purpose. */\n> ```\n> to avoid future discussion of a resolved issue.\n> Arnold\n\n\n\nwhich \nStepan did shortly thereafter\n.",
    "url": "https://unix.stackexchange.com/questions/392341/what-is-the-actual-purpose-of-gnu-greps-x-option-and-why-is-it-undocumented"
  },
  {
    "question_title": "How do I add X days to date and get new date?",
    "question_body": "I  have Linux ( RH 5.3) machine\n\n\nI need to add/calculate 10 days plus date so then I will get new date (expiration date))\n\n\nfor example\n\n\n\n```bash\n# date \n Sun Sep 11 07:59:16 IST 2012\n```\n\n\n\nSo I need to get \n\n\n\n```bash\nNEW_expration_DATE = Sun Sep 21 07:59:16 IST 2012\n```\n\n\n\nPlease advice how to calculate the new expiration date ( with bash , ksh , or manipulate date command ?)",
    "answer": "You can just use the \n-d\n switch and provide a date to be calculated\n\n\n\n```bash\ndate\nSun Sep 23 08:19:56 BST 2012\nNEW_expration_DATE=$(date -d \"+10 days\")\necho $NEW_expration_DATE\nWed Oct 3 08:12:33 BST 2012\n```\n\n\n\n\n> ```bash\n> -d, --date=STRING\n> ```\n\n\n\n\n```bash\ndisplay time described by STRING, not ‘now’\n```\n\n\n\nThis is quite a powerful tool as you can do things like\n\n\n\n```bash\ndate -d \"Sun Sep 11 07:59:16 IST 2012+10 days\"\nFri Sep 21 03:29:16 BST 2012\n```\n\n\n\nor\n\n\n\n```bash\nTZ=IST date -d \"Sun Sep 11 07:59:16 IST 2012+10 days\"\nFri Sep 21 07:59:16 IST 2012\n```\n\n\n\nor\n\n\n\n```bash\nprog_end_date=`date '+%C%y%m%d' -d \"$end_date+10 days\"`\n```\n\n\n\nSo if \n$end_date\n = \n20131001\n then \n$prog_end_date\n = \n20131011\n.",
    "url": "https://unix.stackexchange.com/questions/49053/how-do-i-add-x-days-to-date-and-get-new-date"
  },
  {
    "question_title": "Why ZSH ends a line with a highlighted percent symbol?",
    "question_body": "I've noticed this on occasion with a variety of applications. I've often thought it was because the output was cancelled early (ctrl+c, for example) or something similar, and zsh is filling in a new line character. But now curiosity has gotten the best of me, since it doesn't seem to do this in bash. \n\n\nzsh\n\n\n\n\nbash\n\n\n\n\nThe \nSequence\n program is something I pulled from a book while reading on Java certifications and just wanted to see if it would compile and run. I did notice that it does not use the \nprintln()\n method from the \nSystem.out\n package/class. Instead it uses plain old \nprint()\n.\n\n\nIs the lack of a new line character the reason I get this symbol?",
    "answer": "Yes, this happens because it is a \"partial line\". And by default \nzsh goes to the next line to avoid covering it with the prompt\n.\n\n\n\n> When a partial line is preserved, by default you will see an\n> inverse+bold character at the end of the partial line: a \"%\" for a\n> normal user or a \"#\" for root. If set, the shell parameter\n> PROMPT_EOL_MARK can be used to customize how the end of partial lines\n> are shown.",
    "url": "https://unix.stackexchange.com/questions/167582/why-zsh-ends-a-line-with-a-highlighted-percent-symbol"
  },
  {
    "question_title": "Can grep output only specified groupings that match?",
    "question_body": "Say I have a file:\n\n\n\n```bash\n# file: 'test.txt'\nfoobar bash 1\nbash\nfoobar happy\nfoobar\n```\n\n\n\nI only want to know what words appear after \"foobar\", so I can use this regex:\n\n\n\n```bash\n\"foobar \\(\\w\\+\\)\"\n```\n\n\n\nThe parenthesis indicate that I have a special interest in the word right after foobar.  But when I do a \ngrep \"foobar \\(\\w\\+\\)\" test.txt\n, I get the entire lines that match the entire regex, rather than just \"the word after foobar\":\n\n\n\n```bash\nfoobar bash 1\nfoobar happy\n```\n\n\n\nI would much prefer that the output of that command looked like this:\n\n\n\n```bash\nbash\nhappy\n```\n\n\n\nIs there a way to tell grep to only output the items that match the grouping (or a specific grouping) in a regular expression?",
    "answer": "GNU grep has the \n-P\n option for perl-style regexes, and the \n-o\n option to print only what matches the pattern. These can be combined using look-around assertions (described under \nExtended Patterns in the perlre manpage\n) to remove part of the grep pattern from what is determined to have matched for the purposes of \n-o\n.\n\n\n\n```bash\n$ grep -oP 'foobar \\K\\w+' test.txt\nbash\nhappy\n$\n```\n\n\n\nThe \n\\K\n is the short-form (and more efficient form) of \n(?<=pattern)\n which you use as a zero-width look-behind assertion before the text you want to output. \n(?=pattern)\n can be used as a zero-width look-ahead assertion after the text you want to output.\n\n\nFor instance, if you wanted to match the word between \nfoo\n and \nbar\n, you could use:\n\n\n\n```bash\n$ grep -oP 'foo \\K\\w+(?= bar)' test.txt\n```\n\n\n\nor (for symmetry)\n\n\n\n```bash\n$ grep -oP '(?<=foo )\\w+(?= bar)' test.txt\n```",
    "url": "https://unix.stackexchange.com/questions/13466/can-grep-output-only-specified-groupings-that-match"
  },
  {
    "question_title": "GUI for GIT similar to SourceTree",
    "question_body": "Is there a similar piece of software to \nSourceTree\n, a GUI for git, for Linux? I know about Giggle, git cola, etc. I'm looking for a beautiful, easy to use GUI for git.",
    "answer": "A nice alternative is \nSmartGit\n. It has very similar features to SourceTree and has built in 3-column conflict resolution, visual logs, pulling, pushing, merging, syncing, tagging and all things git :)",
    "url": "https://unix.stackexchange.com/questions/48469/gui-for-git-similar-to-sourcetree"
  },
  {
    "question_title": "scp and compress at the same time, no intermediate save",
    "question_body": "What is the canonical way to:\n\n\n\n\nscp\n a file to a remote location\n\n\ncompress the file in transit (\ntar\n or not, single file or whole folder, \n7za\n or something else even more efficient)\n\n\ndo the above without saving intermediate files\n\n\n\n\nI am familiar with shell pipes like this:\n\n\n\n```bash\ntar cf - MyBackups | 7za a -si -mx=9 -ms=on MyBackups.tar.7z\n```\n\n\n\nessentially:\n\n\n\n\nrolling a whole folder into a single \ntar\n\n\npass data through \nstdout\n to \nstdin\n of the compressing program\n\n\napply \naggressive\n compression\n\n\n\n\nWhat's the best way to do this over an \nssh\n link, with the file landing on the remote filesystem?\n\n\n\n\nI prefer not to \nsshfs\n mount.\n\n\n\n\nThis, does not work:\n\n\n\n```bash\nscp <(tar cvf - MyBackups | 7za a -si -mx=9 -so) localhost:/tmp/tmp.tar.7z\n```\n\n\n\nbecause:\n\n\n\n```bash\n/dev/fd/63: not a regular file\n```",
    "answer": "There are many ways to do what you want. The simplest is to use a pìpe:\n\n\n\n```bash\ntar zcvf -  MyBackups | ssh user@server \"cat > /path/to/backup/foo.tgz\"\n```\n\n\n\nHere, the compression is being handled by \ntar\n which calls \ngzip\n (\nz\n flag). You can also use \ncompress\n (\nZ\n) and \nbzip\n (\nj\n). For \n7z\n, do this:\n\n\n\n```bash\ntar cf - MyBackups | 7za a -si -mx=9 -ms=on MyBackups.tar.7z | \n   ssh user@server \"cat > /path/to/backup/foo.7z\"\n```\n\n\n\nThe \nbest\n way, however, is probably \nrsync\n.\n\n\n\n```bash\nRsync is a fast and extraordinarily versatile  file  copying  tool.   It  can  copy\n   locally, to/from another host over any remote shell, or to/from a remote rsync dae‐\n   mon.  It offers a large number of options that control every aspect of its behavior\n   and  permit  very  flexible  specification of the set of files to be copied.  It is\n   famous for its delta-transfer algorithm, which reduces the amount of data sent over\n   the network by sending only the differences between the source files and the exist‐\n   ing files in the destination.  Rsync is widely used for backups and  mirroring  and\n   as an improved copy command for everyday use.\n```\n\n\n\nrsync\n has \nway\n too many options. It really is worth reading through them but they are scary at first sight. The ones you care about in this context though are:\n\n\n\n```bash\n-z, --compress              compress file data during the transfer\n        --compress-level=NUM    explicitly set compression level\n\n   -z, --compress\n          With this option, rsync compresses the file data as it is sent to the desti‐\n          nation machine, which reduces the amount of data being transmitted --  \n          something that is useful over a slow connection.\n\n          Note  that this option typically achieves better compression ratios than can\n          be achieved by using a compressing remote shell or a  compressing  transport\n          because  it takes advantage of the implicit information in the matching data\n          blocks that are not explicitly sent over the connection.\n```\n\n\n\nSo, in your case, you would want something like this:\n\n\n\n```bash\nrsync -z MyBackups user@server:/path/to/backup/\n```\n\n\n\nThe files would be compressed while in transit and arrive decompressed at the destination. \n\n\n\n\nSome more choices:\n\n\n\n\nscp\n itself can compress the data\n\n\n\n```bash\n-C      Compression enable.  Passes the -C flag to ssh(1) to\n         enable compression.\n\n$ scp -C source user@server:/path/to/backup\n```\n\n\n\nThere may be a way to get \nrsync\n and \n7za\n to play nice but there is no point in doing so. The benefit of \nrsync\n is that it will only copy the bits that have changed between the local and remote files. However, a small local change can result in a very different compressed file so there is no point in using \nrsync\n for this. It just complicates matters with no benefit. Just use direct \nssh\n as shown above. If you \nreally\n want to do this, you can try by giving a subshell as an argument to \nrsync\n. On my system, I could not get this to work with \n7za\n because it does not allow you to write compressed data to a terminal.  Perhaps your implementation is different. Try something like (\nthis does not work for me\n):\n\n\n\n```bash\nrsync $(tar cf - MyBackups | 7za a -an -txz -si -so) \\\n  user@server:/path/to/backup\n```\n\n\n\nAnother point is that \n7z\n \nshould not be used for backups on Linux\n. As stated on the \n7z\n man page:\n\n\n\n> DO NOT USE the 7-zip format for backup purpose  on  Linux/Unix\n> because :\n> - 7-zip does not store the owner/group of the file.",
    "url": "https://unix.stackexchange.com/questions/70581/scp-and-compress-at-the-same-time-no-intermediate-save"
  },
  {
    "question_title": "grep and tail -f?",
    "question_body": "Is it possible to do a \ntail -f\n (or similar) on a file, and \ngrep\n it at the same time? I wouldn't mind other commands just looking for that kind of behavior.",
    "answer": "Using GNU \ntail\n and GNU \ngrep\n, I am able to grep a \ntail -f\n using the straight-forward syntax:\n\n\n\n```bash\ntail -f /var/log/file.log | grep search_term\n```",
    "url": "https://unix.stackexchange.com/questions/3229/grep-and-tail-f"
  },
  {
    "question_title": "Using &quot;${a:-b}&quot; for variable assignment in scripts",
    "question_body": "I have been looking at a few scripts other people wrote (specifically Red Hat), and a lot of their variables are assigned using the following notation\n\nVARIABLE1=\"${VARIABLE1:-some_val}\"\n\nor some expand other variables\n\nVARIABLE2=\"${VARIABLE2:-`echo $VARIABLE1`}\"\n\n\nWhat is the point of using this notation instead of just declaring the values directly (e.g., \nVARIABLE1=some_val\n)?\n\n\nAre there benefits to this notation or possible errors that would be prevented?\n\n\nDoes the \n:-\n have specific meaning in this context?",
    "answer": "This technique allows for a variable to be assigned a value if another variable is either empty or is undefined. \nNOTE:\n This \"other variable\" can be the same or another variable.\n\n\nexcerpt\n\n\n\n```bash\n${parameter:-word}\n    If parameter is unset or null, the expansion of word is substituted. \n    Otherwise, the value of parameter is substituted.\n```\n\n\n\nNOTE:\n This form also works, \n${parameter-word}\n. According to \nthe Bash documentation\n, for all such expansions:\n\n\n\n> Omitting the colon results in a test only for a parameter that is unset. Put another way, if the colon is included, the operator tests for both parameter’s existence and that its value is not null; if the colon is omitted, the operator tests only for existence.\n\n\n\nIf you'd like to see a full list of all forms of parameter expansion available within Bash then I highly suggest you take a look at this topic in the Bash Hacker's wiki titled: \"\nParameter expansion\n\".\n\n\nExamples\n\n\nvariable doesn't exist\n\n\n\n```bash\n$ echo \"$VAR1\"\n\n$ VAR1=\"${VAR1:-default value}\"\n$ echo \"$VAR1\"\ndefault value\n```\n\n\n\nvariable exists\n\n\n\n```bash\n$ VAR1=\"has value\"\n$ echo \"$VAR1\"\nhas value\n\n$ VAR1=\"${VAR1:-default value}\"\n$ echo \"$VAR1\"\nhas value\n```\n\n\n\nThe same thing can be done by evaluating other variables, or running commands within the default value portion of the notation.\n\n\n\n```bash\n$ VAR2=\"has another value\"\n$ echo \"$VAR2\"\nhas another value\n$ echo \"$VAR1\"\n\n$\n\n$ VAR1=\"${VAR1:-$VAR2}\"\n$ echo \"$VAR1\"\nhas another value\n```\n\n\n\nMore Examples\n\n\nYou can also use a slightly different notation where it's just \nVARX=${VARX-<def. value>}\n.\n\n\n\n```bash\n$ echo \"${VAR1-0}\"\nhas another value\n$ echo \"${VAR2-0}\"\nhas another value\n$ echo \"${VAR3-0}\"\n0\n```\n\n\n\nIn the above \n$VAR1\n & \n$VAR2\n were already defined with the string \"has another value\" but \n$VAR3\n was undefined, so the default value was used instead, \n0\n.\n\n\nAnother Example\n\n\n\n```bash\n$ VARX=\"${VAR3-0}\"\n$ echo \"$VARX\"\n0\n```\n\n\n\nChecking and assigning using \n:=\n notation\n\n\nLastly I'll mention the handy operator, \n:=\n. This will do a check and assign a value if the variable under test is empty or undefined.\n\n\nExample\n\n\nNotice that \n$VAR1\n is now set. The operator \n:=\n did the test and the assignment in a single operation.\n\n\n\n```bash\n$ unset VAR1\n$ echo \"$VAR1\"\n\n$ echo \"${VAR1:=default}\"\ndefault\n$ echo \"$VAR1\"\ndefault\n```\n\n\n\nHowever if the value is set prior, then it's left alone.\n\n\n\n```bash\n$ VAR1=\"some value\"\n$ echo \"${VAR1:=default}\"\nsome value\n$ echo \"$VAR1\"\nsome value\n```\n\n\n\nHandy Dandy Reference Table\n\n\n\n\n\n\n\n\n\n\n\n\nParameter set and not null\n\n\nParameter set but null\n\n\nParameter unset\n\n\n\n\n\n\n\n\n\n\n${parameter:-word}\n\n\nsubstitute \nparameter\n\n\nsubstitute \nword\n\n\nsubstitute \nword\n\n\n\n\n\n\n${parameter-word}\n\n\nsubstitute \nparameter\n\n\nsubstitute \nnull\n\n\nsubstitute \nword\n\n\n\n\n\n\n${parameter:=word}\n\n\nsubstitute \nparameter\n\n\nassign \nword\n\n\nassign \nword\n\n\n\n\n\n\n${parameter=word}\n\n\nsubstitute \nparameter\n\n\nsubstitute \nnull\n\n\nassign \nword\n\n\n\n\n\n\n${parameter:?word}\n\n\nsubstitute \nparameter\n\n\nerror, exit\n\n\nerror, exit\n\n\n\n\n\n\n${parameter?word}\n\n\nsubstitute \nparameter\n\n\nsubstitute \nnull\n\n\nerror, exit\n\n\n\n\n\n\n${parameter:+word}\n\n\nsubstitute \nword\n\n\nsubstitute \nnull\n\n\nsubstitute \nnull\n\n\n\n\n\n\n${parameter+word}\n\n\nsubstitute \nword\n\n\nsubstitute \nword\n\n\nsubstitute \nnull\n\n\n\n\n\n\n\n\n\n\n(\nScreenshot of source table\n)\n\n\nThis\n makes the difference between \nassignment\n and \nsubstitution\n explicit: Assignment sets a value for the variable whereas substitution doesn't.\n\n\nReferences\n\n\n\n\nParameter Expansions - Bash Hackers Wiki\n\n\n10.2. Parameter Substitution\n\n\nBash Parameter Expansions",
    "url": "https://unix.stackexchange.com/questions/122845/using-a-b-for-variable-assignment-in-scripts"
  },
  {
    "question_title": "Execute a command once per line of piped input?",
    "question_body": "I want to run a java command once for every match of \nls | grep pattern -\n. In this case, I think I could do \nfind pattern -exec java MyProg '{}' \\;\n but I'm curious about the general case - is there an easy way to say \"run a command once for every line of standard input\"? (In fish or bash.)",
    "answer": "The accepted answer\n has the right idea, but the key is to pass \nxargs\n the \n-n1\n switch, which means \n\"Use at most 1 argument per command line\"\n:\n\n\n\n```bash\ncat file... | xargs -n1 command\n```\n\n\n\nOr, for a single input file you can avoid the pipe from \ncat\n entirely and just go with:\n\n\n\n```bash\n<file xargs -n1 command\n```\n\n\n\nUpdated 2020-08-05:\n\n\nI would also like to respond to the advice found in user Jander's comment, which was heavily upvoted, despite containing some amount of misinformation as I will now explain.\n\n\nDo not be so hasty to recommend the \n-L\n option of \nxargs\n, without mentioning the trouble that its (so called) \ntrailing blank(s) feature\n can lead to. In my opinion this switch causes more harm than good and is certainly a stretch to use to mean, for the case of \n-L 1\n, \nact on one non-empty line at a time\n. To be fair, the man page for \nxargs\n does spell out the \nfeatures\n (i.e., issues) that come for the ride with the \n-L\n switch.\n\n\nSince Jander made no mention of the issues when mentioning \n-L\n to perhaps a hasty unsuspecting StackOverflow audience seeking quick tips and not having the time for such tedious things as reading man pages rather than accepting comments and answers as gospel, I will now present my case for why \n-L\n is a very bad suggestion without a careful understanding of all of the baggage that it brings along for the ride.\n\n\nTo illustrate my disdain for \n-L\n, let's consider a simple input file that consists of the following text someone carelessly entered (perhaps a high-school summer intern that created this data file as part of his/her training, as evidenced by its \nWindowish\n filename. As luck (karma?) would have it, you have been selected by management to be its new custodian):\n\n\ntestdata.txt\n\n\n\n```bash\n1\n2␠\n3\n```\n\n\n\nBecause of the fact that the line that contains the digit \n2\n has a space character (shown as a Unicode \nSYMBOL FOR SPACE\n glyph after the digit \n2\n in the preceding code, in case your browser's font does not have a visual representation for this character), a command that uses \nxargs -L1\n, such as:\n\n\n\n```bash\n<testdata.txt xargs -L1 echo\n```\n\n\n\n..., would produce the following (perhaps surprising) output:\n\n\n\n```bash\n1\n2 3\n```\n\n\n\nThis is caused by the fact that the \n-L\n switch instructs \nxargs\n to \nappend subsequent lines to those that end with blanks\n, a behavior that may only effect the resulting output in those oddball moments where lines are not properly trimmed of trailing blanks - a time bomb bug waiting for the right input file to present itself.\n\n\nOn the other hand, the same command using the \n-n 1\n switch of \nxargs\n, instead of \n-L 1\n would produce a far more acceptable output of:\n\n\n\n```bash\n1\n2␠\n3\n```\n\n\n\nAnd that's not even the worst of it! The \n-L\n switch unlike \n-n\n forces the \"dreaded\" \n-x\n option of \nxargs\n to go into effect. This causes termination of the \nxargs\n process if a command line is encountered that it deems too long for the environment on which it is run.\n\n\nAn input file consisting of many lines with trailing blanks in succession could, as instructed by the \n-L\n switch and its use of the chemical agent known as Agent \n-x\n into the mix, potentially cause \nxargs\n to terminate midstream if the concatenation of all of these into one \nsuperline\n exceeds \nxargs\n' definition of \nline is too long\n for a command line. If things are beginning to look murky, consider that \nline is too long\n is a size determined by \nxargs\n based on the max length specified for the platform on which it is run and further offset by a seemingly \narbitrary constant\n as explained in more detail in the man page. Remember those pesky indefinite integrals from Calculus and their \narbitrary constants\n and losing a point on a quiz or test, because you forgot to write \n+ C\n after your solution for an indefinite integral? Well, that phrase is back with a vengeance, to bite you on the rear once again, if add \n-L\n to your handy \nxargs\n toolkit.\n\n\nA \n-n\n value of \n1\n, on the other hand, would just chop up those long lines into (hopefully) small bite sized one-line chunks and execute the command supplied to \nxargs\n for each of them, one at a time, without any consideration given to whether they end with blanks or not. No more long lines and no more of \nxargs\n stabbing you in the back by terminating abruptly - Et tu, \nBrute\n \n-x\n?\n\n\nAn optional segue regarding phrasing in the xargs man page\n\n\nI don't know why the ambiguous and non-standard word \nblanks\n was used throughout the \nxargs\n man page, instead of far better defined and less ambiguous options such as:\n\n\n\n\nspace(s), if \nblanks\n means one or more \nASCII space\n characters\n\n\nwhitespace(s) other than new-lines (if that's what \nblanks\n implies)\n\n\none or more non-printable characters from the set: \n{space, horizontal tab}\n (if \nblanks\n was used as a synonym for this gruesome twosome)\n\n\n\n\nUpdated 2021-06-15:\n\n\nUser @BjornW asked how \nxargs\n could be used to run a command once per \nline\n of input and not just word of input. (See, I do read the comments, and I'll just blame the seven months it took to respond on Covid :P ).\n\n\nIn the spirit of the original question, as asked, and to make my answer applicable to a greater number of use cases, I would like to address this particular scenario in detail.\n\n\nConsider the following input file. It is chock-full of various edge cases one may actually encounter in the \nReal World\n™ (e.g., leading/trailing spaces, lines consisting only of spaces, empty lines, lines beginning with a hyphen [which should not get interpreted as the introduction of a switch], etc.):\n\n\nlines.txt\n\n\n\n```bash\na1 a22 a333 a4444\nb4444 b333 b22 b1\n␠␠c d e f g\n␣\nhhh\nii jj kk␠\n␣\n␠␠␠\n-L and -x are the gruesome twosome\n␣\n␣\n␣\n```\n\n\n\nIn the preceding input file, the Unicode character \nOPEN BOX\n U+2423 was used to mark empty lines and the Unicode \nSYMBOL FOR SPACE\n was used for leading and trailing spaces, in order to make them more prominent.\n\n\nLet's say we want to run a command on each line of input, taken as a whole, and passed to our command as a single argument, regardless of content (including no content). We would go about this using \nxargs\n, as follows (Note: \nprintf\n will be our sample command and the \n%q\n format specifier will be used in order to enclose the supplied argument in apostrophes for clarity, when spaces are present or the argument is an empty string - all in, only our \nhhh\n input line was left \"unscathed,\" by \n%q\n, as you will see in the output, presented shortly. Had any non-printable characters been present, they would have also gotten escaped by \n%q\n using the POSIX \n$''\n quoting syntax]):\n\n\n\n```bash\n<lines.txt xargs -n1 -d'\\n' printf -- 'Input line: %q\\n'\n```\n\n\n\nThe output is as follows:\n\n\n\n```bash\nInput line: 'a1 a22 a333 a4444'\nInput line: 'b4444 b333 b22 b1'\nInput line: '   c d e f g'\nInput line: ''\nInput line: hhh\nInput line: 'ii jj kk '\nInput line: ''\nInput line: '   '\nInput line: '-L and -x are the gruesome twosome'\nInput line: ''\nInput line: ''\nInput line: ''\n```\n\n\n\nSo, there you have it. Using the \n-d\n switch, we can specify the delimiter that \nxargs\n should look for in our input file to indicate where an argument ends and the next one begins. By setting it to \n'\\n'\n which \nxargs\n itself is smart enough to interpret as a \nC-style character escape\n, as stated in the description of the \n-d\n switch on its \nman\n page, we can use \nxargs\n to forward entire lines of input to our command of choice as arguments, with minimal effort on our part.\n\n\nI would like to also mention that \nxargs\n can be used to concatenate multiple lines of input (with a caveat I will detail at the end of this paragraph), for the rare cases when such behavior is desired, and forward them as a single argument to our command. This can be accomplished by setting the number passed to the \n-n\n switch to \nxargs\n in the above invocation command, to a value that is indicative of the number of lines of input that should get merged into a single argument with their \n\\n\n line endings removed as part of the process. Unfortunately, this new-line stripping behavior makes the aforementioned \nxargs\n approach unsuitable for many use cases, because information that is indicative of where one line ends and the next begins, gets lost in the process.",
    "url": "https://unix.stackexchange.com/questions/7558/execute-a-command-once-per-line-of-piped-input"
  },
  {
    "question_title": "Is there a one-liner that allows me to create a directory and move into it at the same time?",
    "question_body": "I find myself repeating a lot of:\n\n\n\n```bash\nmkdir longtitleproject\ncd longtitleproject\n```\n\n\n\nIs there a way of doing it in one line without repeating the directory name? I'm on bash here.",
    "answer": "This is the one-liner that you need. No other config needed:\n\n\n\n```bash\nmkdir longtitleproject && cd $_\n```\n\n\n\nThe \n$_\n variable, in bash, is the last argument given to the previous command. In this case, the name of the directory you just created. As explained in \nman bash\n:\n\n\n\n```bash\n_         At  shell  startup,  set to the absolute pathname used to invoke\n          the shell or shell script being executed as passed in the  envi‐\n          ronment  or  argument  list.   Subsequently, expands to the last\n          argument to the previous command, after expansion.  Also set  to\n          the  full  pathname  used  to  invoke  each command executed and\n          placed in the environment exported to that command.  When check‐\n          ing  mail,  this  parameter holds the name of the mail file cur‐\n          rently being checked.\"$_\" is the last argument of the previous command.\n```\n\n\n\nUse \ncd $_\n to retrieve the last argument of the previous command instead of \ncd !$\n because \ncd !$\n gives the last argument of previous command \nin the shell history\n:\n\n\n\n```bash\ncd ~/\nmkdir folder && cd !$\n```\n\n\n\nyou end up home (or ~/ )\n\n\n\n```bash\ncd ~/\nmkdir newfolder && cd $_\n```\n\n\n\nyou end up in newfolder under home !! ( or ~/newfolder )",
    "url": "https://unix.stackexchange.com/questions/9123/is-there-a-one-liner-that-allows-me-to-create-a-directory-and-move-into-it-at-th"
  },
  {
    "question_title": "How to find all git repositories within given folders (fast)",
    "question_body": "Naive approach is \nfind dir1 dir2 dir3 -type d -name .git | xargs -I {} dirname {}\n\n, but it's too slow for me, because I have a lot deep folder structures inside git repositories (at least I think that this is the reason). I've read about that I can use \nprune\n to prevent find to recurse into directories once it found something, but there's two things. I'm not sure how this works (I mean I don't understand what \nprune\n does although I've read man page) and the second it wouldn't work in my case, because it would prevent \nfind\n to recurse into \n.git\n folder but not into all other folders.\n\n\nSo what I actually need is:\n\n\nfor all subdirectories check if they contain a \n.git\n folder and if it is then stop searching in this filesystem branch and report result. It would be perfect if this would also exclude any hidden directories from search.",
    "answer": "Okay, I still don't totally sure how this works, but I've tested it and it works.\n\n\n\n```bash\n.\n├── a\n│   ├── .git\n│   └── a\n│       └── .git\n└── b\n    └── .git\n\n6 directories, 0 files\n\n% find . -type d -exec test -e '{}/.git' ';' -print -prune\n./a\n./b\n```\n\n\n\nI'm looking forward into making the same faster.",
    "url": "https://unix.stackexchange.com/questions/333862/how-to-find-all-git-repositories-within-given-folders-fast"
  },
  {
    "question_title": "How to use grep and cut in script to obtain website URLs from an HTML file",
    "question_body": "I am trying to use grep and cut to extract URLs from an HTML file. The links look like:\n\n\n\n```bash\n<a href=\"http://examplewebsite.com/\">\n```\n\n\n\nOther websites have \n.net\n, \n.gov\n, but I assume I could make the cut off point right before \n>\n. So I know I can use grep and cut somehow to cut off everything before http and after .com, but I have been stuck on it for a while.",
    "answer": "Not sure if you are limited on tools:\n\n\nBut regex might not be the best way to go as mentioned, but here is an example that I put together:\n\n\n\n```bash\ncat urls.html | grep -Eo \"(http|https)://[a-zA-Z0-9./?=_%:-]*\" | sort -u\n```\n\n\n\n\n\ngrep -E\n : is the same as egrep\n\n\ngrep -o\n : only outputs what has been grepped\n\n\n(http|https)\n : is an either / or\n\n\na-z\n : is all lower case\n\n\nA-Z\n : is all upper case\n\n\n.\n : is dot\n\n\n/\n : is the slash\n\n\n?\n : is ?\n\n\n=\n : is equal sign\n\n\n_\n : is underscore\n\n\n%\n : is percentage sign\n\n\n:\n : is colon\n\n\n-\n : is dash\n\n\n*\n: is repeat the [...] group\n\n\nsort -u\n : will sort & remove any duplicates\n\n\n\n\nOutput:\n\n\n\n```bash\nbob@bob-NE722:~s$  wget -qO- https://stackoverflow.com/ | grep -Eo \"(http|https)://[a-zA-Z0-9./?=_-]*\" | sort -u\nhttps://stackauth.com\nhttps://meta.stackoverflow.com\nhttps://cdn.sstatic.net/Img/svg-icons\nhttps://stackoverflow.com\nhttps://www.stackoverflowbusiness.com/talent\nhttps://www.stackoverflowbusiness.com/advertising\nhttps://stackoverflow.com/users/login?ssrc=head\nhttps://stackoverflow.com/users/signup?ssrc=head\nhttps://stackoverflow.com\nhttps://stackoverflow.com/help\nhttps://chat.stackoverflow.com\nhttps://meta.stackoverflow.com\n...\n```\n\n\n\nYou can also add in \n\\d\n to catch other numeral types.",
    "url": "https://unix.stackexchange.com/questions/181254/how-to-use-grep-and-cut-in-script-to-obtain-website-urls-from-an-html-file"
  },
  {
    "question_title": "tar extract into directory with same base name?",
    "question_body": "I have a zipped file like \nmyArchive123.tar.gz\n. Inside, it contains a folder like \nhelloWorld\n\n\nIf I extract it with \ntar -xf myArchive123.tar.gz\n, I get the \nhelloWorld\n folder:\n\n\n\n```bash\nls \nmyArchive123.tar.gz\nhelloWorld\n```\n\n\n\nI want the output to be the same as the file name minus the \n.tar.gz\n extension. I.e.:\n\n\n\n```bash\ntar <magic paramaters> myArchive123.tar.gz \nls \n myArchive123.tar.gz\n myArchive123\ncd myArchive123\nls \n  helloWorld\n```\n\n\n\nCan this be done?\n\n\n\n\nI never know what's inside the archive. It could be a folder, could be many files.\n\n\nI'd be ok with using another tool if tar can't do it.\n\n\nI'd be ok with a longer form that can be turned into a script\n\n\n\n\nEDIT\n\nIn the meantime, I wrote myself a script that seems to get the job done (see my posted answer below).\nThe main thing is that it should be packageable into a one-liner like:\n\n\n\n```bash\nextract <file>\n```",
    "answer": "```bash\n$ tar -xf myArchive123.tar.gz --one-top-level\n```\n\n\n\n--one-top-level\n[=DIR] \n\nExtract all files into DIR, or, if used without argument, into a subdirectory named by the base name of the archive (minus standard compression suffixes recognizable by --auto-compress).",
    "url": "https://unix.stackexchange.com/questions/198151/tar-extract-into-directory-with-same-base-name"
  },
  {
    "question_title": "Recursive grep vs find / -type f -exec grep {} \\; Which is more efficient/faster?",
    "question_body": "Which is more efficient for finding which files in an entire filesystem contain a string: recursive grep or find with grep in an exec statement?  I assume find would be more efficient because you can at least do some filtering if you know the file extension or a regex that matches the file name, but when you only know \n-type f\n which is better? GNU grep 2.6.3; find (GNU findutils) 4.4.2\n\n\nExample:\n\n\ngrep -r -i 'the brown dog' /\n\n\nfind / -type f -exec grep -i 'the brown dog' {} \\;",
    "answer": "I'm not sure:\n\n\n\n> ```bash\n> grep -r -i 'the brown dog' /*\n> ```\n\n\n\nis really what you meant. That would mean grep recursively in all the non-hidden files and dirs in \n/\n (but still look inside hidden files and dirs inside those).\n\n\nAssuming you meant:\n\n\n\n```bash\ngrep -r -i 'the brown dog' /\n```\n\n\n\nA few things to note:\n\n\n\n\nNot all \ngrep\n implementations support \n-r\n. And among those that do, the behaviours differ: some follow symlinks to directories when traversing the directory tree (which means you may end up looking several times in the same file or even run in infinite loops), some will not. Some will look inside device files (and it will take quite some time in \n/dev/zero\n for instance) or pipes or binary files..., some will not.\n\n\nIt's efficient as \ngrep\n starts looking inside files as soon as it discovers them. But while it looks in a file, it's no longer looking for more files to search in (which is probably just as well in most cases)\n\n\n\n\nYour:\n\n\n\n```bash\nfind / -type f -exec grep -i 'the brown dog' {} \\;\n```\n\n\n\n(removed the \n-r\n which didn't make sense here) is terribly inefficient because you're running one \ngrep\n per file. \n;\n should only be used for commands that accept only one argument. Moreover here, because \ngrep\n looks only in one file, it will not print the file name, so you won't know where the matches are.\n\n\nYou're not looking inside device files, pipes, symlinks..., you're not following symlinks, but you're still potentially looking inside things like \n/proc/mem\n.\n\n\n\n```bash\nfind / -type f -exec grep -i 'the brown dog' {} +\n```\n\n\n\nwould be a lot better because as few \ngrep\n commands as possible would be run. You'd get the file name unless the last run has only one file. For that it's better to use:\n\n\n\n```bash\nfind / -type f -exec grep -i 'the brown dog' /dev/null {} +\n```\n\n\n\nor with GNU \ngrep\n:\n\n\n\n```bash\nfind / -type f -exec grep -Hi 'the brown dog' {} +\n```\n\n\n\nNote that \ngrep\n will not be started until \nfind\n has found enough files for it to chew on, so there will be some initial delay. And \nfind\n will not carry on searching for more files until the previous \ngrep\n has returned. Allocating and passing the big file list has some (probably negligible) impact, so all in all it's probably going to be less efficient than a \ngrep -r\n that doesn't follow symlink or look inside devices.\n\n\nWith GNU tools:\n\n\n\n```bash\nfind / -type f -print0 | xargs -r0 grep -Hi 'the brown dog'\n```\n\n\n\nAs above, as few \ngrep\n instances as possible will be run, but \nfind\n will carry on looking for more files while the first \ngrep\n invocation is looking inside the first batch. That may or may not be an advantage though. For instance, with data stored on rotational hard drives, \nfind\n and \ngrep\n accessing data stored at different locations on the disk will slow down the disk throughput by causing the disk head to move constantly. In a RAID setup (where \nfind\n and \ngrep\n may access different disks) or on SSDs, that might make a positive difference.\n\n\nIn a RAID setup, running several \nconcurrent\n \ngrep\n invocations might also improve things. Still with GNU tools on RAID1 storage with 3 disks,\n\n\n\n```bash\nfind / -type f -print0 | xargs -r0 -P2 grep -Hi 'the brown dog'\n```\n\n\n\nmight increase the performance significantly. Note however that the second \ngrep\n will only be started once enough files have been found to fill up the first \ngrep\n command. You can add a \n-n\n option to \nxargs\n for that to happen sooner (and pass fewer files per \ngrep\n invocation).\n\n\nAlso note that if you're redirecting \nxargs\n output to anything but a terminal device, then the \ngreps\ns will start buffering their output which means that the output of those \ngrep\ns will probably be incorrectly interleaved. You'd have to use \nstdbuf -oL\n (where available like on GNU or FreeBSD) on them to work around that (you may still have problems with very long lines (typically >4KiB)) or have each write their output in a separate file and concatenate them all in the end.\n\n\nHere, the string you're looking for is fixed (not a regexp) so using the \n-F\n option might make a difference (unlikely as \ngrep\n implementations know how to optimise that already).\n\n\nAnother thing that could make a big difference is fixing the locale to C if you're in a multi-byte locale:\n\n\n\n```bash\nfind / -type f -print0 | LC_ALL=C xargs -r0 -P2 grep -Hi 'the brown dog'\n```\n\n\n\nTo avoid looking inside \n/proc\n, \n/sys\n..., use \n-xdev\n and specify the file systems you want to search in:\n\n\n\n```bash\nLC_ALL=C find / /home -xdev -type f -exec grep -i 'the brown dog' /dev/null {} +\n```\n\n\n\nOr prune the paths you want to exclude explicitly:\n\n\n\n```bash\nLC_ALL=C find / \\( -path /dev -o -path /proc -o -path /sys \\) -prune -o \\\n  -type f -exec grep -i 'the brown dog' /dev/null {} +\n```",
    "url": "https://unix.stackexchange.com/questions/131535/recursive-grep-vs-find-type-f-exec-grep-which-is-more-efficient-faster"
  },
  {
    "question_title": "How do I loop through only directories in bash?",
    "question_body": "I have a folder with some directories and some files (some are hidden, beginning with dot).\n\n\n\n```bash\nfor d in *; do\n echo $d\ndone\n```\n\n\n\nwill loop through all files and directories, but I want to loop only through directories. How do I do that?",
    "answer": "You can specify a slash at the end to match only directories:\n\n\n\n```bash\nfor d in */ ; do\n    echo \"$d\"\ndone\n```\n\n\n\nIf you want to exclude symlinks, use a test to \ncontinue\n the loop if the current entry is a link. You need to remove the trailing slash from the name in order for \n-L\n to be able to recognise it as a symbolic link:\n\n\n\n```bash\nfor d in */ ; do\n    [ -L \"${d%/}\" ] && continue\n    echo \"$d\"\ndone\n```",
    "url": "https://unix.stackexchange.com/questions/86722/how-do-i-loop-through-only-directories-in-bash"
  },
  {
    "question_title": "What does &quot;rc&quot; in .bashrc stand for?",
    "question_body": "Is it \"resource configuration\", by any chance?",
    "answer": "As is often the case with obscure terms, the \nJargon File\n has an answer:\n\n\n\n> [Unix: from runcom files on the CTSS system 1962-63, via the startup script /etc/rc] Script file containing startup instructions for an application program (or an entire operating system), usually a text file containing commands of the sort that might have been invoked manually once the system was running but are to be executed automatically each time the system starts up.\n\n\n\nThus, it would seem that the \"rc\" part stands for \"runcom\", which I believe can be expanded to \"run commands\".  In fact, this is exactly what the file contains, commands that bash should run.",
    "url": "https://unix.stackexchange.com/questions/3467/what-does-rc-in-bashrc-stand-for"
  },
  {
    "question_title": "speed up gzip compression",
    "question_body": "Is it possible to speed up the \ngzip\n process?\n\n\nI'm using\n\n\n\n```bash\nmysqldump \"$database_name\" | gzip > $BACKUP_DIR/$database_name.sql.gz\n```\n\n\n\nto backup a database into a directory, \n$BACKUP_DIR\n.\n\n\nthe manpage says:\n\n\n\n> -# --fast --best\n> Regulate  the  speed  of compression using the\n> specified digit #, where -1  or  --fast  indi‐\n> cates  the  fastest  compression  method (less\n> compression) and -9 or  --best  indicates  the\n> slowest compression method (best compression).\n> The default compression level is -6 (that  is,\n> biased  towards high compression at expense of\n> speed).\n\n\n\n\n\nHow effective would it be to use \n--fast\n?\n\n\nIs this effectively lowering the CPU usage on a modern computer?\n\n\n\n\nMy test results\n\n\nI didn't notice any acceleration:\n\n\n\n\n7 min, 47 seconds (with default ratio \n-6\n)\n\n\n8 min, 36 seconds (with ratio \n--fast\n ( = 9 ))\n\n\n\n\nSo it seems it takes even longer to use the fast compression?\n\n\nOnly higher compression really slows it down:\n\n\n\n\n11 min, 57 seconds (with ratio \n--best\n ( = 1 ))\n\n\n\n\nAfter getting the Idea with \nlzop\n I tested that too and it really is faster:\n\n\n\n\n6 min, 14 seconds with \nlzop -1 -f -o $BACKUP_DIR/$database_name.sql.lzo",
    "answer": "If you have a multi-core machine using \npigz\n is much faster than traditional gzip.\n\n\n\n> pigz, which stands for parallel implementation of gzip, is a fully functional replacement for gzip that exploits multiple processors and multiple cores to the hilt when compressing data. pigz was written by Mark Adler, and uses the zlib and pthread libraries.\n\n\n\nPigz ca be used as a drop-in replacement for gzip. Note than only the compression can be parallelised, not the decompression.\n\n\nUsing pigz the command line becomes\n\n\n\n```bash\nmysqldump \"$database_name\" | pigz > $BACKUP_DIR/$database_name.sql.gz\n```",
    "url": "https://unix.stackexchange.com/questions/88732/speed-up-gzip-compression"
  },
  {
    "question_title": "Installing git &quot;sudo: apt-get: command not found&quot;",
    "question_body": "I am trying to install git. I run the following command: \n\n\nsudo apt-get install git-core git-gui git-doc\n \n\n\nBut receive the following error: \n\nsudo: apt-get: command not found\n\n\nWhat should I do?",
    "answer": "Since you're using CentOS 5, the default \npackage manager\n is \nyum\n, not \napt-get\n. To install a program using it, you'd normally use the following command:\n\n\n\n```bash\n$ sudo yum install <packagename>\n```\n\n\n\nHowever, when trying to install git this way, you'll encounter the following error on CentOS 5:\n\n\n\n```bash\n$ sudo yum install git\nSetting up Install Process\nParsing package install arguments\nNo package git available.\nNothing to do\n```\n\n\n\nThis tells you that the package repositories that \nyum\n knows about don't contain the required rpms (RPM Package Manager files) to install \ngit\n. This is presumably because CentOS 5 is based on RHEL 5, which was released in 2007, before \ngit\n was considered a mature version control system. To get around this problem, we need to add additional repositories to the list that \nyum\n uses (We're going to add the RPMforge repository, as per \nthese instructions\n).\n\n\nThis assumes you want the i386 packages. Test by running \nuname -i\n. If you want the x86_64 packages, replace all occurrences of i386 with x86_64 in the following commands\n\n\nFirst, download the \nrpmforge-release\n package:\n\n\n\n```bash\n$ wget http://packages.sw.be/rpmforge-release/rpmforge-release-0.5.3-1.el5.rf.i386.rpm\n```\n\n\n\nNext, verify and install the package:\n\n\n\n```bash\n$ sudo rpm --import http://apt.sw.be/RPM-GPG-KEY.dag.txt\n$ rpm -K rpmforge-release-0.5.3-1.el5.rf.i386.rpm\n$ sudo rpm -i rpmforge-release-0.5.3-1.el5.rf.i386.rpm\n```\n\n\n\nAnd now we should be able to install \ngit\n:\n\n\n\n```bash\n$ sudo yum install git-gui\n```\n\n\n\nyum\n will work out the dependencies, and ask you at relevant points if you want to proceed. Press \ny\n for Yes, and \nn\n or \nreturn\n for No.",
    "url": "https://unix.stackexchange.com/questions/33688/installing-git-sudo-apt-get-command-not-found"
  },
  {
    "question_title": "Is there a way to prevent git from changing permissions and ownership on pull?",
    "question_body": "Every time I do \ngit pull\n or \ngit reset\n, \ngit\n resets changes to permissions and ownership I made. See for yourself:\n\n\n\n```bash\n#!/usr/bin/env bash\nrm -rf 1 2\n\nmkdir 1\ncd 1\ngit init\necho 1 > 1 && git add 1 && git ci -m 1\n\ngit clone . ../2\ncd $_\nchmod 0640 1\nchgrp http 1\n\ncd ../1\necho 12 > 1 && git ci -am 2\n\ncd ../2\nstat 1\ngit pull\nstat 1\n```\n\n\n\nThe output:\n\n\n\n```bash\n$ ./1.sh 2>/dev/null | grep -F 'Access: ('\nAccess: (0640/-rw-r-----)  Uid: ( 1000/    yuri)   Gid: (   33/    http)\nAccess: (0664/-rw-rw-r--)  Uid: ( 1000/    yuri)   Gid: ( 1000/    yuri)\n```\n\n\n\nIs there a way to work around it?\n\n\nI want to make some files/directories accessible for writing by the web server.",
    "answer": "This sounds like the user you're running has the default group set to \nyuri\n. You can confirm this like so:\n\n\n\n```bash\n$ id -a\nuid=1000(saml) gid=1000(saml) groups=1000(saml),10(wheel),989(wireshark)\n```\n\n\n\nThe UID of your account is this: \nuid=1000(saml)\n whereas the default group is \ngit=1000(saml)\n and any secondary groups are thereafter.\n\n\nNOTE:\n If you want the git clone to have specific ownership, then you have at least 2 options.\n\n\nOption #1\n\n\nSet a parent directory with the permissions as you want like so:\n\n\n\n```bash\n$ mkdir topdir\n$ chgrp http topdir\n$ chmod g+s topdir\n\n$ cd topdir\n$ git clone ....\n```\n\n\n\nThis forced the directory \ntopdir\n to enforce any child directories underneath it to have the group \nhttp\n applied. This will work by in large but can lead to problems, since if you move files into this git clone workspace, those files will not have their groups enforced by the changes made above.\n\n\nOption #2\n\n\nPrior to doing work, change your default group to \nhttp\n like so:\n\n\n\n```bash\n$ newgrp http\n$ git clone ...\n```\n\n\n\nThis method will force any new files created to have their group set to \nhttp\n instead of your normal default group of \nyuri\n, but this will only work so long as you remember to do a \nnewgrp\n prior to working in this workspace.\n\n\nOther options\n\n\nIf neither of these seem acceptable you can try using ACLs instead on the git workspace directory. These are discussed in multiple Q&A's on this site, such as in this Q&A titled: \nGetting new files to inherit group permissions on Linux\n.",
    "url": "https://unix.stackexchange.com/questions/160527/is-there-a-way-to-prevent-git-from-changing-permissions-and-ownership-on-pull"
  },
  {
    "question_title": "Generate File of a certain size?",
    "question_body": "I'd like to generate a file with the name \nexample.file\n. I could use\n\n\n\n```bash\ntouch example.file\n```\n\n\n\nbut I want the file to be exactly 24MB in size. I already checked the manpage of touch, but there is no parameter like this. Is there an easy way to generate files of a certain size?",
    "answer": "You can use dd:\n\n\n\n```bash\ndd if=/dev/zero of=output.dat  bs=24M  count=1\n```\n\n\n\nor\n\n\n\n```bash\ndd if=/dev/zero of=output.dat  bs=1M  count=24\n```\n\n\n\nor, on Mac,\n\n\n\n```bash\ndd if=/dev/zero of=output.dat  bs=1m  count=24\n```",
    "url": "https://unix.stackexchange.com/questions/101332/generate-file-of-a-certain-size"
  },
  {
    "question_title": "How can I find all files that do NOT contain a text string?",
    "question_body": "What \nconcise\n command can I use to find all files that do NOT contain a text string?\n\n\nI tried this (using -v to invert grep's parameters) with no luck:\n\n\n\n```bash\nfind . -exec grep -v -l shared.php {} \\;\n```\n\n\n\nSomeone said this would work:\n\n\n\n```bash\nfind . ! -exec grep -l shared.php {} \\;\n```\n\n\n\nBut it does not seem to work for me.\n\n\nThis page\n has this example:\n\n\n\n```bash\nfind ./logs -size +1c  > t._tmp\nwhile read filename\ndo\n     grep -q \"Process Complete\" $filename\n     if [ $? -ne 0 ] ; then\n             echo $filename\n     fi\ndone < t._tmp\nrm -f t_tmp\n```\n\n\n\nBut that's cumbersome and not at all concise.\n\n\nps: I know that \ngrep -L *\n will do this, but how can I use the find command in combination with grep to excluded files is what I really want to know.\n\n\npss: Also I'm not sure how to have grep include subdirectories with the \ngrep -L *\n syntax, but I still want to know how to use it with \nfind\n :)",
    "answer": "Your find should work if you change \n-v -l\n (files that have any line not matching) to \n-L\n (files with no lines matching), but you could also use \ngrep\n's recursive (\n-r\n) option:\n\n\n\n```bash\ngrep -rL shared.php .\n```",
    "url": "https://unix.stackexchange.com/questions/26836/how-can-i-find-all-files-that-do-not-contain-a-text-string"
  },
  {
    "question_title": "How to kill all process with given name?",
    "question_body": "I run the command \ngrep <application_name>\n and get a list of processes.\n\n\nI want to kill all process from the list: \n19440\n, \n21630\n, \n22694\n.\n\n\nI have tried, but it works with errors.\n\n\n\n```bash\nkill: illegal pid ?\nkill: illegal pid 00:00:00\nkill: illegal pid <application_name>\n```\n\n\n\nHow can I do this gracefully?",
    "answer": "```bash\npkill -f 'PATTERN'\n```\n\n\n\nWill kill all the processes that the pattern \nPATTERN\n matches.  With the \n-f\n option, the whole command line (i.e. including arguments) will be taken into account. Without the \n-f\n option, only the command name will be taken into account.\n\n\nSee also \nman pkill\n on your system.",
    "url": "https://unix.stackexchange.com/questions/316065/how-to-kill-all-process-with-given-name"
  },
  {
    "question_title": "How to recover a corrupted &quot;tar.gz&quot; file",
    "question_body": "I suddenly needed to recover an old \ntar.gz\n file, but as soon as I execute so:\n\n\n\n```bash\ntar -zxvf filename.tar.gz\n```\n\n\n\nI get this:\n\n\n\n```bash\ngzip: stdin: invalid compressed data--format violated  \ntar: Child returned status 1  \ntar: Error is not recoverable: exiting now\n```",
    "answer": "What you should try is the following:\n\n\n\n\nUse \nfile\n command on the archive to see if it's recognized as \ngzip\n-ped data.\n\n\nRun \nstrace gunzip\n on the file.  This will print the last bytes read from the file which might help you identify the point in file where corruption occurs.\n\n\nRun a debug build of \ngunzip\n under \ngdb\n.  Try to correct the corrupted section (you have to be extra lucky to be able to do that) and see if it can continue to the end of the file.\n\n\n\n\nDepending on the nature of corruption, you might or might not be able to recover your data.",
    "url": "https://unix.stackexchange.com/questions/17452/how-to-recover-a-corrupted-tar-gz-file"
  },
  {
    "question_title": "How to XZ a directory with TAR using maximum compression?",
    "question_body": "So I need to compress a directory with max compression. \n\n\nHow can I do it with \nxz\n? I mean I will need \ntar\n too because I can't compress a directory with only \nxz\n. Is there a oneliner to produce e.g. \nfoo.tar.xz\n?",
    "answer": "With GNU \ntar\n 1.22 (2009) or later (on most shells, like Bash):\n\n\n\n```bash\nXZ_OPT=-9 tar cJf tarfile.tar.xz directory\n```\n\n\n\ntar\n's \nuppercase\n \nJ\n switch uses XZ (whereas the lowercase \nj\n switch uses bzip2).\n\n\nThe \nXZ_OPT\n environment variable\n lets you set \nxz\n options.\n\n\nThis is now \nmaximal\n.\n\n\nSee \nman xz\n for other options you can set (\n-e\n/\n--extreme\n \nmight\n give you some additional compression benefit for some datasets).\n\n\n\n```bash\nXZ_OPT=-e9 tar cJf tarfile.tar.xz directory\n```",
    "url": "https://unix.stackexchange.com/questions/28976/how-to-xz-a-directory-with-tar-using-maximum-compression"
  },
  {
    "question_title": "Return owner of process given PID",
    "question_body": "I'm trying to grab the owner of a process from a list.\n\n\nI have the command \npidof nmap\n to get the  then \nps -u <PID> | grep USER\n that I'm currently playing around with. But when I run it, it ends up just printing the titles (top line):\n\n\n\n\nHow can I grab the name of the owner given the process ID?",
    "answer": "```bash\nps -o user= -p PIDHERE\n```\n\n\n\nThis selects the process \nPIDHERE\n with \n-p\n, then instructs \nps\n to format the output by printing only the column named \nuser\n; the \n=\n sign means \"rename the column \nuser\n to (nothing)\", effectively removing the header line.",
    "url": "https://unix.stackexchange.com/questions/284934/return-owner-of-process-given-pid"
  },
  {
    "question_title": "Why would I tar a single file?",
    "question_body": "At my company, we download a local development database snapshot as a \ndb.dump.tar.gz\n file. The compression makes sense, but the tarball only contains a single file (\ndb.dump\n).\n\n\nIs there any point to archiving a single file, or is \n.tar.gz\n just such a common idiom? Why not just \n.gz\n?",
    "answer": "Advantages of using \n.tar.gz\n instead of \n.gz\n are that\n\n\n\n\ntar\n stores more meta-data (UNIX permissions etc.) than \ngzip\n.\n\n\nthe setup can more easily be expanded to store multiple files\n\n\n.tar.gz files are very common, \nonly-gzipped\n files may puzzle some users.\n(cf.  \nMelBurslans comment\n)\n\n\n\n\nThe overhead of using \ntar\n is also very small.\n\n\nIf not really needed, I still do not recommend to tar a single file.\nThere are many useful tools which can access compressed single files directly (such as \nzcat\n, \nzgrep\n etc. - also existing for \nbzip2\n and \nxz\n).",
    "url": "https://unix.stackexchange.com/questions/277793/why-would-i-tar-a-single-file"
  },
  {
    "question_title": "What does --strip-components -C mean in tar?",
    "question_body": "I want to make sure I understand the following code:\n\n\n\n```bash\ntar xvzf FILE --exclude-from=exclude.me --strip-components 1 -C DESTINATION\n```\n\n\n\nwhich was posted in \nthis answer\n.\n\n\nFrom \nman tar\n:\n\n\n\n> --strip-components=NUMBER\n> strip NUMBER leading components from file names on extraction\n> -C, --directory=DIR\n> change to directory DIR\n\n\n\nI didn't understand the manual explanation for \n--strip-components\n.\n\n\nAbout \n-C\n, I understood that it means something like \"put stripped components in a noted directory.\"\n\n\nWhat does \n--strip-components -C\n mean?",
    "answer": "The fragment of manpage you included in your question comes from man\nfor GNU tar. GNU is a software project that prefers \ninfo\n manuals\nover manpages.  In fact, tar manpage \nhas been added to the GNU tar\nsource code tree only in\n2014\n\nand it still is just a reference, not a full-blown manual with\nexamples.  You can invoke a full info manual with \ninfo tar\n, it's\nalso available online\n\nhere\n.  It contains\nseveral examples of \n--strip-components\n usage, the relevant fragments\nare:\n\n\n\n> --strip-components=number\n> Strip given number of leading components from file names before extraction.\n> For example, if archive `archive.tar' contained `some/file/name', then running\n> tar --extract --file archive.tar --strip-components=2\n> would extract this file to file `name'.\n\n\n\nand:\n\n\n\n> --strip-components=number\n> Strip given number of leading components from file names before extraction.\n> For example, suppose you have archived whole `/usr' hierarchy to a tar archive named `usr.tar'. Among other files, this archive contains `usr/include/stdlib.h', which you wish to extract to the current working directory. To do so, you type:\n> $ tar -xf usr.tar --strip=2 usr/include/stdlib.h\n> The option `--strip=2' instructs tar to strip the two leading components (`usr/' and `include/') off the file name.\n\n\n\n\n\nThat said;\n\n\nThere are other implementations of tar out there, for example \nFreeBSD\ntar manpage\n has a\ndifferent explanation of this command:\n\n\n\n> --strip-components count\n> Remove the specified number of leading path elements. Pathnames\n> with fewer elements will be silently skipped. Note that the\n> pathname is edited after checking inclusion/exclusion patterns\n> but before security checks.\n\n\n\nIn other words, you should understand a Unix path as a sequence of\nelements separated by \n/\n (unless there is only one \n/\n).\n\n\nHere is my own example\n (other examples are available in the info manual I linked to above):\n\n\nLet's create a new directory structure:\n\n\n\n```bash\nmkdir -p a/b/c\n```\n\n\n\nPath \na/b/c\n is composed of 3 elements: \na\n, \nb\n, and \nc\n.\n\n\nCreate an empty file in this directory and put it into .tar archive:\n\n\n\n```bash\n$ touch a/b/c/FILE\n$ tar -cf archive.tar a/b/c/FILE\n```\n\n\n\nFILE\n is a 4th element of \na/b/c/FILE\n path.\n\n\nList contents of archive.tar:\n\n\n\n```bash\n$ tar tf archive.tar\na/b/c/FILE\n```\n\n\n\nYou can now extract archive.tar with \n--strip-components\n and an\nargument that will tell it how many path elements you want to be removed from the \na/b/c/FILE\n when extracted. Remove an original \na\n directory:\n\n\n\n```bash\nrm -r a\n```\n\n\n\nExtract with \n--strip-components=1\n - only \na\n has not been recreated:\n\n\n\n```bash\n$ tar xf archive.tar --strip-components=1\n$ ls -Al\ntotal 16\n-rw-r--r-- 1 ja users 10240 Mar 26 15:41 archive.tar\ndrwxr-xr-x 3 ja users  4096 Mar 26 15:43 b\n$ tree b\nb\n└── c\n    └── FILE\n\n1 directory, 1 file\n```\n\n\n\nWith \n--strip-components=2\n you see that \na/b\n - 2 elements have not\nbeen recreated:\n\n\n\n```bash\n$ rm -r b\n$ tar xf archive.tar --strip-components=2\n$ ls -Al\ntotal 16\n-rw-r--r-- 1 ja users 10240 Mar 26 15:41 archive.tar\ndrwxr-xr-x 2 ja users  4096 Mar 26 15:46 c\n$ tree c\nc\n└── FILE\n\n0 directories, 1 file\n```\n\n\n\nWith \n--strip-components=3\n 3 elements \na/b/c\n have not been recreated\nand we got \nFILE\n in the same level directory in which we run \ntar\n:\n\n\n\n```bash\n$ rm -r c\n$ tar xf archive.tar --strip-components=3\n$ ls -Al\ntotal 12\n-rw-r--r-- 1 ja users     0 Mar 26 15:39 FILE\n-rw-r--r-- 1 ja users 10240 Mar 26 15:41 archive.tar\n```\n\n\n\n\n\n-C\n option tells tar to change to a given directory before running a\nrequested operation, extracting but also archiving.  In \nthis\ncomment\n\nyou asked:\n\n\n\n> Asking tar to do cd: why cd? I mean to ask, why it's not just mv?\n\n\n\nWhy do you think that \nmv\n is better? To what directory would you like\nto extract tar archive first:\n\n\n\n\n/tmp - what if it's missing or full?\n\n\n\"$TMPDIR\" - what if it's unset, missing or full?\n\n\ncurrent directory - what if user has no \nw\n permission, just \nr\n and\n\nx\n?\n\n\nwhat if a temporary directory, whatever it is already contained\nfiles with the same names as in \ntar\n archive and extracting would\noverwrite them?\n\n\nwhat if a temporary directory, whatever it is didn't support Unix\nfilesystems and all info about ownership, executable bits etc. would\nbe lost?\n\n\n\n\nAlso notice that \n-C\n is a common \nchange directory\n option in other\nprograms as well, \nGit\n and\n\nmake\n are first that come to my\nmind.",
    "url": "https://unix.stackexchange.com/questions/535772/what-does-strip-components-c-mean-in-tar"
  },
  {
    "question_title": "Colored word-diff just like git&#39;s?",
    "question_body": "I've been for long using git's (colored) \n--word-diff\n which I find awesome. I found myself today in a situation where I had two strings I needed to compare word by word and craving for something just like git's diff.\n\n\nI am aware of \nwdiff\n + \ncolordiff\n, but its results certainly aren't stellar:\n\n\n\n```bash\nme@me:~$ wdiff <(echo -e \"abc\\ndef\") <(echo -e \"dbcx\\ndef\") | colordiff\n[-abc-]{+dbcx+}\ndef\n```\n\n\n\nIs there any better option around? I don't particularly like those \n[-\n, \n-]\n and \n{+\n, \n+}\n.",
    "answer": "You can use \ngit\n's own \ndiff-highlight\n.  It can highlight word differences in unified \ndiff\n output, and it can also cope with ANSI colors on input.  So you can do something like this:\n\n\n\n```bash\ncolordiff -u <(echo -e \"foo abc\\ndef\") <(echo -e \"foo dbcx\\ndef\") | diff-highlight\n```",
    "url": "https://unix.stackexchange.com/questions/307095/colored-word-diff-just-like-gits"
  },
  {
    "question_title": "How to run grep with multiple AND patterns?",
    "question_body": "I would like to get the multi pattern match with implicit \nAND\n between patterns, i.e. equivalent to running several greps in a sequence:\n\n\n\n```bash\ngrep pattern1 | grep pattern2 | ...\n```\n\n\n\nSo how to convert it to something like?\n\n\n\n```bash\ngrep pattern1 & pattern2 & pattern3\n```\n\n\n\nI would like to use single grep because I am building arguments dynamically, so everything has to fit in one string. Using filter is system feature, not grep, so it is not an argument for it.\n\n\n\n\nDon't confuse this question with:\n\n\n\n```bash\ngrep \"pattern1\\|pattern2\\|...\"\n```\n\n\n\nThis is an \nOR\n multi pattern match. I am looking for an \nAND\n pattern match.",
    "answer": "To find the lines that match each and everyone of a list of patterns, \nagrep\n (the original one, now shipped with \nglimpse\n, not the unrelated one in the \nTRE regexp library\n) can do it with this syntax:\n\n\n\n```bash\nagrep 'pattern1;pattern2'\n```\n\n\n\nWith GNU \ngrep\n, when built with PCRE support, you can use several lookahead assertions:\n\n\n\n```bash\ngrep -P '^(?=.*pattern1)(?=.*pattern2)'\n```\n\n\n\nWith \nast \ngrep\n:\n\n\n\n```bash\ngrep -X '.*pattern1.*&.*pattern2.*'\n```\n\n\n\n(adding \n.*\ns as \n<x>&<y>\n matches strings that match both \n<x>\n and \n<y>\n \nexactly\n, \na&b\n would never match as there's no such string that can \nbe\n both \na\n and \nb\n at the same time).\n\n\nIf the patterns don't overlap, you may also be able to do:\n\n\n\n```bash\ngrep -e 'pattern1.*pattern2' -e 'pattern2.*pattern1'\n```\n\n\n\nThe best portable way is probably with \nawk\n as already mentioned:\n\n\n\n```bash\nawk '/pattern1/ && /pattern2/'\n```\n\n\n\nOr with \nsed\n:\n\n\n\n```bash\nsed -e '/pattern1/!d' -e '/pattern2/!d'\n```\n\n\n\nOr \nperl\n:\n\n\n\n```bash\nperl -ne 'print if /pattern1/ && /pattern2/'\n```\n\n\n\nPlease beware that all those will have different regular expression syntaxes.\n\n\nThe \nawk\n/\nsed\n/\nperl\n ones don't reflect whether any line matched the patterns in their exit status. To so that you need:\n\n\n\n```bash\nawk '/pattern1/ && /pattern2/ {print; found = 1}\n     END {exit !found}'\n```\n\n\n\n\n```bash\nperl -ne 'if (/pattern1/ && /pattern2/) {print; $found = 1}\n          END {exit !$found}'\n```\n\n\n\nOr pipe the command to \ngrep '^'\n.\n\n\nFor potentially gzip-compressed files, you can use \nzgrep\n which is generally a shell script wrapper around \ngrep\n, and use one of the \ngrep\n solutions above (not the ast-open one as that \ngrep\n implementation cannot be use by \nzgrep\n) or you could use the \nPerlIO::gzip\n module of \nperl\n which can transparently uncompress files upon input:\n\n\n\n```bash\nperl -MPerlIO::gzip -Mopen='IN,gzip(autopop)' -ne '\n  print \"$ARGV:$_\" if /pattern1/ && /pattern2/' -- *.gz\n```\n\n\n\n(which if the files are small enough at least is even going to be more efficient than \nzgrep\n as the decompression is done internally without having to run \ngunzip\n for each file).",
    "url": "https://unix.stackexchange.com/questions/55359/how-to-run-grep-with-multiple-and-patterns"
  },
  {
    "question_title": "Use git submodule foreach with function",
    "question_body": "My indention is to have a script that updates all git submodules according to which branch is given. If there's no such branch for a submodule, master is used.\n\n\nThis is what I have now:\n\n\n\n```bash\n#!/bin/bash -x\n\nif [ -z $1 ]; then\n    echo \"Branch name required.\"\n    exit\nfi\n\nfunction pbranch {\n    exists=`git show-ref refs/heads/$branch`\n\n    if [ -z $exists ]; then\n        branch=\"master\"\n    fi\n\n    git co $branch\n    git pull origin $branch\n}\n\nbranch=$1\n\ngit submodule foreach pbranch\n```\n\n\n\nBut when running this script, the error is thrown:\n\n\n\n```bash\noleq@pc ~/project> git-fetchmodules major\n+ '[' -z major ']'\n+ branch=major\n+ git submodule foreach pbranch\nEntering 'submodule'\n/usr/lib/git-core/git-submodule: 1: eval: pbranch: not found\nStopping at 'submodule'; script returned non-zero status.\n```\n\n\n\nMy guess is that \ngit submodule foreach\n utilizes eval (according to the \ndocumentation\n), which I don't use correctly in this context. \n\n\nThere are billions of \nexamples\n of how to use this command with \"inline callback\" but I couldn't found a single one with the callback in form of the function. Any idea how to solve this?",
    "answer": "I resolved my issue by putting the function inside of the quotes as a callback:\n\n\n\n```bash\n#!/bin/bash\n\nif [ -z $1 ]; then\n    echo \"Branch name required.\"\n    exit\nfi\n\ngit submodule foreach \"\n    branch=$1;\n    exists=\\$(git show-ref refs/heads/\\$branch | cut -d ' ' -f1);\n\n    if [ -z \\$exists ]; then\n        branch='master';\n    fi;\n\n    echo Checking branch \\$branch for submodule \\$name.;\n\n    git fetch --all -p;\n    git co \\$branch;\n    git reset --hard origin/\\$branch;\n\"\n```\n\n\n\nNote that variables like \n$1\n are those from the script's namespace. The \"escaped ones\" like \n$\\(bar)\n, \n\\$branch\n are evaluated within \"the callback\". It was pretty easy.",
    "url": "https://unix.stackexchange.com/questions/83468/use-git-submodule-foreach-with-function"
  },
  {
    "question_title": "How can I solve this ssh-agent problem?",
    "question_body": "I'm using Linux Mint, and have not been able to get gnome-keyring to unlock automatically at login, it seems.\n\n\nA symptom of my problem is as follows:\n\n\n\n```bash\n$ ssh-add\nIdentity added: /home/me/.ssh/id_rsa (/home/me/.ssh/id_rsa)\n\n$ git pull\nWARNING: gnome-keyring:: couldn't connect to: /tmp/keyring-Nmf3J3/pkcs11: No such file or directory\n```\n\n\n\nHow can I make it that git can push/pull without any passphrase input from me?\n\n\nI realize there's several things here with gnome-keyring, and ssh-agent, but have not been able to nail it down.\n\n\nRunning \nssh-add\n during a session means that I am no longer asked for my passphrase for SSH/git.\n\n\nThe problem is that I would need to run \nssh-add\n during each session - I must be missing how to have Gnome's keyring unlock at login.\n\n\n\n```bash\n$ export | grep GNOME          \nGNOME_KEYRING_CONTROL=/tmp/keyring-hjMM4V\nGNOME_KEYRING_PID=1961\n```\n\n\n\nIt happened again during the same session as the first edit. I did \ngit pull\n and got \nWARNING: gnome-keyring:: couldn't connect to: /tmp/keyring-hjMM4V/pkcs11: No such file or directory\n.\n\n\n\n```bash\n$ env | grep SSH\nSSH_AGENT_PID=2116\nSSH_AUTH_SOCK=/tmp/ssh-OACxJMBY2038/agent.2038\n\n$ ps -fp $SSH_AGENT_PID\nUID        PID  PPID  C STIME TTY          TIME CMD\neoin      2116  2038  0 09:47 ?        00:00:00 /usr/bin/ssh-agent /usr/bin/dbus-launch --exit-with-session x-session-manager\n```",
    "answer": "What is meant to happen is:\n\n\nYou start a gnome session, part of that a gnome-keyring daemon (which also acts as a ssh agent) starts and the environment of anything started during that gnome session is updated with information on how to contact that ssh agent. The password you issue upon graphically logging in is used to unlock the default keyring.\n\n\nWhen you use gnome-keyring as a ssh-agent, you don't want to use another agent like \nssh-agent\n.\n\n\nWhen your X session terminates, so does gnome-keyring. But your tmux session remains. Then, even if you start another gnome-keyring or ssh-agent, the environment of the processes already started by \ntmux\n won't be able to talk to it unless you update their environment with the path of the new socket.\n\n\nWhat you could do is:\n\n\n\n```bash\ngnome-keyring-daemon -r > ~/.gkr\n```\n\n\n\nAnd do \n. ~/.gkr\n in all the shells you want to use the new gnome-keyring\n\n\nBeware though of which DISPLAY that gnome-keyring-daemon is going to connect to.",
    "url": "https://unix.stackexchange.com/questions/52551/how-can-i-solve-this-ssh-agent-problem"
  },
  {
    "question_title": "How to read first and last line from cat output?",
    "question_body": "I have text file. Task - get first and last line from file after \n\n\n\n```bash\n$ cat file | grep -E \"1|2|3|4\" | commandtoprint\n\n$ cat file\n1\n2\n3\n4\n5\n```\n\n\n\nNeed this without cat output (only 1 and 5).\n\n\n\n```bash\n~$ cat file | tee >(head -n 1) >(wc -l)\n1\n2\n3\n4\n5\n5\n1\n```\n\n\n\nMaybe awk and more shorter solution exist...",
    "answer": "sed\n Solution:\n\n\n\n```bash\nsed -e 1b -e '$!d' file\n```\n\n\n\nWhen reading from \nstdin\n if would look like this (for example \nps -ef\n):\n\n\n\n```bash\nps -ef | sed -e 1b -e '$!d'\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      1931  1837  0 20:05 pts/0    00:00:00 sed -e 1b -e $!d\n```\n\n\n\nhead & tail\n Solution:\n\n\n\n```bash\n(head -n1 && tail -n1) <file\n```\n\n\n\nWhen data is coming from a command (\nps -ef\n):\n\n\n\n```bash\nps -ef 2>&1 | (head -n1 && tail -n1)\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      2068  1837  0 20:13 pts/0    00:00:00 -bash\n```\n\n\n\nawk\n Solution:\n\n\n\n```bash\nawk 'NR==1; END{print}' file\n```\n\n\n\nAnd also the piped example with \nps -ef\n:\n\n\n\n```bash\nps -ef | awk 'NR==1; END{print}'\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot      1935  1837  0 20:07 pts/0    00:00:00 awk NR==1; END{print}\n```",
    "url": "https://unix.stackexchange.com/questions/139089/how-to-read-first-and-last-line-from-cat-output"
  },
  {
    "question_title": "How can I check if a gzipped file is empty?",
    "question_body": "Is there a quick way to check if a gzipped file is empty, or do I have to unzip it first?\n\n\nexample:\n\n\n\n```bash\n$ touch foo\n$ if [ -s foo ]; then echo not empty; fi\n$ gzip foo\n$ if [ -s foo.gz ]; then echo not empty; fi\nnot empty\n$ wc -l foo.gz\n      1 foo.gz\n```",
    "answer": "gzip -l foo.gz | awk 'NR==2 {print $2}'\n prints the size of the uncompressed data.\n\n\n\n```bash\nif LC_ALL=C gzip -l foo.gz | awk 'NR==2 {exit($2!=0)}'; then\n  echo foo is empty\nelse\n  echo foo is not empty\nfi\n```\n\n\n\nAlternatively you can start uncompressing the data.\n\n\n\n```bash\nif [ -n \"$(gunzip <foo.gz | head -c 1 | tr '\\0\\n' __)\" ]; then\n    echo \"foo is not empty\"\nelse\n    echo \"foo is empty\"\nfi\n```\n\n\n\n(If your system doesn't have \nhead -c\n to extract the first byte, use \nhead -n 1\n to extract the first line instead.)",
    "url": "https://unix.stackexchange.com/questions/6758/how-can-i-check-if-a-gzipped-file-is-empty"
  },
  {
    "question_title": "7zip, xz, gzip, tar, etc -- what are the differences?",
    "question_body": "what factors should be considered when choosing among 7zip, xz, gzip, tar, etc. for compressing and archiving files?",
    "answer": "I first want to clarify that, of the list you provided, \ntar\n is the only one that is \nnot\n a compression algorithm. \ntar\n is short for \nT\nape \nAr\nchive, and is used to create archive files. In short, a single file that consists of one or more files.  It is used to bundle files together so that they can be compressed by a compressor that is only able to compress a single file.\n\n\nIn terms of availability, \nzip\n is widely available across UNIX (Linux/BSD/MacOS) and Windows systems. Therefore a \nzip\n file is highly portable. Tools to compress/decompress \nxz\n and \ngzip\n files are also available on Windows systems, but are more commonly seen and used on UNIX systems.\n\n\nxz\n and \n7zip\n are known to have a better compression algorithm than \ngzip\n, but use more memory and time to compress/decompress. This topic is nicely discussed \nhere\n.\n\n\nI would recommend using \ngzip\n when less memory is available, and compression/decompression speed is a concern. \n7zip\n and \nxz\n can be used when space is a concern and compression/decompression speed is not.\n\n\nSome nice benchmarks on these algorithms can be found \nhere\n. \nNote:\n \nLZMA\n is the compression algorithm used by \n7zip\n and \nxz\n.",
    "url": "https://unix.stackexchange.com/questions/322746/7zip-xz-gzip-tar-etc-what-are-the-differences"
  },
  {
    "question_title": "Storing username and password in Git",
    "question_body": "When I do \n\n\n\n```bash\ngit push\n```\n\n\n\nI get the command prompt like \n\n\n\n```bash\nUsername for 'https://github.com':\n```\n\n\n\nthen I enter my username manually like\n\n\n\n```bash\nUsername for 'https://github.com': myusername\n```\n\n\n\nand then I hit \nEnter\n and I get prompt for my password\n\n\n\n```bash\nPassword for 'https://myusername@github.com':\n```\n\n\n\nI want the username to be written automatically instead of manually having to type it all the time.\n\n\nI tried doing it with \nxdotool\n but it didn't work out.\n\n\nI have already done\n\n\n\n```bash\ngit config --global user.name myusername\ngit config --global user.email myemail@gmail.com\n```\n\n\n\nbut still it always asks for me to type manually",
    "answer": "In Terminal, enter the following to enable credential memory:\n\n\n\n```bash\n$ git config --global credential.helper cache\n```\n\n\n\nYou may update the default password cache timeout (in seconds):\n\n\n\n```bash\n# This cache timeout is in seconds\n$ git config --global credential.helper 'cache --timeout=3600'\n```\n\n\n\nYou may also use (but please use the \nsingle\n quotes, else double quotes may break for some characters):\n\n\n\n```bash\n$ git config --global user.name 'your user name'\n$ git config --global user.password 'your password'\n```",
    "url": "https://unix.stackexchange.com/questions/379272/storing-username-and-password-in-git"
  },
  {
    "question_title": "How to run grep on a single column?",
    "question_body": "I want to grep the output of my \nls -l\n command:\n\n\n\n```bash\n-rw-r--r--   1 root root       1866 Feb 14 07:47 rahmu.file\n-rw-r--r--   1 rahmu user     95653 Feb 14 07:47 foo.file\n-rw-r--r--   1 rahmu user   1073822 Feb 14 21:01 bar.file\n```\n\n\n\nI want to run \ngrep rahmu\n on column $3 only, so the output of my \ngrep\n command should look like this:\n\n\n\n```bash\n-rw-r--r--   1 rahmu user     95653 Feb 14 07:47 foo.file\n-rw-r--r--   1 rahmu user   1073822 Feb 14 21:01 bar.file\n```\n\n\n\nWhat's the simplest way to do it? The answer must be portable across many Unices, preferably focusing on Linux and Solaris.\n\n\nNB: I'm not looking for a way to find all the files belonging to a given user. This example was only given to make my question clearer.",
    "answer": "One more time \nawk\n saves the day!\n\n\nHere's a straightforward way to do it, with a relatively simple syntax:\n\n\n\n```bash\nls -l | awk '{if ($3 == \"rahmu\") print $0;}'\n```\n\n\n\nor even simpler: \n(Thanks to Peter.O in the comments)\n\n\n\n```bash\nls -l | awk '$3 == \"rahmu\"'\n```",
    "url": "https://unix.stackexchange.com/questions/31753/how-to-run-grep-on-a-single-column"
  },
  {
    "question_title": "How to use rsync to backup a directory without git subdirectory",
    "question_body": "I want to copy my \nc\n directory with all subdirectories excluding \n./git\n subdirectory. I do it using \nrsync\n :\n\n\n\n```bash\necho \"copy c and sh files \"\nrsync -a --include='*.c' --include='*.sh' --include='*/' --exclude='*' ~/c/ ~/Dropbox/Public/c\n# remove .git directory = do not send it to dropbox. Thx to Tomasz Sowa\nrm -rf ~/Dropbox/Public/c/.git\n```\n\n\n\nCan I do it better?",
    "answer": "Just add an explicit exclude for .git:\n\n\nrsync -a --exclude='.git/' --include='*.c' --include='*.sh' --include='*/' --exclude='*' ~/c/ ~/Dropbox/Public/c\n\n\nAnother option is to create \n~/.cvsignore\n containing the following line along with any other directories you'd like to exclude:\n\n\n.git/",
    "url": "https://unix.stackexchange.com/questions/100660/how-to-use-rsync-to-backup-a-directory-without-git-subdirectory"
  },
  {
    "question_title": "Convince grep to output all lines, not just those with matches",
    "question_body": "Say I have the following file:\n\n\n\n```bash\n$ cat test\n\ntest line 1\ntest line 2\nline without the search word\nanother line without it\ntest line 3 with two test words\ntest line 4\n```\n\n\n\nBy default, \ngrep\n returns each line that contains the search term:\n\n\n\n```bash\n$ grep test test\n\ntest line 1\ntest line 2\ntest line 3 with two test words\ntest line 4\n```\n\n\n\nPassing the \n--color\n parameter to \ngrep\n will make it highlight the portion of the line that matches the search expression, but it still only returns lines that contain the expression. Is there a way to get \ngrep\n to output every line in the source file, but highlight the matches?\n\n\nMy current terrible hack to accomplish this (at least on files that don't have 10000+ consecutive lines with no matches) is:\n\n\n\n```bash\n$ grep -B 9999 -A 9999 test test\n```\n\n\n\n\n\nIf \ngrep\n can't accomplish this, is there another command-line tool that offers the same functionality? I've fiddled with \nack\n, but it doesn't seem to have an option for it either.",
    "answer": "```bash\ngrep --color -E \"test|$\" yourfile\n```\n\n\n\nWhat we're doing here is matching against the \n$\n pattern and the test pattern, obviously \n$\n doesn't have anything to colourize so only the test pattern gets color.  The \n-E\n just turns on extended regex matching.\n\n\nYou can create a function out of it easily like this:\n\n\n\n```bash\nhighlight () { grep --color -E \"$1|$\" \"${@:1}\" ; }\n```",
    "url": "https://unix.stackexchange.com/questions/366/convince-grep-to-output-all-lines-not-just-those-with-matches"
  },
  {
    "question_title": "Check validity of gz file",
    "question_body": "How can I check the validity of a gz file, I do not have the hash of the file, I'm using \ngzip -t\n but it is not returning any output.",
    "answer": "The \ngzip -t\n command only returns an exit code to the shell saying whether the file passed the integrity test or not.\n\n\nExample (in a script):\n\n\n\n```bash\nif gzip -t file.gz; then\n    echo 'file is ok'\nelse \n    echo 'file is corrupt'\nfi\n```\n\n\n\nAdding \n-v\n will make it actually report the result with a message.\n\n\nExample:\n\n\n\n```bash\n$ gzip -v -t file.gz\nfile.gz:        OK\n```\n\n\n\nSo the file is ok.  Let's corrupt the file (by writing the character \n0\n at byte 40 in the file) and try again.\n\n\n\n```bash\n$ dd seek=40 bs=1 count=1 of=file.gz <<<\"0\"\n1+0 records in\n1+0 records out\n1 bytes transferred in 0.000 secs (2028 bytes/sec)\n```\n\n\n\n\n```bash\n$ gzip -v -t file.gz\nfile.gz:        gzip: file.gz: Inappropriate file type or format\n```\n\n\n\nThe integrity of a file with respect to its compression does \nnot\n guarantee that the file \ncontents\n is what you believe it is.  If you have an MD5 checksum (or some similar checksum) of the file from whomever provided it, then you would be able to get an additional confirmation that the file not only is a valid \ngzip\n archive, but also that its contents is what you expect it to be.",
    "url": "https://unix.stackexchange.com/questions/359303/check-validity-of-gz-file"
  },
  {
    "question_title": "How to choose directory name during untarring",
    "question_body": "Say I have a file named \nugly_name.tar\n, which when extracted, becomes \nugly_name\n directory. What command can I use so that the resulting directory name is \npretty_name\n instead?",
    "answer": "This should work:\n\n\n\n```bash\nmkdir pretty_name && tar xf ugly_name.tar -C pretty_name --strip-components 1\n```\n\n\n\n-C\n changes to the specified directory before unpacking (or packing). \n--strip-components\n removes the specified number of directories from the filenames stored in the archive.\n\n\nNote that this is not really portable. GNU tar and at least some of the BSD tars have the \n--strip-components\n option, but doesn't seem to exist on other unix-like platforms.\n\n\nThe dumb way of doing this would work pretty much everywhere though.\n\n\n\n```bash\ntar xf ugly_name.tar && mv ugly_name pretty_name\n```",
    "url": "https://unix.stackexchange.com/questions/11018/how-to-choose-directory-name-during-untarring"
  },
  {
    "question_title": "How to debug a bash script?",
    "question_body": "I'm having some problems with some scripts in bash, about errors and unexpected behaviors. I would like to investigate the causes of the problems so I can apply fixes. Is there a way I can turn some kind of \"debug-mode\" for bash, to get more information?",
    "answer": "Start your bash script with \nbash -x ./script.sh\n or add in your script \nset -x\n to see debug output.\n\n\n\n\nAdditional with \nbash\n 4.1 or later:\n\n\nIf you want to write the debug output to a separate file, add this to your script:\n\n\n\n```bash\nexec 5> debug_output.txt\nBASH_XTRACEFD=\"5\"\n```\n\n\n\nSee: \nhttps://stackoverflow.com/a/25593226/3776858\n\n\n\n\nIf you want to see line numbers add this:\n\n\n\n```bash\nPS4='$LINENO: '\n```\n\n\n\n\nIf you have access to \nlogger\n command then you can use this to write debug output via your syslog with timestamp, script name and line number:\n\n\n\n```bash\n#!/bin/bash\n\nexec 5> >(logger -t $0)\nBASH_XTRACEFD=\"5\"\nPS4='$LINENO: '\nset -x\n\n# Place your code here\n```\n\n\n\nYou can use option \n-p\n of \nlogger\n command to set an individual facility and level to write output via local syslog to its own logfile.",
    "url": "https://unix.stackexchange.com/questions/155551/how-to-debug-a-bash-script"
  },
  {
    "question_title": "Bash: What does &quot;&gt;|&quot; do?",
    "question_body": "I have just seen this written down;\n\n\n\n```bash\n$ some-command >| /tmp/output.txt\n```\n\n\n\nVertical pipes are used in standard redirects \"piping\" the output of one command to another, is \n>|\n in fact completely useless as it would be the same as just \n>\n in this scenario?",
    "answer": "It's not useless - it's a specialised form of the plain \n>\n redirect operator (and, perhaps confusingly, nothing to do with pipes). \nbash\n and most other modern shells have an option \nnoclobber\n, which prevents redirection from overwriting or destroying a file that already exists. For example, if \nnoclobber\n is true, and the file \n/tmp/output.txt\n already exists, then this should fail:\n\n\n\n```bash\n$ some-command > /tmp/output.txt\n```\n\n\n\nHowever, you can explicitly override the setting of \nnoclobber\n with the \n>|\n redirection operator - the redirection will work, even if \nnoclobber\n is set.\n\n\nYou can find out if \nnoclobber\n is set in your current environment with \nset -o\n.\n\n\nFor the historical note, both the \"noclobber\" option and its bypass features come from \ncsh\n (late 70s). \nksh\n copied it (early 80s) but used \n>|\n instead of \n>!\n. POSIX specified the \nksh\n syntax (so all POSIX shells including bash, newer ash derivatives used as sh on some systems support it). Zsh supports both syntaxes. I don't think it was added to any Bourne shell variant but I might be wrong.",
    "url": "https://unix.stackexchange.com/questions/45201/bash-what-does-do"
  },
  {
    "question_title": "How can I &quot;grep&quot; patterns across multiple lines?",
    "question_body": "It seems I am misusing \ngrep\n/\negrep\n. \n\n\nI was trying to search for strings in multiple line and could not find a match while I know that what I'm looking for should match. Originally I thought that my regexes were wrong but I eventually read that these tools operate per line (also my regexes were so trivial it could not be the issue).\n\n\nSo which tool would one use to search patterns across multiple lines?",
    "answer": "Here's a \nsed\n one that will give you \ngrep\n-like behavior across multiple lines:\n\n\n\n```bash\nsed -n '/foo/{:start /bar/!{N;b start};/your_regex/p}' your_file\n```\n\n\n\nHow it works\n\n\n\n\n-n\n suppresses the default behavior of printing every line\n\n\n/foo/{}\n instructs it to match \nfoo\n and do what comes inside the squigglies to the matching lines. Replace \nfoo\n with the starting part of the pattern.\n\n\n:start\n is a branching label to help us keep looping until we find the end to our regex.\n\n\n/bar/!{}\n will execute what's in the squigglies to the lines that don't match \nbar\n. Replace \nbar\n with the ending part of the pattern.\n\n\nN\n appends the next line to the active buffer (\nsed\n calls this the pattern space)\n\n\nb start\n will unconditionally branch to the \nstart\n label we created earlier so as to keep appending the next line as long as the pattern space doesn't contain \nbar\n.\n\n\n/your_regex/p\n prints the pattern space if it matches \nyour_regex\n. You should replace \nyour_regex\n by the whole expression you want to match across multiple lines.",
    "url": "https://unix.stackexchange.com/questions/112132/how-can-i-grep-patterns-across-multiple-lines"
  },
  {
    "question_title": "grep lines starting with &quot;1&quot; in Ubuntu",
    "question_body": "I try to search for lines that start with \"1\" using \n\n\n\n```bash\nls -1 | grep ^1*\n```\n\n\n\nbut it returns lines that do not start with 1. What I am missing here?",
    "answer": "Your regular expression doesn't mean what you think it does. It matches all lines starting (^) with one (1) repeated zero or more (*) times. All strings match that regular expression. \ngrep '^1'\n does what you want.",
    "url": "https://unix.stackexchange.com/questions/59893/grep-lines-starting-with-1-in-ubuntu"
  },
  {
    "question_title": "How to convert existing gz (gzip) files to rsyncable",
    "question_body": "I am using rsync to back up a repository that contains many gz files including many new ones each day. The rsync backup proceeds more slowly than it should because these gz files are not built with gzip's --rsyncable option (which makes gz files much more 'rsync-friendly' without significantly increasing their size or affecting their compatibility). And I can't fix the problem at creation time because the files are generated by a python script (rdiff-backup) which uses python's gzip module and this does not support an equivalent to gzip's --rsyncable.\n\n\nSo before running rsync I can identify any new gz files in the source data (i.e. new since the last time rsync was run). Now I want to 're-gzip' these files so that they are gzipped in rsyncable-format. Then I can run rsync from the optimised source.\n\n\nI think this means running each file through gunzip and then gzip --rsyncable but I am not too sure how to do this in a way that won't risk losing data or metadata. Suggestions gratefully received.",
    "answer": "```bash\n#! /bin/bash\n\nset -euo pipefail\n\n##  TOKEN's creation time marks the time since last recompression\nTOKEN=.lastRecompression   \n\nif [ -f ${TOKEN} ]\nthen\n    find -name '*.gz' -cnewer \"${TOKEN}\"\nelse\n    # Process all compressed files if there is no token.\n    find -name '*.gz'\nfi | while read f\ndo\n    # Do it in two steps\n    gunzip < \"$f\" | gzip --rsyncable > \"$f.tmp\"\n\n    # Preserve attributes\n    cp \"$f\" \"$f.tmp\" --attributes-only\n\n    # and rename atomically.\n    # set -e ensures that a problem in the previous step \n    # will stop the full script. \n    mv -v \"$f.tmp\" \"$f\"\ndone\n\n# Update the token\ntouch ${TOKEN}\n```",
    "url": "https://unix.stackexchange.com/questions/244449/how-to-convert-existing-gz-gzip-files-to-rsyncable"
  },
  {
    "question_title": "Is it possible to clone only part of a git project?",
    "question_body": "I found a collection of slackbuilds, some i need\nthere are on GitHub.\n\nhttps://github.com/PhantomX/slackbuilds/\n\nI don't want to get all git.\n\n\n\n```bash\ngit clone https://github.com/PhantomX/slackbuilds.git\n```\n\n\n\nBut only get a slackbuild, for \nthis one\n.\n\n\nHow to do this? Is it possible?",
    "answer": "You will end up downloading the entire history, so I don't see much benefit in it, but you can checkout specific parts using a \"sparse\" checkout. Quoting \nthis Stack Overflow post\n:\n\n\n\n> The steps to do a sparse clone are as follows:\n> ```bash\n> mkdir <repo>\n> cd <repo>\n> git init\n> git remote add -f origin <url>\n> ```\n\n\n\nI'm going to interrupt here. Since I'm quoting another post, I don't want to edit the quoted parts, but \ndo \nnot\n use \n-f\n with \ngit remote add\n. It will do a fetch, which will pull in the entire history. Just add the remote without a fetch:\n\n\n\n```bash\ngit remote add origin <url>\n```\n\n\n\nAnd then do a shallow fetch like described later.\n\n\n\n> This creates an empty repository with your remote, and fetches all\n> objects but doesn't check them out. Then do:\n> ```bash\n> git config core.sparseCheckout true\n> ```\n> Now you need to define which files/folders you want to actually check\n> out. This is done by listing them in .git/info/sparse-checkout, eg:\n> ```bash\n> echo \"some/dir/\" >> .git/info/sparse-checkout\n> echo \"another/sub/tree\" >> .git/info/sparse-checkout\n> ```\n> [...]\n> You might want to have a look at the extended tutorial and you\n> should probably read the official documentation for sparse\n> checkout.\n\n\n\nYou might be better off using \na shallow clone\n. Instead of a normal \ngit pull\n, try:\n\n\n\n```bash\ngit pull --depth=1 origin master\n```\n\n\n\n\n\nI had an occasion to test this again recently, trying to get only the \nUbuntu Mono Powerline fonts\n. The steps above ended up downloading some 11 MB, where the Ubuntu Fonts themselves are ~900 KB:\n\n\n\n```bash\n% git pull --depth=1 origin master\nremote: Enumerating objects: 310, done.\nremote: Counting objects: 100% (310/310), done.\nremote: Compressing objects: 100% (236/236), done.\nremote: Total 310 (delta 75), reused 260 (delta 71), pack-reused 0\nReceiving objects: 100% (310/310), 10.40 MiB | 3.25 MiB/s, done.\nResolving deltas: 100% (75/75), done.\nFrom https://github.com/powerline/fonts\n * branch            master     -> FETCH_HEAD\n * [new branch]      master     -> origin/master\n% du -hxd1 .\n11M     ./.git\n824K    ./UbuntuMono\n12M     .\n```\n\n\n\nA normal \nclone\n took about 20 MB. There's some savings, but not enough.\n\n\nUsing \nthe \n--filter\n + checkout method in Ciro Santilli's answer\n really cuts down the size, but as mentioned there, downloads each blob one by one, which is slow:\n\n\n\n```bash\n% git fetch --depth=1 --filter=blob:none\nremote: Enumerating objects: 52, done.\nremote: Counting objects: 100% (52/52), done.\nremote: Compressing objects: 100% (49/49), done.\nremote: Total 52 (delta 1), reused 35 (delta 1), pack-reused 0\nReceiving objects: 100% (52/52), 14.55 KiB | 1.32 MiB/s, done.\nResolving deltas: 100% (1/1), done.\nFrom https://github.com/powerline/fonts\n * [new branch]      master     -> origin/master\n * [new branch]      terminus   -> origin/terminus\n% git checkout origin/master -- UbuntuMono\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 1.98 KiB | 1.98 MiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 581 bytes | 581.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 121.43 KiB | 609.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 100.66 KiB | 512.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 107.62 KiB | 583.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 112.15 KiB | 791.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 454 bytes | 454.00 KiB/s, done.\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nReceiving objects: 100% (1/1), 468 bytes | 468.00 KiB/s, done.\n% du -hxd1 .\n692K    ./.git\n824K    ./UbuntuMono\n1.5M    .\n```\n\n\n\n\n\nTL;DR: Use all of \n--filter\n, sparse checkout and shallow clone to reduce the total download, or only use sparse checkout + shallow clone if you don't care about the total download and just want that one directory however it may be obtained.",
    "url": "https://unix.stackexchange.com/questions/233327/is-it-possible-to-clone-only-part-of-a-git-project"
  },
  {
    "question_title": "How to gzip 100 GB files faster with high compression?",
    "question_body": "We have 100+ GB files on a Linux machine, and while trying to perform gzip using below command, gzip is taking minimum 1-2 hours to complete:\n\n\n\n```bash\ngzip file.txt\n```\n\n\n\nIs there a way we can make gzip to run fast with the same level of compression happening when we use gzip?\n\n\n\n\nCPU: Intel(R) Core(TM) i3-2350M CPU @2.30 GHz",
    "answer": "If you are using gzip, you use mostly one processor core (well, some parts of the task, like reading and writing data are kernel tasks and kernel will use another core). Have a look at some multicore-capable gzip replacements, e.g. MiGz (\nhttps://github.com/linkedin/migz\n) or Pigz (\nhttps://zlib.net/pigz/\n, for some longer explanation see also e.g. \nhttps://medium.com/ngs-sh/pigz-a-faster-alternative-to-gzip-for-big-files-d5909e46d659\n).",
    "url": "https://unix.stackexchange.com/questions/623881/how-to-gzip-100-gb-files-faster-with-high-compression"
  },
  {
    "question_title": "Will tar -cvzf packed.tar.gz mydir take hidden files into account?",
    "question_body": "I need to create a tarball of a given directory. However, I need to make sure hidden files are included too (such as those beginning with \n.\n).\n\n\nWill the following command automatically take the hidden files into account?\n\n\n\n```bash\ntar -cvzf packed.tar.gz mydir\n```\n\n\n\nIf not, how can I make sure I include hidden files?",
    "answer": "Yes, it will.\n\n\nFiles starting with \n.\n are not \"hidden\" in all contexts.  They aren't expanded by \n*\n, and \nls\n doesn't list them by default, but \ntar\n doesn't care about the leading \n.\n.  (\nfind\n doesn't care either.)\n\n\n(Of course, this is one of those things that's easy to find out by experiment.)",
    "url": "https://unix.stackexchange.com/questions/31975/will-tar-cvzf-packed-tar-gz-mydir-take-hidden-files-into-account"
  },
  {
    "question_title": "How do I count the number of occurrences of a word in a text file with the command line?",
    "question_body": "I have a large JSON file that is on one line, and I want to use the command line to be able to count the number of occurrences of a word in the file. How can I do that?",
    "answer": "```bash\n$ tr ' ' '\\n' < FILE | grep WORD | wc -l\n```\n\n\n\nWhere \ntr\n replaces spaces with newlines, \ngrep\n filters all resulting lines matching WORD and \nwc\n counts the remaining ones.\n\n\nOne can even save the \nwc\n part using the \n-c\n option of grep:\n\n\n\n```bash\n$ tr ' ' '\\n' < FILE | grep -c WORD\n```\n\n\n\nThe \n-c\n option is defined by POSIX.\n\n\nIf it is not guaranteed that there are spaces between the words, you have to use some other character (as delimiter) to replace. For example alternative \ntr\n parts are\n\n\n\n```bash\ntr '\"' '\\n'\n```\n\n\n\nor\n\n\n\n```bash\ntr \"'\" '\\n'\n```\n\n\n\nif you want to replace double or single quotes. Of course, you can also use \ntr\n to replace multiple characters at once (think different kinds of whitespace and punctuation).\n\n\nIn case you need to count WORD but not prefixWORD, WORDsuffix or  prefixWORDsuffix, you can enclose the WORD pattern in begin/end-of-line markers:\n\n\n\n```bash\ngrep -c '^WORD$'\n```\n\n\n\nWhich is equivalent to word-begin/end markers, in our context:\n\n\n\n```bash\ngrep -c '\\<WORD\\>'\n```",
    "url": "https://unix.stackexchange.com/questions/2244/how-do-i-count-the-number-of-occurrences-of-a-word-in-a-text-file-with-the-comma"
  },
  {
    "question_title": "Why is tar archive so much bigger than text file, 10240 bytes?",
    "question_body": "I've checked these two questions (\nquestion one\n, \nquestion two\n), but they were not helpful for me to understand. I have a file \nfile.txt\n with 40 lines of \nHello World!\n string. \nls -l\n shows that its size is 520 bytes. Now I archive this file with \ntar -cvf file.tar file.txt\n and when I do \nls -l\n again I see that \nfile.tar\n is 10240 bytes. Why?\n\n\nI've read some manuals and have understood that archiving and compressing are different things. But can someone please explain how it is working?",
    "answer": "tar\n archives have a minimum size of 10240 bytes by default; see \nthe GNU \ntar\n manual\n for details (but this is not GNU-specific).\n\n\nWith GNU \ntar\n, you can reduce this by specifying either a different block size, or different block factor, or both:\n\n\n\n```bash\ntar -cv -b 1 -f file.tar file.txt\n```\n\n\n\nThe result will still be bigger than \nfile.txt\n, because \nfile.tar\n stores metadata about \nfile.txt\n in addition to \nfile.txt\n itself. In most cases you’ll see one block for the file’s metadata (name, size, timestamps, ownership, permissions), then the file content, then two blocks for the end-of-archive entry, so the smallest archive containing a non-zero-length file is four blocks in size (2,048 bytes with a 512-byte block).",
    "url": "https://unix.stackexchange.com/questions/685766/why-is-tar-archive-so-much-bigger-than-text-file-10240-bytes"
  },
  {
    "question_title": "Can git configuration be set across multiple repositories?",
    "question_body": "Git seems to support configuration values at three levels:\n\n\n\n\nPer-system global settings (stored in \n/etc/git-core\n)\n\n\nPer-user global settings (stored in \n~/.gitconfig\n)\n\n\nPer-repository local settings (stored in \n$REPO/.git/config\n)\n\n\n\n\nThese options cover most of the basis but I'm looking for a way to handle a fourth level. I have a (very) large collection of repositories for which I need to use a different value for \nuser.email\n than my usual. These repositories are often created and manipulated through automated scripts, and setting up per repository local settings is cumbersome.\n\n\nAll of the repositories in question are located under a certain path prefix on my local system. Is there a way to set a configuration value somewhere that will be inherited by all repositories under that path? (Sort of like \n.htaccess\n settings inherit all the way up the file system.) Perhaps there would be a way to set conditional values in the global config file? What other arrangements could be made in a UNIX environment to cope with a set of repositories like mine?",
    "answer": "I have found no way to configure git at this fourth level. The only way seems to be per-command configuration value overrides using \ngit -c key=value\n.\n\n\nMy current hacky solution is to define a shell function that serves as a wrapper for git. When called, it passes the arguments onto the system git command, but not before checking on the present working directory and adding an extra argument to the command if applicable.\n\n\n\n```bash\nfunction git () {\n    case \"$PWD\" in\n        /path/to/repos/*)\n            command git -c user.email=alternate@credentials.org \"$@\"\n            ;;\n        *)\n            command git \"$@\"\n            ;;\n    esac\n}\n```",
    "url": "https://unix.stackexchange.com/questions/42785/can-git-configuration-be-set-across-multiple-repositories"
  },
  {
    "question_title": "How to sort (by whatever key) BibTex entries in `.bib` file?",
    "question_body": "I work with LaTeX and do versioning with Git. For bibliography management I use Mendeley.\n\n\nThe problem is that each time Mendeley synchronizes it's \n.bib\n exports,\nthey are in different order, what makes bibliography versioning much harder.\n\n\nMy idea is to sort BibTex entries in \n.bib\n file, each time before commit.\n\n\nCould you help me, how to do this in smart (short&sweet) way ? :)\n\n\nP.S. I can run this routine manually. I do not need git integration. I just want program/script to sort \n.bib\n file.",
    "answer": "How about \nbibsort\n?\n\n\n\n```bash\nNAME\n     bibsort - sort a BibTeX bibliography file\n\nSYNOPSIS\n     bibsort [optional sort(1) switches] < infile >outfile\n\nDESCRIPTION\n     bibsort filters a BibTeX bibliography, or bibliography frag-\n     ment,  on  its standard input, printing on standard output a\n     sorted bibliography.\n```\n\n\n\nIt's a shell script\n wrapping \nnawk\n (and \ntr\n, \nsort\n and \ngrep\n) and includes two warnings you might have to pay attention to (see the source).\n\n\n(\nEdit\n There're also a lot of \nbibtex-related Perl modules\n...)\n\n\nEdit2\n I just recognized you'd like to sort for \nany\n key, while \nbibsort\n apparently sorts by \nthe bibtex tags\n only -- but maybe its source (it's not too long) is still helpful...?",
    "url": "https://unix.stackexchange.com/questions/31266/how-to-sort-by-whatever-key-bibtex-entries-in-bib-file"
  },
  {
    "question_title": "What does the -f parameter do in the tar command",
    "question_body": "When using \ntar\n I always include \n-f\n in the parameters but I have no idea why.\n\n\nI looked up the man and it said;\n\n\n\n```bash\n-f, --file [HOSTNAME:]F\n\nuse archive file or device F (default\n\"-\", meaning stdin/stdout)\n```\n\n\n\nBut to be honest I have no idea what that means.  Can anyone shed any light on it?",
    "answer": "The \n-f\n option tells \ntar\n that the next argument is the file name of the archive, or standard output if it is \n-\n.",
    "url": "https://unix.stackexchange.com/questions/1280/what-does-the-f-parameter-do-in-the-tar-command"
  },
  {
    "question_title": "How to Count the Number of Lines of an Output?",
    "question_body": "Let say I have the program: \n\n\n\n```bash\nCalculate.py\n```\n\n\n\nIs there a unix command-line that counts the number of lines outputted from my program, Calculate.py?",
    "answer": "You can pipe the output in to \nwc\n. You can use the \n-l\n flag to count lines. Run the program normally and use a pipe to redirect to \nwc.\n\n\n\n```bash\npython Calculate.py | wc -l\n```\n\n\n\nAlternatively, you can redirect the output of your program to a file, say \ncalc.out\n, and run \nwc\n on that file.\n\n\n\n```bash\npython Calculate.py > calc.out\nwc -l calc.out\n```",
    "url": "https://unix.stackexchange.com/questions/248245/how-to-count-the-number-of-lines-of-an-output"
  },
  {
    "question_title": "How to use grep when file does not contain the string? Testing whether a user exists in &quot;passwd&quot; or not",
    "question_body": "In my bash script I'm trying to print a line if a certain string does not exist in a file.\n\n\n\n```bash\nif grep -q \"$user2\" /etc/passwd; then\n    echo \"User does exist!!\"\n```\n\n\n\nThis is how I wrote it if I wanted the string to exist in the file. But how can I change this to make it print \"user does not exist\" if the user is \nnot\n found in the \n/etc/passwd\n file?",
    "answer": "grep\n will return success if it finds at least one instance of the pattern and failure if it does not.  So you could either add an \nelse\n clause if you want both \"does\" and \"does not\" prints, or you could just negate the \nif\n condition to only get failures.  An example of each:\n\n\n\n```bash\nif grep -q \"$user2\" /etc/passwd; then\n    echo \"User does exist!!\"\nelse\n    echo \"User does not exist!!\"\nfi\n\nif ! grep -q \"$user2\" /etc/passwd; then\n    echo \"User does not exist!!\"\nfi\n```",
    "url": "https://unix.stackexchange.com/questions/223503/how-to-use-grep-when-file-does-not-contain-the-string-testing-whether-a-user-ex"
  },
  {
    "question_title": "Prevent showing time stamp message when running &quot;tar xzf&quot;",
    "question_body": "When running \ntar xzf\n, how to prevent this message (Prevent showing time stamp message):\n\n\n\n```bash\ntar: node: time stamp 2011-06-07 02:02:30 is 8309 s in the future\ntar: user/Node: time stamp 2011-06-07 01:56:05 is 7924 s in the future\n```",
    "answer": "tar has an option to suppress this message [1]:\n\n\n\n> -m, --touch\n> don't extract file modified time\n\n\n\nHowever, you should probably also check that you don't have an issue with your system clock.\n\n\n[1] \nhttp://unixhelp.ed.ac.uk/CGI/man-cgi?tar",
    "url": "https://unix.stackexchange.com/questions/14528/prevent-showing-time-stamp-message-when-running-tar-xzf"
  },
  {
    "question_title": "Why can I checkout a branch that was removed on GitHub?",
    "question_body": "In our GitHub repository, a coworker removed a branch named \nrelease\n. But when I run \ngit checkout release\n locally, I always get the removed branch \nrelease\n. Same, even when I checked out another branch, deleted the \nrelease\n branch with \ngit branch -D release\n and ran again \ngit checkout release\n. \n\n\nIs there something to fix on the GitHub repository, or shall I fix something locally?",
    "answer": "After deleting a branch on the remote side you may still see this formerly fetched remote branch locally, see:\n\n\n\n```bash\n$ git branch -a\n[...]\nrelease\nremotes/origin/release\n[...]\n```\n\n\n\nYou only removed the \"release\" but not \"remotes/origin/release\". Delete it like this:\n\n\n\n```bash\n$ git branch -rd origin/release\n```\n\n\n\nOr remove \nall\n fetched branches which do not exist on the remote side anymore:\n\n\n\n```bash\n$ git remote prune origin\n```",
    "url": "https://unix.stackexchange.com/questions/365859/why-can-i-checkout-a-branch-that-was-removed-on-github"
  },
  {
    "question_title": "Creating a user without a password",
    "question_body": "I'm trying to create a user without password like this:\n\n\n\n```bash\nsudo adduser \\\n   --system \\\n   --shell /bin/bash \\\n   --gecos ‘User for managing of git version control’ \\\n   --group \\\n   --disabled-password \\\n   --home /home/git \\\n   git\n```\n\n\n\nIt's created fine. But when I try to login under the git user I'm getting the password entering:\n\n\n\n```bash\nsu git\nPassword:...\n```\n\n\n\nWhen I leave it empty I get an error:\n\n\n\n```bash\nsu: Authentication failed\n```\n\n\n\nWhat's wrong?",
    "answer": "The \n--disabled-password\n option will not set a password, meaning no password is legal, but  login  is still possible (for example with SSH RSA keys).\n\n\nTo create an user without a password, use \npasswd -d $username\n after the user is created to make the password empty. Note not all systems allow users with empty password to log in.",
    "url": "https://unix.stackexchange.com/questions/56765/creating-a-user-without-a-password"
  },
  {
    "question_title": "Why doesn&#39;t grep ignore binary files by default?",
    "question_body": "The manpage for \ngrep\n describes the \n-I\n flag as follows:\n\n\n\n```bash\n-I      Ignore binary files.  This option is equivalent to \n        --binary-file=without-match option.\n```\n\n\n\nIt also says this about binary files:\n\n\n\n```bash\n--binary-files=value Controls searching and printing of binary files.\n         Options are binary, the default: search binary files but do not print\n         them; without-match: do not search binary files; and text: treat all\n         files as text.\n```\n\n\n\nI cannot think of a scenario where I would care about matches in binary files. If such a scenario exists, surely it must be the exception rather than the norm. Why doesn't \ngrep\n ignore binary files by default rather than requiring setting this flag to do so?",
    "answer": "Not everything that grep thinks is a binary file, is actually a binary file. e.g. puppet's logs have ansi color coding in them, which makes grep think they're binary. I'd still want to search them if I'm grepping through /var/log though.",
    "url": "https://unix.stackexchange.com/questions/70438/why-doesnt-grep-ignore-binary-files-by-default"
  },
  {
    "question_title": "Is there a tool that combines zcat and cat transparently?",
    "question_body": "When handling log files, some end up as gzipped files thanks to \nlogrotate\n and others not. So when you try something like this:\n\n\n\n```bash\n$ zcat *\n```\n\n\n\nyou end up with a command line like \nzcat xyz.log xyz.log.1 xyz.log.2.gz xyz.log.3.gz\n and then with:\n\n\n\n```bash\ngzip: xyz.log: not in gzip format\n```\n\n\n\nIs there a tool that will take the magic bytes, similar to how \nfile\n works, and use \nzcat\n or \ncat\n depending on the outcome so that I can pipe the output to \ngrep\n for example?\n\n\nNB: I know I can script it, but I am asking whether there is a tool out there already.",
    "answer": "Try it with \n-f\n or \n--force\n:\n\n\n\n```bash\nzcat -f -- *\n```\n\n\n\nSince \nzcat\n is just a simple script that runs\n\n\n\n```bash\nexec gzip -cd \"$@\"\n```\n\n\n\nwith long options that would translate to\n\n\n\n```bash\nexec gzip --stdout --decompress \"$@\"\n```\n\n\n\nand, as per the \nman gzip\n (emphasize mine):\n\n\n\n```bash\n-f --force\n      Force compression or decompression even if the file has multiple links\n      or the corresponding file already exists, or if the compressed data is\n      read from or written to a terminal. If the input data is not in a format\n      recognized by gzip, and if the option --stdout is also given, copy the\n      input data without change to the standard output: let zcat behave as cat.\n```\n\n\n\nAlso:\n\n\n\n> so that I can pipe the output to grep for example\n\n\n\nYou could use \nzgrep\n for that:\n\n\n\n```bash\nzgrep -- PATTERN *\n```\n\n\n\nthough see Stéphane's comment below.",
    "url": "https://unix.stackexchange.com/questions/77296/is-there-a-tool-that-combines-zcat-and-cat-transparently"
  },
  {
    "question_title": "Is it possible to compress a very large file (~30 GB) using gzip?",
    "question_body": "Is it possible to compress a very large file (~30 GB) using gzip?  If so, what commands, switches, and options should I use?\n\n\nOr is there another program (preferably one commonly available on Ubuntu distributions) that I can use to compress/zip very large files?  Do you have any experience with this?",
    "answer": "AFAIK there is no limit of size for \ngzip\n - at least not 30GB. Of course, you need the space for the zipped file on your disc, both versions will be there simultanously while compressing.\n\n\nbzip2\n compresses files (not only big ones :-) better, but it is (sometimes a lot) slower.",
    "url": "https://unix.stackexchange.com/questions/39934/is-it-possible-to-compress-a-very-large-file-30-gb-using-gzip"
  },
  {
    "question_title": "grep search + next line",
    "question_body": "With the \ngrep\n command, I found the text I need as follows:\n\n\n\n```bash\ngrep 'C02' ~/temp/log.txt\n```\n\n\n\nNow, wherever I find the desired string, I would like to print the line following the found string.\n\n\nFor example, let's say that the desired text is \nabc\n, and \nabc\n is found on line 12, I would like to print line 13 too.",
    "answer": "If you are using Linux system, you can try:\n\n\n\n```bash\ngrep -A1 \"C02\" ~/temp/log.txt\n\n\nOPTIONS\n       -A NUM, --after-context=NUM\n              Print NUM lines of trailing context after matching lines.  Places a line containing -- between contiguous groups of matches.\n       -B NUM, --before-context=NUM\n              Print NUM lines of leading context before matching lines.  Places a line containing -- between contiguous groups of matches.\n       -C NUM, --context=NUM\n              Print NUM lines of output context.  Places a line containing -- between contiguous groups of matches.\n```\n\n\n\nYou can use awk also as:\n\n\n\n```bash\nawk '/C02/{print;getline;print}' ~/temp/log.txt\n```",
    "url": "https://unix.stackexchange.com/questions/320706/grep-search-next-line"
  },
  {
    "question_title": "grep on a variable",
    "question_body": "Let's say I have a variable\n\n\n\n```bash\nline=\"This is where we select from a table.\"\n```\n\n\n\nnow I want to grep how many times does select occur in the sentence. \n\n\n\n```bash\ngrep -ci \"select\" $line\n```\n\n\n\nI tried that, but it did not work. I also tried \n\n\n\n```bash\ngrep -ci \"select\" \"$line\"\n```\n\n\n\nIt still doesn't work.  I get the following error. \n\n\n\n```bash\ngrep: This is where we select from a table.: No such file or directory\n```",
    "answer": "Have \ngrep\n read on its standard input. There you go, using \na pipe\n...\n\n\n\n```bash\n$ echo \"$line\" | grep select\n```\n\n\n\n... or \na here string\n...\n\n\n\n```bash\n$ grep select <<< \"$line\"\n```\n\n\n\nAlso, you might want to replace spaces by newlines before grepping :\n\n\n\n```bash\n$ echo \"$line\" | tr ' ' '\\n' | grep select\n```\n\n\n\n... or you could ask \ngrep\n to print the match only:\n\n\n\n```bash\n$ echo \"$line\" | grep -o select\n```\n\n\n\nThis will allow you to get rid of the rest of the line when there's a match.\n\n\nEdit:\n Oops, read a little too fast, thanks \nMarco\n. In order to count the occurences, just pipe any of these to \nwc(1)\n ;)\n\n\nAnother edit made after \nlzkata\n's comment, quoting \n$line\n when using \necho\n.",
    "url": "https://unix.stackexchange.com/questions/163810/grep-on-a-variable"
  },
  {
    "question_title": "How to turn a tar file to a tgz file?",
    "question_body": "I usually create a tgz file for \nmy_files\n with the command \ntar -czvf my_files.tgz my_files\n, and extract them with \ntar -zxvf my_files.tgz\n. Now I have a tar file created with the command \ntar -cvf my_files.tar my_files\n. I'm wondering how I can turn the \nmy_files.tar\n into \nmy_files.tgz\n so that later I can extract it with the command \ntar -zxvf my_files.tgz\n? Thanks.",
    "answer": "A plain \n.tar\n archive created with \ncf\n (with or without \nv\n) is uncompressed; to get a \n.tar.gz\n or \n.tgz\n archive, compress it:\n\n\n\n```bash\ngzip < my_files.tar > my_files.tgz\n```\n\n\n\nYou might want to add \n-9\n for better compression:\n\n\n\n```bash\ngzip -9 < my_files.tar > my_files.tgz\n```\n\n\n\nBoth variants will leave both archives around; you can use\n\n\n\n```bash\ngzip -9 my_files.tar\n```\n\n\n\ninstead, which will produce \nmy_files.tar.gz\n and delete \nmy_files.tar\n (if everything goes well). You can then rename \nmy_files.tar.gz\n to \nmy_files.tgz\n if you wish.\n\n\nWith many \ntar\n implementations you can extract archives without specifying the \nz\n option, and \ntar\n will figure out what to do — so you can use the same command with compressed and uncompressed archives.",
    "url": "https://unix.stackexchange.com/questions/457949/how-to-turn-a-tar-file-to-a-tgz-file"
  },
  {
    "question_title": "Git pager is less, but what is causing the output coloring?",
    "question_body": "less\n itself isn't capable of doing syntax highlighting, according to \nthis thread\n. \n\n\nHowever, \ngit diff\n nicely shows colored output in less, its default pager. When I redirect the output of \ngit diff\n into a file, no color escape sequences are visible. \n\n\nDoes \ngit diff\n know where it's being sent, and formats the output accordingly? How would one do that?\n\n\n\n\nI just noticed that git colors the \ndiff\n output (e.g. \ngit diff\n), however, it doesn't know how to syntax highlighting in general. e.g.\n\n\n\n```bash\ngit show 415fec6:log.tex\n```\n\n\n\ndoesn't enable any TeX-like syntax. \n\n\n\n\nReading the \ngit\n sources, I found the following hints\n\n\nin \ndiff.h\n:\n\n\n\n```bash\nint use_color;\n```\n\n\n\n\n\nI was previously referring to \nsyntax highlighting\n, but that was not correct. What I mean is output coloring, see e.g.",
    "answer": "Git uses \nisatty()\n to check whether stdout is a tty: this is used to see if a pager must be used (\npager.c\n) as well as colors (\ncolor.c\n).",
    "url": "https://unix.stackexchange.com/questions/153943/git-pager-is-less-but-what-is-causing-the-output-coloring"
  },
  {
    "question_title": "How can I create an alias for a git [action] command (which includes spaces)?",
    "question_body": "Most of my my aliases are of this form: \nalias p='pwd'\n\n\nI want to alias \ngit commit\n so that it does \ngit commit -v\n\n\nBut trying to create an alias with a space gives an error:\n\n\n\n```bash\n$ alias 'git commit'='git commit -v'\n-bash: alias: `git commit': invalid alias name\n```",
    "answer": "Not a direct answer to your question (since aliases can only be one word), but you should be using \ngit-config\n instead:\n\n\n\n```bash\ngit config --global alias.civ commit -v\n```\n\n\n\nThis creates a git alias so that \ngit civ\n runs \ngit commit -v\n.  Unfortunately, AFAIK \nthere is no way to override existing git commands with aliases\n.  However, you can always pick a suitable alias name to live with as an alternative.",
    "url": "https://unix.stackexchange.com/questions/48862/how-can-i-create-an-alias-for-a-git-action-command-which-includes-spaces"
  },
  {
    "question_title": "How can I test if a variable is empty or contains only spaces?",
    "question_body": "The following bash syntax verifies if \nparam\n isn't empty:\n\n\n\n```bash\n[[ !  -z  $param  ]]\n```\n\n\n\nFor example:\n\n\n\n```bash\nparam=\"\"\n[[ !  -z  $param  ]] && echo \"I am not zero\"\n```\n\n\n\nNo output and its fine.\n\n\nBut when \nparam\n is empty except for one (or more) space characters, then the case is different:\n\n\n\n```bash\nparam=\" \" # one space\n[[ !  -z  $param  ]] && echo \"I am not zero\"\n```\n\n\n\n\"I am not zero\" is output.\n\n\nHow can I change the test to consider variables that contain only space characters as empty?",
    "answer": "First, note that the \n-z\n test\n is explicitly for:\n\n\n\n> the length of string is zero\n\n\n\nThat is, a string containing only spaces should \nnot\n be true under \n-z\n, because it has a non-zero length.\n\n\nWhat you want is to remove the spaces from the variable using the \npattern replacement parameter expansion\n:\n\n\n\n```bash\n[[ -z \"${param// }\" ]]\n```\n\n\n\nThis expands the \nparam\n variable and replaces all matches of the pattern \n (a single space) with nothing, so a string that has only spaces in it will be expanded to an empty string.\n\n\n\n\nThe \nnitty-gritty of how that works\n is that \n${var/pattern/string}\n replaces the first longest match of \npattern\n with \nstring\n. When \npattern\n starts with \n/\n (as above) then it replaces \nall\n the matches. Because the replacement is empty, we can omit the final \n/\n and the \nstring\n value:\n\n\n\n> ${parameter/pattern/string}\n> The pattern is expanded to produce a pattern just as in filename expansion. Parameter is expanded and the longest match of pattern against its value is replaced with string. If pattern begins with ‘/’, all matches of pattern are replaced with string. Normally only the first match is replaced. ... If string is null, matches of pattern are deleted and the / following pattern may be omitted.\n\n\n\nAfter all that, we end up with \n${param// }\n to delete all spaces.\n\n\nNote that though present in \nksh\n (where it originated), \nzsh\n and \nbash\n, that syntax is not POSIX and should not be used in \nsh\n scripts.",
    "url": "https://unix.stackexchange.com/questions/146942/how-can-i-test-if-a-variable-is-empty-or-contains-only-spaces"
  },
  {
    "question_title": "How to modify a PKGBUILD which uses git sources to pull only a shallow clone?",
    "question_body": "The other day I tried installing \nopencv-git\n from the \nAUR\n with \nmakepkg\n on Arch Linux. Of course it pulls from the \ngit\n repository as the name indicates. This pulls 1Gb. I am reading about making a \nshallow clone\n with \ngit\n. When I look at the \nPKGBUILD\n file, using \ngrep git PKGBUILD\n, I see:\n\n\n\n```bash\npkgname=\"opencv-git\"\nmakedepends=('git' 'cmake' 'python2-numpy' 'mesa' 'eigen2')\nprovides=(\"${pkgname%-git}\")\nconflicts=(\"${pkgname%-git}\")\nsource=(\"${pkgname%-git}::git+http://github.com/Itseez/opencv.git\"\n    cd \"${srcdir}/${pkgname%-git}\"\n    git describe --long | sed -r 's/([^-]*-g)/r\\1/;s/-/./g'\n    cd \"${srcdir}/${pkgname%-git}\"\n    cd \"${srcdir}/${pkgname%-git}\"\n    cd \"${srcdir}/${pkgname%-git}\"\n    install -Dm644 \"LICENSE\" \"${pkgdir}/usr/share/licenses/${pkgname%-git}/LICENSE\"\n```\n\n\n\nIs there a way to modify the recipe or the \nmakepkg\n command to pull only a shallow clone (the latest version of the source is what I want) and not the full repository to save space and bandwidth? Reading \nman 5 PKGBUILD\n doesn't provide the insight I'm looking for. Also looked quickly through the \nmakepkg\n and \npacman\n \nmanpages\n - can't seem to find how to do that.",
    "answer": "This can be done by using a custom \ndlagent\n. I do not really understand Arch packaging or how the dlagents work, so I only have a hack answer, but it gets the job done.\n\n\nThe idea is to modify the PKGBUILD to use a custom download agent. I modified the source\n\n\n\n```bash\n\"${pkgname%-git}::git+http://github.com/Itseez/opencv.git\"\n```\n\n\n\ninto\n\n\n\n```bash\n\"${pkgname%-git}::mygit://opencv.git\"\n```\n\n\n\nand then defined a new dlagent called \nmygit\n which does a shallow clone by\n\n\n\n```bash\nmakepkg DLAGENTS='mygit::/usr/bin/git clone --depth 1 http://github.com/Itseez/opencv.git'\n```\n\n\n\nAlso notice that the repository that is being cloned is hard-coded into the command. Again, this can probably be avoided. Finally, the download location is not what the PKGBUILD expects. To work around this, I simply move the repository after downloading it. I do this by adding\n\n\n\n```bash\nmv \"${srcdir}/../mygit:/opencv.git\" \"${srcdir}/../${pkgname%-git}\"\n```\n\n\n\nat the beginning of the \npkgver\n function.\n\n\nI think the cleaner solution would be to figure out what the \ngit+http\n dlagent is doing and redfine that temporarily. This should avoid all the hack aspects of the solution.",
    "url": "https://unix.stackexchange.com/questions/154919/how-to-modify-a-pkgbuild-which-uses-git-sources-to-pull-only-a-shallow-clone"
  },
  {
    "question_title": "unpigz (and untar) to a specific directory",
    "question_body": "I know how to \ngunzip\n a file to a selected location.\n\n\nBut when it comes to utilizing all CPU power, many consider \npigz\n instead of \ngzip\n. So, the question is how do I \nunpigz\n (and untar) a \n*.tar.gz\n file to a specific directory?",
    "answer": "I found three solutions:\n\n\n\n\nWith GNU \ntar\n, using the awesome \n-I\n option:\n\n\n\n```bash\ntar -I pigz -xvf /path/to/archive.tar.gz -C /where/to/unpack/it/\n```\n\n\n\n\n\nWith a lot of Linux piping (a \"geek way\"):\n\n\n\n```bash\nunpigz < /path/to/archive.tar.gz | tar -xvC /where/to/unpack/it/\n```\n\n\n\n\n\nMore portable (to other \ntar\n implementations):\n\n\n\n```bash\nunpigz < /path/to/archive.tar.gz | (cd /where/to/unpack/it/ && tar xvf -)\n```\n\n\n\n\n\n\n\n(You can also replace \ntar xvf -\n with \npax -r\n to make it \nPOSIX\n-compliant, though not necessarily more portable on Linux-based systems.)\n\n\nCredits go to \n@PSkocik\n for a proper direction, \n@Stéphane Chazelas\n for the 3rd variant and to the author of \nthis\n answer.",
    "url": "https://unix.stackexchange.com/questions/198958/unpigz-and-untar-to-a-specific-directory"
  },
  {
    "question_title": "How to export variables from a file?",
    "question_body": "I have a \ntmp.txt\n file containing variables to be exported, for example:\n\n\n\n```bash\na=123\nb=\"hello world\"\nc=\"one more variable\"\n```\n\n\n\nHow can I export all these variables using the \nexport\n command, so that they can later be used by child processes?",
    "answer": "```bash\nset -a\n. ./tmp.txt\nset +a\n```\n\n\n\nset -a\n causes variables¹ defined from now on to be automatically exported. It's available in any Bourne-like shell. \n.\n is the standard and Bourne name for the \nsource\n command so I prefer it for portability (\nsource\n comes from \ncsh\n and is now available in most modern Bourne-like shells including \nbash\n though (sometimes with a slightly different behaviour)).\n\n\nIn POSIX shells, you can also use\n\n\n\n```bash\nset -o allexport\n. ./tmp.txt\nset +o allexport\n```\n\n\n\nas a more descriptive alternative way to write it.\n\n\nYou can make it a function with:\n\n\n\n```bash\nexport_from() {\n  # local is not a standard command but is pretty common. It's needed here\n  # for this code to be re-entrant (for the case where sourced files to\n  # call export_from). We still use _export_from_ prefix to namespace\n  # those variables to reduce the risk of those variables being some of\n  # those exported by the sourced file.\n  local _export_from_ret _export_from_restore _export_from_file\n\n  _export_from_ret=0\n\n  # record current state of the allexport option. Some shells (ksh93/zsh)\n  # have support for local scope for options, but there's no standard\n  # equivalent.\n  case $- in\n    (*a*) _export_from_restore=;;\n    (*)   _export_from_restore='set +a';;\n  esac\n\n  for _export_from_file do\n    # using the command prefix removes the \"special\" attribute of the \".\"\n    # command so that it doesn't exit the shell when failing.\n    command . \"$_export_from_file\" || _export_from_ret=\"$?\"\n  done\n  eval \"$_export_from_restore\"\n  return \"$_export_from_ret\"\n}\n```\n\n\n\n\n\n¹ In \nbash\n, beware that it also causes all \nfunctions\n declared while \nallexport\n is on to be exported to the environment (as \nBASH_FUNC_myfunction%%\n environment variables that are then imported by all \nbash\n shells run in that environment, even when running as \nsh\n).",
    "url": "https://unix.stackexchange.com/questions/79064/how-to-export-variables-from-a-file"
  },
  {
    "question_title": "Batch renaming files",
    "question_body": "I have a directory full of images:\n\n\n\n```bash\nimage0001.png\nimage0002.png\nimage0003.png\n...\n```\n\n\n\nAnd I would like a one-liner to rename them to (say).\n\n\n\n```bash\n0001.png\n0002.png\n0003.png\n...\n```\n\n\n\nHow do I do this?",
    "answer": "On Debian and derivatives, \nPerl's \nrename\n commandline\n works similarly to \nsed\n like this:\n\n\n\n```bash\nrename -v 's/image//' ./*.png\n```\n\n\n\nThere's also the \nrename\n from \nutil-linux\n that works like this, instead:\n\n\n\n```bash\nrename -- image '' *.png\n```",
    "url": "https://unix.stackexchange.com/questions/1136/batch-renaming-files"
  },
  {
    "question_title": "How to append a line in a zipped file without unzipping?",
    "question_body": "```bash\nmknod /tmp/oracle.pipe p\n\nsqlplus / as sysdba << _EOF\nset escape on\n\nhost nohup gzip -c < /tmp/oracle.pipe > /tmp/out1.gz \\&\nspool /tmp/oracle.pipe\nselect * from employee;\nspool off\n\n_EOF\n\nrm /tmp/oracle.pip\n```\n\n\n\nI need to insert a trailer at the end of the zipped file out1.gz ,\nI can count the lines using\n\n\n\n```bash\ncount=zcat out1.gz |wc -l\n```\n\n\n\nHow do i insert the trailer\n\n\n\n```bash\nT5 (assuming count=5)\n```\n\n\n\nAt the end of out1.gz without unzipping it.",
    "answer": "From \nman gzip\n you can read that \ngzip\nped files can simply be concatenated:\n\n\n\n> ADVANCED USAGE\n> Multiple compressed files can be concatenated. In this case, gunzip will extract all members at once. For example:\n> ```bash\n> gzip -c file1  > foo.gz\n> gzip -c file2 >> foo.gz\n> Then\n> gunzip -c foo\n> is equivalent to\n> ```\n\n\n\n\n```bash\ncat file1 file2\n```\n\n\n\nThis could also be done using \ncat\n for the \ngzip\nped files, e.g.:\n\n\n\n```bash\nseq 1 4 > A && gzip A\necho 5 > B && gzip B\n#now 1 to 4 is in A.gz and 5 in B.gz, we want 1 to 5 in C.gz:\ncat A.gz B.gz > C.gz && zcat C.gz\n1\n2\n3\n4\n5\n#or for appending B.gz to A.gz:\ncat B.gz >> A.gz\n```\n\n\n\nFor doing it without external file for you line to be appended, do as follows:\n\n\n\n```bash\necho \"this is the new line\" | gzip - >> original_file.gz\n```",
    "url": "https://unix.stackexchange.com/questions/264980/how-to-append-a-line-in-a-zipped-file-without-unzipping"
  },
  {
    "question_title": "Git based package manager/installer for Linux",
    "question_body": "Is there any linux distro that has a git based package manager/installer. I want something similar to FreeBSD Ports (which is CVS based, I think) or Mac OS X \nHomebrew\n (git based).",
    "answer": "There's \nExherbo\n, which uses multiple Git repositories to store its exheres (its term for build recipes similar to Gentoo's ebuilds or BSD's ports).  It's still a pretty young distribution, though.\n\n\nUpdate:\n \nGentoo\n recently \nmoved to Git for its package repository\n.  However, I don't think it's set up yet to have normal users get updates via Git (although I believe it's planned to allow that).",
    "url": "https://unix.stackexchange.com/questions/36357/git-based-package-manager-installer-for-linux"
  },
  {
    "question_title": "Compress a folder with tar?",
    "question_body": "I'm trying to compress a folder (\n/var/www/\n) to \n~/www_backups/$time.tar\n where \n$time\n is the current date.\n\n\nThis is what I have:\n\n\n\n```bash\ncd /var/www && sudo tar -czf ~/www_backups $time\"\n```\n\n\n\nI am completely lost and I've been at this for hours now. Not sure if \n-czf\n is correct. I simply want to copy all of the content in \n/var/www\n into a \n$time.tar\n file, and I want to maintain the file permissions for all of the files. Can anyone help me out?",
    "answer": "To \ntar\n and \ngzip\n a folder, the syntax is:\n\n\n\n```bash\ntar czf name_of_archive_file.tar.gz name_of_directory_to_tar\n```\n\n\n\nAdding \n-\n before the options (\nczf\n) is optional with \ntar\n. The effect of \nczf\n is as follows:\n\n\n\n\nc\n — create an archive file (as opposed to extract, which is \nx\n)\n\n\nf\n — filename of the archive file\n\n\nz\n — filter archive through \ngzip\n (remove this option to create a \n.tar\n file)\n\n\n\n\nIf you want to \ntar\n the current directory, use \n.\n to designate that.\n\n\nTo construct filenames dynamically, use the \ndate\n utility (look at its man page for the available format options). For example:\n\n\n\n```bash\ncd /var/www &&\ntar czf ~/www_backups/$(date +%Y%m%d-%H%M%S).tar.gz .\n```\n\n\n\nThis will create a file named something like \n20120902-185558.tar.gz\n.\n\n\nOn Linux, chances are your \ntar\n also supports BZip2 compression with the \nj\n rather than \nz\n option. And possibly others. Check the man page on your local system.",
    "url": "https://unix.stackexchange.com/questions/46969/compress-a-folder-with-tar"
  },
  {
    "question_title": "Combined `mkdir` and `cd`?",
    "question_body": "is there any way (what is the easiest way in bash) to combine the following:\n\n\n\n```bash\nmkdir foo\ncd foo\n```\n\n\n\nThe manpage for \nmkdir\n does not describe anything like that, maybe there is a fancy version of \nmkdir\n? I know that \ncd\n has to be shell builtin, so the same would be true for the fancy \nmkdir\n...\n\n\nAliasing?",
    "answer": "I think creating a function is the most appropriate way to do this, but just for listing all alternative ways, you could write:\n\n\n\n```bash\nmkdir foo && cd \"$_\"\n```\n\n\n\n$_\nis a special parameter that holds the last argument of the previous command. The quote around \n$_\n make sure it works even if the folder name contains spaces.\n\n\nWhy use double quotes?\n\n\nIn some shells, such as \nzsh\n, the double quotes surrounding the \n$_\n are not necessary even when the directory name contains spaces. They are required for this command to work in \nbash\n, however.\n\n\nFor example, running this command in bash 3.2.57 on macOS 10.13.6:\n\n\n\n```bash\nmkdir \"my directory\" && cd $_\n```\n\n\n\nresults in this output:\n\n\n\n```bash\nbash: cd: my: No such file or directory\n```\n\n\n\nHowever, if we surround \n$_\n with double quotes, the command returns successfully.\n\n\n\n```bash\nbash-3.2$ mkdir \"my directory\" && cd \"$_\"\nbash-3.2$ echo $?\n0\nbash-3.2$\n```",
    "url": "https://unix.stackexchange.com/questions/125385/combined-mkdir-and-cd"
  },
  {
    "question_title": "Are there pitfalls to putting $HOME in git instead of symlinking dotfiles?",
    "question_body": "I have for many years had my entire \n$HOME\n directory checked into subversion. This has included all my dotfiles and application profiles, many scripts, tools and hacks, my preferred basic home directory structure, not a few oddball projects and a warehouse worth of random data. This was a good thing. While it lasted.\n\n\nBut it's gotten out of hand. The basic checkout is the same across dozens of systems, but not all that stuff is appropriate for all my machines. It doesn't even all play nicely with different distros.\n\n\nI'm in the process of cleaning house -- separating the data out where it belongs, splitting out some scripts as separate projects, fixing some broken links in stuff that should be automated, etc.\n\n\nMy intent is to replace \nsubversion\n with \ngit\n for the toplevel checkout of \n$HOME\n, but I'd like to pare this down to just the things I'd like to have on ALL my systems, meaning dotfiles, a few directories and some basic custom scripts.\n\n\nIn reading up online a lot of people seem to be doing this using the symlink approach: clone into a subdirectory then create symlinks from \n$HOME\n into the repository.\n Having had my \n$HOME\n under full version control for over a decade, I don't like the idea of this approach and I can't figure out why people seem so averse to the straight checkout method. \nAre there pitfalls I need to know about specific to \ngit\n as a top level checkout for \n$HOME\n?\n\n\nP.S. Partly as an exercise in good coding, I'm also planning on making my root checkout public on GitHub. It's scary how much security sensitive information I've allowed to collect in files that ought to be sharable without a second thought! WiFi password, un-passphrased RSA keys, etc. Eeek!",
    "answer": "Yes\n, there is at least one major pitfall when considering \ngit\n to manage a home directory that is not a concern with \nsubversion\n.\n\n\nGit is both greedy and recursive by default\n.\n\n\nSubversion will naively ignore anything it doesn't know about and it stops processing folders either up or down from your checkout when it reaches one that it doesn't know about (or that belongs to a different repository). Git, on the other hand, keeps recursing into all child directories making nested checkouts very complicated due to namespace issues. Since your home directory is likely also the place where you checkout and work on various other git repositories, having your home directory in git is almost certainly going to make your life an impossible mess.\n\n\nAs it turns out, this is the main reason people checkout their dotfiles into an isolated folder and then symlink into it. It keeps git out of the way when doing anything else in any child directory of your \n$HOME\n. While this is purely a matter of preference if checking your home into subversion, it becomes a matter of necessity if using git.\n\n\nHowever\n, there is an alternate solution. Git allows for something called a \"fake root\" where all the repository machinery is hidden in an alternate folder that can be physically separated from the checkout working directory. The result is that the git toolkit won't get confused: it won't even SEE your repository, only the working copy. By setting a couple environment variables you can tip off git where to find the goods for those moments when you are managing your home directory. Without the environment variables set nobody is the wiser and your home looks like it's classic file-y self.\n\n\nTo make this trick flow a little smoother, there are some great tools out there. The \nvcs-home mailing list\n seems like the defacto place to start, and the about page has a convenient wrap up of howtos and people's experiences. Along the way are some nifty little tools like \nvcsh\n, \nmr\n. If you want to keep your home directory directly in git, vcsh is almost a must have tool. If you end up splitting your home directory into several repostories behind the scenes, combine \nvcsh\n with \nmr\n for quick and not very dirty way to manage it all at once.",
    "url": "https://unix.stackexchange.com/questions/46538/are-there-pitfalls-to-putting-home-in-git-instead-of-symlinking-dotfiles"
  },
  {
    "question_title": "How do I grep for multiple patterns with pattern having a pipe character?",
    "question_body": "I want to find all lines in several files that match one of two patterns. I tried to find the patterns I'm looking for by typing \n\n\n\n```bash\ngrep (foo|bar) *.txt\n```\n\n\n\nbut the shell interprets the \n|\n as a pipe and complains when \nbar\n isn't an executable.\n\n\nHow can I grep for multiple patterns in the same set of files?",
    "answer": "First, you need to protect the pattern from expansion by the shell. The easiest way to do that is to put single quotes around it. Single quotes prevent expansion of anything between them (including backslashes); the only thing you can't do then is have single quotes in the pattern.\n\n\n\n```bash\ngrep -- 'foo*' *.txt\n```\n\n\n\n(also note the \n--\n end-of-option-marker to stop some \ngrep\n implementations including GNU \ngrep\n from treating a file called \n-foo-.txt\n for instance (that would be expanded by the shell from \n*.txt\n) to be taken as an option (even though it follows a non-option argument here)).\n\n\nIf you do need a single quote, you can write it as \n'\\''\n (end string literal, literal quote, open string literal).\n\n\n\n```bash\ngrep -- 'foo*'\\''bar' *.txt\n```\n\n\n\nSecond, grep supports at least¹ two syntaxes for patterns. The old, default syntax (\nbasic regular expressions\n) doesn't support the alternation (\n|\n) operator, though some versions have it as an extension, but written with a backslash.\n\n\n\n```bash\ngrep -- 'foo\\|bar' *.txt\n```\n\n\n\nThe portable way is to use the newer syntax, \nextended regular expressions\n. You need to pass the \n-E\n option to \ngrep\n to select it (formerly that was done with the \negrep\n separate command²)\n\n\n\n```bash\ngrep -E -- 'foo|bar' *.txt\n```\n\n\n\nAnother possibility when you're just looking for any of several patterns (as opposed to building a complex pattern using disjunction) is to pass multiple patterns to \ngrep\n. You can do this by preceding each pattern with the \n-e\n option.\n\n\n\n```bash\ngrep -e foo -e bar -- *.txt\n```\n\n\n\nOr put patterns on several lines:\n\n\n\n```bash\ngrep -- 'foo\nbar' *.txt\n```\n\n\n\nOr store those patterns in a file, one per line and run\n\n\n\n```bash\ngrep -f that-file -- *.txt\n```\n\n\n\nNote that if \n*.txt\n expands to a single file, \ngrep\n won't prefix matching lines with its name like it does when there are more than one file. To work around that, with some \ngrep\n implementations like GNU \ngrep\n, you can use the \n-H\n option, or with any implementation, you can pass \n/dev/null\n as an extra argument.\n\n\n\n\n¹ some \ngrep\n implementations support even more like perl-compatible ones with \n-P\n, or \naugmented\n ones with \n-X\n, \n-K\n for ksh wildcards...\n\n\n² while \negrep\n has been deprecated by POSIX and is sometimes no longer found on some systems, on some other systems like Solaris when the POSIX or GNU utilities have not been installed, then \negrep\n is your only option as its \n/bin/grep\n supports none of \n-e\n, \n-f\n, \n-E\n, \n\\|\n or multi-line patterns",
    "url": "https://unix.stackexchange.com/questions/37313/how-do-i-grep-for-multiple-patterns-with-pattern-having-a-pipe-character"
  },
  {
    "question_title": "How do I delete the first n lines of an ascii file using shell commands?",
    "question_body": "I have multiple files that contain ascii text information in the first 5-10 lines, followed by well-tabulated matrix information.  In a shell script, I want to remove these first few lines of text so that I can use the pure matrix information in another program.  How can I use bash shell commands to do this?  \n\n\nIf it's any help, I'm using RedHat and an Ubuntu linux systems.",
    "answer": "As long as the file is not a symlink or hardlink, you can use sed, tail, or awk. Example below.\n\n\n\n```bash\n$ cat t.txt\n12\n34\n56\n78\n90\n```\n\n\n\nsed\n\n\n\n```bash\n$ sed -e '1,3d' < t.txt\n78\n90\n```\n\n\n\nYou can also use sed in-place without a temp file: \nsed -i -e 1,3d yourfile\n. This won't echo anything, it will just modify the file in-place. If you don't need to pipe the result to another command, this is easier.\n\n\ntail\n\n\n\n```bash\n$ tail -n +4 t.txt\n78\n90\n```\n\n\n\nawk\n\n\n\n```bash\n$ awk 'NR > 3 { print }' < t.txt\n78\n90\n```",
    "url": "https://unix.stackexchange.com/questions/37790/how-do-i-delete-the-first-n-lines-of-an-ascii-file-using-shell-commands"
  },
  {
    "question_title": "Does git contain information about used id / group id changes?",
    "question_body": "I have googling awhile, but can not find such information. Looks like git doesn't contain users and groups, only permissions. Am I right?",
    "answer": "See the \nContent Limitations\n section of the git Wiki: git does not track file ownership, group membership, doesn't track most permission bits, ACLs, access and modification times, etc.\n\n\nGit tracks contents, and doesn't care much about pretty much everything else.",
    "url": "https://unix.stackexchange.com/questions/101510/does-git-contain-information-about-used-id-group-id-changes"
  },
  {
    "question_title": "Tool for searching across branches and through history in a Git repository",
    "question_body": "I'm looking for a tool, or for suggestions towards a script, that would be able to search a Git repository for files based on both filenames and file contents (\nfind\n/\ngrep\n-like).  It would need to be able to search not just in the currently checked out branch, but through that branch's history as well as in other branches.\n\n\nThe very specific example of a Git repository that I have is a checkout of \ndspinellis' \nunix-history-repo\n.  I often look for historical implementations of things in there, but it's a pain to track down files as I often need to \nguess\n what branch I need to be looking at (there are 165 branches in that repository).\n\n\nExample of thing I would like to do with this tool\n is to \nfind the \nwow\n command\n, which may have been an external command or a built-in command in \nsh\n or \ncsh\n at some point, if it existed as part of some early BSD Unix (if it existed at all).  To do this, I would want to search for \nwow.*\n as a filename pattern, and also for files that may at some point have included a \nwow\n C function, across the 165 branches of dspinellis' Git repository.",
    "answer": "Heh, guess what I’ve been doing too...\n\n\nTo look for a file name across all branches, I use\n\n\n\n```bash\ngit log --all --name-only --pretty=format:%H -- wow\\*\n```\n\n\n\nwow\n can be replaced by any glob. This runs quite quickly on the Unix history repository. The format shows the hash leading to the creation of the matching file, so you can then check the tree out at that point and explore further.\n\n\nTo search file contents across all branches, I use\n\n\n\n```bash\ngit rev-list --all | xargs git grep \"excited too\"\n```\n\n\n\nwhich lists all commit objects and searches through them. This is very, very slow on the Unix history repository; listing all the branches and grepping there is quicker:\n\n\n\n```bash\ngit grep \"excited too\" $(git branch -r | awk '{print $1}')\n```",
    "url": "https://unix.stackexchange.com/questions/493747/tool-for-searching-across-branches-and-through-history-in-a-git-repository"
  },
  {
    "question_title": "read first line from .gz compressed file without decompressing entire file",
    "question_body": "I have a huge log file compressed in .gz format and I want to just read the first line of it without uncompressing it to just check the date of the oldest log in the file.\n\n\nThe logs are of the form:\n\n\n\n```bash\nYYYY-MM-DD Log content asnsenfvwen eaifnesinrng\nYYYY-MM-DD Log content asnsenfvwen eaifnesinrng\nYYYY-MM-DD Log content asnsenfvwen eaifnesinrng\n```\n\n\n\nI just want to read the date in the first line which I would do like this for an uncompressed file:\n\n\n\n```bash\nread logdate otherstuff < logfile.gz\necho $logdate\n```\n\n\n\nUsing zcat is taking too long.",
    "answer": "Piping \nzcat\n’s output to \nhead -n 1\n will decompress a small amount of data, guaranteed to be enough to show the first line, but typically no more than a few buffer-fulls (96 KiB in my experiments):\n\n\n\n```bash\nzcat logfile.gz | head -n 1\n```\n\n\n\nOnce \nhead\n has finished reading one line, it closes its input, which closes the pipe, and \nzcat\n stops after receiving a \nSIGPIPE\n (which happens when it next tries to write into the closed pipe). You can see this by running\n\n\n\n```bash\n(zcat logfile.gz; echo $? >&2) | head -n 1\n```\n\n\n\nThis will show that \nzcat\n exits with code 141, which indicates it stopped because of a \nSIGPIPE\n (13 + 128).\n\n\nYou can add more post-processing, \ne.g.\n with AWK, to only extract the date:\n\n\n\n```bash\nzcat logfile.gz | awk '{ print $1; exit }'\n```\n\n\n\n(On macOS you might need to use \ngzcat\n rather than \nzcat\n to handle gzipped files.)",
    "url": "https://unix.stackexchange.com/questions/452011/read-first-line-from-gz-compressed-file-without-decompressing-entire-file"
  },
  {
    "question_title": "git pull from remote but no such ref was fetched?",
    "question_body": "I have a git mirror on my disk and when I want to update my repo with git pull it gives me error message:\n\n\n\n```bash\nYour configuration specifies to merge with the ref '3.5/master' from the remote, but no such ref was fetched.\n```\n\n\n\nIt also gives me: \n\n\n\n```bash\n1ce6dac..a5ab7de  3.4/bfq    -> origin/3.4/bfq\n  fa52ab1..f5d387e  3.4/master -> origin/3.4/master\n  398cc33..1c3000a  3.4/upstream-updates -> origin/3.4/upstream-updates\n  d01630e..6b612f7  3.7/master -> origin/3.7/master\n  491e78a..f49f47f  3.7/misc   -> origin/3.7/misc\n  5b7be63..356d8c6  3.7/upstream-updates -> origin/3.7/upstream-updates\n  636753a..027c1f3  3.8/master -> origin/3.8/master\n  b8e524c..cfcf7b5  3.8/misc   -> origin/3.8/misc\n  * [neuer Zweig]     3.8/upstream-updates -> origin/3.8/upstream-updates\n```\n\n\n\nWhen I run make menuconfig it gives me Linux version 3.5.7? What does this mean? How can I update my repo?",
    "answer": "Check the branch you are on (\ngit branch\n), check the configuration for that branch (in \n.../.git/config\n), you probably are on the wrong branch or your configuration for it tells to merge with a (now?) non-existent remote branch.",
    "url": "https://unix.stackexchange.com/questions/66548/git-pull-from-remote-but-no-such-ref-was-fetched"
  },
  {
    "question_title": "Tar a folder without .git files?",
    "question_body": "If I tar a folder that is a git repository, can I do so without including the \n.git\n related files? If not, how would I go about doing that via a command?",
    "answer": "Simplest answer: Add \n--exclude-vcs\n. This excludes all version control system directories\n\n\nPersonally I use\n\n\n\n```bash\ntar --exclude-vcs -zcvf foo.tar.gz ./FOLDER_NAME\n```\n\n\n\nso all you need to do is add the \n--exclude-vcs\n at the end of the command.",
    "url": "https://unix.stackexchange.com/questions/18657/tar-a-folder-without-git-files"
  },
  {
    "question_title": "Excluding files in etckeeper with .gitignore doesn&#39;t work",
    "question_body": "I use etckeeper for changes in my configfiles (on Debian squeeze)\n\n\nSince I also have an ircdeamon running, there are some files, that change every minute in the folder\n\n\n\n```bash\n/etc/hybserv/\n```\n\n\n\nI don't want to version control them anymore, so I added \n\n\n\n```bash\nhybserv/*\n```\n\n\n\nto the end of\n\n\n\n```bash\n/etc/.gitignore\n```\n\n\n\nbut they are not ignored! They keep showing up every hour in the hourly commit.\n\n\nWhat am I doing wrong?",
    "answer": "You need to delete (=unregister) them from git.\n\n\nUse something like \n\n\n\n```bash\ncd /etc\ngit rm --cached hybserv/*\ngit commit -m \"Remove hybserv/* files from git\"\n```\n\n\n\nNote the \n--cached\n option. With it, the files are \nonly\n removed from git and are not deleted from the disk.",
    "url": "https://unix.stackexchange.com/questions/63627/excluding-files-in-etckeeper-with-gitignore-doesnt-work"
  },
  {
    "question_title": "How to run a program in a clean environment in bash?",
    "question_body": "I want to run a program in an empty environment (i.e. with no envariables set). How to do this in bash?",
    "answer": "You can do this with \nenv\n:\n\n\n\n```bash\nenv -i your_command\n```\n\n\n\nContrary to comments below, this \ndoes\n completely clear out the environment, but it does not prevent \nyour_command\n setting new variables. In particular, running a shell will cause the \n/etc/profile\n to run, and the shell may have some built in settings also.\n\n\nYou can check this with:\n\n\n\n```bash\nenv -i env\n```\n\n\n\ni.e. wipe the environment and then print it. The output will be blank.",
    "url": "https://unix.stackexchange.com/questions/48994/how-to-run-a-program-in-a-clean-environment-in-bash"
  },
  {
    "question_title": "git grep colors differ from grep custom colors",
    "question_body": "I've setup grep colors in my \n~/.bashrc\n :\n\n\n\n```bash\nexport GREP_COLORS='ms=01;34:mc=01;34:sl=:cx=:fn=35:ln=32:bn=32:se=36'\n```\n\n\n\nThey work for \n\n\n\n\ngrep --color=auto\n\n\ngrep --color=always\n\n\n\n\nUnfortunately, those custom colors are ignored by:\n\n\n\n\ngit grep --color=auto\n\n\ngit grep --color=always\n\n\n\n\nHow to make \ngit grep\n to use above \n$GREP_COLORS\n colors ?",
    "answer": "Git grep is not using the \nGREP_COLORS\n environment variable. Instead you should add custom entries in you \n~/.gitconfig\n\n\nFor example: \n\n\n\n```bash\n[color \"grep\"]\n    linenumber = yellow bold\n    match = red\n    filename = magenta\n```",
    "url": "https://unix.stackexchange.com/questions/30591/git-grep-colors-differ-from-grep-custom-colors"
  },
  {
    "question_title": "Why does gzipping a file on stdin yield a smaller output than the same file given as an argument?",
    "question_body": "When I do:\n\n\n\n```bash\n# gzip -c foo > foo1.gz \n# gzip < foo > foo2.gz\n```\n\n\n\nWhy does \nfoo2.gz\n end up being smaller in size than \nfoo1.gz\n?",
    "answer": "Because it's saving the filename and timestamp so that it can try to restore both after you decompress it later. Since \nfoo\n is given to \ngzip\n via \n<stdin>\n in your second example, it can't store the filename and timestamp information.\n\n\nFrom the manpage:\n\n\n\n```bash\n-n --no-name\n          When compressing, do not save the original file name and time stamp by default. (The original name is always saved if the name had\n          to  be truncated.) When decompressing, do not restore the original file name if present (remove only the gzip suffix from the com-\n          pressed file name) and do not restore the original time stamp if present (copy it from the compressed file). This  option  is  the\n          default when decompressing.\n\n   -N --name\n          When compressing, always save the original file name and time stamp; this is the default. When decompressing, restore the original\n          file name and time stamp if present. This option is useful on systems which have a limit on file name  length  or  when  the  time\n          stamp has been lost after a file transfer.\n```\n\n\n\nI've recreated the issue here:\n\n\n\n```bash\n[root@xxx601 ~]# cat /etc/fstab > file.txt\n[root@xxx601 ~]# gzip < file.txt > file.txt.gz\n[root@xxx601 ~]# gzip -c file.txt > file2.txt.gz\n[root@xxx601 ~]# ll -h file*\n-rw-r--r--. 1 root root  465 May 17 19:35 file2.txt.gz\n-rw-r--r--. 1 root root 1.2K May 17 19:34 file.txt\n-rw-r--r--. 1 root root  456 May 17 19:34 file.txt.gz\n```\n\n\n\nIn my example, \nfile.txt.gz\n is the equivalent of your \nfoo2.gz\n. Using the \n-n\n option disables this behavior when it otherwise \nwould\n have access to the information:\n\n\n\n```bash\n[root@xxx601 ~]# gzip -nc file.txt > file3.txt.gz\n[root@xxx601 ~]# ll -h file*\n-rw-r--r--. 1 root root  465 May 17 19:35 file2.txt.gz\n-rw-r--r--. 1 root root  456 May 17 19:43 file3.txt.gz\n-rw-r--r--. 1 root root 1.2K May 17 19:34 file.txt\n-rw-r--r--. 1 root root  456 May 17 19:34 file.txt.gz\n```\n\n\n\nAs you can see above, the file sizes for \nfile.txt\n and \nfile3.txt\n match since they're now both omitting name and date.",
    "url": "https://unix.stackexchange.com/questions/203977/why-does-gzipping-a-file-on-stdin-yield-a-smaller-output-than-the-same-file-give"
  },
  {
    "question_title": "command line method or programmatically add ssh key to github.com user account",
    "question_body": "Is there a way to identify with a username and password to github.com servers for the purpose of adding an ssh key to the github user account? So far everything I've read suggests that a user's ssh key must be added via the web GUI. I'm looking for the method or process of adding a key via a command line interface or else a bash/ansible/something script.",
    "answer": "Update 2020\n\n\nAs stated in \ndeveloper changes\n, Password authentication is going to be deprecated at:\n\n\nNovember 13, 2020 at 16:00 UTC\n\n\nAdditionally, as @trysis asked in the comments, we need a solution for 2FA.\n\n\nThe new way is to use a \npersonal access token\n:\n\n\n\nFor our specific example (adding a ssh key), we only need write permissions (read permissions are added automatically on using write permissions):\n\n\n\nThe updated command (via curl):\n\n\n\n```bash\ncurl -H \"Authorization: token YourGeneratedToken\" --data '{\"title\":\"test-key\",\"key\":\"ssh-rsa AAA...\"}' https://api.github.com/user/keys\n```\n\n\n\nThis does also work when 2FA is enabled.\n\n\n\n\nOLD\n\n\nAuth with username and password is \nsupported by github api\n:\n\n\n\n> There are three ways to authenticate through GitHub API v3.\n> ...\n> Basic Authentication\n> $ curl -u \"username\" https://api.github.com\n> ...\n\n\n\nSo just \nchoose a lib\n in the language you prefer\nand use the implemented version of the \nCreate a Public Key\n \"Public Key\" API Section:\n\n\n\n> Creates a public key. Requires that you are authenticated via Basic Auth, or OAuth with at least [write:public_key] scope.\n\n\n\nINPUT\n\n\nPOST /user/keys\n\n\n\n```bash\n{\n    \"title\": \"octocat@octomac\",\n    \"key\": \"ssh-rsa AAA...\"\n}\n```\n\n\n\nIf you want to use it from command line (via curl):\n\n\n\n```bash\ncurl -u \"username\" --data '{\"title\":\"test-key\",\"key\":\"ssh-rsa AAA...\"}' https://api.github.com/user/keys\n```\n\n\n\nor even without prompting for password:\n\n\n\n```bash\ncurl -u \"username:password\" --data '{\"title\":\"test-key\",\"key\":\"ssh-rsa AAA...\"}' https://api.github.com/user/keys\n```\n\n\n\nhere is a nice little tutorial for \nusing curl to interact with github API",
    "url": "https://unix.stackexchange.com/questions/136894/command-line-method-or-programmatically-add-ssh-key-to-github-com-user-account"
  },
  {
    "question_title": "Tips for putting ~ under source control",
    "question_body": "I want to put my home directory (~) under source control (git, in this case), as I have many setting files (.gitconfig, .gitignore, .emacs, etc.) in there I would like to carry across machines, and having them in Git would make it nice for retrieving them.  \n\n\nMy main machine is my MacBook, and the way that OS X is set up, there are many folders I want to ignore (Documents, Downloads, .ssh).  There are also folders which are already using Git (.emacs.d).  \n\n\nMy thought was to just add all these directories to my .gitignore file, but that seems kind of tiresome, and could potentially lead to some unforeseen consequences.  My next thought was to periodically copy the files I want to store into some folder in home, then commit that folder.  The problem with that will be that I have to remember to move them before committing.\n\n\nIs there a clean way to do this?",
    "answer": "I have \n$HOME\n under git. The first line of my .gitignore file is\n\n\n\n```bash\n/*\n```\n\n\n\nThe rest are patterns to not ignore using the \n!\n modifier. This first line means the default is to ignore all files in my home directory. Those files that I want to version control go into \n.gitignore\n like this:\n\n\n\n```bash\n!/.gitignore\n!/.profile\n[...]\n```\n\n\n\nA trickier pattern I have is:\n\n\n\n```bash\n!/.ssh\n/.ssh/*\n!/.ssh/config\n```\n\n\n\nThat is, I only want to version \n.ssh/config\n - I don't want my keys and other files in .ssh to go into git. The above is how I achieve that.\n\n\nEdit: Added slashes to start of all paths. This makes the ignore patterns match from the top of the repository ($HOME) instead of anywhere. For example, if \n!lib/\n was a pattern (don't ignore everything in the lib directory) and you add a file \n.gitignore\n, previously the pattern (\n!.gitignore\n) was matching that. With the leading slash (\n!/.gitignore\n), it will only match \n.gitignore\n in my home directory and not in any subdirectories.\n\n\nI haven't seen a case where this makes a practical difference with my ignore list, but it appears to me to be more technically accurate.",
    "url": "https://unix.stackexchange.com/questions/1875/tips-for-putting-under-source-control"
  },
  {
    "question_title": "Unzipping a .gz file without removing the gzipped file",
    "question_body": "I have a file \nfile.gz\n, when I try to unzip this file by using \ngunzip file.gz\n, it unzipped the file but only contains extracted and removes the \nfile.gz\n file.\n\n\nHow can I unzip by keeping both unzipped file and zipped file?",
    "answer": "Here are several alternatives:\n\n\n\n\nGive \ngunzip\n the \n--keep\n option (version 1.6 or later)\n\n\n\n> -k   --keep\n> Keep (don't delete) input files during compression or decompression.\n\n\n\n\n```bash\ngunzip -k file.gz\n```\n\n\n\nPass the file to \ngunzip\n as stdin\n\n\n\n```bash\ngunzip < file.gz > file\n```\n\n\n\nUse \nzcat\n (or, on older systems, \ngzcat\n)\n\n\n\n```bash\nzcat file.gz > file\n```",
    "url": "https://unix.stackexchange.com/questions/156261/unzipping-a-gz-file-without-removing-the-gzipped-file"
  },
  {
    "question_title": "Terminal prompt not wrapping correctly",
    "question_body": "I have an issue where if I type in very long commands in bash the terminal will not render what I'm typing correctly. I'd expect that if I had a command like the following:\n\n\n\n```bash\nusername@someserver ~/somepath $ ssh -i /path/to/private/key\nmyusername@something.someserver.com\n```\n\n\n\nThe command should render on two lines. Instead it will often wrap around and start writing over the top of my prompt, somewhat like this:\n\n\n\n```bash\nmyreallylongusername@something.somelongserver.comh -i /path/to/private/key\n```\n\n\n\nIf I decide to go back and change some argument there's no telling where the cursor will show up, sometimes in the middle of the prompt, but usually on the line \nabove\n where I'm typing. \n\n\nAdditional fun happens when when I \nUp\n to a previous command.\nI've tried this in both gnome-terminal and terminator and on i3 and Cinnamon. Someone suggested it was my prompt, so here that is:\n\n\n\n```bash\n\\[\\033[01;32m\\]\\u:\\[\\033[01;34m\\] \\W\\033[01;34m \\$\\[\\033[00m\\]\n```\n\n\n\nCtrl\nl\n, \nreset\n, and \nclear\n all do what they say, but when I type the command back in or \nUp\n the same things happens.\n\n\nI checked and \ncheckwinsize\n is enabled in bash. This happens on 80x24 and other window sizes.\n\n\nIs this just something I learn to live with? Is there some piece of magic which I should know? I've settled for just using a really short prompt, but that doesn't fix the issue.",
    "answer": "Non-printable sequences should be \nenclosed in \n\\[\n and \n\\]\n. Looking at your \nPS1\n it has a unenclosed sequence after \n\\W\n. But, the second entry is redundant as well as it repeats the previous statement \n\"1;34\"\n.\n\n\n\n```bash\n\\[\\033[01;32m\\]\\u:\\[\\033[01;34m\\] \\W\\033[01;34m \\$\\[\\033[00m\\]\n                  |_____________|               |_|\n                         |                       |\n                         +--- Let this apply to this as well.\n```\n\n\n\nAs such this should have intended coloring:\n\n\n\n```bash\n\\[\\033[1;32m\\]\\u:\\[\\033[1;34m\\] \\W \\$\\[\\033[0m\\]\n                               |_____|\n                                  |\n                                  +---- Bold blue.\n```\n\n\n\nKeeping the \n\"original\"\n this should also work:\n\n\n\n```bash\n\\[\\033[1;32m\\]\\u:\\[\\033[1;34m\\] \\W\\[\\033[1;34m\\] \\$\\[\\033[0m\\]\n                                  |_|         |_|\n                                   |           |\n                                   +-----------+-- Enclose in \\[ \\]\n```\n\n\n\n\n\nEdit:\n\n\nThe reason for the behavior is because \nbash\n believes the prompt is longer then it actually is. As a simple example, if one use:\n\n\n\n```bash\nPS1=\"\\033[0;34m$\"\n       1 2345678\n```\n\n\n\nThe prompt is believed to be 8 characters and not 1. As such if terminal window is 20 columns, after typing 12 characters, it is believed to be 20 and wraps around. This is also evident if one then try to do backspace or \nCtrl+u\n. It stops at column 9.\n\n\nHowever it also does not start new line unless one are on last column, as a result the first line is overwritten.\n\n\nIf one keep typing the line should wrap to next line after 32 characters.",
    "url": "https://unix.stackexchange.com/questions/105958/terminal-prompt-not-wrapping-correctly"
  },
  {
    "question_title": "Fastest and most efficient way to get number of records (lines) in a gzip-compressed file",
    "question_body": "I am trying to do a record count on a 7.6 GB gzip file. I found few approaches using the \nzcat\n command.\n\n\n\n```bash\n$ zcat T.csv.gz | wc -l\n423668947\n```\n\n\n\nThis works but it takes too much time (more than 10 minutes to get the count). I tried a few more approaches like\n\n\n\n```bash\n$ sed -n '$=' T.csv.gz\n28173811\n$ perl -lne 'END { print $. }' < T.csv.gz\n28173811\n$ awk 'END {print NR}' T.csv.gz\n28173811\n```\n\n\n\nAll three of these commands are executing pretty fast but giving an incorrect count of 28173811.\n\n\nHow can I perform a record count in a minimal amount of time?",
    "answer": "The \nsed\n, \nperl\n and \nawk\n commands that you mention may be correct, but they all read the \ncompressed\n data and counts newline characters in that.  These newline characters have nothing to do with the newline characters in the uncompressed data.\n\n\nTo count the number of lines in the uncompressed data, there is no way around uncompressing it.  Your approach with \nzcat\n is the correct approach and since the data is so large, it \nwill\n take time to uncompress it.\n\n\nMost utilities that deals with \ngzip\n compression and decompression will most likely use the same shared library routines to do so.  The only way to speed it up would be to find an implementation of the \nzlib\n routines that are somehow faster than the default ones, and rebuild e.g. \nzcat\n to use those.",
    "url": "https://unix.stackexchange.com/questions/363644/fastest-and-most-efficient-way-to-get-number-of-records-lines-in-a-gzip-compre"
  },
  {
    "question_title": "Splitting gzip-file into smaller gz-files without recompressing",
    "question_body": "I have a big .gz file. I would like to split it into 100 smaller gzip files, that can each be decompressed by itself. In other words: I am not looking for a way of chopping up the .gz file into chunks that would have to be put back together to be able to decompress it. I want to be able to decompress each of the smaller files independently.\n\n\nCan it be done without recompressing the whole file?\n\n\nCan it be done if the original file is compressed with \n--rsyncable\n? (\"Cater better to the rsync program by periodically resetting the internal structure of the compressed data stream.\" sounds like these reset points might be good places to split at and probably prepend a header.)\n\n\nCan it be done for any of the other compressed formats? I would imagine \nbzip2\n would be doable - as it is compressed in blocks.",
    "answer": "Split and join of the big file works, but it is impossible to decompress pieces of the compressed file, because essential informations are distributed through the whole dataset. Another way; split the uncompressed file and compress the single parts. Now you can decompress each pieces. But why? You have to merge all decompressed parts before further processing.",
    "url": "https://unix.stackexchange.com/questions/338195/splitting-gzip-file-into-smaller-gz-files-without-recompressing"
  },
  {
    "question_title": "grep inside less?",
    "question_body": "I'm currently sifting through a lot of unfamiliar logs looking for some issues.  The first file I look at is Events.log, and I get at least three pages in \nless\n which appear to display the same event at different times – an event that appears to be fairly benign.\nI would like to filter this event out, and currently I quit \nless\n and do something like\n\n\n\n```bash\ngrep -v \"event text\" Events.log | less\n```\n\n\n\nThis now brings a number of other common, uninteresting events that I would also like to filter out.  Is there a way I can \ngrep -v\n \ninside\n of \nless\n?  Rather than having to do\n\n\n\n```bash\negrep -v \"event text|something else|the other thing|foo|bar\" Events.log | less\n```\n\n\n\nIt strikes me as a useful feature when looking at any kind of log file – and if \nless\n isn't the tool, is there another with the qualities I seek?  Just a \nless\n-style viewer with built in \ngrep\n.",
    "answer": "less\n has very powerful pattern matching.  From the \nman page\n:\n\n\n\n> &pattern\n> Display only lines which match the pattern;\n> lines which do not match the pattern\n> are not displayed.  If pattern is empty\n> (if you type & immediately followed by ENTER),\n> any filtering is turned off, and all lines are displayed.\n> While filtering is in effect,\n> an ampersand is displayed at the beginning of the prompt,\n> as a reminder that some lines in the file may be hidden.\n> Certain characters are special as in the / command†:\n> ^N or !\n> Display only lines which do NOT match the pattern.\n> ^R\n> Don't interpret regular expression metacharacters;\n> that is, do a simple textual comparison.\n> ____________\n> † Certain characters are special\n> if entered at the beginning of the pattern;\n> they modify the type of search\n> rather than become part of the pattern.\n\n\n\n   (Of course \n^N\n and \n^R\n represent \nCtrl\n+\nN\n\nand \nCtrl\n+\nR\n, respectively.) \n\n\nSo, for example, \n&dns\n will display only lines that match the pattern \ndns\n,\nand \n&!dns\n will filter out (exclude) those lines,\ndisplaying only lines that don't match the pattern.\n\n\nIt is noted in the description of the \n/\n command that\n\n\n\n> The pattern is a regular expression,\n> as recognized by the regular expression library supplied by your system.\n\n\n\nSo\n\n\n\n\n&eth[01]\n  will display lines containing \neth0\n or \neth1\n\n\n&arp.*eth0\n will display lines containing \narp\n followed by \neth0\n\n\n&arp|dns\n  will display lines containing \narp\n or \ndns\n\n\n\n\nAnd the \n!\n can invert any of the above. \nSo the command you would want to use for the example in your question is:\n\n\n\n```bash\n&!event text|something else|the other thing|foo|bar\n```\n\n\n\nAlso use \n/\npattern\n and \n?\npattern\n\nto search (and \nn\n/\nN\n to go to next/previous).",
    "url": "https://unix.stackexchange.com/questions/179238/grep-inside-less"
  },
  {
    "question_title": "Wget returning binary instead of html?",
    "question_body": "I am using wget to download a static html page.  The W3C Validator tells me the page is encoded in UTF-8.  Yet when I cat the file after download, I get a bunch of binary nonsense.  I'm on Ubuntu, and I thought the default encoding was UTF-8?  That's what my locale file seems to say.  Why is this happening and how can I correct it?\n\n\nAlso, looks like \nContent-Encoding: gzip\n.  Perhaps this makes a diff?\n\n\nThis is the simple request:\n\n\n\n```bash\nwget https://www.example.com/page.html\n```\n\n\n\nI also tried this: \n\n\n\n```bash\nwget https://www.example.com/page.html -q -O - | iconv -f utf-16 -t utf-8 > output.html\n```\n\n\n\nWhich returned: \niconv: illegal input sequence at position 40\n\n\ncat'ing the file returns binary that looks like this:\n\n\n\n```bash\nl�?חu�`�q\"�:)s��dġ__��~i��6n)T�$H�#���QJ\n```\n\n\n\nResult of \nxxd output.html | head -20\n :\n\n\n\n```bash\n00000000: 1f8b 0800 0000 0000 0003 bd56 518f db44  ...........VQ..D\n00000010: 107e a6bf 62d4 8a1e 48b9 d8be 4268 9303  .~..b...H...Bh..\n00000020: 8956 082a 155e 7a02 21dd cbd8 3bb6 97ae  .V.*.^z.!...;...\n00000030: 77cd ee38 39f7 a1bf 9d19 3bb9 0bbd 9c40  w..89.....;....@\n00000040: 2088 12c5 de9d 9df9 be99 6f67 f751 9699   .........og.Q..\n00000050: 500d 1d79 5eee a265 faec 7151 e4ab 6205  P..y^..e..qQ..b.\n00000060: 4dd3 0014 1790 e7d0 77c0 ef2f cbf8 cde3  M.......w../....\n00000070: cf1f 7d6c 7d69 ec16 d0d9 c67f 7d7d 56c9  ..}l}i......}}V.\n00000080: 04c5 eb33 35fc e49e 2563 e908 ca10 0d45  ...35...%c.....E\n00000090: 31ce afcf a022 e77a 34c6 fa46 46be d88f  1....\".z4..FF...\n000000a0: a41e ab79 446d 76d6 702b cf45 9e7f ba77  ...yDmv.p+.E...w\n000000b0: 7dc2 779c 274e cc18 483c 3a12 0f75 f07c  }.w.'N..H<:..u.|\n000000c0: 5e63 67dd b886 ab48 e550 b5c4 f0e3 db0d  ^cg....H.P......\n000000d0: 54c1 85b8 8627 2ff3 2ff3 17f9 0626 d31d  T....'/./....&..\n000000e0: d9a6 e5b5 4076 663f 94ec 7b5a 17cf 7ade  ....@vf?..{Z..z.\n000000f0: 00d3 0d9f 4fcc d733 ef8d a0bb 0a06 c7eb  ....O..3........\n00000100: b304 6fb1 b1cc 18ed 90e0 8710 43aa 424f  ..o.........C.BO\n00000110: 50c7 d0c1 2bac 09be 4d1c 2566 335e 666c  P...+...M.%f3^fl\n00000120: 1e20 951d 58fd 6774 f3e9 f317 749f 7fc4  . ..X.gt....t...\n00000130: d651 cdca f5a7 b0a5 aea4 08ab 055c e4c5  .Q...........\\..\n```\n\n\n\nAlso, strangely, the output file seems to open properly in TextWrangler!",
    "answer": "This is a \ngzip\n compressed file. You can find this out by running the \nfile\n command, which figures out the file format from \nmagic numbers\n in the data (this is how programs such as Text Wrangler figure out that the file is compressed as well):\n\n\n\n```bash\nfile output.html\nwget -O - … | file -\n```\n\n\n\nThe server (I guessed it from the content you showed) is sending gzipped data and correctly setting the header\n\n\n\n```bash\nContent-Encoding: gzip\n```\n\n\n\nbut wget doesn't support that. In recent versions, wget sends \nAccept-encoding: identity\n, to tell the server not to compress or otherwise encode the data. In older versions, you can send the header manually:\n\n\n\n```bash\nwget --header 'Accept-encoding: identity' …\n```\n\n\n\nHowever this particular server appears to be broken: it sends compressed data even when told not to encode the data in any way. So you'll have to decompress the data manually.\n\n\n\n```bash\nwget -O output.html.gz … && gunzip output.html.gz\n```",
    "url": "https://unix.stackexchange.com/questions/321492/wget-returning-binary-instead-of-html"
  },
  {
    "question_title": "diff --git unknown option",
    "question_body": "when I try to run \n\n\n\n```bash\ndiff --git a/drivers/cpufreq/intel_pstate.c b/drivers/cpufreq/intel_pstate.c\n```\n\n\n\nI get unknown option (I have installed git via \napt-get install git\n)",
    "answer": "Either use \n\n\n\n```bash\ndiff -u file1 file2\n```\n\n\n\nor\n\n\n\n```bash\ngit diff branch/commit1 branch/commit2\n```\n\n\n\nMore on git diff at \nhttps://www.kernel.org/pub/software/scm/git/docs/git-diff.html\n\n\nI am not aware of any \n--git\n option however for \ndiff\n and the man page doesn't show it.",
    "url": "https://unix.stackexchange.com/questions/102287/diff-git-unknown-option"
  },
  {
    "question_title": "Can I zip an entire folder using gzip?",
    "question_body": "I'm trying to zip a folder in unix.\nCan that be done using the gzip command?",
    "answer": "No. \n\n\nUnlike \nzip\n, \ngzip\n functions as a compression algorithm \nonly\n.\n\n\nBecause of various reasons some of which hearken back to the era of tape drives, Unix uses a program named \ntar\n to archive data, which can then be compressed with a compression program like \ngzip\n, \nbzip2\n, \n7zip\n, etc.\n\n\nIn order to \"zip\" a directory, the correct command would be\n\n\n\n```bash\ntar -zcvf archive.tar.gz directory/\n```\n\n\n\nThis will tell \ntar\n to\n\n\n\n\ncompress it using the \nz\n (gzip) algorithm\n\n\nc\n (create) an archive from the files in \ndirectory\n (\ntar\n is recursive by default)\n\n\nv\n (verbosely) list (on /dev/stderr so it doesn't affect piped commands) all the files it adds to the archive.\n\n\nand store the output as a \nf\n (file) named \narchive.tar.gz\n\n\n\n\nThe \ntar\n command offers \ngzip\n support (via the \n-z\n flag) purely for your convenience. The \ngzip\n command/lib is completely separate. The command above is effectively the same as\n\n\n\n```bash\ntar -cv directory | gzip > archive.tar.gz\n```\n\n\n\nTo decompress and unpack the archive into the current directory you would use\n\n\n\n```bash\ntar -zxvf archive.tar.gz\n```\n\n\n\nThat command is effectively the same as\n\n\n\n```bash\ngunzip < archive.tar.gz | tar -xv\n```\n\n\n\ntar\n has many, many, MANY other options and uses as well; I heartily recommend reading through its manpage sometime.",
    "url": "https://unix.stackexchange.com/questions/93139/can-i-zip-an-entire-folder-using-gzip"
  },
  {
    "question_title": "How to &quot;grep&quot; for line length in a given range?",
    "question_body": "NOTE:\n This question is the complement of this Q&A: \nHow to \"grep\" for line length *not* in a given range?\n\n\n\n\nI need to get only the lines from a textfile (a wordlist, separated with newline) that has a length range of minimum or equal than 3 characters, but not longer or equal than 10. \n\n\nExample: \n\n\nINPUT: \n\n\n\n```bash\negyezményét\nmegkíván\nki\nalma\nkevesen\nmeghatározó\n```\n\n\n\nOUTPUT: \n\n\n\n```bash\nmegkíván\nalma\nkevesen\n```\n\n\n\nQuestion:\n How can I do this in \nbash\n?",
    "answer": "```bash\ngrep -x '.\\{3,10\\}'\n```\n\n\n\nwhere\n\n\n\n\n-x\n (also \n--line-regexp\n with GNU \ngrep\n) match pattern to whole line\n\n\n.\n any single character\n\n\n\\{3,10\\}\n quantify from 3 to 10 times previous symbol (in the case any ones)",
    "url": "https://unix.stackexchange.com/questions/184519/how-to-grep-for-line-length-in-a-given-range"
  },
  {
    "question_title": "How can I delete a word backward at the command line (bash and zsh)?",
    "question_body": "How can I delete a word backward at the command line?  I'm truly used to some editors deleting the last 'word' using \nCtrl\n+\nBackspace\n, and I'd like that functionality at the command line too.\n\n\nI am using Bash at the moment and although I could jump backward a word and then delete forward a word, I'd rather have this as a quick-key, or event as \nCtrl\n+\nBackspace\n.\n\n\nHow can accomplish this?",
    "answer": "Ctrl\n+\nW\n is the standard \"kill word\" (aka \nwerase\n).\n\nCtrl\n+\nU\n kills the whole line (\nkill\n).\n\n\nYou can change them with \nstty\n.\n\n\n\n```bash\n-bash-4.2$ stty -a\nspeed 38400 baud; 24 rows; 80 columns;\nlflags: icanon isig iexten echo echoe -echok echoke -echonl echoctl\n        -echoprt -altwerase -noflsh -tostop -flusho pendin -nokerninfo\n        -extproc -xcase\niflags: -istrip icrnl -inlcr -igncr -iuclc ixon -ixoff ixany imaxbel\n        -ignbrk brkint -inpck -ignpar -parmrk\noflags: opost onlcr -ocrnl -onocr -onlret -olcuc oxtabs -onoeot\ncflags: cread cs8 -parenb -parodd hupcl -clocal -cstopb -crtscts -mdmbuf\ncchars: discard = ^O; dsusp = ^Y; eof = ^D; eol = <undef>;\n        eol2 = <undef>; erase = ^?; intr = ^C; kill = ^U; lnext = ^V;\n        min = 1; quit = ^\\; reprint = ^R; start = ^Q; status = <undef>;\n        stop = ^S; susp = ^Z; time = 0; werase = ^W;\n-bash-4.2$ stty werase ^p\n-bash-4.2$ stty kill ^a\n-bash-4.2$\n```\n\n\n\nNote that one does not have to put the actual control character on the line, \nstty\n understands putting \n^\n and then the character you would hit with control.\n\n\nAfter doing this, if I hit \nCtrl\n+\nP\n it will erase a word from the line.  And if I hit \nCtrl\n+\nA\n, it will erase the whole line.",
    "url": "https://unix.stackexchange.com/questions/94331/how-can-i-delete-a-word-backward-at-the-command-line-bash-and-zsh"
  },
  {
    "question_title": "Uncompressed file estimation wrong?",
    "question_body": "I had a large (~60G) compressed file (\ntar.gz\n).\n\n\nI used \nsplit\n to break it into 4 parts and then \ncat\n to join them back together.\n\n\nHowever, now, when I am trying to estimate the size of the uncompressed file, it turns out it is smaller than the original? How is this possible?\n\n\n\n```bash\n$ gzip -l myfile.tar.gz \n         compressed        uncompressed  ratio uncompressed_name\n        60680003101          3985780736 -1422.4% myfile.tar\n```",
    "answer": "This is caused by the size of the field used to store the uncompressed size in gzipped files: it’s only 32 bits, so \ngzip\n can only store sizes of files up to 4 GiB. Anything larger is compressed and uncompressed correctly, but \ngzip -l\n gives an incorrect uncompressed size in versions 1.11 and older.\n\n\nSo splitting the tarball and reconstructing it hasn’t caused this, and shouldn’t have affected the file — if you want to make sure, you can check it with \ngzip -tv\n.\n\n\nSee \nFastest way of working out uncompressed size of large GZIPPED file\n for more details, and \nthe \ngzip\n manual\n:\n\n\n\n> The gzip format represents the input size modulo 2³², so the uncompressed size and compression ratio are listed incorrectly for uncompressed files 4 GiB and larger.",
    "url": "https://unix.stackexchange.com/questions/472013/uncompressed-file-estimation-wrong"
  },
  {
    "question_title": "&#39;ls -1&#39; : how to list filenames without extension",
    "question_body": "ls -1\n lists my elements like so:\n\n\n\n```bash\nfoo.png\nbar.png\nfoobar.png\n...\n```\n\n\n\nI want it listed without the \n.png\n like so:\n\n\n\n```bash\nfoo\nbar\nfoobar\n...\n```\n\n\n\n(the dir only contains \n.png\n files)\n\n\nCan somebody tell me how to use \ngrep\n in this case?\n\n\nPurpose:\nI have a text file where all the names are listed without the extension. I want to make a script that compares the text file with the folder to see which file is missing.",
    "answer": "```bash\nls -1 | sed -e 's/\\.png$//'\n```\n\n\n\nThe \nsed\n command removes (that is, it replaces with the empty string) any string \n.png\n found at the \nend\n of a filename.\n\n\nThe \n.\n is escaped as \n\\.\n so that it is interpreted by \nsed\n as a literal \n.\n character rather than the regexp \n.\n (which means match any character).  The \n$\n is the end-of-line anchor, so it doesn't match \n.png\n in the middle of a filename.",
    "url": "https://unix.stackexchange.com/questions/283886/ls-1-how-to-list-filenames-without-extension"
  },
  {
    "question_title": "How can I resolve a hostname to an IP address in a Bash script?",
    "question_body": "What's the most concise way to resolve a hostname to an IP address in a Bash script? I'm using \nArch Linux\n.",
    "answer": "You can use \ngetent\n, which comes with \nglibc\n (so you almost certainly have it on Linux). This resolves using gethostbyaddr/gethostbyname2, and so also will check \n/etc/hosts\n/NIS/etc:\n\n\n\n```bash\ngetent hosts unix.stackexchange.com | awk '{ print $1 }'\n```\n\n\n\nOr, as Heinzi said below, you can use \ndig\n with the \n+short\n argument (queries DNS servers directly, does not look at \n/etc/hosts\n/NSS/etc) :\n\n\n\n```bash\ndig +short unix.stackexchange.com\n```\n\n\n\nIf \ndig +short\n is unavailable, any one of the following should work. All of these query DNS directly and ignore other means of resolution:\n\n\n\n```bash\nhost unix.stackexchange.com | awk '/has address/ { print $4 }'\nnslookup unix.stackexchange.com | awk '/^Address: / { print $2 }'\ndig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 }'\n```\n\n\n\nIf you want to only print one IP, then add the \nexit\n command to \nawk\n's workflow.\n\n\n\n```bash\ndig +short unix.stackexchange.com | awk '{ print ; exit }'\ngetent hosts unix.stackexchange.com | awk '{ print $1 ; exit }'\nhost unix.stackexchange.com | awk '/has address/ { print $4 ; exit }'\nnslookup unix.stackexchange.com | awk '/^Address: / { print $2 ; exit }'\ndig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 ; exit }'\n```",
    "url": "https://unix.stackexchange.com/questions/20784/how-can-i-resolve-a-hostname-to-an-ip-address-in-a-bash-script"
  },
  {
    "question_title": "Limit grep context to N characters on line",
    "question_body": "I have to grep through some JSON files in which the line lengths exceed a few thousand characters. \nHow can I limit grep to display context up to N characters to the left and right of the match?\n Any tool other than grep would be fine as well, so long as it available in common Linux packages.\n\n\nThis would be example output, for the \nimaginary grep switch Ф\n:\n\n\n\n```bash\n$ grep -r foo *\nhello.txt: Once upon a time a big foo came out of the woods.\n\n$ grep -Ф 10 -r foo *\nhello.txt: ime a big foo came of t\n```",
    "answer": "Try to use this one:\n\n\n\n```bash\ngrep -r -E -o \".{0,10}wantedText.{0,10}\" *\n```\n\n\n\n-E\n tells, that you want to use extended regex\n\n\n-o\n tells, that you want to print only the match\n\n\n-r\n grep is looking for result recursively in the folder\n\n\nREGEX:\n\n\n{0,10}\n tells, how many arbitrary characters you want to print\n\n\n.\n represents an arbitrary character (a character itself wasn't important here, just their number)\n\n\nEdit:\n Oh, I see, that Joseph recommends almost the same solution as I do :D",
    "url": "https://unix.stackexchange.com/questions/163726/limit-grep-context-to-n-characters-on-line"
  },
  {
    "question_title": "remove particular characters from a variable using bash",
    "question_body": "I want to parse a \nvariable\n (in my case it's development kit version) to make it dot(\n.\n) free. If \nversion='2.3.3'\n, desired output is \n233\n.\n\n\nI tried as below, but it requires \n.\n to be replaced with another character giving me \n2_3_3\n. It would have been fine if \ntr . ''\n would have worked.\n\n\n\n```bash\n1 VERSION='2.3.3' \n  2 echo \"2.3.3\" | tr . _\n```",
    "answer": "There is no need to execute an external program. \nbash\n's \nstring manipulation\n can handle it (also available in \nksh93\n (where it comes from), \nzsh\n and recent versions of \nmksh\n, \nyash\n and busybox \nsh\n (at least)):\n\n\n\n```bash\n$ VERSION='2.3.3'\n$ echo \"${VERSION//.}\"\n233\n```\n\n\n\n(In those shells' manuals you can generally find this in the \nparameter expansion\n section.)",
    "url": "https://unix.stackexchange.com/questions/104881/remove-particular-characters-from-a-variable-using-bash"
  },
  {
    "question_title": "watch command not showing colors for &#39;git status&#39;",
    "question_body": "I'm trying to get \nwatch\n to display colors from 'git status'.\n\n\nI've tried running watch with the \n--color\n option, as suggested elsewhere here, but, still, \nwatch --color 'git status'\n doesn't display colors.",
    "answer": "When \ngit status\n is run under \nwatch\n, it is able to detect that its standard output is not a terminal, meaning it will not output colors if the \ncolor.status\n setting is set to \nauto\n.  To force \ngit status\n to always output colors (even under \nwatch\n), set \ncolor.stats\n to \nalways\n, e.g. \n\n\n\n```bash\ngit config color.status always\n```\n\n\n\nto set the setting permanently, or as \n@ChrisJonsen points out\n, use \ngit -c color.status=always status\n to run \ngit status\n with a one-time override.",
    "url": "https://unix.stackexchange.com/questions/46814/watch-command-not-showing-colors-for-git-status"
  },
  {
    "question_title": "How do I securely extract an untrusted tar file?",
    "question_body": "I would like to be able to extract a tar file, such that all extracted files are placed under a certain prefix directory.  Any attempt by the tar files to write to outside directories should cause the extraction to fail.\n\n\nAs you might imagine, this is so that I can securely extract an untrusted tar file.\n\n\nHow can I do this with GNU \ntar\n?\n\n\nI came up with:\n\n\n\n\n```bash\ntar --exclude='/*' --exclude='*/../*' --exclude='../*' -xvf untrusted_file.tar\n```\n\n\n\nbut I am not sure that this is paranoid enough.",
    "answer": "You don't need the paranoia at all. GNU \ntar\n — and in fact \nany\n well-written \ntar\n program produced in the past 30 years or so — will refuse to extract files in the tarball that begin with a slash or that contain \n..\n elements, by default.\n\n\nYou have to go out of your way to force modern \ntar\n programs to extract such potentially-malicious tarballs: both GNU and BSD \ntar\n need the \n-P\n option to make them disable this protection. See the section \nAbsolute File Names\n in the GNU tar manual.\n\n\nThe \n-P\n flag isn't specified by POSIX,¹ though, so other \ntar\n programs may have different ways of coping with this. For example, the Schily Tools' \nstar\n program\n uses \n-/\n and \n-..\n to disable these protections.\n\n\nThe only thing you might consider adding to a naïve \ntar\n command is a \n-C\n flag to force it to extract things in a safe temporary directory, so you don't have to \ncd\n there first.\n\n\n\n\nAsides\n:\n\n\n\n\nTechnically, \ntar\n isn't specified by POSIX any more at all. They tried to tell the Unix computing world that we should be using \npax\n now instead of \ntar\n and \ncpio\n, but the computing world largely ignored them. \n\n\nIt's relevant here to note that the POSIX specification for \npax\n doesn't say how it should handle leading slashes or embedded \n..\n elements. There's a nonstandard \n--insecure\n flag for \nBSD \npax\n to suppress protections against embedded \n..\n path elements, but there is apparently no default protection against leading slashes; the BSD \npax\n man page indirectly recommends writing \n-s\n substitution rules to deal with the absolute path risk.\n\n\nThat's the sort of thing that happens when a de facto standard remains in active use while the de jure standard is largely ignored.",
    "url": "https://unix.stackexchange.com/questions/276959/how-do-i-securely-extract-an-untrusted-tar-file"
  },
  {
    "question_title": "File permission with six octal digits in git. What does it mean?",
    "question_body": "I performed a \ngit commit\n command and it gave me the following reply:\n\n\n\n```bash\n7 files changed, 93 insertions(+), 15 deletions(-)\nmode change 100644 => 100755 assets/internal/fonts/icomoon.svg\nmode change 100644 => 100755 assets/internal/fonts/icomoon.ttf\nmode change 100644 => 100755 assets/internal/fonts/icomoon.woff\n```\n\n\n\nI know files can have user / group / other rwx permissions and those can be expressed as three octal digits, like \"644\" or \"755\". But why is git showing six digits here?\n\n\nI've read the following articles but didn't find an answer:\n\n\n\n\nWikipedia's article on \"File system permissions\"\n\n\nHow do I remove files saying “old mode 100755 new mode 100644” from unstaged changes in Git?\n\n\nUnix permissions made easy\n\n\nChmod permissions (flags) explained: 600, 0600, 700, 777, 100 etc..",
    "answer": "The values shown are the 16-bit file modes \nas stored by Git\n, following the layout of \nPOSIX types and modes\n:\n\n\n\n```bash\n32-bit mode, split into (high to low bits)\n\n    4-bit object type\n      valid values in binary are 1000 (regular file), 1010 (symbolic link)\n      and 1110 (gitlink)\n\n    3-bit unused\n\n    9-bit unix permission. Only 0755 and 0644 are valid for regular files.\n    Symbolic links and gitlinks have value 0 in this field.\n```\n\n\n\nThat file doesn’t mention directories; they are represented using object type 0100. Gitlinks are used for \nsubmodules\n.\n\n\nEach digit in the six-digit value is in octal, representing three bits; 16 bits thus need six digits, the first of which only represents one bit:\n\n\n\n```bash\nType|---|Perm bits\n\n1000 000 111101101\n1 0   0   7  5  5\n\n1000 000 110100100\n1 0   0   6  4  4\n```\n\n\n\nGit doesn’t store arbitrary modes, only a subset of the values are allowed, from the usual POSIX types and modes (in octal, 12 for a symbolic link, 10 for a regular file, 04 for a directory) to which git adds 16 for Git links. The mode is appended, using four octal digits. For files, you’ll only ever see 100755 or 100644 (although 100664 is also technically possible); directories are 040000 (permissions are ignored), symbolic links 120000. The set-user-ID, set-group-ID and sticky bits aren’t supported at all (they would be stored in the unused bits).\n\n\nSee also \nthis related answer\n.",
    "url": "https://unix.stackexchange.com/questions/450480/file-permission-with-six-octal-digits-in-git-what-does-it-mean"
  },
  {
    "question_title": "How to add a newline to the end of a file?",
    "question_body": "Using version control systems I get annoyed at the noise when the diff says \nNo newline at end of file\n.\n\n\nSo I was wondering: How to add a newline at the end of a file to get rid of those messages?",
    "answer": "Here you go\n:\n\n\n\n```bash\nsed -i -e '$a\\' file\n```\n\n\n\nAnd alternatively for OS X \nsed\n:\n\n\n\n```bash\nsed -i '' -e '$a\\' file\n```\n\n\n\nThis adds \n\\n\n at the end of the file \nonly\n if it doesn’t already end with a newline. So if you run it twice, it will not add another newline:\n\n\n\n```bash\n$ cd \"$(mktemp -d)\"\n$ printf foo > test.txt\n$ sed -e '$a\\' test.txt > test-with-eol.txt\n$ diff test*\n1c1\n< foo\n\\ No newline at end of file\n---\n> foo\n$ echo $?\n1\n$ sed -e '$a\\' test-with-eol.txt > test-still-with-one-eol.txt\n$ diff test-with-eol.txt test-still-with-one-eol.txt\n$ echo $?\n0\n```\n\n\n\nHow it works:\n\n\n\n\n$\n denotes the end of file\n\n\na\\\n appends the following text (which is nothing, in this case) on a new line\n\n\n\n\nIn other words, if the last line contains a character that is not newline, append a newline.",
    "url": "https://unix.stackexchange.com/questions/31947/how-to-add-a-newline-to-the-end-of-a-file"
  },
  {
    "question_title": "How to download an archive and extract it without saving the archive to disk?",
    "question_body": "I'd like to download, and extract an archive under a given directory. Here is how I've been doing it so far:\n\n\n\n```bash\nwget http://downloads.mysql.com/source/dbt2-0.37.50.3.tar.gz\ntar zxf dbt2-0.37.50.3.tar.gz\nmv dbt2-0.37.50.3 dbt2\n```\n\n\n\nI'd like instead to download and extract the archive \non the fly\n, without having the \ntar.gz\n written to the disk. I think this is possible by piping the output of \nwget\n to \ntar\n, and giving \ntar\n a target, but in practice I don't know how to put the pieces together.",
    "answer": "You can do it by telling \nwget\n to output its payload to stdout (with the flag \n-O-\n) and suppress its own output (with the flag \n-q\n). The output is then made the input (via stdin) to the \ntar\n command by a \npipe\n (\n|\n):\n\n\n\n```bash\nwget -qO- your_link_here | gunzip | tar xvf -\n```\n\n\n\nf -\n tells \ntar\n the archive is to be read from stdin. With some \ntar\n implementations, that's the default, in others, that's often a tape device.\n\n\nSome \ntar\n implementations can detect compressions and decompress by themselves in which case you can remove the \n| gunzip\n, some support a \nz\n option to decompress gzip-compressed archives on the fly by themselves (often by invoking \ngunzip\n themselves).\n\n\nTo specify a target directory, if your \ntar\n supports \n-C\n:\n\n\n\n```bash\nwget -qO- your_link_here | gunzip | tar xvf - -C /target/directory\n```\n\n\n\nIf not:\n\n\n\n```bash\n(cd /target/directory && wget -qO- your_link_here | gunzip | tar xvf -)\n```\n\n\n\nIf you happen to have GNU \ntar\n, you can also rename the output dir:\n\n\n\n```bash\nwget -qO- your_link_here | tar --transform 's/^dbt2-0.37.50.3/dbt2/' -xvzf -\n```\n\n\n\nIn libarchive's \ntar\n (\nbsdtar\n), or \nstar\n, the equivalent is with the \n-s/pattern/replacement/\n option like in the standard \npax\n command.",
    "url": "https://unix.stackexchange.com/questions/85194/how-to-download-an-archive-and-extract-it-without-saving-the-archive-to-disk"
  },
  {
    "question_title": "git diff displays colors incorrectly",
    "question_body": "In order to get coloured output from all git commands, I set the following:\n\n\n\n```bash\ngit config --global color.ui true\n```\n\n\n\nHowever, this produces an output like this for \ngit diff\n, \ngit log\n\n\n\n\nwhereas commands like \ngit status\n display fine\n\n\n\n\nWhy is it not recognizing the escaped color codes in only some of the commands and how can I fix it?\n\n\nI'm using iTerm 2 (terminal type \nxterm-256color\n) on OS X 10.8.2 and zsh as my shell\n\n\n\n```bash\nzsh --version\nzsh 5.0.0 (x86_64-apple-darwin12.0.0)\n\ngit --version                                                                                                                      \ngit version 1.7.9.6 (Apple Git-31.1)\n```",
    "answer": "You're seeing the escape sequences that tell the terminal to change colors displayed with the escape character shown as \nESC\n, whereas the desired behavior would be that the escape sequences have their intended effect.\n\n\nCommands such as \ngit diff\n and \ngit log\n pipe their output into a \npager\n, \nless\n by default. Git tries to tell \nless\n to allow control characters to have their control effect, but this isn't working for you.\n\n\nIf \nless\n is your pager but you have the environment variable \nLESS\n set to a value that doesn't include \n-r\n or \n-R\n, git is unable to tell \nless\n to display colors. It normally passes \nLESS=-FRSX\n, but not if \nLESS\n is already set in the environment. A fix is to explicitly pass the \n-R\n option to tell \nless\n to display colors when invoked by git:\n\n\n\n```bash\ngit config --global core.pager 'less -R'\n```\n\n\n\nIf \nless\n isn't your pager, either switch to \nless\n or figure out how to make your pager display colors.\n\n\nIf you don't want git to display colors when it's invoking a pager, set \ncolor.ui\n to \nauto\n instead of \ntrue\n.",
    "url": "https://unix.stackexchange.com/questions/64927/git-diff-displays-colors-incorrectly"
  },
  {
    "question_title": "What color codes can I use in my Bash PS1 prompt?",
    "question_body": "I used several colors in my bash PS1 prompt such as:\n\n\n\n```bash\n\\033]01;31\\] # pink\n\\033]00m\\]   # white\n\\033]01;36\\] # bold green\n\\033]02;36\\] # green\n\\033]01;34\\] # blue\n\\033]01;33\\] # bold yellow\n```\n\n\n\nWhere can I find a list of the color codes I can use?\n\n\nI looked at \nColorize Bash Console Color\n but it didn't answer my question about a list of the actual codes.\n\n\nIt would be nice if there was a more readable form also.\n\n\nSee also: \nHow can I get my PS1 prompt to show time, user, host, directories, and Git branch",
    "answer": "Those are \nANSI escape sequences\n; that link is to a chart of color codes but there are other interesting things on that Wikipedia page as well.  Not all of them work on (e.g.) a normal Linux console.\n\n\nThis is incorrect:\n\n\n\n> \\033]00m\\]   # white\n\n\n\n0\n resets the terminal to its default (which is probably white).  The actual code for white foreground is 37.  Also, the escaped closing brace at the end (\n\\]\n) is not part of the color sequence (see the last few paragraphs below for an explanation of their purpose in setting a prompt).\n\n\nNote that some GUI terminals allow you to specify a customized color scheme. This will affect the output.\n\n\nThere's \na list here\n which adds 7 foreground and 7 background colors I had not seen before, but they seem to work:\n\n\n\n```bash\n# Foreground colors\n90   Dark gray  \n91   Light red  \n92   Light green    \n93   Light yellow   \n94   Light blue \n95   Light magenta  \n96   Light cyan  \n\n# Background colors\n100  Dark gray  \n101  Light red  \n102  Light green    \n103  Light yellow   \n104  Light blue \n105  Light magenta  \n106  Light cyan\n```\n\n\n\nIn addition, if you have a 256 color GUI terminal (I think most of them are now), you can apply colors from this chart: \n\n\n\n\nThe ANSI sequence to select these, using the number in the bottom left corner, starts \n38;5;\n for the foreground and \n48;5;\n for the background, then the color number, so e.g.:\n\n\n\n```bash\necho -e \"\\\\033[48;5;95;38;5;214mhello world\\\\033[0m\"\n```\n\n\n\nGives me a light orange on tan (meaning, the color chart is roughly approximated).\n\n\nYou can see the colors in this chart\n1\n as they would appear on your terminal fairly easily:\n\n\n\n```bash\n#!/bin/bash\n\ncolor=16;\n\nwhile [ $color -lt 245 ]; do\n    echo -e \"$color: \\\\033[38;5;${color}mhello\\\\033[48;5;${color}mworld\\\\033[0m\"\n    ((color++));\ndone\n```\n\n\n\nThe output is self-explanatory.  \n\n\nSome systems set the $TERM variable to \nxterm-256color\n if you are on a 256 color terminal via some shell code in \n/etc/profile\n.  On others, you should be able to configure your terminal to use this.  That will let TUI applications know there are 256 colors, and allow you to add something like this to your \n~/.bashrc\n:\n\n\n\n```bash\nif [[ \"$TERM\" =~ 256color ]]; then\n     PS1=\"MyCrazyPrompt...\"\nfi\n```\n\n\n\nBeware that when you use color escape sequences in your prompt, you should enclose them in escaped (\n\\\n prefixed) square brackets, like this:\n\n\n\n```bash\nPS1=\"\\[\\033[01;32m\\]MyPrompt: \\[\\033[0m\\]\"\n```\n\n\n\nNotice the \n[\n's interior to the color sequence are not escaped, but the enclosing ones are.  The purpose of the latter is to indicate to the shell that the enclosed sequence does not count toward the character length of the prompt.  If that count is wrong, weird things will happen when you scroll back through the history, e.g., if it is too long, the excess length of the last scrolled string will appear attached to your prompt and you won't be able to backspace into it (it's ignored the same way the prompt is).\n\n\nAlso note that if you want to include the output of a command run every time the prompt is used (as opposed to just once when the prompt is set), you should set it as a literal string with single quotes, e.g.:\n\n\n\n```bash\nPS1='\\[\\033[01;32m\\]$(date): \\[\\033[0m\\]'\n```\n\n\n\nAlthough this is not a great example if you are happy with using bash's special \n\\d\n or \n\\D{format}\n prompt escapes -- which are not the topic of the question but can be found in \nman bash\n under \nPROMPTING\n.  There are various other useful escapes such as \n\\w\n for current directory, \n\\u\n for current user, etc.\n\n\n\n\n1. The main portion of this chart, colors 16 - 231 (notice they are not in numerical order) are a 6 x 6 x 6 RGB color cube. \"Color cube\" refers to the fact that an RGB color space can be represented using a three dimensional array (with one axis for red, one for green, and one for blue).  Each color in the cube here can be represented as coordinates in a 6 x 6 x 6 array, and the index in the chart calculated thusly:\n\n\n\n```bash\n16 + R * 36 + G * 6 + B\n```\n\n\n\nThe first color in the cube, at index 16 in the chart, is black (RGB 0, 0, 0).  You could use this formula in shell script:\n\n\n\n```bash\n#!/bin/sh                                                         \n\nfunction RGBcolor {                                               \n    echo \"16 + $1 * 36 + $2 * 6 + $3\" | bc                        \n}                                                                 \n\nfg=$(RGBcolor 1 0 2)  # Violet                                            \nbg=$(RGBcolor 5 3 0)  # Bright orange.                                            \n\necho -e \"\\\\033[1;38;5;$fg;48;5;${bg}mviolet on tangerine\\\\033[0m\"\n```",
    "url": "https://unix.stackexchange.com/questions/124407/what-color-codes-can-i-use-in-my-bash-ps1-prompt"
  },
  {
    "question_title": "tar extraction depends on filename?",
    "question_body": "I often download tarballs with wget from sourceforge.net.\n\n\nThe downloaded files then are named, e.g \nSQliteManager-1.2.4.tar.gz?r=http:%2F%2Fsourceforge.net%2Fprojects%2Fsqlitemanager%2Ffiles%2F&ts=1305711521&use_mirror=switch\n\n\nWhen I try to\n\n\n\n```bash\ntar xzf SQliteManager-1.2.4.tar.gz\\?r\\=http\\:%2F%2Fsourceforge.net%2Fprojects%2Fsqlitemanager%2Ffiles%2F\\&ts\\=1305711521\\&use_mirror\\=switch\n```\n\n\n\nI receive the following error message:\n\n\n\n```bash\ntar (child): Cannot connect to SQliteManager-1.2.4.tar.gz?r=http: resolve failed\n\ngzip: stdin: unexpected end of file\ntar: Child returned status 128\ntar: Error is not recoverable: exiting now\n```\n\n\n\nAfter renaming the file to \nfoo.tar.gz\n the extraction works perfect.\n\n\nIs there a way, that i am not forced to rename each time the target file before extracting?",
    "answer": "The reason for the error you are seeing can be found in the \nGNU tar documentation\n:\n\n\n\n> If the archive file name includes a\n> colon (‘:’), then it is assumed to be\n> a file on another machine[...]\n\n\n\nThat is, it is interpretting \nSQliteManager-1.2.4.tar.gz?r=http\n as a host name and trying to resolve it to an IP address, hence the \"resolve failed\" error.\n\n\nThat same documentation goes on to say:\n\n\n\n> If you need to use a file whose name\n> includes a colon, then the remote tape\n> drive behavior can be inhibited by\n> using the ‘--force-local’ option.",
    "url": "https://unix.stackexchange.com/questions/13377/tar-extraction-depends-on-filename"
  },
  {
    "question_title": "How to uncompress zlib data in UNIX?",
    "question_body": "I have created zlib-compressed data in Python, like this:\n\n\n\n```bash\nimport zlib\ns = '...'\nz = zlib.compress(s)\nwith open('/tmp/data', 'w') as f:\n    f.write(z)\n```\n\n\n\n(or one-liner in shell: \necho -n '...' | python2 -c 'import sys,zlib; sys.stdout.write(zlib.compress(sys.stdin.read()))' > /tmp/data\n)\n\n\nNow, I want to uncompress the data in shell. Neither \nzcat\n nor \nuncompress\n work:\n\n\n\n```bash\n$ cat /tmp/data | gzip -d -\ngzip: stdin: not in gzip format\n\n$ zcat /tmp/data \ngzip: /tmp/data.gz: not in gzip format\n\n$ cat /tmp/data | uncompress -\ngzip: stdin: not in gzip format\n```\n\n\n\nIt seems that I have created gzip-like file, but without any headers. Unfortunately I don't see any option to uncompress such raw data in gzip man page, and the zlib package does not contain any executable utility.\n\n\nIs there a utility to uncompress raw zlib data?",
    "answer": "It is also possible to decompress it using standard \nshell-script\n + \ngzip\n, if you don't have, or want to use \nopenssl\n or other tools.\nThe trick is to prepend the \ngzip magic number and compress method\n to the actual data from \nzlib.compress\n:\n\n\n\n```bash\nprintf \"\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00\" |cat - /tmp/data |gzip -dc >/tmp/out\n```\n\n\n\nEdits:\n\n@d0sboots commented: For RAW Deflate data, you need to add 2 more null bytes: \n → \n\"\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n\n\nThis Q on SO\n gives more information about this approach. An answer there suggests that there is also an 8 byte footer.\n\n\nUsers @Vitali-Kushner and @mark-bessey reported success even with truncated files, so a gzip footer does not seem strictly required.\n\n\n@tobias-kienzler suggested this function for the \nbashrc\n:\n\n\nzlibd() (printf \"\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00\" | cat - \"$@\" | gzip -dc)",
    "url": "https://unix.stackexchange.com/questions/22834/how-to-uncompress-zlib-data-in-unix"
  },
  {
    "question_title": "tar.gpg vs tar.gz.gpg",
    "question_body": "Is it redundant to compress a tar archive before encrypting it using GPG? From my understanding, GPG will compress the file that it encrypts. Is it preferable to do \ntar -> gpg\n instead of \ntar -> gzip -> gpg\n?",
    "answer": "GnuPG uses ZIP compression by default, which is slightly less performant in many cases than \ngzip\n. This would suggest that a separate \ngzip\n phase could be useful, but GnuPG also supports zlib compression, using the same compression algorithms as \ngzip\n; to select this, specify \n--compress-algo zlib\n.\n\n\nYou can also specify \nbzip2\n instead to get better compression, with \n--compress-algo bzip2\n.\n\n\nIn both cases, the compression “level” can be specified with \n-z\n, \ne.g.\n \n-z 9\n.\n\n\nPGP only supports ZIP compression, but that’s probably not much of a concern nowadays.",
    "url": "https://unix.stackexchange.com/questions/697224/tar-gpg-vs-tar-gz-gpg"
  },
  {
    "question_title": "How to conditionally do something if a command succeeded or failed",
    "question_body": "How can I do something like this in bash?\n\n\n\n```bash\nif \"`command` returns any error\";\nthen\n    echo \"Returned an error\"\nelse\n    echo \"Proceed...\"\nfi\n```",
    "answer": "> How to conditionally do something if a command succeeded or failed\n\n\n\nThat's exactly what bash's \nif\n statement does:\n\n\n\n```bash\nif command ; then\n    echo \"Command succeeded\"\nelse\n    echo \"Command failed\"\nfi\n```\n\n\n\nAdding information from comments: you \ndon't\n need to use the \n[\n ... \n]\n syntax in this case. \n[\n is itself a command, very nearly equivalent to \ntest\n. It's probably the most common command to use in an \nif\n, which can lead to the assumption that it's part of the shell's syntax. But if you want to test whether a command succeeded or not, use the command itself directly with \nif\n, as shown above.",
    "url": "https://unix.stackexchange.com/questions/22726/how-to-conditionally-do-something-if-a-command-succeeded-or-failed"
  },
  {
    "question_title": "GPG Hangs When Private Keys are Accessed",
    "question_body": "I like to sign my git commits with my PGP key, so I was quite alarmed when I went to \ngit commit -S\n but instead of prompting for my PGP key passphrase, git just started hanging. I haven't made a change to my GPG setup in several months and have made many commits since then with no problem. Additionally, when I attempt to view my private keys with \ngpg -K\n, gpg hangs. However, when I run \ngpg -k\n to view my public keys, it returns the list like normal. Hopefully someone will have some idea of what is causing this problem and how to fix it.",
    "answer": "I came across this exact issue (OSX Sierra 10.12.6, gpg/GnuPG 2.2.5)\n\n\nCommands that would hang:\n\n\n\n```bash\ngpg -K # --list-secret-keys\ngpg -d # --decrypt\ngpg --edit-key\ngpgconf --kill gpg-agent\n```\n\n\n\nMy solution was the same as mentioned by \nJohn\n above (ie. kill gpg-agent) as most other methods on \nhow-can-i-restart-gpg-agent\n would also hang.\n\n\n\n```bash\n# Solution    \npkill -9 gpg-agent\n```\n\n\n\nThen for signing git commits I set the tty env as mentioned by \ncas\n above and also at \ngpg-failed-to-sign-commit-object\n.\n\n\n\n```bash\nexport GPG_TTY=$(tty)\n```",
    "url": "https://unix.stackexchange.com/questions/382279/gpg-hangs-when-private-keys-are-accessed"
  },
  {
    "question_title": "How do I pass a list of files to grep",
    "question_body": "I am using \nfind\n and getting a list of files I want to \ngrep\n through. How do I pipe that list to \ngrep\n?",
    "answer": "Well, the generic case that works with any command that writes to stdout is to use \nxargs\n, which will let you attach any number of command-line arguments to the end of a command:\n\n\n\n```bash\n$ find … | xargs grep 'search'\n```\n\n\n\nOr to embed the command in your \ngrep\n line with backticks or \n$()\n, which will run the command and substitute its output:\n\n\n\n```bash\n$ grep 'search' $(find …)\n```\n\n\n\nNote that these commands don't work if the file names contain whitespace, or certain other “weird characters” (\n\\'\"\n for xargs, \n\\[*?\n for \n$(find …)\n).\n\n\n\n\nHowever, in the specific case of \nfind\n the ability to execute a program on the given arguments is built-in:\n\n\n\n```bash\n$ find … -exec grep 'search' {} \\;\n```\n\n\n\nEverything between \n-exec\n and \n;\n is the command to execute; \n{}\n is replaced with the filename found by \nfind\n. That will execute a separate \ngrep\n for each file; since \ngrep\n can take many filenames and search them all, you can change the \n;\n to \n+\n to tell find to pass all the matching filenames to \ngrep\n at once:\n\n\n\n```bash\n$ find … -exec grep 'search' {} \\+\n```",
    "url": "https://unix.stackexchange.com/questions/20262/how-do-i-pass-a-list-of-files-to-grep"
  },
  {
    "question_title": "How to grep lines between start and end pattern?",
    "question_body": "I have a file which is having following content:\n\n\n\n```bash\nzdk\naaa\nb12\ncdn\ndke\nkdn\n```\n\n\n\nInput1: \naaa\n and \ncdn\n\n\nOutput 1:\n\n\n\n```bash\naaa\nb12\ncdn\n```\n\n\n\nInput 2: \nzdk\n and \ndke\n\n\nOutput 2:\n\n\n\n```bash\nzdk\naaa\nb12\ncdn\ndke\n```\n\n\n\nI could use below commands to achieve:\n\n\n\n```bash\ngrep -a aaa -A2 file # Output 1\ngrep -a aaa -A4 file # Output 2\n```\n\n\n\nBut in the file I don't know what is the exact occurrence (position) of end string pattern (file is having 20000 rows)\n\n\nHow can I achieve this?",
    "answer": "grep\n won't help you here. This is a job better accomplished with \nsed\n using range expressions:\n\n\n\n```bash\n$ sed -n '/aaa/,/cdn/p' file\naaa\nb12\ncdn\n$ sed -n '/zdk/,/dke/p' file\nzdk\naaa\nb12\ncdn\ndke\n```\n\n\n\nsed -n\n suppresses the automatic printing, so that lines are printed just if explicitly asked to. And this happens when the range \n/aaa/,/cdn/\n happens.\n\n\nThese range expressions are also available in \nawk\n, where you can say:\n\n\n\n```bash\nawk '/zdk/,/dke/' file\n```\n\n\n\nOf course, all these conditions can be expanded to a more strict regex like \nsed -n '/^aaa$/,/^cdn$/p' file\n to check that the lines consist on exactly \naaa\n and \ncdn\n, nothing else.",
    "url": "https://unix.stackexchange.com/questions/236751/how-to-grep-lines-between-start-and-end-pattern"
  },
  {
    "question_title": "What is the difference between `grep`, `egrep`, and `fgrep`?",
    "question_body": "Can anyone tell me the technical differences between \ngrep\n, \negrep\n, and \nfgrep\n and provide suitable examples?\n\n\nWhen do I need to use \ngrep\n over \negrep\n and vice versa?",
    "answer": "egrep\n is 100% equivalent to \ngrep -E\n\n\nfgrep\n is 100% equivalent to \ngrep -F\n\n\n\n\nHistorically these switches were provided in separate binaries. On some really old Unix systems you will find that you need to call the separate binaries, but on all modern systems the switches are preferred. The man page for grep has details about this. \n\n\nAs for what they do, \n-E\n switches grep into a special mode so that the expression is evaluated as an ERE (Extended Regular Expression) as opposed to its normal pattern matching. Details of this syntax are on the man page.\n\n\n\n> -E, --extended-regexp\n> Interpret PATTERN as an extended regular expression\n\n\n\nThe \n-F\n switch switches grep into a different mode where it accepts a pattern to match, but then splits that pattern up into one search string per line and does an OR search on any of the strings without doing any special pattern matching.\n\n\n\n> -F, --fixed-strings\n> Interpret  PATTERN  as  a  list of fixed strings, separated by newlines, any of which is to be matched.\n\n\n\nHere are some example scenarios:\n\n\n\n\nYou have a file with a list of say ten Unix usernames in plain text. You want to search the group file on your machine to see if any of the ten users listed are in any special groups:\n\n\n\n```bash\ngrep -F -f user_list.txt /etc/group\n```\n\n\n\nThe reason the \n-F\n switch helps here is that the usernames in your pattern file are interpreted as plain text strings. Dots for example would be interpreted as dots rather than wild-cards.\n\n\nYou want to search using a fancy expression. For example parenthesis \n()\n can be used to indicate groups with \n|\n used as an OR operator. You could run this search using \n-E\n:\n\n\n\n```bash\ngrep -E '^no(fork|group)' /etc/group\n```\n\n\n\n...to return lines that start with either \"nofork\" or \"nogroup\". Without the \n-E\n switch you would have to escape the special characters involved because with normal pattern matching they would just search for that exact pattern;\n\n\n\n```bash\ngrep '^no\\(fork\\|group\\)' /etc/group\n```",
    "url": "https://unix.stackexchange.com/questions/17949/what-is-the-difference-between-grep-egrep-and-fgrep"
  },
  {
    "question_title": "Efficiently remove file(s) from large .tgz",
    "question_body": "Assume i have an gzip compressed tar-ball compressedArchive.tgz (+100 files, totaling +5gb). \n\n\nWhat would be the fastest way to remove all entries matching a given filename pattern for example prefix*.jpg and then store the remains in a gzip:ed tar-ball again?\n\n\nReplacing the old archive or creating a new one is not important, whichever is fastest.",
    "answer": "With GNU \ntar\n, you can do:\n\n\n\n```bash\npigz -d < file.tgz |\n  tar --delete --wildcards -f - '*/prefix*.jpg' |\n  pigz > newfile.tgz\n```\n\n\n\nWith \nbsdtar\n:\n\n\n\n```bash\npigz -d < file.tgz |\n  bsdtar -cf - --exclude='*/prefix*.jpg' @- |\n  pigz > newfile.tgz\n```\n\n\n\n(\npigz\n being the multi-threaded version of \ngzip\n).\n\n\nYou could overwrite the file over itself like:\n\n\n\n```bash\n{ pigz -d < file.tgz |\n    tar --delete --wildcards -f - '*/prefix*.jpg' |\n    pigz &&\n    perl -e 'truncate STDOUT, tell STDOUT'\n} 1<> file.tgz\n```\n\n\n\nBut that's quite risky, especially if the result ends up being less compressed than the original file (in which case, the second \npigz\n may end up overwriting areas of the file which the first one has not read yet).",
    "url": "https://unix.stackexchange.com/questions/80239/efficiently-remove-files-from-large-tgz"
  },
  {
    "question_title": "why should I use tar.xz instead of tar.gz? xz is a lossless data compression program and file format",
    "question_body": "Today first time in my life I saw \ntar.xz\n download. I searched the internet and found Wikipedia articles (\nxz\n and \nXZ Utils\n)\n\n\nInteresting quote about the users of \nxz\n\n\n\n> xz has gained notability for compressing packages in the GNU coreutils\n> project,[7] Debian family of systems deb (file format), openSUSE,[8]\n> Fedora,[9] Arch Linux,[10] Slackware,[11] FreeBSD,[12] Gentoo,[13]\n> GNOME,[14] and TeX Live,[15] as well as being an option to compress a\n> compiled Linux kernel.[16] In March 2013, kernel.org announced the use\n> of xz as the default compressed file format for distributing kernel\n> archive files.[17]\n\n\n\nI always use \ntar.gz\n. When and why should I use \ntar.xz\n? What's the use case?\n\n\nI found out after first comment that a \nsimilar question\n already posted. I often compress mongodump/mongoexport (BSON/JSON) and mysqldump (SQL text). Is there an advantage to use tar.xz for those backups?",
    "answer": "gzip\n and \nxz\n uses two different algorithms, and therefore they perform differently, both in terms of what level of compression they achieve and in terms of the amount of resources they consume while compressing or decompressing.\n\n\nIn \ngeneral\n, \nxz\n achieves higher compression ratios, but needs a lot more memory and time.\n\n\nI personally use \nxz\n for archiving data; big files that I need to put away for a long time. I use \ngzip\n otherwise, since it's usually quicker.\n\n\nDo test them both and see how they perform on \nyour\n average \ntar\n (or whatever) file.",
    "url": "https://unix.stackexchange.com/questions/301587/why-should-i-use-tar-xz-instead-of-tar-gz-xz-is-a-lossless-data-compression-pro"
  },
  {
    "question_title": "Extract multiple .tar.gz files with a single tar call",
    "question_body": "I was wondering whether (and, of course, how) it’s possible to tell \ntar\n to extract multiple files in a single run.\n\n\nI’m an experienced Unix user for several years and of course I know that you can use \nfor\n or \nfind\n or things like that to call \ntar\n once for each archive you want to extract, but I couldn’t come up with a working command line that caused my \ntar\n to extract two .tar.gz files at once. (And no, there’s nothing wrong with \nfor\n, I’m merely asking whether it’s possible to do without.)\n\n\nI’m asking this question rather out of curiosity, maybe\n\n\n\n\nthere’s a strange fork of \ntar\n somewhere that supports this\n\n\nsomeone knows how to use the \n-M\n parameter that \ntar\n suggested to me when I tried \ntar -zxv -f a.tgz -f b.tgz\n\n\nwe’re all blind and it’s totally easy to do — but I couldn’t find any hint in the web that didn’t utilize \nfor\n or \nfind\n or \nxargs\n or the like.\n\n\n\n\nPlease don’t reply with \ntar -zxvf *.tar.gz\n (because that does \nnot\n work) and only reply with “doesn’t work” if you’re absolutely sure about it (and maybe have a good explanation \nwhy\n, too).\n\n\nEdit:\n I was pointed to \nan answer to this question on Stack Overflow\n which says in great detail that it’s not possible without breaking current \ntar\n syntax, but I don’t think that’s true. Using \ntar -zxv -f a.tgz -f b.tgz\n or \ntar -zxv --all-args-are-archives *.tar.gz\n would break no existing syntax, imho.",
    "answer": "This is possible, the syntax is pretty easy:\n\n\n\n```bash\n$ cat *.tar | tar -xvf - -i\n```\n\n\n\nThe -i option ignores the EOF at the end of the tar archives, from the man page:\n\n\n\n```bash\n-i, --ignore-zeros\nignore blocks of zeros in archive (normally mean EOF)\n```",
    "url": "https://unix.stackexchange.com/questions/19840/extract-multiple-tar-gz-files-with-a-single-tar-call"
  },
  {
    "question_title": "How do I encrypt git on my server?",
    "question_body": "Heres the closest I've gotten: I installed gitolite in the \n/Private\n folder using \necryptfs-utils\n  (\nsudo apt-get install ecryptfs-utils\n \nadduser git\n \necryptfs-setup-private\n then the rest was configuring gitolite using a root install).\n\n\nIt worked just fine as long as someone was logged in as the user \ngit\n using a password (\nsu git\n using root does not work). Since the private folder activates through logging in with a password and gitolite uses RSA keys (required) the private folder is hidden thus error occurs.\n\n\nIs there a way I can log into my server after a reboot, type in the password and have the git user private folder available until next time the machine restarts?\n\n\nOr maybe theres an easy way to encrypt a folder for git repositories?",
    "answer": "You simply need to remove the file \n~/.ecryptfs/auto-umount\n.\n\n\nThis file is a flag that pam_ecryptfs checks on logout.  This file exists by default at setup, along with \n~/.ecryptfs/auto-mount\n, such that your private directory is automatically mounted and unmounted at login/logout.  But each can be removed independently to change that behavior.  Enjoy!",
    "url": "https://unix.stackexchange.com/questions/10378/how-do-i-encrypt-git-on-my-server"
  },
  {
    "question_title": "Why doesn&#39;t my Bash script recognize aliases?",
    "question_body": "In my \n~/.bashrc\n file reside two definitions:\n\n\n\n\ncommandA\n, which is an alias to a longer path\n\n\ncommandB\n, which is an alias to a Bash script\n\n\n\n\nI want to process the same file with these two commands, so I wrote the following Bash script:\n\n\n\n\n\n```bash\n#!/bin/bash\n\nfor file in \"$@\"\n    do\n    commandA $file\n    commandB $file\ndone\n```\n\n\n\nEven after logging out of my session and logging back in, Bash prompts me with \ncommand not found\n errors for both commands when I run this script.\n\n\nWhat am I doing wrong?",
    "answer": "If you look into the bash manpage you find:\n\n\n\n> Aliases are not expanded when the\n> shell is not interactive, unless the\n> expand_aliases shell  option  is  set\n> using shopt (see the description of\n> shopt under SHELL BUILTIN COMMANDS\n> below).\n\n\n\nSo put a\n\n\n\n```bash\nshopt -s expand_aliases\n```\n\n\n\nin your script.\n\n\nMake sure to source your aliases file after setting this in your script.\n\n\n\n```bash\nshopt -s expand_aliases\nsource ~/.bash_aliases\n```",
    "url": "https://unix.stackexchange.com/questions/1496/why-doesnt-my-bash-script-recognize-aliases"
  },
  {
    "question_title": "How to remove a file from the git index",
    "question_body": "How do I remove a file from a git repositorie's \nindex\n without removing the file from the working tree?\n\n\nIf I had a file \n./notes.txt\n that was being tracked by git, I could run \ngit rm notes.txt\n. But that would remove the file. I'd rather want git just to stop tracking the file.",
    "answer": "You could just use \ngit rm --cached notes.txt\n. This will keep the file but remove it from the index.",
    "url": "https://unix.stackexchange.com/questions/1818/how-to-remove-a-file-from-the-git-index"
  },
  {
    "question_title": "Quoting within $(command substitution) in Bash",
    "question_body": "In my Bash environment I use variables containing spaces, and I use these variables within command substitution.\n\n\nWhat is the correct way to quote my variables? And how should I do it if these are nested?\n\n\n\n```bash\nDIRNAME=$(dirname \"$FILE\")\n```\n\n\n\nor do I quote outside the substitution?\n\n\n\n```bash\nDIRNAME=\"$(dirname $FILE)\"\n```\n\n\n\nor both?\n\n\n\n```bash\nDIRNAME=\"$(dirname \"$FILE\")\"\n```\n\n\n\nor do I use back-ticks?\n\n\n\n```bash\nDIRNAME=`dirname \"$FILE\"`\n```\n\n\n\nWhat is the right way to do this? And how can I easily check if the quotes are set right?",
    "answer": "In order from worst to best:\n\n\n\n\nDIRNAME=\"$(dirname $FILE)\"\n \nwill not do what you want\n if \n$FILE\n contains whitespace (or whatever characters \n$IFS\n currently contains) or globbing characters \n\\[?*\n.\n\n\nDIRNAME=`dirname \"$FILE\"`\n is technically correct, but \nbackticks are not recommended for command expansion\n because of the extra complexity when nesting them and the extra backslash processing that happens within them.\n\n\nDIRNAME=$(dirname \"$FILE\")\n is correct, but \nonly because this is an assignment to a scalar (not array) variable\n. If you use the command substitution in any other context, such as \nexport DIRNAME=$(dirname \"$FILE\")\n or \ndu $(dirname -- \"$FILE\")\n, the lack of quotes will cause trouble if the result of the expansion contain whitespace or globbing characters.\n\n\nDIRNAME=\"$(dirname \"$FILE\")\"\n (except for the missing \n--\n, see below) is the recommended way. You can replace \nDIRNAME=\n with a command and a space without changing anything else, and \ndirname\n receives the correct string.\n\n\n\n\nTo improve even further:\n\n\n\n\nDIRNAME=\"$(dirname -- \"$FILE\")\"\n works if \n$FILE\n starts with a dash.\n\n\nDIRNAME=\"$(dirname -- \"$FILE\" && printf x)\" && DIRNAME=\"${DIRNAME%?x}\" || exit\n works even if \n$FILE\n's dirname ends with a newline, since \n$()\n chops off newlines at the end of output, both the one added by \ndirname\n and the ones that may be part of the actual data.\n\n\n\n\nYou can nest command expansions as much as you like. With \n$()\n you always create a new quoting context, so you can do things like this:\n\n\n\n```bash\nfoo \"$(bar \"$(baz \"$(ban \"bla\")\")\")\"\n```\n\n\n\nYou do \nnot\n want to try that with backticks.",
    "url": "https://unix.stackexchange.com/questions/118433/quoting-within-command-substitution-in-bash"
  },
  {
    "question_title": "gitk crashes when viewing commit containing emoji: X Error of failed request: BadLength (poly request too large or internal Xlib length error)",
    "question_body": "I'm able to open \ngitk\n but it crashes as soon as I open a commit whom changes contains an emoji (not the commit message).\n\n\nError\n\n\n\n```bash\n❯ gitk --all\nX Error of failed request:  BadLength (poly request too large or internal Xlib length error)\n  Major opcode of failed request:  139 (RENDER)\n  Minor opcode of failed request:  20 (RenderAddGlyphs)\n  Serial number of failed request:  6687\n  Current serial number in output stream:  6706\n```\n\n\n\nEnv\n\n\n\n```bash\n❯ cat /etc/os-release --plain\nNAME=\"Linux Mint\"\nVERSION=\"20 (Ulyana)\"\nID=linuxmint\nID_LIKE=ubuntu\nPRETTY_NAME=\"Linux Mint 20\"\nVERSION_ID=\"20\"\nHOME_URL=\"https://www.linuxmint.com/\"\nSUPPORT_URL=\"https://forums.linuxmint.com/\"\nBUG_REPORT_URL=\"http://linuxmint-troubleshooting-guide.readthedocs.io/en/latest/\"\nPRIVACY_POLICY_URL=\"https://www.linuxmint.com/\"\nVERSION_CODENAME=ulyana\nUBUNTU_CODENAME=focal\n```\n\n\n\nGit\n\n\n❯ git --version\ngit version 2.25.1\n\n\nExamples\n\n\n\n> 6e05ecd add v3->v4 migration script to update variables\n> https://github.com/rafaelrinaldi/pure/pull/271/commits/6e05ecdad0e4f623050e154e16c0af0315767940\n\n\n\nQuestions\n\n\nI tried various things:\n\n\n\n\nremoving \n~/.Xresources\n config related to fonts\n\n\nediting then removing \n~/.config/fontconfig/conf.d/30-icons.conf\n\n\n\n\nWithout success, most of the \nissues\n I found were related to \nst\n terminal\n. However, I'm not using it but \nguake\n, and the issue also happened with \nyakuake\n, \ngnome-terminal\n and \nhyper\n\n\nHow could I fix this?",
    "answer": "As suggested in \nhttps://bugs.launchpad.net/ubuntu/+source/git/+bug/1852985/comments/11\n, I am able to stop gitk from crashing by installing unifont:\n\n\n\n```bash\nsudo apt install unifont\n```\n\n\n\nLike in the comment, the emoji is displayed in gitk as an empty square, but gitk no longer crashes.",
    "url": "https://unix.stackexchange.com/questions/629281/gitk-crashes-when-viewing-commit-containing-emoji-x-error-of-failed-request-ba"
  },
  {
    "question_title": "tar without preserving user",
    "question_body": "I'd like to compress some files for http distribution, but found that \n.tar.gz\n keeps the user name and user ID and there doesn't seem to be any way to not do that? (There is a \n--numeric-owner\n option for tar which seems to ignore the user name, but still keeps the user ID.)\n\n\nDoesn't that mean that \n.tar.gz\n is a poor choice for file distribution as my system probably is the only one with my user ID and my user name? Is \n.7z\n a better format for file distribution, or do you have any other recommendation?",
    "answer": "Generally .tar.gz is a usable file distribution format. GNU tar allows you not to preserve the owner and permissions.\n\n\n\n```bash\n$ tar -c -f archive.tar --owner=0 --group=0 --no-same-owner --no-same-permissions .\n```\n\n\n\nhttps://www.gnu.org/software/tar/manual/html_section/tar_33.html#SEC69\n\n\nIf your version of tar does not support the GNU options you can copy your source files to another directory tree and update group and ownership there, prior to creating your tar.gz file for distribution.\n\n\n--owner=0\n and \n--group=0\n works only in compression phase of the file while in decompression phase it has no effect.\n\n\n--no-same-owner\n \n--no-same-permissions\n works only in decompression phase while in compression phase it has no effect.\n\nPut together they can constitute a default function in which tar assumes the characteristics of not remembering the user who compressed or decompressed the files.\n\nWhen during compression the files are stored with user and group 0, during the decompression via GUI, they assume the permissions of the user who extracts the files, so it is a valid solution to forget the user in the compression phase.",
    "url": "https://unix.stackexchange.com/questions/285237/tar-without-preserving-user"
  },
  {
    "question_title": "Print archive file list instantly (without decompressing entire archive)",
    "question_body": "A problem with \n.tar.gz\n archives is that, when I try to just list an archive's content, the computer actually decompresses it, which would take a very long time if the file is large.\n\n\nOther file formats like \n.7z\n, \n.rar\n,\n.zip\n don't have this problem. Listing their contents takes just an instant.\n\n\nIn my naive opinion, this is a huge drawback of the \n.tar.gz\n archive format. \n\n\nSo I actually have 2 questions:  \n\n\n\n\nwhy do people use \n.tar.gz\n so much, despite this drawback?  \n\n\nwhat choices (I mean other software or tools) do I have if I want the \"instant content listing\" capability?",
    "answer": "It's important to understand there's a trade-off here.\n\n\ntar\n means \ntape archiver\n. On a tape, you do mostly sequential reading and writing. Tapes are rarely used nowadays, but \ntar\n is still used for its ability to read and write its data as a stream.\n\n\nYou can do:\n\n\n\n```bash\ntar cf - files | gzip | ssh host 'cd dest && gunzip | tar xf -'\n```\n\n\n\nYou can't do that with \nzip\n or the like.\n\n\nYou can't even list the content of a \nzip\n archive without storing it locally in a seekable file first. Things like:\n\n\n\n```bash\ncurl -s https://github.com/dwp-forge/columns/archive/v.2016-02-27.zip | unzip -l /dev/stdin\n```\n\n\n\nwon't work.\n\n\nTo achieve that quick reading of the content, \nzip\n or the like need to build an index. That index can be stored at the beginning of the file (in which case it can only be written to regular files, not streams), or at the end, which means the archiver needs to remember all the archive members before printing it in the end and means a truncated archive may not be recoverable.\n\n\nThat also means archive members need to be compressed individually which means a much lower compression ratio especially if there's a lot of small files.\n\n\nAnother drawback with formats like \nzip\n is that the archiving is linked to the compressing, you can't choose the compression algorithm. See how \ntar\n archives used to be compressed with \ncompress\n (\ntar.Z\n), then with \ngzip\n, then \nbzip2\n, then \nxz\n as new more performant compression algorithms were devised. Same goes for encryption. Who would trust \nzip\n's encryption nowadays?\n\n\nNow, the problem with \ntar.gz\n archives is not that much that you need to uncompress them. Uncompressing is often faster than reading off a disk (you'll probably find that listing the content of a large tgz archive is quicker that listing the same one uncompressed when not cached in memory), but that you need to read the whole archive.\n\n\nNot being able to read the index quickly is not really a problem. If you do foresee needing to read the table content of an archive often, you can just store that list in a separate file. For instance, at creation time, you can do:\n\n\n\n```bash\ntar cvvf - dir 2> file.tar.xz.list | xz > file.tar.xz\n```\n\n\n\nA bigger problem IMO is the fact that because of the sequential aspect of the archive, you can't extract individual files without reading the whole beginning section of the archive that leads to it. IOW, you can't do random reads within the archive.\n\n\nNow, for seekable files, it doesn't have to be that way.\n\n\nIf you compress your \ntar\n archive with \ngzip\n, that compresses it as a whole, the compression algorithm uses data seen at the beginning to compress, so you have to start from the beginning to uncompress.\n\n\nBut the \nxz\n format can be configured to compress data in separate individual chunks (large enough so as the compression to be efficient), that means that as long as you keep an index at the end of those compressed chunks, for seekable files, you access the uncompressed data randomly (in chunks at least).\n\n\npixz\n (parallel \nxz\n) uses that capability when compressing \ntar\n archives to also add an index of the start of each member of the archive at the end of the \nxz\n file.\n\n\nSo, for seekable files, not only can you get a list of the content of the tar archive instantly (without metadata though) if they have been compressed with \npixz\n:\n\n\n\n```bash\npixz -l file.tar.xz\n```\n\n\n\nBut you can also extract individual elements without having to read the whole archive:\n\n\n\n```bash\npixz -x archive/member.txt < file.tar.xz | tar xpf -\n```\n\n\n\nNow, as to why things like \n7z\n or \nzip\n are rarely used on Unix is mostly because they can't archive Unix files. They've been designed for other operating systems. You can't do a faithful backup of data using those. They can't store metadata like owner (id and name), permission, they can't store symlinks, devices, fifos..., they can't store information about hard links, and other metadata information like extended attributes or ACLs.\n\n\nSome of them can't even store members with arbitrary names (some will choke on backslash or newline or colon, or non-ascii filenames) (some \ntar\n formats also have limitations though).\n\n\nNever uncompress a tgz/tar.xz file to disk!\n\n\nIn case it is not obvious, one doesn't use a \ntgz\n or \ntar.bz2\n, \ntar.xz\n... archive as:\n\n\n\n```bash\nunxz file.tar.xz\ntar tvf file.tar\nxz file.tar\n```\n\n\n\nIf you've got an uncompressed \n.tar\n file lying about on your file system, it's that you've done something wrong.\n\n\nThe whole point of those \nxz\n/\nbzip2\n/\ngzip\n being stream compressors is that they can be used on the fly, in pipelines as in\n\n\n\n```bash\nunxz < file.tar.xz | tar tvf -\n```\n\n\n\nThough modern \ntar\n implementations know how to invoke \nunxz\n/\ngunzip\n/\nbzip2\n by themselves, so:\n\n\n\n```bash\ntar tvf file.tar.xz\n```\n\n\n\nwould generally also work (and again uncompress the data on the fly and not store the uncompressed version of the archive on disk).\n\n\nExample\n\n\nHere's a Linux kernel source tree compressed with various formats.\n\n\n\n```bash\n$ ls --block-size=1 -sS1\n666210304 linux-4.6.tar\n173592576 linux-4.6.zip\n 97038336 linux-4.6.7z\n 89468928 linux-4.6.tar.xz\n```\n\n\n\nFirst, as noted above, the 7z and zip ones are slightly different because they can't store the few symlinks in there and are missing most of the metadata.\n\n\nNow a few timings to list the content after having flushed the system caches:\n\n\n\n```bash\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches\n3\n$ time tar tvf linux-4.6.tar > /dev/null\ntar tvf linux-4.6.tar > /dev/null  0.56s user 0.47s system 13% cpu 7.428 total\n$ time tar tvf linux-4.6.tar.xz > /dev/null\ntar tvf linux-4.6.tar.xz > /dev/null  8.10s user 0.52s system 118% cpu 7.297 total\n$ time unzip -v linux-4.6.zip > /dev/null\nunzip -v linux-4.6.zip > /dev/null  0.16s user 0.08s system 86% cpu 0.282 total\n$ time 7z l linux-4.6.7z > /dev/null\n7z l linux-4.6.7z > /dev/null  0.51s user 0.15s system 89% cpu 0.739 total\n```\n\n\n\nYou'll notice listing the \ntar.xz\n file is quicker than the \n.tar\n one even on this 7 years old PC as reading those extra megabytes from the disk takes longer than reading and decompressing the smaller file.\n\n\nThen OK, listing the archives with 7z or zip is quicker but that's a non-problem as as I said, it's easily worked around by storing the file list alongside the archive:\n\n\n\n```bash\n$ tar tvf linux-4.6.tar.xz | xz > linux-4.6.tar.xz.list.xz\n$ ls --block-size=1 -sS1 linux-4.6.tar.xz.list.xz\n434176 linux-4.6.tar.xz.list.xz\n$ time xzcat linux-4.6.tar.xz.list.xz > /dev/null\nxzcat linux-4.6.tar.xz.list.xz > /dev/null  0.05s user 0.00s system 99% cpu 0.051 total\n```\n\n\n\nEven faster than 7z or zip even after dropping caches. You'll also notice that the cumulative size of the archive and its index is still smaller than the zip or 7z archives.\n\n\nOr use the \npixz\n indexed format:\n\n\n\n```bash\n$ xzcat linux-4.6.tar.xz | pixz -9  > linux-4.6.tar.pixz\n$ ls --block-size=1 -sS1 linux-4.6.tar.pixz\n89841664 linux-4.6.tar.pixz\n$ echo 3 | sudo tee /proc/sys/vm/drop_caches\n3\n$ time pixz -l linux-4.6.tar.pixz > /dev/null\npixz -l linux-4.6.tar.pixz > /dev/null  0.04s user 0.01s system 57% cpu 0.087 total\n```\n\n\n\nNow, to extract individual elements of the archive, the worst case scenario for a tar archive is when accessing the last element:\n\n\n\n```bash\n$ xzcat linux-4.6.tar.xz.list.xz|tail -1\n-rw-rw-r-- root/root      5976 2016-05-15 23:43 linux-4.6/virt/lib/irqbypass.c\n$ time tar xOf linux-4.6.tar.xz linux-4.6/virt/lib/irqbypass.c | wc\n    257     638    5976\ntar xOf linux-4.6.tar.xz linux-4.6/virt/lib/irqbypass.c  7.27s user 1.13s system 115% cpu 7.279 total\nwc  0.00s user 0.00s system 0% cpu 7.279 total\n```\n\n\n\nThat's pretty bad as it needs to read (and uncompress) the whole archive. Compare with:\n\n\n\n```bash\n$ time unzip -p linux-4.6.zip linux-4.6/virt/lib/irqbypass.c | wc\n    257     638    5976\nunzip -p linux-4.6.zip linux-4.6/virt/lib/irqbypass.c  0.02s user 0.01s system 19% cpu 0.119 total\nwc  0.00s user 0.00s system 1% cpu 0.119 total\n```\n\n\n\nMy version of 7z seems not to be able to do random access, so it seems to be even worse than \ntar.xz\n:\n\n\n\n```bash\n$ time 7z e -so linux-4.6.7z linux-4.6/virt/lib/irqbypass.c 2> /dev/null | wc\n    257     638    5976\n7z e -so linux-4.6.7z linux-4.6/virt/lib/irqbypass.c 2> /dev/null  7.28s user 0.12s system 89% cpu 8.300 total\nwc  0.00s user 0.00s system 0% cpu 8.299 total\n```\n\n\n\nNow since we have our \npixz\n generated one from earlier:\n\n\n\n```bash\n$ time pixz < linux-4.6.tar.pixz -x linux-4.6/virt/lib/irqbypass.c  | tar xOf - | wc\n    257     638    5976\npixz -x linux-4.6/virt/lib/irqbypass.c < linux-4.6.tar.pixz  1.37s user 0.06s system 84% cpu 1.687 total\ntar xOf -  0.00s user 0.01s system 0% cpu 1.693 total\nwc  0.00s user 0.00s system 0% cpu 1.688 total\n```\n\n\n\nIt's faster but still relatively slow because the archive contains few large blocks:\n\n\n\n```bash\n$ pixz -tl linux-4.6.tar.pixz\n 17648865 / 134217728\n 15407945 / 134217728\n 18275381 / 134217728\n 19674475 / 134217728\n 18493914 / 129333248\n   336945 /   2958887\n```\n\n\n\nSo \npixz\n still needs to read and uncompress a (up to a) ~19MB large chunk of data.\n\n\nWe can make random access faster by making archives will smaller blocks (and sacrifice a bit of disk space):\n\n\n\n```bash\n$ pixz -f0.25 -9 < linux-4.6.tar > linux-4.6.tar.pixz2\n$ ls --block-size=1 -sS1 linux-4.6.tar.pixz2\n93745152 linux-4.6.tar.pixz2\n$ time pixz < linux-4.6.tar.pixz2 -x linux-4.6/virt/lib/irqbypass.c  | tar xOf - | wc\n    257     638    5976\npixz -x linux-4.6/virt/lib/irqbypass.c < linux-4.6.tar.pixz2  0.17s user 0.02s system 98% cpu 0.189 total\ntar xOf -  0.00s user 0.00s system 1% cpu 0.188 total\nwc  0.00s user 0.00s system 0% cpu 0.187 total\n```",
    "url": "https://unix.stackexchange.com/questions/287087/print-archive-file-list-instantly-without-decompressing-entire-archive"
  },
  {
    "question_title": "Grep word boundaries",
    "question_body": "Accorging to GNU documentation:\n\n\n\n```bash\n‘\\<’ Match the empty string at the beginning of word.\n‘\\>’ Match the empty string at the end of word.\n```\n\n\n\nMy /etc/fstab looks like this:\n\n\n\n```bash\n/dev/sdb1       /media/fresh      ext2   defaults     0 0\n```\n\n\n\nI want grep to return TRUE/FALSE for the existence of /media/fresh. I tried to use \n\\<\n and \n\\>\n but it didn't work. Why?\n\n\n\n```bash\negrep '\\</media/fresh\\>' /etc/fstab\n```\n\n\n\nWorkaround:\n\n\n\n```bash\negrep '[[:blank:]]/media/fresh[[:blank:]]' /etc/fstab\n```\n\n\n\nBut it looks uglier.\n\n\nMy grep is 2.5.1",
    "answer": "\\<\n and \n\\>\n match empty string at the begin and end of a word respectively and only word constituent characters are:\n\n\n\n```bash\n[[:alnum:]_]\n```\n\n\n\nFrom \nman grep\n:\n\n\n\n```bash\nWord-constituent characters are letters, digits, and the underscore.\n```\n\n\n\nSo, your Regex is failing because \n/\n is not a valid word constituent character.\n\n\nInstead as you have spaces around, you can use \n-w\n option of \ngrep\n to match a word:\n\n\n\n```bash\ngrep -wo '/media/fresh' /etc/fstab\n```\n\n\n\nExample:\n\n\n\n```bash\n$ grep -wo '/media/fresh' <<< '/dev/sdb1       /media/fresh      ext2   defaults     0 0'\n/media/fresh\n```",
    "url": "https://unix.stackexchange.com/questions/281938/grep-word-boundaries"
  },
  {
    "question_title": "Is there a basic tutorial for grep, awk and sed?",
    "question_body": "I've been a Linux user for a while, and I've a pretty decent understanding of most the common command line utilities. However, ones that come up and up again in relation to programming are \ngrep\n, \nawk\n, and \nsed\n.\n\n\nAbout the only thing I've used grep for is piping stuff into it to find files in log files, the output of \nps\n etc. I haven't used \nawk\n or \nsed\n at all. Are there any good tutorials for these utilities?",
    "answer": "AWK is particularly well suited for tabular data and has a lower learning curve than some alternatives.\n\n\nAWK: A Tutorial and Introduction\n\n\nAn AWK Primer\n (\nalt link\n)\n\n\nRegularExpressions.info\n\n\nsed tutorial\n\n\ngrep tutorial\n\n\ninfo sed\n, \ninfo grep\n and \ninfo awk\n or \ninfo gawk",
    "url": "https://unix.stackexchange.com/questions/2434/is-there-a-basic-tutorial-for-grep-awk-and-sed"
  },
  {
    "question_title": "How to grep for same string but multiple files at the same time?",
    "question_body": "I have a set of log files that I need to review and I would like to search specific strings on the same files at once Is this possible? Currently I am using \n\n\n\n```bash\ngrep -E 'fatal|error|critical|failure|warning|' /path_to_file\n```\n\n\n\nHow do I use this and search for the strings of multiple files at once? If this is something that needs to be scripted, can someone provide a simple script to do this?",
    "answer": "```bash\ngrep -E 'fatal|error|critical|failure|warning|' *.log\n```",
    "url": "https://unix.stackexchange.com/questions/171314/how-to-grep-for-same-string-but-multiple-files-at-the-same-time"
  },
  {
    "question_title": "How to correctly add a path to PATH?",
    "question_body": "I'm wondering where a new path has to be added to the \nPATH\n environment variable. I know this can be accomplished by editing \n.bashrc\n (for example), but it's not clear how to do this.\n\n\nThis way:\n\n\n\n```bash\nexport PATH=~/opt/bin:$PATH\n```\n\n\n\nor this?\n\n\n\n```bash\nexport PATH=$PATH:~/opt/bin\n```",
    "answer": "The simple stuff\n\n\n\n```bash\nPATH=$PATH:~/opt/bin\n```\n\n\n\nor\n\n\n\n```bash\nPATH=~/opt/bin:$PATH\n```\n\n\n\ndepending on whether you want to add \n~/opt/bin\n at the end (to be searched after all other directories, in case there is a program by the same name in multiple directories) or at the beginning (to be searched before all other directories).\n\n\nYou can add multiple entries at the same time. \nPATH=$PATH:~/opt/bin:~/opt/node/bin\n or variations on the ordering work just fine. Don't put \nexport\n at the beginning of the line as it has additional complications (see below under “Notes on shells other than bash”).\n\n\nIf your \nPATH\n gets built by many different components, you might end up with duplicate entries. See \nHow to add home directory path to be discovered by Unix which command?\n and \nRemove duplicate $PATH entries with awk command\n to avoid adding duplicates or remove them.\n\n\nSome distributions automatically put \n~/bin\n in your PATH if it exists, by the way.\n\n\nWhere to put it\n\n\nPut the line to modify \nPATH\n in \n~/.profile\n, or in \n~/.bash_profile\n or if that's what you have. (If your login shell is zsh and not bash, put it in \n~/.zprofile\n instead.)\n\n\nThe profile file is read by login shells, so it will only take effect the next time you log in. (Some systems configure terminals to read a login shell; in that case you can start a new terminal window, but the setting will take effect only for programs started via a terminal, and how to set \nPATH\n for all programs depends on the system.)\n\n\nNote that \n~/.bash_rc\n is not read by any program, and \n~/.bashrc\n is the configuration file of interactive instances of bash. You should not define environment variables in \n~/.bashrc\n. The right place to define environment variables such as \nPATH\n is \n~/.profile\n (or \n~/.bash_profile\n if you don't care about shells other than bash). See \nWhat's the difference between them and which one should I use?\n\n\nDon't put it in \n/etc/environment\n or \n~/.pam_environment\n: these are not shell files, you can't use substitutions like \n$PATH\n in there. In these files, you can only override a variable, not add to it.\n\n\nPotential complications in some system scripts\n\n\nYou don't need \nexport\n if the variable is already in the environment: any change of the value of the variable is reflected in the environment.¹ \nPATH\n is pretty much always in the environment; all unix systems set it very early on (usually in the very first process, in fact).\n\n\nAt login time, you can rely on \nPATH\n being already in the environment, and already containing some system directories. If you're writing a script that may be executed early while setting up some kind of virtual environment, you may need to ensure that \nPATH\n is non-empty and exported: if \nPATH\n is still unset, then something like \nPATH=$PATH:/some/directory\n would set \nPATH\n to \n:/some/directory\n, and the empty component at the beginning means the current directory (like \n.:/some/directory\n).\n\n\n\n```bash\nif [ -z \"${PATH-}\" ]; then export PATH=/usr/local/bin:/usr/bin:/bin; fi\n```\n\n\n\nNotes on shells other than bash\n\n\nIn bash, ksh and zsh, \nexport\n is special syntax, and both \nPATH=~/opt/bin:$PATH\n and \nexport PATH=~/opt/bin:$PATH\n do the right thing even. In other Bourne/POSIX-style shells such as dash (which is \n/bin/sh\n on many systems), \nexport\n is parsed as an ordinary command, which implies two differences:\n\n\n\n\n~\n is only parsed at the beginning of a word, except in assignments (see \nHow to add home directory path to be discovered by Unix which command?\n for details);\n\n\n$PATH\n outside double quotes \nbreaks if \nPATH\n contains whitespace or \n\\[*?\n.\n\n\n\n\nSo in shells like dash, \nexport PATH=~/opt/bin:$PATH\n sets \nPATH\n to the literal string \n~/opt/bin/:\n followed by the value of \nPATH\n up to the first space.\n\nPATH=~/opt/bin:$PATH\n (a bare assignment) \ndoesn't require quotes\n and does the right thing. If you want to use \nexport\n in a portable script, you need to write \nexport PATH=\"$HOME/opt/bin:$PATH\"\n, or \nPATH=~/opt/bin:$PATH; export PATH\n (or \nPATH=$HOME/opt/bin:$PATH; export PATH\n for portability to even the Bourne shell that didn't accept \nexport var=value\n and didn't do tilde expansion).\n\n\n¹ \n This wasn't true in Bourne shells (as in the actual Bourne shell, not modern POSIX-style shells), but you're highly unlikely to encounter such old shells these days.",
    "url": "https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path"
  },
  {
    "question_title": "How can I get the size of a file in a bash script?",
    "question_body": "How can I get the size of a file in a bash script? \n\n\nHow do I assign this to a bash variable so I can use it later?",
    "answer": "Your best bet if on a GNU system:\n\n\n\n```bash\nstat --printf=\"%s\" file.any\n```\n\n\n\nFrom \nman stat\n:\n\n\n\n> %s total size, in bytes\n\n\n\nIn a bash script :\n\n\n\n```bash\n#!/bin/bash\nFILENAME=/home/heiko/dummy/packages.txt\nFILESIZE=$(stat -c%s \"$FILENAME\")\necho \"Size of $FILENAME = $FILESIZE bytes.\"\n```\n\n\n\nNOTE: see \n@chbrown's answer\n for how to use \nstat\n on BSD or macOS systems.",
    "url": "https://unix.stackexchange.com/questions/16640/how-can-i-get-the-size-of-a-file-in-a-bash-script"
  },
  {
    "question_title": "Github adding a repository as a fork from an existing clone",
    "question_body": "So I have a git repository that I cloned from an upstream source on github. I made a few changes to it (that are uncommitted in the master branch). What I want to do is push my changes onto my github page as a new branch and have github still see it as a fork.\n\n\nIs that possible? I'm fairly new to git and github. Did my question even make sense?\n\n\nThe easiest way that I can think of (which I'm sure is the most aroundabout way), is to fork the repo on github. Clone it locally to a different directory. Add the upstream origin repo. Create a branch in that new forked repo. Copy my code changes by hand into the new local repo. And then push it back up to my github.\n\n\nIs this a common use case that there's a simpler way of doing it without duplicating directories?\n\n\nI guess I'm asking here as opposed to SO since I'm on linux using command line git and the people here give better answers imo =]",
    "answer": "You can do it all from your existing repository (no need to clone the fork into a new (local) repository, create your branch, copy your commits/changes, etc.).\n\n\n\n\nGet your commits ready to be published.\n\n\nRefine any existing local commits (e.g. with \ngit commit --amend\n and/or \ngit rebase --interactive\n).\n\n\nCommit any of your uncommitted changes that you want to publish (I am not sure if you meant to imply that you have some commits on your local \nmaster\n and some uncommitted changes, or just some uncommitted changes; incidentally, uncommitted changes are not “on a branch”, they are strictly in your working tree).\n\n\nRename your \nmaster\n branch to give it the name you want for your “new branch”. This is not strictly necessary (you can push from any branch to any other branch), but it will probably reduce confusion in the long run if your local branch and the branch in your GitHub fork have the same name.\n\n\n\n```bash\ngit branch -m master my-feature\n```\n\n\n\nFork the upstream GitHub repository\n\n(e.g.) \ngithub.com:UpstreamOwner/repostory_name.git\n as\n\n(e.g.) \ngithub.com:YourUser/repository_name.git\n.\n\n\nThis is done on the GitHub website (or a “client” that uses the GitHub APIs), there are no local Git commands involved.\n\n\nIn your local repository (the one that was originally cloned from the upstream GitHub repository and has your changes in its \nmaster\n), add your fork repository as a remote:\n\n\n\n```bash\ngit remote add -f github github.com:YourUser/repository_name.git\n```\n\n\n\nPush your branch to your fork repository on GitHub.\n\n\n\n```bash\ngit push github my-feature\n```\n\n\n\nOptionally, rename the remotes so that your fork is known as “origin” and the upstream as “upstream”.\n\n\n\n```bash\ngit remote rename origin upstream\ngit remote rename github origin\n```\n\n\n\nOne reason to rename the remotes would be because you want to be able to use \ngit push\n without specifying a repository (it defaults to “origin”).",
    "url": "https://unix.stackexchange.com/questions/16743/github-adding-a-repository-as-a-fork-from-an-existing-clone"
  },
  {
    "question_title": "What&#39;s the difference between `Commit hash`, `Parent Hash` and `Tree hash` in git?",
    "question_body": "Today I'm learning some basic git knowledge via reading this doc online:\n\n\nhttp://git-scm.com/book/en/v2/Git-Basics-Viewing-the-Commit-Hi\n\n\nAnd at that chapter, I'm start to learn using \ngit log --pretty=format:\" \"\n to show log info at my taste.\n\n\nBut some how, I saw in the format table two similar options, \n%H\n for \nCommit Hash\n, \n%P\n for \nParent Hash\n and \n%T\n for \nTree Hash\n.\n\n\nI experimented them on my command line, it comes out they are all hash values of same length with different value.\n\n\nI googled and stackoverflowed, no obvious hints so far.\n\n\nI have idea about this \nHash value\n, it's a checksum of that git commit.\n\n\nBut what does \nParent Hash\n and \nTree hash\n do?\n\n\n\n\nPS: Ah, I got some ideas now, did the \nParent Hash\n mean the hash value of the directly origin of a branch?",
    "answer": "Parent hashes:\n\n\n\n```bash\n$ git log --graph\n*   commit c06c4c912dbd9ee377d14ec8ebe2847cf1a3ec7e\n|\\  Merge: 79e6924 3113760\n| | Author: linjie <linjielig@gmail.com>\n| | Date:   Mon Mar 14 16:02:09 2016 +0800\n| |\n| |     commit5\n| |\n| |     Merge branch 'dev'\n| |\n| * commit 31137606f85d8960fa1640d0881682a081ffa9d0\n| | Author: linjie <linjielig@gmail.com>\n| | Date:   Mon Mar 14 16:01:26 2016 +0800\n| |\n| |     commit3\n| |\n* | commit 79e69240ccd218d49d78a72f33002fd6bc62f407\n|/  Author: linjie <linjielig@gmail.com>\n|   Date:   Mon Mar 14 16:01:59 2016 +0800\n|\n|       commit4\n|\n* commit 7fd4e3fdddb89858d925a89767ec62985ba07f3d\n| Author: linjie <linjielig@gmail.com>\n| Date:   Mon Mar 14 16:01:00 2016 +0800\n|\n|     commit2\n|\n* commit 316dd3fb3c7b501bc9974676adcf558a18508dd4\n  Author: linjie <linjielig@gmail.com>\n  Date:   Mon Mar 14 16:00:34 2016 +0800\n\n     commit1\n\n$ git log --pretty=format:'%<(82)%P %s'\n79e69240ccd218d49d78a72f33002fd6bc62f407 31137606f85d8960fa1640d0881682a081ffa9d0  commit5\n7fd4e3fdddb89858d925a89767ec62985ba07f3d                                           commit4\n7fd4e3fdddb89858d925a89767ec62985ba07f3d                                           commit3\n316dd3fb3c7b501bc9974676adcf558a18508dd4                                           commit2\n                                                                                   commit1\n```\n\n\n\nYou can see \ncommit4\n and \ncommit3\n is parent of \ncommit5\n, \ncommit2\n is parent of \ncommit3\n and \ncommit4\n, \ncommit1\n is parent of \ncommit2\n.\n\n\nTree hash:\n\n\n\n```bash\n$ git log --pretty=format:'%T %s'\nf3c7cee96f33938631a9b023ccf5d8743b00db0e commit5\ne0ecb42ae45ddc91c947289f928ea5085c70b208 commit4\nd466aea17dc07516c449c58a73b2dc3faa9d11a1 commit3\nb39f2e707050e0c5bbb3b48680f416ef05b179ba commit2\n5706ec2b32605e27fa04cbef37d582325d14dda9 commit1\n\n$ git cat-file -p f3c7ce\n100644 blob 8bb2e871e94c486a867f5cfcbc6f30d004f6a9e5    dev\n100644 blob 47f16c8e00adba77ec5c176876e99c8e9f05d69b    master\n\n$ git cat-file -p 5706ec\n100644 blob fc0bfde0d44bb4d6c7d27b6e587ebedd34ba5911    master\n```\n\n\n\nThe command's function: Pretty-print the contents of \n<object>\n based on it's type.\n\n\n\n```bash\ngit cat-file -p\n```\n\n\n\nIn git,all the content is stored as tree and blob objects, with trees corresponding to UNIX directory entries and blobs corresponding more or less to inodes or file contents. A single tree object contains one or more tree entries, each of which contains a SHA-1 pointer to a blob or subtree with its associated mode, type, and filename. Git normally creates a tree by taking the state of your staging area or index and writing a series of tree objects from it. Commit objects have the information about who saved the tree object, when they saved or why they were saved. This is the basic information that the commit object stores for you.\n\n\nConclusion:\n\n\nCommit hash, Parent hash, Tree hash are all SHA-1. Commit hash and Parent hash is identical except Parent hash has child. Tree hash is represent a Tree object. Commit hash and Parent hash represent a commit object.\n\n\nReference:\n\n\n\n\nGit Internals - Git Objects\n\n\ngit-cat-file - Provide content or type and size information for repository objects",
    "url": "https://unix.stackexchange.com/questions/165369/whats-the-difference-between-commit-hash-parent-hash-and-tree-hash-in-gi"
  },
  {
    "question_title": "How can I check if two gzipped files are equal?",
    "question_body": "I am trying to save space while doing a \"dumb\" backup by simply dumping data into a text file. My backup script is executed daily and looks like this:\n\n\n\n\nCreate a directory named after the backup date.\n\n\nDump some data into a text file \n\"$name\"\n.\n\n\nIf the file is valid, gzip it: \ngzip \"$name\"\n. Otherwise, \nrm \"$name\"\n.\n\n\n\n\nNow I want to add an additional step to remove a file if the same data was also available in the day before (and create symlink or hardlink).\n\n\nAt first I thought of using \nmd5sum \"$name\"\n, but this does not work because I also store the filename and creation date.\n\n\nDoes \ngzip\n have an option to compare two gzipped files and tell me whether they are equal or not? If \ngzip\n does not have such an option, is there another way to achieve my goal?",
    "answer": "@derobert\ns answer is great, though I want to share some other information that I have found.\n\n\ngzip -l -v\n\n\ngzip-compressed files contain already a hash (not secure though, see \nthis SO post\n):\n\n\n\n```bash\n$ echo something > foo\n$ gzip foo\n$ gzip -v -l foo.gz \nmethod  crc     date  time           compressed        uncompressed  ratio uncompressed_name\ndefla 18b1f736 Feb  8 22:34                  34                  10 -20.0% foo\n```\n\n\n\nOne can combine the CRC and uncompressed size to get a quick fingerprint:\n\n\n\n```bash\ngzip -v -l foo.gz | awk '{print $2, $7}'\n```\n\n\n\ncmp\n\n\nFor checking whether two bytes are equal or not, use \ncmp file1 file2\n. Now, a gzipped file has some header with the data and footer (CRC plus original size) appended. The \ndescription of the gzip format\n shows that the header contains the time when the file was compressed and that the file name is a nul-terminated string that is appended after the 10-byte header.\n\n\nSo, assuming that the file name is constant and the same command (\ngzip \"$name\"\n) is used, one can check whether two files are different by using \ncmp\n and skipping the first bytes including the time:\n\n\n\n```bash\ncmp -i 8 file1 file2\n```\n\n\n\nNote\n: the assumption that the same compression options is important, otherwise the command will always report the file as different. This happens because the compression options are stored in the header and may affect the compressed data. \ncmp\n just looks at raw bytes and do not interpret it as gzip.\n\n\nIf you have filenames of the same length, then you could try to calculate the bytes to be skipped after reading the filename. When the filenames are of different size, you could run \ncmp\n after skipping bytes, like \ncmp <(cut -b9- file1) <(cut -b10- file2)\n.\n\n\nzcmp\n\n\nThis is definitely the best way to go, it first compresses data and starts comparing the bytes with \ncmp\n (really, this is what is done in the \nzcmp\n (\nzdiff\n) shellscript).\n\n\nOne note, do not be afraid of the following note in the manual page:\n\n\n\n> When both files must be uncompressed before comparison, the second is uncompressed to /tmp.  In all other cases, zdiff and zcmp use only a pipe.\n\n\n\nWhen you have a sufficiently new Bash, compression will not use a temporary file, just a pipe. Or, as the \nzdiff\n source says:\n\n\n\n```bash\n# Reject Solaris 8's buggy /bin/bash 2.03.\n```",
    "url": "https://unix.stackexchange.com/questions/64200/how-can-i-check-if-two-gzipped-files-are-equal"
  },
  {
    "question_title": "How to uppercase the command line argument?",
    "question_body": "I searched SO and found that to uppercase a string following would work\n\n\n\n```bash\nstr=\"Some string\"\necho ${str^^}\n```\n\n\n\nBut I tried to do a similar thing on a command-line argument, which gave me the following error\n\n\nTried\n\n\n\n```bash\n#!/bin/bash\n             ## Output\necho ${1^^}  ## line 3: ${1^^}: bad substitution\necho {$1^^}  ## No error, but output was still smaller case i.e. no effect\n```\n\n\n\nHow could we do this?",
    "answer": "The syntax \nstr^^\n which you are trying is available from Bash 4.0 and above. Perhaps yours is an older version (or you ran the script with \nsh\n explicitly):\n\n\nTry this:\n\n\n\n```bash\nstr=\"Some string\"\nprintf '%s\\n' \"$str\" | awk '{ print toupper($0) }'\n```",
    "url": "https://unix.stackexchange.com/questions/51983/how-to-uppercase-the-command-line-argument"
  },
  {
    "question_title": "How to redirect output to a file from within cron?",
    "question_body": "I have a backup script which I need to run at a particular time of a day so I am using \ncron\n for this task and from within cron am also trying to redirect the output of backup script to a \nlogfile\n.\n\n\ncrontab -e\n\n\n\n```bash\n*/1 * * * * /home/ranveer/backup.sh &>> /home/ranveer/backup.log\n```\n\n\n\nIn the above cron entry I am redirecting both \nstderr and stdout\n to a log file.\n\n\nThe above cron job executes fine according to \nsyslog\n  and it performs the task mentioned in the \nbackup.sh\n file but it doesn't write anything to the log file.\n\n\n/var/log/syslog\n\n\n\n```bash\nOct 19 20:26:01 ranveer CRON[15214]: (ranveer) CMD (/home/ranveer/backup.sh &>> /home/ranveer/backup.log)\n```\n\n\n\nWhen I run the script from cli it works as required and output is written to a log file\n\n\n\n```bash\nranveer@ranveer:~$ ./backup.sh &>> backup.log \nranveer@ranveer:~$ cat backup.log\nFri Oct 19 20:28:01 IST 2012\nsuccessfully copied testdir\ntest.txt successfully copied\n-------------------------------------------------------------------------------------\nranveer@ranveer:~$\n```\n\n\n\nSo, why the output of file is not getting redirected to the file from within cron.",
    "answer": "I solved the problem. There are two ways:\n\n\nM1\n\n\nChange the redirection from \n&>>\n to \n2>&1\n. So now \ncrontab -e\n looks like\n\n\n\n```bash\n*/1 * * * * /home/ranveer/vimbackup.sh >> /home/ranveer/vimbackup.log 2>&1\n```\n\n\n\nI believe\n the above works because by default \ncron\n is using \nsh\n to run the task instead of \nbash\n so \n&>>\n is not supported by \nsh\n.\n\n\nM2\n\n\nChange the default shell by adding \nSHELL=/bin/bash\n in the \ncrontab -e\n file.",
    "url": "https://unix.stackexchange.com/questions/52330/how-to-redirect-output-to-a-file-from-within-cron"
  },
  {
    "question_title": "How to combine 2 -name conditions in find?",
    "question_body": "I would like to search for files that would not match 2 \n-name\n conditions. I can do it like so : \n\n\n\n```bash\nfind /media/d/ -type f -size +50M ! -name \"*deb\" ! -name \"*vmdk\"\n```\n\n\n\nand this will yield proper result but can I join these 2 condition with OR somehow ?",
    "answer": "yes, you can:\n\n\n\n```bash\nfind /media/d/ -type f -size +50M ! \\( -name \"*deb\" -o -name \"*vmdk\" \\)\n```\n\n\n\nExplanation from the \nPOSIX spec\n:\n\n\n\n> ! expression : Negation of a primary; the unary NOT operator.\n> ( expression ): True if expression is true.\n> expression -o expression: Alternation of primaries; the OR operator. The second expression shall not be evaluated if the first expression is true.\n\n\n\nNote that parenthesis, both opening and closing, are prefixed by a backslash (\n\\\n) to prevent evaluation by the shell.",
    "url": "https://unix.stackexchange.com/questions/50612/how-to-combine-2-name-conditions-in-find"
  },
  {
    "question_title": "gzip - redirection or piping?",
    "question_body": "I will be backing up a large (750GB) disk to an external USB disk using dd.\n\nShould I be using redirection or piping? Which is more efficient? Or is there a difference?\n\n\nAlso, what is the best block size? USB is likely to be the bottleneck here.\n\n\n\n```bash\ndd if=/dev/sda bs=1M | gzip -c > /mnt/sdb1/backups/disk.img.gz\n\ngzip -dc /mnt/sdb1/backups/disk.img.gz | dd of=/dev/sda bs=1M\n```\n\n\n\nvs\n\n\n\n```bash\ndd if=/dev/sda bs=1M | gzip -c | dd of=/mnt/sdb1/backups/disk.img.gz\n\ndd if=/mnt/sdb1/backups/disk.img.gz | gzip -dc | dd of=/dev/sda bs=1M\n```\n\n\n\nThanks.",
    "answer": "You don't need to use \ndd\n or piping at all.\n\n\n\n```bash\n</dev/sda gzip >/mnt/sdb1/backups/disk.img.gz\n</mnt/sdb1/backups/disk.img.gz gunzip >/dev/sda\n```\n\n\n\nI once \nmade a benchmark\n and found using \ndd\n slower than \ncat\n for a straight copy between different disks. I would expect the pipe to make any solution involving \ndd\n even slower in this case.",
    "url": "https://unix.stackexchange.com/questions/34003/gzip-redirection-or-piping"
  },
  {
    "question_title": "Gzip decompress on file with other extension?",
    "question_body": "Is it possible to use \ngzip\n to decompress a gzipped file, without the gz extension, and without moving the file?",
    "answer": "You can pass the \n-S\n option to use a suffix other than \n.gz\n.\n\n\n\n```bash\ngunzip -S .compressed file.compressed\n```\n\n\n\nIf you want the uncompressed file to have some other name, run\n\n\n\n```bash\ngzip -dc <compressed-file >uncompressed-file\ngunzip <compressed-file >uncompressed-file\n```\n\n\n\n(these commands are equivalent).\n\n\nNormally unzipping restores the name and date of the original file (when it was compressed); this doesn't happen with \n-c\n.\n\n\nIf you want the compressed file and the uncompressed file to have the same name, you can't do it directly, you need to either rename the compressed file or rename the uncompressed file. In particular, \ngzip\n removes and recreates its target file, so if you need to modify the file in place because you don't have write permission in the directory, you need to use \n-c\n or redirection.\n\n\n\n```bash\ncp somefile /tmp\ngunzip </tmp/somefile >|somefile\n```\n\n\n\nNote that \ngunzip <somefile >somefile\n will not work, because the \ngunzip\n process would see a file truncated to 0 bytes when it starts reading. If you could invoke the truncation, then \ngunzip\n would feed back on its own output; either way, this one can't be done in place.",
    "url": "https://unix.stackexchange.com/questions/32953/gzip-decompress-on-file-with-other-extension"
  },
  {
    "question_title": "How can I best copy large numbers of small files over scp?",
    "question_body": "I have a directory that's got several gigabytes and several thousand small files. I want to copy it over the network with scp more than once. CPU time on the source and destination machines is cheap, but the network overhead added by copying each file individually is huge. I would tar/gzip it up and ship it over, but the source machine is short on disk.\n\n\nIs there a way for me to pipe the output of \ntar -czf <output> <directory>\n to scp? If not, is there another easy solution? My source machine is ancient (SunOS) so I'd rather not go installing things on it.",
    "answer": "You can pipe tar across an ssh session:\n\n\n\n```bash\n$ tar czf - <files> | ssh user@host \"cd /wherever && tar xvzf -\"\n```",
    "url": "https://unix.stackexchange.com/questions/10026/how-can-i-best-copy-large-numbers-of-small-files-over-scp"
  },
  {
    "question_title": "tar exits on &quot;Cannot stat: No such file of directory&quot;, why?",
    "question_body": "I'm trying to create tar.gz file using the following command:\n\n\n\n```bash\nsudo tar -vcfz dvr_rdk_v1.tar.gz dvr_rdk/\n```\n\n\n\nIt then start to create files (many files in folder), but then I get the following error:\n\n\n\n```bash\ntar: dvr_rdk_v1.tar.gz: Cannot stat: No such file or directory\ntar: Exiting with failure status due to previous errors\n```\n\n\n\nI don't see any description of this error, what does it mean?",
    "answer": "Remove \n-\n from \nvcfz\n options. \ntar\n does not need hyphen for options.\n\n\nWith a hyphen, the argument for the \n-f\n option is \nz\n. So the command is in effect trying to archive \ndvr_rdk_v1.tar.gz\n and \ndvr_rdk\n into an archive called \nz\n. Without the hyphen, the semantics of the options changes, so that the next argument on the command line, i.e. your archive's filename, becomes the argument to the \nf\n flag.\n\n\nAlso check your write permission to the directory from which you are executing the command.",
    "url": "https://unix.stackexchange.com/questions/149494/tar-exits-on-cannot-stat-no-such-file-of-directory-why"
  },
  {
    "question_title": "When is double-quoting necessary?",
    "question_body": "The old advice used to be to double-quote any expression involving a \n$VARIABLE\n, at least if one wanted it to be interpreted by the shell as one single item, otherwise, any spaces in the content of \n$VARIABLE\n would throw off the shell.\n\n\nI understand, however, that in more recent versions of shells, double-quoting is no longer always needed (at least for the purpose described above).  For instance, in \nbash\n:\n\n\n\n```bash\n% FOO='bar baz'\n% [ $FOO = 'bar baz' ] && echo OK\nbash: [: too many arguments\n% [[ $FOO = 'bar baz' ]] && echo OK\nOK\n% touch 'bar baz'\n% ls $FOO\nls: cannot access bar: No such file or directory\nls: cannot access baz: No such file or directory\n```\n\n\n\nIn \nzsh\n, on the other hand, the same three commands succeed.  Therefore, based on this experiment, it seems that, in \nbash\n, one can omit the double quotes inside \n[[ ... ]]\n, but not inside \n[ ... ]\n nor in command-line arguments, whereas, in \nzsh\n, the double quotes may be omitted in all these cases.\n\n\nBut inferring general rules from anecdotal examples like the above is a chancy proposition.  It would be nice to see a summary of when double-quoting is necessary.  I'm primarily interested in \nzsh\n, \nbash\n, and \n/bin/sh\n.",
    "answer": "First, separate zsh from the rest. It's not a matter of old vs modern shells: zsh behaves differently. The zsh designers decided to make it incompatible with traditional shells (Bourne, ksh, bash), but easier to use.\n\n\nSecond, it is far easier to use double quotes all the time than to remember when they are needed. They are needed most of the time, so you'll need to learn when they aren't needed, not when they are needed.\n\n\nIn a nutshell, \ndouble quotes are necessary wherever a list of words or a pattern is expected\n. They are optional in contexts where a single raw string is expected by the parser.\n\n\nWhat happens without quotes\n\n\nNote that without double quotes, two things happen.\n\n\n\n\nFirst, the result of the expansion (the value of the variable for a parameter substitution like \n$foo\n or \n${foo}\n, or the output of the command for a command substitution like \n$(foo)\n) is split into words wherever it contains whitespace.\n\nMore precisely, the result of the expansion is split at each character that appears in the value of the \nIFS\n variable (separator character). If a sequence of separator characters contains whitespace (space, tab, or newline), the whitespace counts as a single character; leading, trailing, or repeated non-whitespace separators lead to empty fields. For example, with \nIFS=\" :\"\n (space and colon), \n:one::two : three: :four \n produces empty fields before \none\n, between \none\n and \ntwo\n, and (a single one) between \nthree\n and \nfour\n.\n\n\nEach field that results from splitting is interpreted as a glob (a wildcard pattern) if it contains one of the characters \n[*?\n. If that pattern matches one or more file names, the pattern is replaced by the list of matching file names.\n\n\n\n\nAn unquoted variable expansion \n$foo\n is colloquially known as the “split+glob operator”, in contrast with \n\"$foo\"\n which just takes the value of the variable \nfoo\n. The same goes for command substitution: \n\"$(foo)\"\n is a command substitution, \n$(foo)\n is a command substitution followed by split+glob.\n\n\nWhere you can omit the double quotes\n\n\nHere are all the cases I can think of in a Bourne-style shell where you can write a variable or command substitution without double quotes, and the value is interpreted literally.\n\n\n\n\nOn the right-hand side of a scalar (not array) variable assignment.\n\n\n\n```bash\nvar=$stuff\n a_single_star=*\n```\n\n\n\nNote that you do need the double quotes after \nexport\n or \nreadonly\n, because in a few shells, they are still ordinary builtins, not a keyword. This is only true in some shells such as some older versions of dash, older versions of zsh (in sh emulation), yash, or posh; in bash, ksh, newer versions of dash and zsh \nexport\n / \nreadonly\n and co are treated specially as dual builtin / keyword (under some conditions) as POSIX now more clearly requires.\n\n\n\n```bash\nexport VAR=\"$stuff\"\n```\n\n\n\n\n\nIn a \ncase\n statement.\n\n\n\n```bash\ncase $var in …\n```\n\n\n\nNote that you do need double quotes in a case pattern. Word splitting doesn't happen in a case pattern, but an unquoted variable is interpreted as a glob-style pattern whereas a quoted variable is interpreted as a literal string.\n\n\n\n```bash\na_star='a*'\n case $var in\n   \"$a_star\") echo \"'$var' is the two characters a, *\";;\n    $a_star) echo \"'$var' begins with a\";;\n esac\n```\n\n\n\n\n\nWithin double brackets. Double brackets are shell special syntax.\n\n\n\n```bash\n[[ -e $filename ]]\n```\n\n\n\nExcept that you do need double quotes where a pattern or regular expression is expected: on the right-hand side of \n=\n or \n==\n or \n!=\n or \n=~\n (though for the latter, behaviour varies between shells).\n\n\n\n```bash\na_star='a*'\n if [[ $var == \"$a_star\" ]]; then echo \"'$var' is the two characters a, *\"\n elif [[ $var == $a_star ]]; then echo \"'$var' begins with a\"\n fi\n```\n\n\n\nYou do need double quotes as usual within single brackets \n[ … ]\n because they are ordinary shell syntax (it's a command that happens to be called \n[\n). See \nWhy does parameter expansion with spaces without quotes work inside double brackets \"[[\" but not inside single brackets \"[\"?\n.\n\n\n\n\nIn a redirection in non-interactive POSIX shells (not \nbash\n, nor \nksh88\n).\n\n\n\n```bash\necho \"hello world\" >$filename\n```\n\n\n\nSome shells, when interactive, do treat the value of the variable as a wildcard pattern. POSIX prohibits that behaviour in non-interactive shells, but a few shells including bash (except in POSIX mode) and ksh88  (including when found as the (supposedly) POSIX \nsh\n of some commercial Unices like Solaris) still do it there (\nbash\n does also attempt \nsplitting\n and the redirection fails unless that \nsplit+globbing\n results in exactly one word), which is why it's better to quote targets of redirections in a \nsh\n script in case you want to convert it to a \nbash\n script some day, or run it on a system where \nsh\n is non-compliant on that point, or it may be \nsourced\n from interactive shells.\n\n\n\n\nInside an arithmetic expression. In fact, you need to leave the quotes out in order for a variable to be parsed as an arithmetic expression in several shells.\n\n\n\n```bash\nexpr=2*2\n echo \"$(($expr))\"\n```\n\n\n\nHowever, you do need the quotes around the arithmetic expansion as it is subject to word splitting in most shells as POSIX requires (!?).\n\n\n\n\nIn an associative array subscript.\n\n\n\n```bash\ntypeset -A a\n i='foo bar*qux'\n a[foo\\ bar\\*qux]=hello\n echo \"${a[$i]}\"\n```\n\n\n\n\n\n\n\nAn unquoted variable and command substitution can be useful in some rare circumstances:\n\n\n\n\nWhen the variable value or command output consists of a list of glob patterns and you want to expand these patterns to the list of matching files.\n\n\nWhen you know that the value doesn't contain any wildcard character, that \n$IFS\n was not modified and you want to split it at whitespace (well, only space, tab and newline) characters.\n\n\nWhen you want to split a value at a certain character: disable globbing with \nset -o noglob\n / \nset -f\n, set \nIFS\n to the separator character (or leave it alone to split at whitespace), then do the expansion.\n\n\n\n\nZsh\n\n\nIn zsh, you can omit the double quotes most of the time, with a few exceptions.\n\n\n\n\n$var\n never expands to multiple words (assuming \nvar\n isn't an array), but it expands to the empty list (as opposed to a list containing a single, empty word) if the value of \nvar\n is the empty string. Contrast:\n\n\n\n```bash\nvar=\n print -l -- $var foo        # prints just foo\n print -l -- \"$var\" foo      # prints an empty line, then foo\n```\n\n\n\nSimilarly, \n\"${array[@]}\"\n expands to all the elements of the array, while \n$array\n only expands to the non-empty elements.\n\n\n\n\nLike in ksh and bash, inside \n[[ … ]]\n, a variable in the right-hand side of a \n==\n, \n!=\n or \n=~\n operator needs to be double-quoted if it contains a string, and not quoted if it contains a pattern/regex: \np='a*'; [[ abc == $p ]]\n is true but \np='a*'; [[ abc == \"$p\" ]]\n is false.\n\n\n\n\nThe \n@\n parameter expansion flag sometimes requires double quotes around the whole substitution: \n\"${(@)foo}\"\n.\n\n\n\n\nCommand substitution undergoes field splitting if unquoted: \necho $(echo 'a'; echo '*')\n prints \na *\n (with a single space) whereas \necho \"$(echo 'a'; echo '*')\"\n prints the unmodified two-line string. Use \n\"$(somecommand)\"\n to get the output of the command in a single word, sans final newlines. Use \n\"${$(somecommand; echo .)%?}\"\n to get the exact output of the command including final newlines. Use \n\"${(@f)$(somecommand)}\"\n to get an array of lines from the command's output (removing trailing empty lines if any though).",
    "url": "https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary"
  },
  {
    "question_title": "Why does my shell script choke on whitespace or other special characters?",
    "question_body": "… or an introductory guide to robust filename handling and other string passing in shell scripts.\n\n\nI wrote a shell script which works well most of the time. But it chokes on some inputs (e.g. on some file names).\n\n\nI encountered a problem such as the following:\n\n\n\n\nI have a file name containing a space \nhello world\n, and it was treated as two separate files \nhello\n and \nworld\n.\n\n\nI have an input line with two consecutive spaces and they shrank to one in the input.\n\n\nLeading and trailing whitespace disappears from input lines.\n\n\nSometimes, when the input contains one of the characters \n\\[*?\n, they are replaced by some text which is actually the names of some files.\n\n\nThere is an apostrophe \n'\n (or a double quote \n\"\n) in the input, and things got weird after that point.\n\n\nThere is a backslash in the input (or: I am using Cygwin and some of my file names have Windows-style \n\\\n separators).\n\n\n\n\nWhat is going on, and how do I fix this?",
    "answer": "Always use double quotes around variable substitutions and command substitutions: \n\"$foo\"\n, \n\"$(foo)\"\n\n\nIf you use \n$foo\n unquoted, your script will choke on input or parameters (or command output, with \n$(foo)\n) containing whitespace or \n\\[*?\n.\n\n\nThere, you can stop reading. Well, ok, here are a few more:\n\n\n\n\nread\n — \nTo read input line by line with the \nread\n builtin, use \nwhile IFS= read -r line; do …\n\nPlain \nread\n treats backslashes and whitespace specially.\n\n\nxargs\n — \nAvoid \nxargs\n. If you must use \nxargs\n, make that \nxargs -0\n. Instead of \nfind … | xargs\n, \nprefer \nfind … -exec …\n.\n\n\nxargs\n treats whitespace and the characters \n\\\"'\n specially.\n\n\n\n\nThis answer applies to Bourne/POSIX-style shells (\nsh\n, \nash\n, \ndash\n, \nbash\n, \nksh\n, \nmksh\n, \nyash\n…). Zsh users should skip it and read the end of \nWhen is double-quoting necessary?\n instead. If you want the whole nitty-gritty, \nread the standard\n or your shell's manual.\n\n\n\n\nNote that the explanations below contains a few approximations (statements that are true in most conditions but can be affected by the surrounding context or by configuration).\n\n\nWhy do I need to write \n\"$foo\"\n? What happens without the quotes?\n\n\n$foo\n does not mean “take the value of the variable \nfoo\n”. It means something much more complex:\n\n\n\n\nFirst, take the value of the variable.\n\n\nField splitting: treat that value as a whitespace-separated list of fields, and build the resulting list. For example, if the variable contains \nfoo *  bar ​\n then the result of this step is the 3-element list \nfoo\n, \n*\n, \nbar\n.\n\n\nFilename generation: treat each field as a glob, i.e. as a wildcard pattern, and replace it by the list of file names that match this pattern. If the pattern doesn't match any files, it is left unmodified. In our example, this results in the list containing \nfoo\n, following by the list of files in the current directory, and finally \nbar\n. If the current directory is empty, the result is \nfoo\n, \n*\n, \nbar\n.\n\n\n\n\nNote that the result is a list of strings. There are two contexts in shell syntax: list context and string context. Field splitting and filename generation only happen in list context, but that's most of the time. Double quotes delimit a string context: the whole double-quoted string is a single string, not to be split. (Exception: \n\"$@\"\n to expand to the list of positional parameters, e.g. \n\"$@\"\n is equivalent to \n\"$1\" \"$2\" \"$3\"\n if there are three positional parameters. See \nWhat is the difference between $* and $@?\n)\n\n\nThe same happens to command substitution with \n$(foo)\n or with \n`foo`\n. On a side note, don't use \n`foo`\n: its quoting rules are weird and non-portable, and all modern shells support \n$(foo)\n which is absolutely equivalent except for having intuitive quoting rules.\n\n\nThe output of arithmetic substitution also undergoes the same expansions, but that isn't normally a concern as it only contains non-expandable characters (assuming \nIFS\n doesn't contain digits or \n-\n).\n\n\nSee \nWhen is double-quoting necessary?\n for more details about the cases when you can leave out the quotes.\n\n\nUnless you mean for all this rigmarole to happen, just remember to always use double quotes around variable and command substitutions. Do take care: leaving out the quotes can lead not just to errors but to \nsecurity holes\n.\n\n\nHow do I process a list of file names?\n\n\nIf you write \nmyfiles=\"file1 file2\"\n, with spaces to separate the files, this can't work with file names containing spaces. Unix file names can contain any character other than \n/\n (which is always a directory separator) and null bytes (which you can't use in shell scripts with most shells).\n\n\nSame problem with \nmyfiles=*.txt; … process $myfiles\n. When you do this, the variable \nmyfiles\n contains the 5-character string \n*.txt\n, and it's when you write \n$myfiles\n that the wildcard is expanded. This example will actually work, until you change your script to be \nmyfiles=\"$someprefix*.txt\"; … process $myfiles\n. If \nsomeprefix\n is set to \nfinal report\n, this won't work.\n\n\nTo process a list of any kind (such as file names), put it in an array. This requires mksh, ksh93, yash or bash (or zsh, which doesn't have all these quoting issues); a plain POSIX shell (such as ash or dash) doesn't have array variables.\n\n\n\n```bash\nmyfiles=(\"$someprefix\"*.txt)\nprocess \"${myfiles[@]}\"\n```\n\n\n\nKsh88 has array variables with a different assignment syntax \nset -A myfiles \"someprefix\"*.txt\n (see \nassignation variable under different ksh environment\n if you need ksh88/bash portability). Bourne/POSIX-style shells have a single one array, the array of positional parameters \n\"$@\"\n which you set with \nset\n and which is local to a function:\n\n\n\n```bash\nset -- \"$someprefix\"*.txt\nprocess -- \"$@\"\n```\n\n\n\nWhat about file names that begin with \n-\n?\n\n\nOn a related note, keep in mind that file names can begin with a \n-\n (dash/minus), which most commands interpret as denoting an option. Some commands (like \nsh\n, \nset\n or \nsort\n) also accept options that start with \n+\n. If you have a file name that begins with a variable part, be sure to pass \n--\n before it, as in the snippet above. This indicates to the command that it has reached the end of options, so anything after that is a file name even if it starts with \n-\n or \n+\n.\n\n\nAlternatively, you can make sure that your file names begin with a character other than \n-\n. Absolute file names begin with \n/\n, and you can add \n./\n at the beginning of relative names. The following snippet turns the content of the variable \nf\n into a “safe” way of referring to the same file that's guaranteed not to start with \n-\n nor \n+\n.\n\n\n\n```bash\ncase \"$f\" in -* | +*) \"f=./$f\";; esac\n```\n\n\n\nOn a final note on this topic, beware that some commands interpret \n-\n as meaning standard input or standard output, even after \n--\n. If you need to refer to an actual file named \n-\n, or if you're calling such a program and you don't want it to read from stdin or write to stdout, make sure to rewrite \n-\n as above. See \nWhat is the difference between \"du -sh *\" and \"du -sh ./*\"?\n for further discussion.\n\n\nHow do I store a command in a variable?\n\n\n“Command” can mean three things: a command name (the name as an executable, with or without full path, or the name of a function, builtin or alias), a command name with arguments, or a piece of shell code. There are accordingly different ways of storing them in a variable.\n\n\nIf you have a command name, just store it and use the variable with double quotes as usual.\n\n\n\n```bash\ncommand_path=\"$1\"\n…\n\"$command_path\" --option --message=\"hello world\"\n```\n\n\n\nIf you have a command with arguments, the problem is the same as with a list of file names above: this is a list of strings, not a string. You can't just stuff the arguments into a single string with spaces in between, because if you do that you can't tell the difference between spaces that are part of arguments and spaces that separate arguments. If your shell has arrays, you can use them.\n\n\n\n```bash\ncmd=(/path/to/executable --option --message=\"hello world\" --)\ncmd=(\"${cmd[@]}\" \"$file1\" \"$file2\")\n\"${cmd[@]}\"\n```\n\n\n\nWhat if you're using a shell without arrays? You can still use the positional parameters, if you don't mind modifying them.\n\n\n\n```bash\nset -- /path/to/executable --option --message=\"hello world\" --\nset -- \"$@\" \"$file1\" \"$file2\"\n\"$@\"\n```\n\n\n\nWhat if you need to store a complex shell command, e.g. with redirections, pipes, etc.? Or if you don't want to modify the positional parameters? Then you can build a string containing the command, and use the \neval\n builtin.\n\n\n\n```bash\ncode='/path/to/executable --option --message=\"hello world\" -- /path/to/file1 | grep \"interesting stuff\"'\neval \"$code\"\n```\n\n\n\nNote the nested quotes in the definition of \ncode\n: the single quotes \n'…'\n delimit a string literal, so that the value of the variable \ncode\n is the string \n/path/to/executable --option --message=\"hello world\" -- /path/to/file1\n. The \neval\n builtin tells the shell to parse the string passed as an argument as if it appeared in the script, so at that point the quotes and pipe are parsed, etc.\n\n\nUsing \neval\n is tricky. Think carefully about what gets parsed when. In particular, you can't just stuff a file name into the code: you need to quote it, just like you would if it was in a source code file. There's no direct way to do that. Something like \ncode=\"$code $filename\"\n breaks if the file name contains any shell special character (spaces, \n$\n, \n;\n, \n|\n, \n<\n, \n>\n, etc.). \ncode=\"$code \\\"$filename\\\"\"\n still breaks on \n\"$\\`\n. Even \ncode=\"$code '$filename'\"\n breaks if the file name contains a \n'\n. There are two solutions.\n\n\n\n\nAdd a layer of quotes around the file name. The easiest way to do that is to add single quotes around it, and replace single quotes by \n'\\''\n.\n\n\n\n```bash\nquoted_filename=$(printf %s. \"$filename\" | sed \"s/'/'\\\\\\\\''/g\")\n code=\"$code '${quoted_filename%.}'\"\n```\n\n\n\n\n\nKeep the variable expansion inside the code, so that it's looked up when the code is evaluated, not when the code fragment is built. This is simpler but only works if the variable is still around with the same value at the time the code is executed, not e.g. if the code is built in a loop.\n\n\n\n```bash\ncode=\"$code \\\"\\$filename\\\"\"\n```\n\n\n\n\n\n\n\nFinally, do you really need a variable containing code? The most natural way to give a name to a code block is to define a function.\n\n\nWhat's up with \nread\n?\n\n\nWithout \n-r\n, \nread\n allows continuation lines — this is a single logical line of input:\n\n\n\n```bash\nhello \\\nworld\n```\n\n\n\nread\n splits the input line into fields delimited by characters in \n$IFS\n (without \n-r\n, backslash also escapes those). For example, if the input is a line containing three words, then \nread first second third\n sets \nfirst\n to the first word of input, \nsecond\n to the second word and \nthird\n to the third word. If there are more words, the last variable contains everything that's left after setting the preceding ones. Leading and trailing whitespace are trimmed.\n\n\nSetting \nIFS\n to the empty string avoids any trimming. See \nWhy is `while IFS= read` used so often, instead of `IFS=; while read..`?\n for a longer explanation.\n\n\nWhat's wrong with \nxargs\n?\n\n\nThe input format of \nxargs\n is whitespace-separated strings which can optionally be single- or double-quoted. No standard tool outputs this format.\n\n\nxargs -L1\n or \nxargs -l\n is not to split the input on \nlines\n, but to run one command per line of input (that line still split to make up the arguments, and continued on the next line if ending in blanks).\n\n\nxargs -I PLACEHOLDER\n does use one line of input to substitute the \nPLACEHOLDER\n but quotes and backslashes are still processed and leading blanks trimmed.\n\n\nYou can use \nxargs -r0\n where applicable (and where available: GNU (Linux, Cygwin), BusyBox, BSDs, OSX, but it isn't in POSIX). That's safe, because null bytes can't appear in most data, in particular in file names and external command arguments. To produce a null-separated list of file names, use \nfind … -print0\n (or you can use \nfind … -exec …\n as explained below).\n\n\nHow do I process files found by \nfind\n?\n\n\n\n```bash\nfind … -exec some_command a_parameter another_parameter {} +\n```\n\n\n\nsome_command\n needs to be an external command, it can't be a shell function or alias. If you need to invoke a shell to process the files, call \nsh\n explicitly.\n\n\n\n```bash\nfind … -exec sh -c '\n  for x do\n    … # process the file \"$x\"\n  done\n' find-sh {} +\n```\n\n\n\nI have some other question\n\n\nBrowse the \nquoting\n tag on this site, or \nshell\n or \nshell-script\n. (Click on “learn more…” to see some general tips and a hand-selected list of common questions.) If you've searched and you can't find an answer, \nask away\n.",
    "url": "https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters"
  },
  {
    "question_title": "Preventing grep from causing premature termination of &quot;bash -e&quot; script",
    "question_body": "This script does not output \nafter\n:\n\n\n\n```bash\n#!/bin/bash -e\n\necho \"before\"\n\necho \"anything\" | grep e # it would if I searched for 'y' instead\n\necho \"after\"\nexit\n```\n\n\n\nI could solve this by removing the \n-e\n option on the shebang line, but I wish to keep it so my script stops if there is an error. I do not consider \ngrep\n finding no match as an error. How may I prevent it from exiting so abruptly?",
    "answer": "```bash\necho \"anything\" | { grep e || true; }\n```\n\n\n\nExplanation:\n\n\n\n\nThis will throw an error\n\n\n```bash\n$ echo \"anything\" | grep e\n$ echo $?\n1\n```\n\n\n\n\n\nThis will not throw an error\n\n\n```bash\n$ echo \"anything\" | { grep e || true; }\n$ echo $?\n0\n```\n\n\n\n\n\nDopeGhoti's \"no-op\" version (Potentially avoids spawning a process, if \ntrue\n is not a builtin), this will not throw an error\n\n\n```bash\n$ echo \"anything\" | { grep e || :; }\n$ echo $?\n0\n```\n\n\n\n\n\n\n\nThe \n||\n means \"or\". If the first part of the command \"fails\" (meaning \ngrep e\n returns a non-zero exit code) then the part after the \n||\n is executed, succeeds and returns zero as the exit code  (\ntrue\n always returns zero).",
    "url": "https://unix.stackexchange.com/questions/330660/preventing-grep-from-causing-premature-termination-of-bash-e-script"
  },
  {
    "question_title": "Counting occurrences of word in text file",
    "question_body": "I have a text file containing tweets and I'm required to count the number of times a word is mentioned in the tweet. For example, the file contains:\n\n\n\n```bash\nApple iPhone X is going to worth a fortune\nThe iPhone X is Apple's latest flagship iPhone. How will it pit against it's competitors?\n```\n\n\n\nAnd let's say I want to count how many times the word iPhone is mentioned in the file. So here's what I've tried.\n\n\n\n```bash\ncut -f 1 Tweet_Data | grep -i \"iPhone\" | wc -l\n```\n\n\n\nit certainly works but I'm confused about the 'wc' command in unix. What is the difference if I try something like:\n\n\n\n```bash\ncut -f 1 Tweet_Data | grep -c \"iPhone\"\n```\n\n\n\nwhere -c is used instead? Both of these yield different results in a large file full of tweets and I'm confused on how it works. Which method is the correct way of counting the occurrence?",
    "answer": "Given such a requirement, I would use a GNU grep (for the \n-o\n option\n), \nthen\n pass it through \nwc\n to count the total number of occurrences:\n\n\n\n```bash\n$ grep -o -i iphone Tweet_Data | wc -l\n3\n```\n\n\n\nPlain \ngrep -c\n on the data will count the number of \nlines\n that match, not the total number of \nwords\n that match. Using the \n-o\n option tells grep to output each match on its own line, no matter how many times the match was found in the original line.\n\n\nwc -l\n tells the \nwc\n utility to count the number of lines. After grep puts each match in its own line, this is the total number of occurrences of the word in the input.\n\n\n\n\nIf GNU grep is not available (or desired), you could transform the input with \ntr\n so that each word is on its own line, then use \ngrep -c\n to count:\n\n\n\n```bash\n$ tr '[:space:]' '[\\n*]' < Tweet_Data | grep -i -c iphone\n3\n```",
    "url": "https://unix.stackexchange.com/questions/398413/counting-occurrences-of-word-in-text-file"
  },
  {
    "question_title": "Can I redirect output to a log file and background a process at the same time?",
    "question_body": "Can I redirect output to a log file and a background process at the same time?\n\n\nIn other words, can I do something like this?\n\n\n\n```bash\nnohup java -jar myProgram.jar 2>&1 > output.log &\n```\n\n\n\nOr, is that not a legal command?  Or, do I need to manually move it to the background, like this:\n\n\n\n```bash\njava -jar myProgram.jar 2>$1 > output.log\njobs\n[CTRL-Z]\nbg 1\n```",
    "answer": "One problem with your first command is that you redirect stderr to where stdout is (if you changed the $ to a & as suggested in the comment) and then, you redirected stdout to some log file, but that does not pull along the redirected stderr. You must do it in the other order, first send stdout to where you want it to go, and then send stderr to the address stdout is at\n\n\n\n```bash\nsome_cmd > some_file 2>&1 &\n```\n\n\n\nand then you could throw the & on to send it to the background. Jobs can be accessed with the \njobs\n command. \njobs\n will show you the running jobs, and number them. You could then talk about the jobs using a % followed by the number like \nkill %1\n or so.  \n\n\nAlso, without the & on the end you can suspend the command with \nCtrl\nz\n, use the \nbg\n command to put it in the background and \nfg\n to bring it back to the foreground.  In combination with the \njobs\n command, this is powerful.\n\n\nto clarify the above part about the order you write the commands. Suppose stderr is address 1002, stdout is address 1001, and the file is 1008. The command reads left to right, so the first thing it sees in yours is \n2>&1\n which moves stderr to the address 1001, it then sees \n> file\n which moves stdout to 1008, but keeps stderr at 1001. It does not pull everything pointing at 1001 and move it to 1008, but simply references stdout and moves it to the file.\n\nThe other way around, it moves stdout to 1008, and then moves stderr to the point that stdout is pointing to, 1008 as well. This way both can point to the single file.",
    "url": "https://unix.stackexchange.com/questions/74520/can-i-redirect-output-to-a-log-file-and-background-a-process-at-the-same-time"
  },
  {
    "question_title": "creating a tar archive without including parent directory",
    "question_body": "I am trying to create a graphical program for my script.\n\n\nInside the script I use tar to create a tar archive.\n\n\nFrom the graphical program I get the full name of file that I want to create a tar archive.\n\n\n\n```bash\ntar -cvf temp.tar /home/username/dir1/dir2/selecteddir\n```\n\n\n\nMy tar archive includes home, username, dir1, dir2 and selecteddir while i want tar to create archive only including selecteddir.",
    "answer": "You can use the \n-C\n option of \ntar\n to accomplish this:\n\n\n\n```bash\ntar -C /home/username/dir1/dir2 -cvf temp.tar selecteddir\n```\n\n\n\nFrom the man page of \ntar\n:\n\n\n\n```bash\n-C directory\n         In c and r mode, this changes the directory before adding the following files.  \n         In x mode, change directories after opening the archive but before extracting \n         entries from the archive.\n```",
    "url": "https://unix.stackexchange.com/questions/168357/creating-a-tar-archive-without-including-parent-directory"
  },
  {
    "question_title": "Get `grep` to not output file name",
    "question_body": "When I use \ngrep -o\n to search in multiple files, it outputs each result prefixed with the file name. How can I prevent this prefix? I want the results without the file names.",
    "answer": "With the GNU implementation of \ngrep\n (the one that also introduced \n-o\n) or compatible, you can use the \n-h\n option.\n\n\n\n> ```bash\n> -h, --no-filename\n> Suppress the prefixing of file names on  output.   This  is  the\n> default  when there is only one file (or only standard input) to\n> search.\n> ```\n\n\n\nWith other implementations, you can always concatenate the files with \ncat\n and \ngrep\n that output:\n\n\n\n```bash\ncat ./*.txt | grep regexp\n```\n\n\n\nOr use \nsed\n or \nawk\n instead of \ngrep\n:\n\n\n\n```bash\nawk '/regexp/' ./*.txt\n```\n\n\n\n(extended regexps like with \ngrep -E\n).\n\n\n\n```bash\nsed '/regexp/!d/' ./*.txt\n```\n\n\n\n(basic regexps like with \ngrep\n without \n-E\n. Many \nsed\n implementations now also support a \n-E\n option for extended regexps).",
    "url": "https://unix.stackexchange.com/questions/204607/get-grep-to-not-output-file-name"
  },
  {
    "question_title": "How to cycle through reverse-i-search in Bash?",
    "question_body": "In the GNU bash shell, I can type \nCtrl\n + \nR\n to search for a matching command previously run. E.g., if I type \nCtrl\n + \nR\n and then \"\ngrep\n\", it lists my last \ngrep\n command, and I can hit \nEnter\n to use it.\n\n\nThis only gives one suggestion though. Is there any way to cycle through other previously typed matching commands?",
    "answer": "If I understand the question correctly you should be able to cycle through\nalternatives by repeatedly hitting \nCtrl\n + \nR\n.\n\n\nE.g.:\n\n\n\n\nCtrl\n + \nR\n\n\ngrep\n\n\nCtrl\n + \nR\n\n\nCtrl\n + \nR\n\n...\n\n\n\n\nThat searches backwards through your history.  To search forward instead, use \nCtrl\n + \nS\n, but you may need to have set: \nstty -ixon\n (either by \n.bash_profile\n or manually) prior to that to disable the XON/XOFF feature which takes over \nCtrl\n + \nS\n. If it happens anyway, use \nCtrl\n + \nQ\n to re-enable screen output (More details \nhere\n.)",
    "url": "https://unix.stackexchange.com/questions/73498/how-to-cycle-through-reverse-i-search-in-bash"
  },
  {
    "question_title": "Is `git diff` related to `diff`?",
    "question_body": "Is \ngit diff\n related to \ndiff\n?\n\n\n\n\nIs \ngit diff\n implemented based on \ndiff\n?\n\n\nIs \nthe command line syntax of \ngit diff\n similar to \nthat of \ndiff\n? Does learning one help  using the other much?\n\n\nare their output files following the same format? Can they be both used by \ngit patch\n and \npatch\n? (Is there \ngit patch\n? How is it related to \npatch\n?)\n\n\n\n\nThanks.",
    "answer": "The file format is interoperable.  Git uses the best format, \ndiff -u\n.  It also extends it to represent additional types of changes.\n\n\nThe equivalent to \npatch\n is \ngit apply\n.  It stages the changes in the index as well as applying them to the working tree.\n\n\nI remember \ngit apply\n being stricter than \npatch\n, although the reference documentation doesn't seem to make an explicit comparison.  It does mention several tests / errors which can be enabled or disabled.\n\n\nThe reference documentation also suggests that it could be used as \"a replacement for GNU patch\" - even outside of a git repository, if you use a certain option.",
    "url": "https://unix.stackexchange.com/questions/356652/is-git-diff-related-to-diff"
  },
  {
    "question_title": "Determine if Git working directory is clean from a script",
    "question_body": "I have a script which runs \nrsync\n with a Git working directory as destination. I want the script to have different behavior depending on if the working directory is clean (no changes to commit), or not. For instance, if the output of \ngit status\n is as below, I want the script to exit:\n\n\n\n```bash\ngit status\nAlready up-to-date.\n# On branch master\nnothing to commit (working directory clean)\nEverything up-to-date\n```\n\n\n\nIf the directory is not clean then I would like it to execute some more commands.\n\n\nHow can I check for output like the above in a shell script?",
    "answer": "Parsing the output of \ngit status\n is a bad idea because the output is intended to be human readable, not machine-readable. There's no guarantee that the output will remain the same in future versions of Git or in differently configured environments.\n\n\nUVVs comment\n is on the right track, but unfortunately the return code of \ngit status\n doesn't change when there are uncommitted changes. It does, however, provide the \n--porcelain\n option, which causes the output of \ngit status --porcelain\n to be formatted in an easy-to-parse format for scripts, \nand\n will remain stable across Git versions and regardless of user configuration.\n\n\nWe can use empty output of \ngit status --porcelain\n as an indicator that there are no changes to be committed:\n\n\n\n```bash\nif [ -z \"$(git status --porcelain)\" ]; then \n  # Working directory clean\nelse \n  # Uncommitted changes\nfi\n```\n\n\n\nIf we do not care about untracked files in the working directory, we can use the \n--untracked-files=no\n option to disregard those:\n\n\n\n```bash\nif [ -z \"$(git status --untracked-files=no --porcelain)\" ]; then \n  # Working directory clean excluding untracked files\nelse \n  # Uncommitted changes in tracked files\nfi\n```\n\n\n\nTo make this more robust against conditions which \nactually\n cause \ngit status\n to fail without output to \nstdout\n, we can refine the check to:\n\n\n\n```bash\nif output=$(git status --porcelain) && [ -z \"$output\" ]; then\n  # Working directory clean\nelse \n  # Uncommitted changes\nfi\n```\n\n\n\nIt's also worth noting that, although \ngit status\n does not give meaningful exit code when the working directory is unclean, \ngit diff\n provides the \n--exit-code\n option, which makes it behave similar to the \ndiff\n utility, that is, exiting with status \n1\n when there were differences and \n0\n when none were found.\n\n\nUsing this, we can check for unstaged changes with:\n\n\n\n```bash\ngit diff --exit-code\n```\n\n\n\nand staged, but not committed changes with:\n\n\n\n```bash\ngit diff --cached --exit-code\n```\n\n\n\nAlthough \ngit diff\n can report on untracked files in submodules via appropriate arguments to \n--ignore-submodules\n, unfortunately it seems that there is no way to have it report on untracked files in the actual working directory. If untracked files in the working directory are relevant, \ngit status --porcelain\n is probably the best bet.",
    "url": "https://unix.stackexchange.com/questions/155046/determine-if-git-working-directory-is-clean-from-a-script"
  },
  {
    "question_title": "On-the-fly stream compression that doesn&#39;t spill over into hardware resources?",
    "question_body": "I have 200 GB free disk space, 16 GB of RAM (of which ~1 GB is occupied by the  desktop and kernel) and 6 GB of swap.\n\n\nI have a 240 GB external SSD, with 70 GB used\n1\n and the rest free, which I need to back up to my disk.\n\n\nNormally, I would \ndd if=/dev/sdb of=Desktop/disk.img\n the disk first, and then compress it, but making the image first is not an option since doing so would require far more disk space than I have, even though the compression step will result in the free space being squashed so the final archive can easily fit on my disk.\n\n\ndd\n writes to STDOUT by default, and \ngzip\n can read from STDIN, so in theory I can write \ndd if=/dev/sdb | gzip -9 -\n, but \ngzip\n takes significantly longer to read bytes than \ndd\n can produce them.\n\n\nFrom \nman pipe\n: \n\n\n\n> Data written to the write end of the pipe is buffered by the kernel until it is read from the read end of the pipe.\n\n\n\nI visualise a \n|\n as being like a real pipe -- one application shoving data in and the other taking data out of the pipe's queue as quickly as possible. \n\n\nWhat when the program on the left side writes more data more quickly than the other side of the pipe can hope to process it? Will it cause extreme memory or swap usage, or will the kernel try to create a FIFO on disk, thereby filling up the disk? Or will it just fail with \nSIGPIPE Broken pipe\n if the buffer is too large?\n\n\nBasically, this boils down to two questions:\n \n\n\n\n\nWhat are the implications and outcomes of shoving more data into a pipe than is read at a time?\n\n\nWhat's the reliable way to compress a datastream to disk without putting the entire uncompressed datastream on the disk?\n\n\n\n\nNote 1: I cannot just copy exactly the first 70 used GB and expect to get a working system or filesystem, because of fragmentation and other things which will require the full contents to be intact.",
    "answer": "dd\n reads and writes data one block at a time, and it only ever has one block outstanding. So\n\n\n\n```bash\nvalgrind dd if=/dev/zero status=progress of=/dev/null bs=1M\n```\n\n\n\nshows that \ndd\n uses approximately 1MB of memory. You can play around with the block size, and drop \nvalgrind\n, to see the effect on \ndd\n’s speed.\n\n\nWhen you pipe into \ngzip\n, \ndd\n simply slows down to match \ngzip\n’s speed. Its memory usage doesn’t increase, nor does it cause the kernel to store the buffers on disk (the kernel doesn’t know how to do that, except \nvia\n swap). A broken pipe only happens when one of the ends of the pipe dies; see \nsignal(7)\n and \nwrite(2)\n for details.\n\n\nThus\n\n\n\n```bash\ndd if=... iconv=fullblock bs=1M | gzip -9 > ...\n```\n\n\n\nis a safe way to do what you’re after.\n\n\nWhen piping, the writing process ends up being blocked by the kernel if the reading process isn’t keeping up. You can see this by running\n\n\n\n```bash\nstrace dd if=/dev/zero bs=1M | (sleep 60; cat > /dev/null)\n```\n\n\n\nYou’ll see that \ndd\n reads 1MB, then issues a \nwrite()\n which sits there waiting for one minute while \nsleep\n runs. That’s how both sides of a pipe balance out: the kernel blocks writes if the writing process is too fast, and it blocks reads if the reading process is too fast.",
    "url": "https://unix.stackexchange.com/questions/371789/on-the-fly-stream-compression-that-doesnt-spill-over-into-hardware-resources"
  },
  {
    "question_title": "tar --exclude doesn&#39;t exclude. Why?",
    "question_body": "I have this very simple line in a bash script which executes successfully (i.e. producing the \n_data.tar\n file), except that it \ndoesn't\n exclude the sub-directories it is told exclude via the \n--exclude\n option:\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  --exclude='/data/sub1/*'  --exclude='/data/sub2/*' --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'  /data\n```\n\n\n\nInstead, it produces a \n_data.tar\n file that contains everything under /data, including the files in the subdirectories I wanted to exclude.\n\n\nAny idea why? and how to fix this?\n\n\nUpdate\n I implemented my observations based on the link provided in the first answer below (top level dir first, no whitespace after last exclude):\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1/*'  --exclude='/data/sub2/*'  --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'\n```\n\n\n\nBut that didn't help. All \"excluded\" sub-directories are present in the resulting \n_data.tar\n file.\n\n\nThis is puzzling. Whether this is a bug in current tar (GNU tar 1.23, on a CentOS 6.2, Linux 2.6.32)  or \"extreme sensitivity\" of tar to whitespaces and other easy-to-miss typos, I consider this a bug. For now.\n\n\nThis is horrible\n: I tried the insight suggested below (no trailing \n/*\n) and it still doesn't work in the production script:\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1'  --exclude='/data/sub2'  --exclude='/data/sub3'  --exclude='/data/sub4'\n```\n\n\n\nI can't see any difference between what I tried and what @Richard Perrin tried, except for the quotes and 2 spaces instead of 1. I am going to try this (must wait for the nightly script to run as the directory to be backed up is huge) and report back.\n\n\n\n```bash\n/bin/tar -cf /home/_data.tar  /data --exclude=/data/sub1 --exclude=/data/sub2 --exclude=/data/sub3 --exclude=/data/sub4\n```\n\n\n\nI am beginning to think that all these \ntar --exclude\n sensitivities aren't tar's but something in my environment, but then what could that be?\n\n\nIt worked!\n The last variation tried (no single-quotes and single-space instead of double-space between the \n--exclude\ns) tested working. Weird but accepting.\n\n\nUnbelievable!\n It turns out that an older version of \ntar\n (1.15.1) would only exclude if the top-level dir is \nlast\n on the command line. This is the exact opposite of how version 1.23 requires. FYI.",
    "answer": "It may be that your version of \ntar\n requires that the \n--exclude\n options have to be placed at the beginning of the \ntar\n command.\n\n\nSee: \nhttps://stackoverflow.com/q/984204\n\n\n\n```bash\ntar --exclude='./folder' --exclude='./upload/folder2' \\\n    -zcvf /backup/filename.tgz .\n```\n\n\n\nSee: \nhttp://mandrivausers.org/index.php?/topic/8585-multiple-exclude-in-tar/\n\n\n\n```bash\ntar --exclude=<first> --exclude=<second> -cjf backupfile.bz2 /home/*\n```\n\n\n\nAlternative:\n\n\n\n```bash\nEXCLD='first second third'\ntar -X <(for i in ${EXCLD}; do echo $i; done) -cjf backupfile.bz2 /home/*\n```\n\n\n\nYet another \ntar\n command tip is from \nhere\n:\n\n\n\n```bash\ntar cvfz myproject.tgz --exclude='path/dir_to_exclude1' \\\n                       --exclude='path/dir_to_exclude2' myproject\n```",
    "url": "https://unix.stackexchange.com/questions/32845/tar-exclude-doesnt-exclude-why"
  },
  {
    "question_title": "Dereferencing hard links",
    "question_body": "In the manual page of \ntar\n command, an option for following hard links is listed.\n\n\n\n```bash\n-h, --dereference\n      follow symlinks; archive and dump the files they point to\n\n--hard-dereference\n      follow hard links; archive and dump the files they refer to\n```\n\n\n\nHow does \ntar\n know that a file is a hard link? How does it \nfollow\n it?\n\n\nWhat if I don't choose this option? How does it \nnot\n hard-dereference?",
    "answer": "By default, if you tell \ntar\n to archive a file with hard links, and more than one such link is included among the files to be archived, it archives the file only once, and records the second (and any additional names) as hard links.  This means that when you extract that archive, the hard links will be restored.\n\n\nIf you use the \n--hard-dereference\n option, then \ntar\n does \nnot\n preserve hard links.  Instead, it treats them as independent files that just happen to have the same contents and metadata.  When you extract the archive, the files will be independent.\n\n\nNote:  It recognizes hard links by first checking the link count of the file.  It records the device number and inode of each file with more than one link, and uses that to detect when the same file is being archived again.  (When you use \n--hard-dereference\n, it does not do this.)",
    "url": "https://unix.stackexchange.com/questions/43037/dereferencing-hard-links"
  },
  {
    "question_title": "How do I compress files in-place?",
    "question_body": "I have a machine with 90% hard-disk usage.  I want to compress its 500+ log files into a smaller new file. However, the hard disk is too small to keep both the original files and the compressed ones.\n\n\nSo what I need is to compress all log files into a single new file one by one, deleting each original once compressed.\n\n\nHow can I do that in Linux?",
    "answer": "gzip\n or \nbzip2\n will compress the file and remove the non-compressed one automatically (this is their default behaviour).\n\n\nHowever, keep in mind that while the compressing process, both files will exists.\n\n\nIf you want to compress log files (ie: files containing text), you may prefer \nbzip2\n, since it has a better ratio for text files.\n\n\n\n```bash\nbzip2 -9 myfile       # will produce myfile.bz2\n```\n\n\n\nComparison and examples:\n\n\n\n```bash\n$ ls -l myfile\n-rw-rw-r-- 1 apaul apaul 585999 29 april 10:09 myfile\n\n$ bzip2 -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 115780 29 april 10:09 myfile.bz2\n\n$ bunzip2 myfile.bz2\n\n$ gzip -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 146234 29 april 10:09 myfile.gz\n```\n\n\n\nUPDATE\n as @Jjoao told me in a comment, interestingly, \nxz\n seems to have a best ratio on plain files with its default options:\n\n\n\n```bash\n$ xz -9 myfile\n\n$ ls -l myfile*\n-rw-rw-r-- 1 apaul apaul 109384 29 april 10:09 myfile.xz\n```\n\n\n\nFor more informations, here is an interesting benchmark for different tools: \nhttp://binfalse.de/2011/04/04/comparison-of-compression/\n\n\nFor the example above, I use \n-9\n for a best compression ratio, but if the time needed to compress data is more important than the ratio, you'd better not use it (use a lower option, ie \n-1\n, or something between).",
    "url": "https://unix.stackexchange.com/questions/199328/how-do-i-compress-files-in-place"
  },
  {
    "question_title": "How to fix &quot;Hunk #1 FAILED at 1 (different line endings)&quot; message?",
    "question_body": "I am trying to create a patch with the command\n\n\n\n```bash\ngit diff sourcefile >/var/lib/laymab/overlay/category/ebuild/files/thepatch.patch\n```\n\n\n\nwhen I apply the patch, it gives me \n\n\n\n```bash\n$ patch -v\nGNU patch 2.7.5\n\n$ /usr/bin/patch -p1 </var/lib/laymab/overlay/category/ebuild/files/thepatch.patch\npatching file sourcefile\nHunk #1 FAILED at 1 (different line endings).\nHunk #2 FAILED at 23 (different line endings).\nHunk #3 FAILED at 47 (different line endings).\nHunk #4 FAILED at 65 (different line endings).\nHunk #5 FAILED at 361 (different line endings).\n5 out of 5 hunks FAILED -- saving rejects to file sourcefile.rej\n```\n\n\n\nI tried to apply dos2unix to both src file and patch file, but the message don't gone...\n\n\nUPD: --ignore-whitespace doesn't help too\n\n\n\n```bash\nPATCH COMMAND:  patch -p1 -g0 -E --no-backup-if-mismatch --ignore-whitespace --dry-run -f < '/var/lib/layman/dotnet/dev-dotnet/slntools/files/remove-wix-project-from-sln-file-v2.patch'\n\n=====================================================\nchecking file Main/SLNTools.sln\nHunk #1 FAILED at 14 (different line endings).\nHunk #2 FAILED at 49 (different line endings).\nHunk #3 FAILED at 69 (different line endings).\nHunk #4 FAILED at 102 (different line endings).\n4 out of 4 hunks FAILED\n```\n\n\n\nUPD: found a very good article: \nhttps://stackoverflow.com/a/4425433/1709408",
    "answer": "I had the same problem using the \npatch\ncommand that comes with MSYS2 on Windows. In my case both the source file and the patch had CRLF line-ending, and converting both to LF didn't work either. What worked was the following: \n\n\n\n```bash\n$ dos2unix patch-file.patch\n$ patch -p1 < patch-file.patch\n$ unix2dos modified-files...\n```\n\n\n\npatch\n will convert the line-endings to LF on all the patched files, so it's necessary to convert them back to CRLF.\n\n\nObs: the \npatch\n version I'm using is 2.7.5",
    "url": "https://unix.stackexchange.com/questions/239364/how-to-fix-hunk-1-failed-at-1-different-line-endings-message"
  },
  {
    "question_title": "How to return both file name and line number with find ... -exec grep?",
    "question_body": "When using \nfind\n, how do I return the file name and the line number when searching for a string? I manage to return the file name in one command and the line numbers with another one, but I can't seem to combine them.\n\n\nFile names: \nfind . -type f -exec grep -l 'string to search' {} \\;\n\n\nLine numbers: \nfind . -type f -exec grep -n 'string to search' {} \\;",
    "answer": "The command line switch \n-H\n forces \ngrep\n to print the file name, even with just one file.\n\n\n\n```bash\n% grep -n 7 test.in\n7:7\n% grep -Hn 7 test.in\ntest.in:7:7\n```\n\n\n\n\n\n\n```bash\n-H, --with-filename\n          Print the filename for each match.\n```\n\n\n\nNote that as \nKojiro\n says in \na comment\n, this is not part of the POSIX standard; it is in both GNU and BSD grep, but it's possible some systems don't have it (e.g. Solaris).",
    "url": "https://unix.stackexchange.com/questions/53072/how-to-return-both-file-name-and-line-number-with-find-exec-grep"
  }
]